<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-08-04T00:00:00Z">2025-08-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">115</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test Set Quality in Multilingual LLM Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kranti Chalamalasetti, Gabriel Bernier-Colborne, Yvan Gauthier, Sowmya Vajjala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several multilingual benchmark datasets have been developed in a
semi-automatic manner in the recent past to measure progress and understand the
state-of-the-art in the multilingual capabilities of Large Language Models.
However, there is not a lot of attention paid to the quality of the datasets
themselves, despite the existence of previous work in identifying errors in
even fully human-annotated test sets. In this paper, we manually analyze recent
multilingual evaluation sets in two languages - French and Telugu, identifying
several errors in the process. We compare the performance difference across
several LLMs with the original and revised versions of the datasets and
identify large differences (almost 10% in some cases) in both languages). Based
on these results, we argue that test sets should not be considered immutable
and should be revisited, checked for correctness, and potentially versioned. We
end with some recommendations for both the dataset creators as well as
consumers on addressing the dataset quality issues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 1st Workshop on Multilingual Data Quality Signals,
  COLM 2025, Short paper. 10 pages in total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pointer: Linear-Complexity Long-Range Modeling without <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Pointer, a novel architecture that achieves linear $O(NK)$
complexity for long-range sequence modeling while maintaining superior
performance without requiring pre-training. Unlike standard attention
mechanisms that compute $O(N^2)$ pairwise interactions, our approach uses
layer-wise pointer chaining where each layer's pointer selection depends on
previous layer's pointer positions, creating explicit long-distance connections
through pointer chains. We demonstrate that this architecture achieves
$2$--$10\times$ speedup on long sequences compared to standard transformers,
maintains $>95\%$ accuracy on copy tasks at distances up to 2048 tokens, and
learns interpretable pointer patterns that reveal structured dependency
modeling. Our experiments on efficiency benchmarks, long-range dependency
tasks, and interpretability analysis show that Pointer offers a compelling
alternative to attention mechanisms for scenarios requiring efficient
long-range modeling without pre-training dependencies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Nordic AI Meet 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and
  Decision in Embodied Agents <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Liu, Zhixuan Liang, Zanxin Chen, Tianxing Chen, Mengkang Hu, Wanxi Dong, Congsheng Xu, Zhaoming Han, Yusen Qin, Yao Mu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in multimodal large language models (MLLMs) have enabled
richer perceptual grounding for code policy generation in embodied agents.
However, most existing systems lack effective mechanisms to adaptively monitor
policy execution and repair codes during task completion. In this work, we
introduce HyCodePolicy, a hybrid language-based control framework that
systematically integrates code synthesis, geometric grounding, perceptual
monitoring, and iterative repair into a closed-loop programming cycle for
embodied agents. Technically, given a natural language instruction, our system
first decomposes it into subgoals and generates an initial executable program
grounded in object-centric geometric primitives. The program is then executed
in simulation, while a vision-language model (VLM) observes selected
checkpoints to detect and localize execution failures and infer failure
reasons. By fusing structured execution traces capturing program-level events
with VLM-based perceptual feedback, HyCodePolicy infers failure causes and
repairs programs. This hybrid dual feedback mechanism enables self-correcting
program synthesis with minimal human supervision. Our results demonstrate that
HyCodePolicy significantly improves the robustness and sample efficiency of
robot manipulation policies, offering a scalable strategy for integrating
multimodal reasoning into autonomous decision-making pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noosemia: toward a Cognitive and Phenomenological Account of
  Intentionality Attribution in Human-Generative AI Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrico De Santis, Antonello Rizzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces and formalizes Noosemia, a novel
cognitive-phenomenological phenomenon emerging from human interaction with
generative AI systems, particularly those enabling dialogic or multimodal
exchanges. We propose a multidisciplinary framework to explain how, under
certain conditions, users attribute intentionality, agency, and even
interiority to these systems - a process grounded not in physical resemblance,
but in linguistic performance, epistemic opacity, and emergent technological
complexity. By linking an LLM declination of meaning holism to our technical
notion of the LLM Contextual Cognitive Field, we clarify how LLMs construct
meaning relationally and how coherence and a simulacrum of agency arise at the
human-AI interface. The analysis situates noosemia alongside pareidolia,
animism, the intentional stance and the uncanny valley, distinguishing its
unique characteristics. We also introduce a-noosemia to describe the
phenomenological withdrawal of such projections. The paper concludes with
reflections on the broader philosophical, epistemological, and social
implications of noosemic dynamics and directions for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous
  Healthcare Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Zhu, Yifan Qi, Zixiang Wang, Lei Gu, Dehao Sui, Haoran Hu, Xichen Zhang, Ziyi He, Liantao Ma, Lequan Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The efficacy of AI agents in healthcare research is hindered by their
reliance on static, predefined strategies. This creates a critical limitation:
agents can become better tool-users but cannot learn to become better strategic
planners, a crucial skill for complex domains like healthcare. We introduce
HealthFlow, a self-evolving AI agent that overcomes this limitation through a
novel meta-level evolution mechanism. HealthFlow autonomously refines its own
high-level problem-solving policies by distilling procedural successes and
failures into a durable, strategic knowledge base. To anchor our research and
facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark
featuring complex, realistic health data analysis tasks derived from
peer-reviewed clinical research. Our comprehensive experiments demonstrate that
HealthFlow's self-evolving approach significantly outperforms state-of-the-art
agent frameworks. This work marks a necessary shift from building better
tool-users to designing smarter, self-evolving task-managers, paving the way
for more autonomous and effective AI for scientific discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/yhzhu99/HealthFlow</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Attention Hacking in Preference-Based Reward Modeling via
  Interaction Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianxiang Zang, Meiling Ning, Shihan Dou, Jiazheng Zhang, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reward model (RM), as the core component of reinforcement learning from
human feedback (RLHF) for large language models (LLMs), responsible for
providing reward signals to generated responses. However, mainstream preference
modeling in RM is inadequate in terms of token-level interaction, making its
judgment signals vulnerable to being hacked by misallocated attention to
context. This stems from two fundamental limitations: (1) Current preference
modeling employs decoder-only architectures, where the unidirectional causal
attention mechanism leads to forward-decaying intra-sequence attention within
the prompt-response sequence. (2) The independent Siamese-encoding paradigm
induces the absence of token-level inter-sequence attention between chosen and
rejected sequences. To address this "attention hacking", we propose
"Interaction Distillation", a novel training framework for more adequate
preference modeling through attention-level optimization. The method introduces
an interaction-based natural language understanding model as the teacher to
provide sophisticated token interaction patterns via comprehensive attention,
and guides the preference modeling to simulate teacher model's interaction
pattern through an attentional alignment objective. Through extensive
experiments, interaction distillation has demonstrated its ability to provide
more stable and generalizable reward signals compared to state-of-the-art RM
optimization methods that target data noise, highlighting the attention hacking
constitute a more fundamental limitation in RM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CharBench: Evaluating the Role of Tokenization in Character-Level Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omri Uzan, Yuval Pinter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tasks that require character-level reasoning, such as counting or locating
characters within words, remain challenging for contemporary language models. A
common conjecture is that language models' reliance on subword units, rather
than characters, contributes to their struggles with character-level tasks, yet
recent studies offer conflicting conclusions about the role of tokenization,
leaving its impact unclear. To address this gap, we introduce CharBench, a
comprehensive benchmark of character-level tasks that is two orders of
magnitude larger than existing alternatives. We evaluate a diverse range of
leading open-weight and proprietary models on CharBench and find that it
presents a significant challenge to modern LLMs, with an average accuracy of
43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic
properties of words and their segmentations into tokens correspond to model
performance. For counting tasks, we find that tokenization properties are
weakly correlated with correctness, while the length of the queried word and
the actual character count play a more significant part. In contrast, for tasks
requiring intra-word positional understanding, performance is negatively
correlated with the length of the token containing the queried character,
suggesting that longer tokens obscure character position information for LLMs.
We encourage future work to build on the benchmark and evaluation methodology
introduced here as tools for improving model performance on such tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-Efficient Routed Fine-Tuning: Mixture-of-Experts Demands
  Mixture of Adaptation Modules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Liu, Yunpu Ma, Yuetian Lu, Shuo Chen, Zifeng Ding, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) benefits from a dynamic routing mechanism among
their specialized experts, which existing Parameter- Efficient Fine-Tuning
(PEFT) strategies fail to leverage. This motivates us to investigate whether
adaptation modules themselves should incorporate routing mechanisms to align
with MoE's multi-expert architecture. We analyze dynamics of core components
when applying PEFT to MoE language models and examine how different routing
strategies affect adaptation effectiveness. Extensive experiments adapting
OLMoE-1B-7B and Mixtral-8x7B on various commonsense and math reasoning tasks
validate the performance and efficiency of our routed approach. We identify the
optimal configurations for different scenarios and provide empirical analyses
with practical insights to facilitate better PEFT and MoE applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is a preprint under review. arXiv admin note: text overlap
  with arXiv:2411.08212</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MArgE: Meshing Argumentative Evidence from Multiple Large Language
  Models for Justifiable Claim Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Pok Ng, Junqi Jiang, Gabriel Freedman, Antonio Rago, Francesca Toni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging outputs from multiple large language models (LLMs) is emerging as
a method for harnessing their power across a wide range of tasks while
mitigating their capacity for making errors, e.g., hallucinations. However,
current approaches to combining insights from multiple LLMs often involve
unstructured interactions (e.g., free debate), resulting in model generations
that are not faithfully justifiable. In this work, we introduce MArgE, a novel
framework to provide formal structure to the evidence from each LLM, in the
form of a tree of extracted arguments, for the task of claim verification. We
use a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks
and semantics from the field of computational argumentation, to construct
structured argument trees for given claims. This process creates an inspectable
pathway from the initial arguments to the final claim verification decisions,
providing a faithful justification thereof. We show experimentally that MArgE
can significantly outperform single LLMs, including three open-source models
(4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior
methods for unstructured multi-LLM debates. We thus demonstrate the advantages
of incorporating formal, argumentative reasoning mechanisms when combining
multiple LLM outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EHSAN: Leveraging Chat<span class="highlight-title">GPT</span> in a Hybrid Framework for Arabic Aspect-Based
  Sentiment Analysis in Healthcare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eman Alamoudi, Ellis Solaiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arabic-language patient feedback remains under-analysed because dialect
diversity and scarce aspect-level sentiment labels hinder automated assessment.
To address this gap, we introduce EHSAN, a data-centric hybrid pipeline that
merges ChatGPT pseudo-labelling with targeted human review to build the first
explainable Arabic aspect-based sentiment dataset for healthcare. Each sentence
is annotated with an aspect and sentiment label (positive, negative, or
neutral), forming a pioneering Arabic dataset aligned with healthcare themes,
with ChatGPT-generated rationales provided for each label to enhance
transparency. To evaluate the impact of annotation quality on model
performance, we created three versions of the training data: a fully supervised
set with all labels reviewed by humans, a semi-supervised set with 50% human
review, and an unsupervised set with only machine-generated labels. We
fine-tuned two transformer models on these datasets for both aspect and
sentiment classification. Experimental results show that our Arabic-specific
model achieved high accuracy even with minimal human supervision, reflecting
only a minor performance drop when using ChatGPT-only labels. Reducing the
number of aspect classes notably improved classification metrics across the
board. These findings demonstrate an effective, scalable approach to Arabic
aspect-based sentiment analysis (SA) in healthcare, combining large language
model annotation with human expertise to produce a robust and explainable
dataset. Future directions include generalisation across hospitals, prompt
refinement, and interpretable data-driven modelling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guess or Recall? Training CNNs to Classify and Localize Memorization in
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jérémie Dentan, Davide Buscaldi, Sonia Vanier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verbatim memorization in Large Language Models (LLMs) is a multifaceted
phenomenon involving distinct underlying mechanisms. We introduce a novel
method to analyze the different forms of memorization described by the existing
taxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the
attention weights of the LLM and evaluate the alignment between this taxonomy
and the attention weights involved in decoding.
  We find that the existing taxonomy performs poorly and fails to reflect
distinct mechanisms within the attention blocks. We propose a new taxonomy that
maximizes alignment with the attention weights, consisting of three categories:
memorized samples that are guessed using language modeling abilities, memorized
samples that are recalled due to high duplication in the training set, and
non-memorized samples. Our results reveal that few-shot verbatim memorization
does not correspond to a distinct attention mechanism. We also show that a
significant proportion of extractable samples are in fact guessed by the model
and should therefore be studied separately. Finally, we develop a custom visual
interpretability technique to localize the regions of the attention weights
involved in each form of memorization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and
parallel decoding but suffer from prohibitive quadratic computational
complexity and memory overhead during inference. Current caching techniques
accelerate decoding by storing full-layer states, yet impose substantial memory
usage that limit long-context applications. Our analysis of attention patterns
in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining
salient across decoding steps and low-relevance tokens staying unimportant,
motivating selective cache eviction. We propose Sparse-dLLM, the first
training-free framework integrating dynamic cache eviction with sparse
attention via delayed bidirectional sparse caching. By leveraging the stability
of token saliency over steps, it retains critical tokens and dynamically evicts
unimportant prefix/suffix entries using an attention-guided strategy. Extensive
experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to
10$\times$ higher throughput than vanilla dLLMs, with comparable performance
and similar peak memory costs, outperforming previous methods in efficiency and
effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Noori, Pratik Devkota, Somya Mohanty, Prashanti Manda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated annotation of clinical text with standardized medical concepts is
critical for enabling structured data extraction and decision support. SNOMED
CT provides a rich ontology for labeling clinical entities, but manual
annotation is labor-intensive and impractical at scale. This study introduces a
neural sequence labeling approach for SNOMED CT concept recognition using a
Bidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text
with domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences
into overlapping 19-token chunks enriched with contextual, syntactic, and
morphological features. The Bi-GRU model assigns IOB tags to identify concept
spans and achieves strong performance with a 90 percent F1-score on the
validation set. These results surpass traditional rule-based systems and match
or exceed existing neural models. Qualitative analysis shows effective handling
of ambiguous terms and misspellings. Our findings highlight that lightweight
RNN-based architectures can deliver high-quality clinical concept annotation
with significantly lower computational cost than transformer-based models,
making them well-suited for real-world deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building and Aligning Comparable Corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Motaz Saad, David Langlois, Kamel Smaili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Comparable corpus is a set of topic aligned documents in multiple languages,
which are not necessarily translations of each other. These documents are
useful for multilingual natural language processing when there is no parallel
text available in some domains or languages. In addition, comparable documents
are informative because they can tell what is being said about a topic in
different languages. In this paper, we present a method to build comparable
corpora from Wikipedia encyclopedia and EURONEWS website in English, French and
Arabic languages. We further experiment a method to automatically align
comparable documents using cross-lingual similarity measures. We investigate
two cross-lingual similarity measures to align comparable documents. The first
measure is based on bilingual dictionary, and the second measure is based on
Latent Semantic Indexing (LSI). Experiments on several corpora show that the
Cross-Lingual LSI (CL-LSI) measure outperforms the dictionary based measure.
Finally, we collect English and Arabic news documents from the British
Broadcast Corporation (BBC) and from ALJAZEERA (JSC) news website respectively.
Then we use the CL-LSI similarity measure to automatically align comparable
documents of BBC and JSC. The evaluation of the alignment shows that CL-LSI is
not only able to align cross-lingual documents at the topic level, but also it
is able to do this at the event level.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What are you sinking? A geometric approach on attention sink 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valeria Ruscio, Umberto Nanni, Fabrizio Silvestri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention sink (AS) is a consistent pattern in transformer attention maps
where certain tokens (often special tokens or positional anchors)
disproportionately attract attention from other tokens. We show that in
transformers, AS is not an architectural artifact, but it is the manifestation
of a fundamental geometric principle: the establishment of reference frames
that anchor representational spaces. We analyze several architectures and
identify three distinct reference frame types, centralized, distributed, and
bidirectional, that correlate with the attention sink phenomenon. We show that
they emerge during the earliest stages of training as optimal solutions to the
problem of establishing stable coordinate systems in high-dimensional spaces.
We show the influence of architecture components, particularly position
encoding implementations, on the specific type of reference frame. This
perspective transforms our understanding of transformer attention mechanisms
and provides insights for both architecture design and the relationship with
AS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What's in the News? Towards Identification of Bias by Commission,
  Omission, and Source Selection (COSS) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasia Zhukova, Terry Ruas, Felix Hamborg, Karsten Donnay, Bela Gipp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a world overwhelmed with news, determining which information comes from
reliable sources or how neutral is the reported information in the news
articles poses a challenge to news readers. In this paper, we propose a
methodology for automatically identifying bias by commission, omission, and
source selection (COSS) as a joint three-fold objective, as opposed to the
previous work separately addressing these types of bias. In a pipeline concept,
we describe the goals and tasks of its steps toward bias identification and
provide an example of a visualization that leverages the extracted features and
patterns of text reuse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published in the Proceedings of the 2023 ACM/IEEE Joint Conference on
  Digital Libraries</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextual Graph <span class="highlight-title">Transformer</span>: A Small Language Model for Enhanced
  Engineering Document Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karan Reddy, Mayukha Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard transformer-based language models, while powerful for general text,
often struggle with the fine-grained syntax and entity relationships in complex
technical, engineering documents. To address this, we propose the Contextual
Graph Transformer (CGT), a hybrid neural architecture that combines Graph
Neural Networks (GNNs) and Transformers for domain-specific question answering.
CGT constructs a dynamic graph over input tokens using sequential, skip-gram,
and semantic similarity edges, which is processed by GATv2Conv layers for local
structure learning. These enriched embeddings are then passed to a Transformer
encoder to capture global dependencies. Unlike generic large models, technical
domains often require specialized language models with stronger
contextualization and structure awareness. CGT offers a parameter-efficient
solution for such use cases. Integrated into a Retrieval-Augmented Generation
(RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7%
higher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from
CGTs ability to jointly model structural token interactions and long-range
semantic coherence. The model is trained from scratch using a two-phase
approach: pretraining on general text followed by fine-tuning on
domain-specific manuals. This highlights CGTs adaptability to technical
language, enabling better grounding, entity tracking, and retrieval-augmented
responses in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic
  Representations in LLaMA 3.2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Merullo, Arjun Khurana, Oliver McLaughlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models demonstrate proficiency on phonetic tasks, such as
rhyming, without explicit phonetic or auditory grounding. In this work, we
investigate how \verb|Llama-3.2-1B-Instruct| represents token-level phonetic
information. Our results suggest that Llama uses a rich internal model of
phonemes to complete phonetic tasks. We provide evidence for high-level
organization of phoneme representations in its latent space. In doing so, we
also identify a ``phoneme mover head" which promotes phonetic information
during rhyming tasks. We visualize the output space of this head and find that,
while notable differences exist, Llama learns a model of vowels similar to the
standard IPA vowel chart for humans, despite receiving no direct supervision to
do so.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PoeTone: A Framework for Constrained Generation of Structured Chinese
  Songci with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Qu, Shuzhou Yuan, Michael Färber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a systematic investigation into the constrained
generation capabilities of large language models (LLMs) in producing Songci, a
classical Chinese poetry form characterized by strict structural, tonal, and
rhyme constraints defined by Cipai templates. We first develop a comprehensive,
multi-faceted evaluation framework that includes: (i) a formal conformity
score, (ii) automated quality assessment using LLMs, (iii) human evaluation,
and (iv) classification-based probing tasks. Using this framework, we evaluate
the generative performance of 18 LLMs, including 3 proprietary models and 15
open-source models across four families, under five prompting strategies:
zero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.
Finally, we propose a Generate-Critic architecture in which the evaluation
framework functions as an automated critic. Leveraging the critic's feedback as
a reward signal, we fine-tune three lightweight open-source LLMs via supervised
fine-tuning (SFT), resulting in improvements of up to 5.88% in formal
conformity. Our findings offer new insights into the generative strengths and
limitations of LLMs in producing culturally significant and formally
constrained literary texts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modular Arithmetic: Language Models Solve Math Digit by Digit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanja Baeumel, Daniil Gurgurov, Yusser al Ghussin, Josef van Genabith, Simon Ostermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent work has begun to uncover the internal strategies that Large
Language Models (LLMs) employ for simple arithmetic tasks, a unified
understanding of their underlying mechanisms is still lacking. We extend recent
findings showing that LLMs represent numbers in a digit-wise manner and present
evidence for the existence of digit-position-specific circuits that LLMs use to
perform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that
operate independently on different digit positions (units, tens, hundreds).
Notably, such circuits exist independently of model size and of tokenization
strategy, i.e. both for models that encode longer numbers digit-by-digit and as
one token. Using Feature Importance and Causal Interventions, we identify and
validate the digit-position-specific circuits, revealing a compositional and
interpretable structure underlying the solving of arithmetic problems in LLMs.
Our interventions selectively alter the model's prediction at targeted digit
positions, demonstrating the causal role of digit-position circuits in solving
arithmetic tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-time <span class="highlight-title">Prompt</span> Intervention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxu Yang, Qingyi Si, Mz Dai, Dingyu Yao, Mingyu Zheng, Minghui Chen, Zheng Lin, Weiping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time compute has led to remarkable success in the large language model
(LLM) community, particularly for complex tasks, where longer chains of thought
(CoTs) are generated to enhance reasoning capabilities. However, growing
evidence reveals that such reasoning models often produce CoTs plagued by
excessive redundancy, including unnecessary verification steps and repetitive
reasoning shifts. The root cause lies in post-training of them that overly rely
on outcome reward paradigms, as the data of process reward paradigms, which
regulate intermediate reasoning steps, is difficult to construct at scale. To
address this, we propose PI, a novel framework for Test-time Prompt
Intervention. PI provides an interface to dynamically guide and regulate
reasoning paths during inference through timely (When module) and proper (How
module) interventions and post-intervention sampling (Which module). This
allows human problem-solving expertise and cognitive science principles to be
seamlessly integrated into LLMs' reasoning processes, enhancing controllability
and interpretability. Extensive experiments across multiple models and datasets
demonstrate that PI significantly shortens CoTs while reducing hallucination,
yielding more concise and reliable reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 16 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Bouscary, Saurabh Amin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based solvers have emerged as a promising means of automating problem
modeling and solving. However, they remain unreliable and often depend on
iterative repair loops that result in significant latency. We introduce
OptiHive, an LLM-based framework that produces high-quality solvers for
optimization problems from natural-language descriptions without iterative
self-correction. OptiHive uses a single batched LLM query to generate diverse
components (solvers, problem instances, and validation tests) and filters out
erroneous components to ensure fully interpretable outputs. Taking into account
the imperfection of the generated components, we employ a statistical model to
infer their true performance, enabling principled uncertainty quantification
and solver selection. On tasks ranging from traditional optimization problems
to challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive
significantly outperforms baselines, increasing the optimality rate from 5\% to
92\% on the most complex problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Monolingual to Bilingual: Investigating Language Conditioning in
  Large Language Models for Psycholinguistic Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuzhou Yuan, Zhan Qu, Mario Tawfelis, Michael Färber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) exhibit strong linguistic capabilities, but
little is known about how they encode psycholinguistic knowledge across
languages. We investigate whether and how LLMs exhibit human-like
psycholinguistic responses under different linguistic identities using two
tasks: sound symbolism and word valence. We evaluate two models,
Llama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and
bilingual prompting in English, Dutch, and Chinese. Behaviorally, both models
adjust their outputs based on prompted language identity, with Qwen showing
greater sensitivity and sharper distinctions between Dutch and Chinese. Probing
analysis reveals that psycholinguistic signals become more decodable in deeper
layers, with Chinese prompts yielding stronger and more stable valence
representations than Dutch. Our results demonstrate that language identity
conditions both output behavior and internal representations in LLMs, providing
new insights into their application as models of cross-linguistic cognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monsoon Uprising in Bangladesh: How Facebook Shaped Collective Identity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Tasin Abir, Arpita Chowdhury, Ashfia Rahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates how Facebook shaped collective identity during the
July 2024 pro-democracy uprising in Bangladesh, known as the Monsoon Uprising.
During government repression, protesters turned to Facebook as a central space
for resistance, where multimodal expressions, images, memes, videos, hashtags,
and satirical posts played an important role in unifying participants. Using a
qualitative approach, this research analyzes visual rhetoric, verbal discourse,
and digital irony to reveal how shared symbols, protest art, and slogans built
a sense of solidarity. Key elements included the symbolic use of red, the
ironic metaphorical use of the term "Razakar", and the widespread sharing of
visuals representing courage, injustice, and resistance. The findings show that
the combination of visual and verbal strategies on Facebook not only mobilized
public sentiment, but also built a strong collective identity that challenged
authoritarian narratives. This study tries to demonstrate how online platforms
can serve as powerful tools for identity construction and political
mobilization in the digital age.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language
  and Multi-Agent Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunjn An, Yongwon Kim, Wonduk Seo, Joonil Park, Daye Kang, Changhoon Oh, Dokyun Kim, Seunghyun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While many tools are available for designing AI, non-experts still face
challenges in clearly expressing their intent and managing system complexity.
We introduce AIAP, a no-code platform that integrates natural language input
with visual workflows. AIAP leverages a coordinated multi-agent system to
decompose ambiguous user instructions into modular, actionable steps, hidden
from users behind a unified interface. A user study involving 32 participants
showed that AIAP's AI-generated suggestions, modular workflows, and automatic
identification of data, actions, and context significantly improved
participants' ability to develop services intuitively. These findings highlight
that natural language-based visual programming significantly reduces barriers
and enhances user experience in AI service design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent<span class="highlight-title">Prompt</span>: Optimizing Promts in Latent Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Bystroński, Grzegorz Piotrowski, Nitesh V. Chawla, Tomasz Kajdanowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have shown that optimizing prompts for Large Language Models
(LLMs) can significantly improve task performance, yet many optimization
techniques rely on heuristics or manual exploration. We present LatentPrompt, a
model-agnostic framework for prompt optimization that leverages latent semantic
space to automatically generate, evaluate, and refine candidate prompts without
requiring hand-crafted rules. Beginning with a set of seed prompts, our method
embeds them in a continuous latent space and systematically explores this space
to identify prompts that maximize task-specific performance. In a
proof-of-concept study on the Financial PhraseBank sentiment classification
benchmark, LatentPrompt increased classification accuracy by approximately 3
percent after a single optimization cycle. The framework is broadly applicable,
requiring only black-box access to an LLM and an automatic evaluation metric,
making it suitable for diverse domains and tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Based Measurement of Innovation: Mapping Expert Insight into Large
  Language Model Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Nowak, Patrick Figge, Carolin Haeussler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measuring innovation often relies on context-specific proxies and on expert
evaluation. Hence, empirical innovation research is often limited to settings
where such data is available. We investigate how large language models (LLMs)
can be leveraged to overcome the constraints of manual expert evaluations and
assist researchers in measuring innovation. We design an LLM framework that
reliably approximates domain experts' assessment of innovation from
unstructured text data. We demonstrate the performance and broad applicability
of this framework through two studies in different contexts: (1) the
innovativeness of software application updates and (2) the originality of
user-generated feedback and improvement ideas in product reviews. We compared
the performance (F1-score) and reliability (consistency rate) of our LLM
framework against alternative measures used in prior innovation studies, and to
state-of-the-art machine learning- and deep learning-based models. The LLM
framework achieved higher F1-scores than the other approaches, and its results
are highly consistent (i.e., results do not change across runs). This article
equips R&D personnel in firms, as well as researchers, reviewers, and editors,
with the knowledge and tools to effectively use LLMs for measuring innovation
and evaluating the performance of LLM-based innovation measures. In doing so,
we discuss, the impact of important design decisions-including model selection,
prompt engineering, training data size, training data distribution, and
parameter settings-on performance and reliability. Given the challenges
inherent in using human expert evaluation and existing text-based measures, our
framework has important implications for harnessing LLMs as reliable,
increasingly accessible, and broadly applicable research tools for measuring
innovation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyu Li, Zhi Jin, Yuanpeng He, Dongming Jin, Yichi Zhang, Haoran Duan, Nyima Tash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since knowledge graphs (KG) will continue to evolve in real scenarios,
traditional KGE models are only suitable for static knowledge graphs.
Therefore, continual knowledge graph embedding (CKGE) has attracted the
attention of researchers. Currently, a key challenge facing CKGE is that the
model is prone to "catastrophic forgetting", resulting in the loss of
previously learned knowledge. In order to effectively alleviate this problem,
we propose a new CKGE model BAKE. First, we note that the Bayesian posterior
update principle provides a natural continual learning strategy that is
insensitive to data order and can theoretically effectively resist the
forgetting of previous knowledge during data evolution. Different from the
existing CKGE method, BAKE regards each batch of new data as a Bayesian update
of the model prior. Under this framework, as long as the posterior distribution
of the model is maintained, the model can better preserve the knowledge of
early snapshots even after evolving through multiple time snapshots. Secondly,
we propose a continual clustering method for CKGE, which further directly
combats knowledge forgetting by constraining the evolution difference (or
change amplitude) between new and old knowledge between different snapshots. We
conduct extensive experiments on BAKE on multiple datasets, and the results
show that BAKE significantly outperforms existing baseline models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination
  via Attention Lens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohan Zheng, Zhenguo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have demonstrated remarkable multimodal
comprehension and reasoning capabilities, but they still suffer from severe
object hallucination. Previous studies primarily attribute the flaw to
linguistic prior caused by the scale mismatch between visual encoders and large
language models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon
LLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,
generating descriptions inconsistent with visual cues. However, through an
in-depth investigation of the hallucinated mechanisms, we empirically reveal a
previously overlooked phenomenon: LVLMs may ignore not only visual information
but also textual modality during hallucination, a behavior termed as modality
bias, which indicates that LVLMs struggle to simultaneously attend to both
visual and textual modalities, leading to fragmented understanding of
user-provided instructions. Based on this observation, we propose a simple yet
effective training-free method to mitigate object hallucination. Concretely, we
intervene and adjust the attention weights of textual and visual tokens,
balancing cross-modal compatibility for better alignment with user intentions.
Furthermore, we adopt a contrastive decoding strategy to reduce the LVLM's
overreliance on its parametric knowledge, synergistically enhancing our
attention manipulation. Extensive experiments confirm the widespread presence
of modality bias in LVLMs. Notably, our method effectively mitigates
hallucination across multiple open-source LVLMs and benchmarks, highlighting
its generalizability and efficacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important
  Before Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolin Lin, Jingcun Wang, Olga Kondrateva, Yiyu Shi, Bing Li, Grace Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have significantly boosted
long-context processing. However, the increasing key-value (KV) cache size
poses critical challenges to memory and execution efficiency. Most KV cache
compression methods rely on heuristic token eviction using all attention heads
in Grouped Query Attention (GQA)-based LLMs. This method ignores the different
functionalities of attention heads, leading to the eviction of critical tokens
and thus degrades the performance of LLMs.
  To address the issue above, instead of using all the attention heads in
GQA-based LLMs to determine important tokens as in the previous work, we first
identify the attention heads in each layer that are not only capable of
retrieving the initial and final tokens of a prompt, but also capable of
retrieving important tokens within the text and attending to their surrounding
semantic context. Afterwards, we exploit such heads to determine the important
tokens and retain their corresponding KV cache pairs. Furthermore, we analyze
the cache eviction error of each layer individually and introduce a
layer-adaptive KV cache allocation strategy. Experimental results demonstrate
the proposed CompressKV consistently outperforms state-of-the-art approaches
under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.
Our code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Six Guidelines for Trustworthy, Ethical and Responsible Automation
  Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matouš Jelínek, Nadine Schlicker, Ewart de Visser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Calibrated trust in automated systems (Lee and See 2004) is critical for
their safe and seamless integration into society. Users should only rely on a
system recommendation when it is actually correct and reject it when it is
factually wrong. One requirement to achieve this goal is an accurate
trustworthiness assessment, ensuring that the user's perception of the system's
trustworthiness aligns with its actual trustworthiness, allowing users to make
informed decisions about the extent to which they can rely on the system
(Schlicker et al. 2022). We propose six design guidelines to help designers
optimize for accurate trustworthiness assessments, thus fostering ethical and
responsible human-automation interactions. The proposed guidelines are derived
from existing literature in various fields, such as human-computer interaction,
cognitive psychology, automation research, user-experience design, and ethics.
We are incorporating key principles from the field of pragmatics, specifically
the cultivation of common ground (H. H. Clark 1996) and Gricean communication
maxims (Grice 1975). These principles are essential for the design of automated
systems because the user's perception of the system's trustworthiness is shaped
by both environmental contexts, such as organizational culture or societal
norms, and by situational context, including the specific circumstances or
scenarios in which the interaction occurs (Hoff and Bashir 2015). Our proposed
guidelines provide actionable insights for designers to create automated
systems that make relevant trustworthiness cues available. This would ideally
foster calibrated trust and more satisfactory, productive, and safe
interactions between humans and automated systems. Furthermore, the proposed
heuristics might work as a tool for evaluating to what extent existing systems
enable users to accurately assess a system's trustworthiness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Model Guided Reinforcement Learning in Quantitative Trading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Darmanin, Vince Vella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithmic trading requires short-term decisions aligned with long-term
financial goals. While reinforcement learning (RL) has been explored for such
tactical decisions, its adoption remains limited by myopic behavior and opaque
policy rationale. In contrast, large language models (LLMs) have recently
demonstrated strategic reasoning and multi-modal financial signal
interpretation when guided by well-designed prompts.
  We propose a hybrid system where LLMs generate high-level trading strategies
to guide RL agents in their actions. We evaluate (i) the rationale of
LLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and
Maximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results
show improved return and risk metrics over standard RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages (4 pages appendix and references), 6 figures, preprint under
  review for FLLM 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding and Mitigating Political Stance Cross-topic Generalization
  in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Zhang, Shu Yang, Junchao Wu, Derek F. Wong, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning Large Language Models on a political topic will significantly
manipulate their political stance on various issues and unintentionally affect
their stance on unrelated topics. While previous studies have proposed this
issue, there is still a lack of understanding regarding the internal
representations of these stances and the mechanisms that lead to unintended
cross-topic generalization. In this paper, we systematically explore the
internal mechanisms underlying this phenomenon from a neuron-level perspective
and how to mitigate the cross-topic generalization of political fine-tuning.
Firstly, we propose Political Neuron Localization through Activation
Contrasting (PNLAC) to identify two distinct types of political neurons:
general political neurons, which govern stance across multiple political
topics, and topic-specific neurons} that affect the model's political stance on
individual topics. We find the existence of these political neuron types across
four models and datasets through activation patching experiments. Leveraging
these insights, we introduce InhibitFT, an inhibition-based fine-tuning method,
effectively mitigating the cross-topic stance generalization. Experimental
results demonstrate the robustness of identified neuron types across various
models and datasets, and show that InhibitFT significantly reduces the
cross-topic stance generalization by 20% on average, while preserving
topic-specific performance. Moreover, we demonstrate that selectively
inhibiting only 5% of neurons is sufficient to effectively mitigate the
cross-topic stance generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding User Preferences for Interaction Styles in Conversational
  Recommender Systems: The Predictive Role of System Qualities, User
  Experience, and Traits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raj Mahmud, Shlomo Berkovsky, Mukesh Prasad, A. Baki Kocaballi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Recommender Systems (CRSs) deliver personalised
recommendations through multi-turn natural language dialogue and increasingly
support both task-oriented and exploratory interactions. Yet, the factors
shaping user interaction preferences remain underexplored. In this
within-subjects study (\(N = 139\)), participants experienced two scripted CRS
dialogues, rated their experiences, and indicated the importance of eight
system qualities. Logistic regression revealed that preference for the
exploratory interaction was predicted by enjoyment, usefulness, novelty, and
conversational quality. Unexpectedly, perceived effectiveness was also
associated with exploratory preference. Clustering uncovered five latent user
profiles with distinct dialogue style preferences. Moderation analyses
indicated that age, gender, and control preference significantly influenced
these choices. These findings integrate affective, cognitive, and trait-level
predictors into CRS user modelling and inform autonomy-sensitive,
value-adaptive dialogue design. The proposed predictive and adaptive framework
applies broadly to conversational AI systems seeking to align dynamically with
evolving user needs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at OZCHI 2025. 21 pages, 9 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert
  Redundancy Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhuang Xu, Xu Han, Yuanchi Zhang, Yixuan Wang, Yijun Liu, Shiyu Ji, Qingfu Zhu, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are
distinguished by their strong performance scaling with increasing parameters
across a wide range of tasks, yet they also suffer from substantial
computational and storage overheads. Notably, the performance gains of MoE
models do not scale proportionally with the growth in expert parameters. While
prior works attempt to reduce parameters via expert-level pruning, merging, or
decomposition, they still suffer from challenges in both performance and
computational efficiency. In this paper, we address these challenges by
introducing micro-expert as a finer-grained compression unit that spans across
matrices. We first establish a more fundamental perspective, viewing MoE layers
as mixtures of micro-experts, and present CAMERA, a lightweight and
training-free framework for identifying micro-expert redundancy. Our analysis
uncovers significant variance in micro-expert contributions during decoding.
Based on this insight, we further propose CAMERA-P, a structured micro-expert
pruning framework, and CAMERA-Q, a mixed-precision quantization idea designed
for micro-experts. Extensive experiments on nine downstream tasks show that
CAMERA-P consistently outperforms strong baselines under pruning ratios ranging
from 20% to 60%. Furthermore, CAMERA-Q achieves superior results under
aggressive 2-bit quantization, surpassing existing matrix- and channel-level
ideas. Notably, our method enables complete micro-expert analysis of
Qwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VeOmni: Scaling Any Modality Model Training with Model-Centric
  Distributed Recipe Zoo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie Li, Jiacheng Yang, Yanghua Peng, Zhi Zhang, Xin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have driven impressive
progress in omni-modal understanding and generation. However, training
omni-modal LLMs remains a significant challenge due to the heterogeneous model
architectures required to process diverse modalities, necessitating
sophisticated system design for efficient large-scale training. Existing
frameworks typically entangle model definition with parallel logic, incurring
limited scalability and substantial engineering overhead for end-to-end
omni-modal training. % We present \veomni, a modular and efficient training
framework to accelerate the development of omni-modal LLMs. \veomni introduces
model-centric distributed recipes that decouples communication from
computation, enabling efficient 3D parallelism on omni-modal LLMs. \veomni also
features a flexible configuration interface supporting seamless integration of
new modalities with minimal code change. % Using \veomni, a omni-modal
mixture-of-experts (MoE) model with 30B parameters can be trained with over
2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D
parallelism on 128 GPUs, showcasing its superior efficiency and scalability for
training large omni-modal LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaMPE: Length-aware Multi-grained Position Encoding for Adaptive
  Long-context Scaling Without Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sikui Zhang, Guangze Gao, Ziyun Gan, Chunfeng Yuan, Zefeng Lin, Houwen Peng, Bing Li, Weiming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) experience significant performance degradation
when the input exceeds the pretraining context window, primarily due to the
out-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent
studies mitigate this problem by remapping OOD positions into the
in-distribution range with fixed mapping strategies, ignoring the dynamic
relationship between input length and the model's effective context window. To
this end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a
training-free method that fully utilizes the model's effective context window
for adaptive long-context scaling in LLMs. Motivated by the left-skewed
frequency distribution of relative positions, LaMPE establishes a dynamic
relationship between mapping length and input length through a parametric
scaled sigmoid function to adaptively allocate positional capacity across
varying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention
mechanism that strategically allocates positional resolution across different
sequence regions to capture both fine-grained locality and long-range
dependencies. Our method can be seamlessly applied to a wide range of
RoPE-based LLMs without training. Extensive experiments on three representative
LLMs across five mainstream long-context benchmarks demonstrate that LaMPE
achieves significant performance improvements compared to existing length
extrapolation methods. The code will be released at
https://github.com/scar-on/LaMPE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative
  Credit Assignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guofu Xie, Yunsheng Shi, Hongtao Tian, Ting Yao, Xiao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Verifiable Rewards (RLVR) has improved the
reasoning abilities of Large Language Models (LLMs) by using rule-based binary
feedback, helping to mitigate reward hacking. However, current RLVR methods
typically treat whole responses as single actions, assigning the same reward to
every token. This coarse-grained feedback hampers precise credit assignment,
making it hard for models to identify which reasoning steps lead to success or
failure, and often results in suboptimal policies and inefficient learning.
Methods like PPO provide credit assignment through value estimation, but often
yield inaccurate and unverifiable signals due to limited sampling. On the other
hand, methods using Process Reward Models can provide step-by-step judgments
for each reasoning step, but they require high-quality process supervision
labels and are time-consuming when applied in online reinforcement learning
(RL). To overcome these limitations, we introduce a simple but efficient method
Credit Assignment Policy Optimization (CAPO). Given a reasoning response
rollout from the policy model, CAPO directly leverages an off-the-shelf,
general-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to
generate all step-wise critique by one pass, thereby providing verifiable
token-level rewards to refine the tokens that were originally assigned
identical rule-based rewards. This enables more fine-grained credit assignment
in an effective way. Furthermore, to enhance the accuracy and robustness of
CAPO, we employ voting mechanisms that scale with the number of generated
critiques. Extensive experiments using different backbones like Llama and Qwen
models and in different sizes show that CAPO consistently outperforms
supervised learning-based and RL-based fine-tuning methods across six
challenging mathematical benchmarks and three out-of-domain benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simple Methods Defend RAG Systems Well Against Real-World Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Triantafyllopoulos, Renyi Qu, Salvatore Giorgi, Brenda Curtis, Lyle H. Ungar, João Sedoc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring safety and in-domain responses for Retrieval-Augmented Generation
(RAG) systems is paramount in safety-critical applications, yet remains a
significant challenge. To address this, we evaluate four methodologies for
Out-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal
Component Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG
system only responds to queries confined to the system's knowledge base.
Specifically, our evaluation explores two novel dimensionality reduction and
feature separation strategies: \textit{PCA}, where top components are selected
using explained variance or OOD separability, and an adaptation of
\textit{Neural Collapse Feature Separation}. We validate our approach on
standard datasets (StackExchange and MSMARCO) and real-world applications
(Substance Use and COVID-19), including tests against LLM-simulated and actual
attacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations
of response correctness and relevance, we confirm that an external OOD detector
is crucial for maintaining response relevance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A French Version of the OLDI Seed Corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malik Marmonier, Benoît Sagot, Rachel Bawden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first French partition of the OLDI Seed Corpus, our submission
to the WMT 2025 Open Language Data Initiative (OLDI) shared task. We detail its
creation process, which involved using multiple machine translation systems and
a custom-built interface for post-editing by qualified native speakers. We also
highlight the unique translation challenges presented by the source data, which
combines highly technical, encyclopedic terminology with the stylistic
irregularities characteristic of user-generated content taken from Wikipedia.
This French corpus is not an end in itself, but is intended as a crucial pivot
resource to facilitate the collection of parallel corpora for the
under-resourced regional languages of France.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dialogue Systems Engineering: A <span class="highlight-title">Survey</span> and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikio Nakano, Hironori Takeuchi, Sadahiro Yoshikawa, Yoichi Matsuyama, Kazunori Komatani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes to refer to the field of software engineering related to
the life cycle of dialogue systems as Dialogue Systems Engineering, and surveys
this field while also discussing its future directions. With the advancement of
large language models, the core technologies underlying dialogue systems have
significantly progressed. As a result, dialogue system technology is now
expected to be applied to solving various societal issues and in business
contexts. To achieve this, it is important to build, operate, and continuously
improve dialogue systems correctly and efficiently. Accordingly, in addition to
applying existing software engineering knowledge, it is becoming increasingly
important to evolve software engineering tailored specifically to dialogue
systems. In this paper, we enumerate the knowledge areas of dialogue systems
engineering based on those of software engineering, as defined in the Software
Engineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based
on this survey, we identify unexplored topics in each area and discuss the
future direction of dialogue systems engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CellForge: Agentic Design of Virtual Cell Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangru Tang, Zhuoyun Yu, Jiapeng Chen, Yan Cui, Daniel Shao, Weixu Wang, Fang Wu, Yuchen Zhuang, Wenqi Shi, Zhi Huang, Arman Cohan, Xihong Lin, Fabian Theis, Smita Krishnaswamy, Mark Gerstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual cell modeling represents an emerging frontier at the intersection of
artificial intelligence and biology, aiming to predict quantities such as
responses to diverse perturbations quantitatively. However, autonomously
building computational models for virtual cells is challenging due to the
complexity of biological systems, the heterogeneity of data modalities, and the
need for domain-specific expertise across multiple disciplines. Here, we
introduce CellForge, an agentic system that leverages a multi-agent framework
that transforms presented biological datasets and research objectives directly
into optimized computational models for virtual cells. More specifically, given
only raw single-cell multi-omics data and task descriptions as input, CellForge
outputs both an optimized model architecture and executable code for training
virtual cell models and inference. The framework integrates three core modules:
Task Analysis for presented dataset characterization and relevant literature
retrieval, Method Design, where specialized agents collaboratively develop
optimized modeling strategies, and Experiment Execution for automated
generation of code. The agents in the Design module are separated into experts
with differing perspectives and a central moderator, and have to
collaboratively exchange solutions until they achieve a reasonable consensus.
We demonstrate CellForge's capabilities in single-cell perturbation prediction,
using six diverse datasets that encompass gene knockouts, drug treatments, and
cytokine stimulations across multiple modalities. CellForge consistently
outperforms task-specific state-of-the-art methods. Overall, CellForge
demonstrates how iterative interaction between LLM agents with differing
perspectives provides better solutions than directly addressing a modeling
challenge. Our code is publicly available at
https://github.com/gersteinlab/CellForge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynaword: From One-shot to Continuously Developed <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenneth Enevoldsen, Kristian Nørgaard Jensen, Jan Kostkan, Balázs Szabó, Márton Kardos, Kirten Vad, Andrea Blasi Núñez, Gianluca Barmina, Jacob Nielsen, Rasmus Larsen, Peter Vahlstrup, Per Møldrup Dalum, Desmond Elliott, Lukas Galke, Peter Schneider-Kamp, Kristoffer Nielbo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale datasets are foundational for research and development in natural
language processing. However, current approaches face three key challenges: (1)
reliance on ambiguously licensed sources restricting use, sharing, and
derivative works; (2) static dataset releases that prevent community
contributions and diminish longevity; and (3) quality assurance processes
restricted to publishing teams rather than leveraging community expertise.
  To address these limitations, we introduce two contributions: the Dynaword
approach and Danish Dynaword. The Dynaword approach is a framework for creating
large-scale, open datasets that can be continuously updated through community
collaboration. Danish Dynaword is a concrete implementation that validates this
approach and demonstrates its potential. Danish Dynaword contains over four
times as many tokens as comparable releases, is exclusively openly licensed,
and has received multiple contributions across industry and research. The
repository includes light-weight tests to ensure data formatting, quality, and
documentation, establishing a sustainable framework for ongoing community
contributions and dataset evolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic
  Bidirectional Machine Translation System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serry Sibaee, Omer Nacar, Yasser Al-Habashi, Adel Ammar, Wadii Boulila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rich linguistic landscape of the Arab world is characterized by a
significant gap between Modern Standard Arabic (MSA), the language of formal
communication, and the diverse regional dialects used in everyday life. This
diglossia presents a formidable challenge for natural language processing,
particularly machine translation. This paper introduces \textbf{SHAMI-MT}, a
bidirectional machine translation system specifically engineered to bridge the
communication gap between MSA and the Syrian dialect. We present two
specialized models, one for MSA-to-Shami and another for Shami-to-MSA
translation, both built upon the state-of-the-art AraT5v2-base-1024
architecture. The models were fine-tuned on the comprehensive Nabra dataset and
rigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami
model achieved an outstanding average quality score of \textbf{4.01 out of 5.0}
when judged by OPENAI model GPT-4.1, demonstrating its ability to produce
translations that are not only accurate but also dialectally authentic. This
work provides a crucial, high-fidelity tool for a previously underserved
language pair, advancing the field of dialectal Arabic translation and offering
significant applications in content localization, cultural heritage, and
intercultural communication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decomposing the Entropy-Performance Exchange: The Missing Keys to
  Unlocking Effective Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Deng, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, reinforcement learning with verifiable rewards (RLVR) has been
widely used for enhancing the reasoning abilities of large language models
(LLMs). A core challenge in RLVR involves managing the exchange between entropy
and performance of policies. Despite the importance of this exchange, a
fine-grained understanding of when and how this exchange operates most
effectively remains limited. To bridge this gap, we conduct a systematic
empirical analysis of the entropy-performance exchange mechanism of RLVR across
different levels of granularity. Specifically, we first divide the training
process into two distinct stages based on entropy dynamics, i.e., rising stage
and plateau stage, and then systematically investigate how this mechanism
varies across stage-level, instance-level, and token-level granularitiess. Our
analysis reveals that, in the rising stage, entropy reduction in negative
samples facilitates the learning of effective reasoning patterns, which in turn
drives rapid performance gains. Moreover, in the plateau stage, learning
efficiency strongly correlates with high-entropy tokens present in
low-perplexity samples and those located at the end of sequences. Motivated by
these findings, we propose two methods that dynamically adjust the reward
signal using perplexity and positional information to focus RL updates on
tokens that exhibit high learning potential, achieving improvements compared to
the baseline methods on various LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interference Matrix: Quantifying Cross-Lingual Interference in
  <span class="highlight-title">Transformer</span> Encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Belen Alastruey, João Maria Janeiro, Alexandre Allauzen, Maha Elbayad, Loïc Barrault, Marta R. Costa-jussà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a comprehensive study of language interference in
encoder-only Transformer models across 83 languages. We construct an
interference matrix by training and evaluating small BERT-like models on all
possible language pairs, providing a large-scale quantification of
cross-lingual interference. Our analysis reveals that interference between
languages is asymmetrical and that its patterns do not align with traditional
linguistic characteristics, such as language family, nor with proxies like
embedding similarity, but instead better relate to script. Finally, we
demonstrate that the interference matrix effectively predicts performance on
downstream tasks, serving as a tool to better design multilingual models to
obtain optimal performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Isolating Culture Neurons in Multilingual Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danial Namazifard, Lukas Galke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language and culture are deeply intertwined, yet it is so far unclear how and
where multilingual large language models encode culture. Here, we extend upon
an established methodology for identifying language-specific neurons and extend
it to localize and isolate culture-specific neurons, carefully disentangling
their overlap and interaction with language-specific neurons. To facilitate our
experiments, we introduce MUREL, a curated dataset of 85.2 million tokens
spanning six different cultures. Our localization and intervention experiments
show that LLMs encode different cultures in distinct neuron populations,
predominantly in upper layers, and that these culture neurons can be modulated
independently from language-specific neurons or those specific to other
cultures. These findings suggest that cultural knowledge and propensities in
multilingual language models can be selectively isolated and edited - promoting
fairness, inclusivity, and alignment. Code and data is available at
https://github.com/namazifard/Culture_Neurons .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LeanK: Learnable K Cache Channel Pruning for Efficient Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) enable long-context tasks but face efficiency
challenges due to the growing key-value (KV) cache. We propose LeanK, a
learning-based method that prunes unimportant key (K) cache channels by
leveraging static channel sparsity. With a novel two-stage training process,
LeanK learns channel-wise static mask that could satisfy specific sparsity
ratio and hardware alignment requirement. LeanK reduces GPU memory and
accelerates decoding without sacrificing accuracy. Experiments demonstrate up
to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel
enables 1.3x speedup for attention computation. We also provide insights into
model channels and attention heads during long-context inference by analyzing
the learned importance distribution. Our code is available at
https://aka.ms/LeanK.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for
  Proof-Centric Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yebo Peng, Zixiang Liu, Yaoming Li, Zhizhuo Yang, Xinye Xu, Bowen Ye, Weijun Yuan, Zihan Wang, Tong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the mathematical capability of Large Language Models (LLMs) is a
critical yet challenging frontier. Existing benchmarks fall short, particularly
for proof-centric problems, as manual creation is unscalable and costly,
leaving the true mathematical abilities of LLMs largely unassessed. To overcome
these barriers, we propose Proof2Hybrid, the first fully automated framework
that synthesizes high-quality, proof-centric benchmarks from natural language
mathematical corpora. The key novelty of our solution is Proof2X, a roadmap of
converting mathematical proofs into various kinds of questions that are easy to
verify. Instructed by this roadmap, we propose a new type of hybrid-formatted
questions, named ``$m$-out-of-$n$ multiple judge questions'', specifically
designed to enable robust, automatic evaluation while being resilient to
guessing and superficial pattern matching inherent in traditional formats. As a
demonstration of our framework, we introduce AlgGeoTest, a benchmark for
algebraic geometry--a frontier domain of modern mathematics--comprising 456
challenging items. Our extensive evaluations on state-of-the-art LLMs using
AlgGeoTest reveal profound deficits in their comprehension of algebraic
geometry, providing a more precise measure of their true mathematical
capabilities. Our framework and benchmark pave the way for a new wave of
in-depth research into the mathematical intelligence of AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, Yuwei Fu, Jing Su, Ge Zhang, Wenhao Huang, Mingxuan Wang, Lin Yan, Xiaoying Jia, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Yonghui Wu, Hao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Seed Diffusion Preview, a large-scale language model based on
discrete-state diffusion, offering remarkably fast inference speed. Thanks to
non-sequential, parallel generation, discrete diffusion models provide a
notable speedup to mitigate the inherent latency of token-by-token decoding, as
demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion
Preview achieves an inference speed of 2,146 token/s over H20 GPUs while
maintaining competitive performance across a sweep of standard code evaluation
benchmarks, significantly faster than contemporary Mercury and Gemini
Diffusion, establishing new state of the art on the speed-quality Pareto
frontier for code models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Demo is available at https://studio.seed.ai/exp/seed_diffusion/;
  Project page is https://seed.bytedance.com/seed_diffusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Dynamics of Meta-Learning in Small Model <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Demitri Africa, Yuval Weiss, Paula Buttery, Richard Diehl Martinez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are powerful but costly. We ask whether meta-learning
can make the pretraining of small language models not only better but also more
interpretable. We integrate first-order MAML with subset-masked LM pretraining,
producing four LLama-style decoder-only models (11M-570M params), and evaluate
it on a fundamental NLP task with many settings and real-world applications.
Compared with vanilla training, our model (i) reaches the same loss up to 1.6x
sooner, (ii) improves F1 on multilingual Universal NER under equal compute, and
(iii) makes the training dynamics easy to read: first the network's
representations fan out ("diversify") and later they collapse into a smaller,
shared subspace ("compress"). This two-stage shift shows up as a rise-and-fall
in both effective-rank curves and attention-head entropy. The same curves
pinpoint which layers specialise earliest and which later reconverge, giving a
compact, interpretable signature of meta-adaptation. Code, checkpoints and
WandB logs are released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through
  Latent Acoustic Pattern Triggers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Lin, Miao Yu, Kaiwen Luo, Yibo Zhang, Lilan Peng, Dexian Wang, Xuehai Tang, Yuanhe Zhang, Xikang Yang, Zhenhong Zhou, Kun Wang, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Audio Large Language Models (ALLMs) emerge as powerful tools for speech
processing, their safety implications demand urgent attention. While
considerable research has explored textual and vision safety, audio's distinct
characteristics present significant challenges. This paper first investigates:
Is ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In
response to this issue, we introduce Hidden in the Noise (HIN), a novel
backdoor attack framework designed to exploit subtle, audio-specific features.
HIN applies acoustic modifications to raw audio waveforms, such as alterations
to temporal dynamics and strategic injection of spectrally tailored noise.
These changes introduce consistent patterns that an ALLM's acoustic feature
encoder captures, embedding robust triggers within the audio stream. To
evaluate ALLM robustness against audio-feature-based triggers, we develop the
AudioSafe benchmark, assessing nine distinct risk types. Extensive experiments
on AudioSafe and three established safety datasets reveal critical
vulnerabilities in existing ALLMs: (I) audio features like environment noise
and speech rate variations achieve over 90% average attack success rate. (II)
ALLMs exhibit significant sensitivity differences across acoustic features,
particularly showing minimal response to volume as a trigger, and (III)
poisoned sample inclusion causes only marginal loss curve fluctuations,
highlighting the attack's stealth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subject or Style: Adaptive and Training-Free Mixture of LoRAs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Chen Zhang, Yu-Jie Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning models via Low-Rank Adaptation (LoRA) demonstrates remarkable
performance in subject-driven or style-driven generation tasks. Studies have
explored combinations of different LoRAs to jointly generate learned styles and
content. However, current methods struggle to balance the original subject and
style, and often require additional training. Recently, K-LoRA proposed a
training-free LoRA fusion method. But it involves multiple hyperparameters,
making it difficult to adapt to all styles and subjects. In this paper, we
propose EST-LoRA, a training-free adaptive LoRA fusion method. It
comprehensively considers three critical factors: \underline{E}nergy of matrix,
\underline{S}tyle discrepancy scores and \underline{T}ime steps. Analogous to
the Mixture of Experts (MoE) architecture, the model adaptively selects between
subject LoRA and style LoRA within each attention layer. This integrated
selection mechanism ensures balanced contributions from both components during
the generation process. Experimental results show that EST-LoRA outperforms
state-of-the-art methods in both qualitative and quantitative evaluations and
achieves faster generation speed compared to other efficient fusion approaches.
Our code is publicly available at:
https://anonymous.4open.science/r/EST-LoRA-F318.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trainable Dynamic Mask Sparse Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingze Shi, Yifan Wu, Bingheng Wu, Yiran Peng, Liangdong Wang, Guang Liu, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large language models, the demand for modeling long contexts is constantly
increasing, but the quadratic complexity of the standard self-attention
mechanism often becomes a bottleneck. Although existing sparse attention
mechanisms have improved efficiency, they may still encounter issues such as
static patterns or information loss. We introduce a trainable dynamic mask
sparse attention mechanism, Dynamic Mask Attention, which effectively utilizes
content-aware and position-aware sparsity. DMA achieves this through two key
innovations: First, it dynamically generates content-aware sparse masks from
value representations, enabling the model to identify and focus on critical
information adaptively. Second, it implements position-aware sparse attention
computation that effectively skips unnecessary calculation regions. This
dual-sparsity design allows the model to significantly reduce the computational
complexity of important information while retaining complete information,
achieving an excellent balance between information fidelity and computational
efficiency. We have verified the performance of DMA through comprehensive
experiments. Comparative studies show that DMA outperforms multi-head
attention, sliding window attention, multi-head latent attention, and native
sparse attention in terms of perplexity under Chinchilla Scaling Law settings.
Moreover, in challenging multi-query associative recall tasks, DMA also
demonstrates superior performance and efficiency compared to these methods.
Crucially, in the evaluation of a 1.7B parameter model, DMA significantly
outperforms multi-head attention in both standard benchmark performance and the
challenging needle-in-a-haystack task. These experimental results highlight its
capability to balance model efficiency and long-context modeling ability
effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "Harmless to You, Hurtful to Me!": Investigating the Detection of Toxic
  Languages Grounded in the Perspective of Youth <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaqiong Li, Peng Zhang, Lin Wang, Hansu Gu, Siyuan Qiao, Ning Gu, Tun Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Risk perception is subjective, and youth's understanding of toxic content
differs from that of adults. Although previous research has conducted extensive
studies on toxicity detection in social media, the investigation of youth's
unique toxicity, i.e., languages perceived as nontoxic by adults but toxic as
youth, is ignored. To address this gap, we aim to explore: 1) What are the
features of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing
toxicity detection techniques accurately detect these languages (RQ2). For
these questions, we took Chinese youth as the research target, constructed the
first Chinese ``youth-toxicity'' dataset, and then conducted extensive
analysis. Our results suggest that youth's perception of these is associated
with several contextual factors, like the source of an utterance and
text-related features. Incorporating these meta information into current
toxicity detection methods significantly improves accuracy overall. Finally, we
propose several insights into future research on youth-centered toxicity
detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 20th International AAAI Conference on Web and Social
  Media (ICWSM 2026)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRINN: Contrastive Reinforcement Learning for Approximate Nearest
  Neighbor Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoya Li, Xiaofei Sun, Albert Wang, Chris Shum, Jiwei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate nearest-neighbor search (ANNS) algorithms have become
increasingly critical for recent AI applications, particularly in
retrieval-augmented generation (RAG) and agent-based LLM applications. In this
paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS
optimization as a reinforcement learning problem where execution speed serves
as the reward signal. This approach enables the automatic generation of
progressively faster ANNS implementations while maintaining accuracy
constraints. Our experimental evaluation demonstrates CRINN's effectiveness
across six widely-used NNS benchmark datasets. When compared against
state-of-the-art open-source ANNS algorithms, CRINN achieves best performance
on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and
GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean
and GloVe-25-angular). The implications of CRINN's success reach well beyond
ANNS optimization: It validates that LLMs augmented with reinforcement learning
can function as an effective tool for automating sophisticated algorithmic
optimizations that demand specialized knowledge and labor-intensive manual
refinement.Code can be found at https://github.com/deepreinforce-ai/CRINN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy
  in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Li, Keyu Wang, Shu Yang, Zhuoran Zhang, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing
with user-stated opinions even when those contradict factual knowledge. While
prior work has documented this tendency, the internal mechanisms that enable
such behavior remain poorly understood. In this paper, we provide a mechanistic
account of how sycophancy arises within LLMs. We first systematically study how
user opinions induce sycophancy across different model families. We find that
simple opinion statements reliably induce sycophancy, whereas user expertise
framing has a negligible impact. Through logit-lens analysis and causal
activation patching, we identify a two-stage emergence of sycophancy: (1) a
late-layer output preference shift and (2) deeper representational divergence.
We also verify that user authority fails to influence behavior because models
do not encode it internally. In addition, we examine how grammatical
perspective affects sycophantic behavior, finding that first-person prompts
(``I believe...'') consistently induce higher sycophancy rates than
third-person framings (``They believe...'') by creating stronger
representational perturbations in deeper layers. These findings highlight that
sycophancy is not a surface-level artifact but emerges from a structural
override of learned knowledge in deeper layers, with implications for alignment
and truthful AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Capital Visualization using Speech Amount during Meetings <span class="chip">SIGDIAL
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekai Hashimoto, Takeshi Mizumoto, Kohei Nagira, Shun Shiramatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, many companies have recognized the importance of human
resources and are investing in human capital to revitalize their organizations
and enhance internal communication, thereby fostering innovation. However,
conventional quantification methods have mainly focused on readily measurable
indicators without addressing the fundamental role of conversations in human
capital. This study focuses on routine meetings and proposes strategies to
visualize human capital by analyzing speech amount during these meetings. We
employ conversation visualization technology, which operates effectively, to
quantify speech. We then measure differences in speech amount by attributes
such as gender and job post, changes in speech amount depending on whether
certain participants are present, and correlations between speech amount and
continuous attributes. To verify the effectiveness of our proposed methods, we
analyzed speech amounts by departmental affiliation during weekly meetings at
small to medium enterprises.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at the 26th Annual
  Meeting of the Special Interest Group on Discourse and Dialogue(SIGDIAL
  2025). It represents the author's version of the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The SMeL Test: A simple benchmark for media literacy in language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustaf Ahdritz, Anat Kleiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The internet is rife with unattributed, deliberately misleading, or otherwise
untrustworthy content. Though large language models (LLMs) are often tasked
with autonomous web browsing, the extent to which they have learned the simple
heuristics human researchers use to navigate this noisy environment is not
currently known. In this paper, we introduce the Synthetic Media Literacy Test
(SMeL Test), a minimal benchmark that tests the ability of language models to
actively filter out untrustworthy information in context. We benchmark a
variety of commonly used instruction-tuned LLMs, including reasoning models,
and find that no model consistently trusts more reliable sources; while
reasoning in particular is associated with higher scores, even the best API
model we test hallucinates up to 70% of the time. Remarkably, larger and more
capable models do not necessarily outperform their smaller counterparts. We
hope our work sheds more light on this important form of hallucination and
guides the development of new methods to combat it.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MolReasoner: Toward Effective and Interpretable Reasoning for Molecular
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guojiang Zhao, Sihang Li, Zixiang Lu, Zheng Cheng, Haitao Lin, Lirong Wu, Hanchen Xia, Hengxing Cai, Wentao Guo, Hongshuai Wang, Mingjun Xu, Siyu Zhu, Guolin Ke, Linfeng Zhang, Zhifeng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models(LLMs) have demonstrated remarkable performance across
various domains, yet their capabilities in molecular reasoning remain
insufficiently explored. Current approaches tend to rely heavily on
general-purpose prompting, which lacks domain-specific molecular semantics,
while those that use fine-tuning strategies often face challenges with
interpretability and reasoning depth. To address these issues, we introduce
MolReasoner, a two-stage framework designed to transition LLMs from
memorization towards chemical reasoning. First, we propose Mol-SFT, which
initializes the model's reasoning abilities via synthetic Chain-of-Thought(CoT)
samples generated by GPT-4o and verified for chemical accuracy. Subsequently,
Mol-RL applies reinforcement learning with specialized reward functions
designed explicitly to align chemical structures with linguistic descriptions,
thereby enhancing molecular reasoning capabilities. Our approach notably
enhances interpretability, improving the model 's molecular understanding and
enabling better generalization. Extensive experiments demonstrate that
MolReasoner outperforms existing methods, and marking a significant shift from
memorization-based outputs to robust chemical reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProCut: LLM <span class="highlight-title">Prompt</span> Compression via Attribution Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhentao Xu, Fengyi Li, Albert Chen, Xiaofeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large-scale industrial LLM systems, prompt templates often expand to
thousands of tokens as teams iteratively incorporate sections such as task
instructions, few-shot examples, and heuristic rules to enhance robustness and
coverage. This expansion leads to bloated prompts that are difficult to
maintain and incur significant inference latency and serving costs. To address
this, we introduce Prompt Compression via Attribution Estimation (ProCut), a
flexible, LLM-agnostic, training-free framework that compresses prompts through
attribution analysis. ProCut segments prompt templates into semantically
meaningful units, quantifies their impact on task performance, and prunes
low-utility components. Through extensive experiments on five public benchmark
datasets and real-world industrial prompts, we show that ProCut achieves
substantial prompt size reductions (78% fewer tokens in production) while
maintaining or even slightly improving task performance (up to 62% better than
alternative methods). We further introduce an LLM-driven attribution estimator
that reduces compression latency by over 50%, and demonstrate that ProCut
integrates seamlessly with existing prompt-optimization frameworks to produce
concise, high-performing prompts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing Temporal Databases for Systematic Evaluation of Factual
  Time-Sensitive Question-Answering in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soyeon Kim, Jindong Wang, Xing Xie, Steven Euijong Whang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facts evolve over time, making it essential for Large Language Models (LLMs)
to handle time-sensitive factual knowledge accurately and reliably. While
factual Time-Sensitive Question-Answering (TSQA) tasks have been widely
studied, existing benchmarks often rely on manual curation or a small, fixed
set of predefined templates, which restricts scalable and comprehensive TSQA
evaluation. To address these challenges, we propose TDBench, a new benchmark
that systematically constructs TSQA pairs by harnessing temporal databases and
database techniques such as temporal SQL and functional dependencies. We also
introduce a fine-grained evaluation metric called time accuracy, which assesses
the validity of time references in model explanations alongside traditional
answer accuracy to enable a more reliable TSQA evaluation. Extensive
experiments on contemporary LLMs show how \ours{} enables scalable and
comprehensive TSQA evaluation while reducing the reliance on human labor,
complementing existing Wikipedia/Wikidata-based TSQA evaluation approaches by
enabling LLM evaluation on application-specific data and seamless multi-hop
question generation. Code and data are publicly available at:
https://github.com/ssoy0701/tdbench.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Marco-Voice Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengping Tian, Chenyang Lyu, Xuanfan Ni, Haoqin Sun, Qingjuan Li, Zhiqiang Qian, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo, Kaifu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a multifunctional speech synthesis system that integrates
voice cloning and emotion control speech synthesis within a unified framework.
The goal of this work is to address longstanding challenges in achieving highly
expressive, controllable, and natural speech generation that faithfully
preserves speaker identity across diverse linguistic and emotional contexts.
Our approach introduces an effective speaker-emotion disentanglement mechanism
with in-batch contrastive learning, enabling independent manipulation of
speaker identity and eemotional style, as well as rotational emotional
embedding integration method for smooth emotion control. To support
comprehensive training and evaluation, we construct CSEMOTIONS, a high-quality
emotional speech dataset containing 10 hours of Mandarin speech from six
professional speakers across seven emotional categories. Extensive experiments
demonstrate that our system, Marco-Voice, achieves substantial improvements in
both objective and subjective metrics. Comprehensive evaluations and analysis
were conducted, results show that MarcoVoice delivers competitive performance
in terms of speech clarity and emotional richness, representing a substantial
advance in the field of expressive neural speech synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a
  Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huihan Li, You Chen, Siyuan Wang, Yixin He, Ninareh Mehrabi, Rahul Gupta, Xiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) perform well on reasoning benchmarks but often
fail when inputs alter slightly, raising concerns about the extent to which
their success relies on memorization. This issue is especially acute in
Chain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger
intermediate errors that cascade into incorrect final answers. We introduce
STIM, a novel framework for Source-aware Token-level Identification of
Memorization, which attributes each token in a reasoning chain to one of
multiple memorization sources - local, mid-range, or long-range - based on
their statistical co-occurrence with the token in the pretraining corpus. Our
token-level analysis across tasks and distributional settings reveals that
models rely more on memorization in complex or long-tail cases, and that local
memorization is often the dominant driver of errors, leading to up to 67% of
wrong tokens. We also show that memorization scores from STIM can be effective
in predicting the wrong tokens in the wrong reasoning step. STIM offers a
powerful tool for diagnosing and improving model reasoning and can generalize
to other structured step-wise generation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanqi Yang, Yanda Li, Yunchao Wei, Meng Fang, Ling Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large audio-language models (LALMs) have achieved near-human performance in
sentence-level transcription and emotion recognition. However, existing
evaluations focus mainly on surface-level perception, leaving the capacity of
models for contextual and inference-driven reasoning in speech-based scenarios
insufficiently examined. To address this gap, we introduce SpeechR, a unified
benchmark for evaluating reasoning over speech in large audio-language models.
SpeechR evaluates models along three key dimensions: factual retrieval,
procedural inference, and normative judgment. It includes three distinct
evaluation formats. The multiple-choice version measures answer selection
accuracy. The generative version assesses the coherence and logical consistency
of reasoning chains. The acoustic-feature version investigates whether
variations in stress and emotion affect reasoning performance. Evaluations on
eleven state-of-the-art LALMs reveal that high transcription accuracy does not
translate into strong reasoning capabilities. SpeechR establishes a structured
benchmark for evaluating reasoning in spoken language, enabling more targeted
analysis of model capabilities across diverse dialogue-based tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpeechRole: A Large-Scale <span class="highlight-title">Dataset</span> and Benchmark for Evaluating Speech
  Role-Playing Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changhao Jiang, Jiajun Sun, Yifei Cao, Jiabao Zhuang, Hui Li, Xiaoran Fan, Ming Zhang, Junjie Ye, Shihan Dou, Zhiheng Xi, Jingqi Tong, Yilong Wu, Baoyu Fan, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, role-playing agents have emerged as a promising paradigm for
achieving personalized interaction and emotional resonance. Existing research
primarily focuses on the textual modality, neglecting the critical dimension of
speech in realistic interactive scenarios. In particular, there is a lack of
systematic evaluation for Speech Role-Playing Agents (SRPAs). To address this
gap, we construct SpeechRole-Data, a large-scale, high-quality dataset that
comprises 98 diverse roles and 112k speech-based single-turn and multi-turn
conversations. Each role demonstrates distinct vocal characteristics, including
timbre and prosody, thereby enabling more sophisticated speech role-playing.
Furthermore, we propose SpeechRole-Eval, a multidimensional evaluation
benchmark that systematically assesses SRPAs performance in key aspects such as
fundamental interaction ability, speech expressiveness, and role-playing
fidelity. Experimental results reveal the advantages and challenges of both
cascaded and end-to-end speech role-playing agents in maintaining vocal style
consistency and role coherence. We release all data, code, and baseline models
to provide a solid foundation for speech-driven multimodal role-playing
research and to foster further developments in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>ing Large Language Models to Detect Dementia Family Caregivers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Badsha Biswas, Özlem Uzuner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media, such as Twitter, provides opportunities for caregivers of
dementia patients to share their experiences and seek support for a variety of
reasons. Availability of this information online also paves the way for the
development of internet-based interventions in their support. However, for this
purpose, tweets written by caregivers of dementia patients must first be
identified. This paper demonstrates our system for the SMM4H 2025 shared task
3, which focuses on detecting tweets posted by individuals who have a family
member with dementia. The task is outlined as a binary classification problem,
differentiating between tweets that mention dementia in the context of a family
member and those that do not. Our solution to this problem explores large
language models (LLMs) with various prompting methods. Our results show that a
simple zero-shot prompt on a fine-tuned model yielded the best results. Our
final system achieved a macro F1-score of 0.95 on the validation set and the
test set. Our full code is available on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextually Aware E-Commerce Product Question Answering using RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Praveen Tangarajan, Anand A. Rajasekar, Manish Rathi, Vinay Rao Dandin, Ozan Ersoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  E-commerce product pages contain a mix of structured specifications,
unstructured reviews, and contextual elements like personalized offers or
regional variants. Although informative, this volume can lead to cognitive
overload, making it difficult for users to quickly and accurately find the
information they need. Existing Product Question Answering (PQA) systems often
fail to utilize rich user context and diverse product information effectively.
We propose a scalable, end-to-end framework for e-commerce PQA using Retrieval
Augmented Generation (RAG) that deeply integrates contextual understanding. Our
system leverages conversational history, user profiles, and product attributes
to deliver relevant and personalized answers. It adeptly handles objective,
subjective, and multi-intent queries across heterogeneous sources, while also
identifying information gaps in the catalog to support ongoing content
improvement. We also introduce novel metrics to measure the framework's
performance which are broadly applicable for RAG system evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure, 5 tables. Preprint under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TIBSTC-CoT: A Multi-Domain Instruction <span class="highlight-title">Dataset</span> for Chain-of-Thought
  Reasoning in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Gao, Cheng Huang, Nyima Tashi, Yutong Liu, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Xiao Feng, Hao Wang, Yongbin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the severe data scarcity in Tibetan, a low-resource language
spoken by over six million people, we introduce TIBSTC-CoT, the large-scale,
multi-domain Tibetan dataset automatically constructed via chain-of-thought
prompting with large language models (LLMs). TIBSTC-CoT establishes a scalable
and reproducible framework for dataset creation in low-resource settings,
covering diverse domains and reasoning patterns essential for language
understanding and generation. Building on this dataset, we develop the
Sunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with
chain-of-thought capabilities. Trained entirely on TIBSTC-CoT,
Sunshine-thinking has demonstrated strong reasoning and generation performance,
comparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a
significant step toward inclusive AI by enabling high-quality Tibetan language
processing through both resource creation and model innovation. All data are
available: https://github.com/Vicentvankor/sun-shine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extrapolation by Association: Length Generalization Transfer in
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Cai, Nayoung Lee, Avi Schwarzschild, Samet Oymak, Dimitris Papailiopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer language models have demonstrated impressive generalization
capabilities in natural language domains, yet we lack a fine-grained
understanding of how such generalization arises. In this paper, we investigate
length generalization--the ability to extrapolate from shorter to longer
inputs--through the lens of \textit{task association}. We find that length
generalization can be \textit{transferred} across related tasks. That is,
training a model with a longer and related auxiliary task can lead it to
generalize to unseen and longer inputs from some other target task. We
demonstrate this length generalization transfer across diverse algorithmic
tasks, including arithmetic operations, string transformations, and maze
navigation. Our results show that transformer models can inherit generalization
capabilities from similar tasks when trained jointly. Moreover, we observe
similar transfer effects in pretrained language models, suggesting that
pretraining equips models with reusable computational scaffolding that
facilitates extrapolation in downstream settings. Finally, we provide initial
mechanistic evidence that length generalization transfer correlates with the
re-use of the same attention heads between the tasks. Together, our findings
deepen our understanding of how transformers generalize to out-of-distribution
inputs and highlight the compositional reuse of inductive structure across
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gandalf the Red: Adaptive Security for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07927v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07927v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Pfister, Václav Volhejn, Manuel Knott, Santiago Arias, Julia Bazińska, Mykhailo Bichurin, Alan Commike, Janet Darling, Peter Dienes, Matthew Fiedler, David Haber, Matthias Kraft, Marco Lancini, Max Mathys, Damián Pascual-Ortiz, Jakub Podolak, Adrià Romero-López, Kyriacos Shiarlis, Andreas Signer, Zsolt Terek, Athanasios Theocharis, Daniel Timbrell, Samuel Trautwein, Samuel Watts, Yun-Han Wu, Mateo Rojas-Carulla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current evaluations of defenses against prompt attacks in large language
model (LLM) applications often overlook two critical factors: the dynamic
nature of adversarial behavior and the usability penalties imposed on
legitimate users by restrictive defenses. We propose D-SEC (Dynamic Security
Utility Threat Model), which explicitly separates attackers from legitimate
users, models multi-step interactions, and expresses the security-utility in an
optimizable form. We further address the shortcomings in existing evaluations
by introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed
to generate realistic, adaptive attack. Using Gandalf, we collect and release a
dataset of 279k prompt attacks. Complemented by benign user data, our analysis
reveals the interplay between security and utility, showing that defenses
integrated in the LLM (e.g., system prompts) can degrade usability even without
blocking requests. We demonstrate that restricted application domains,
defense-in-depth, and adaptive defenses are effective strategies for building
secure and useful LLM applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Niklas Pfister, V\'aclav Volhejn and Manuel Knott contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCOPE: Stochastic and Counterbiased Option Placement for Evaluating
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18182v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18182v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonjun Jeong, Dongseok Kim, Taegkeun Whangbo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) can achieve inflated scores on multiple-choice
tasks by exploiting inherent biases in option positions or labels, rather than
demonstrating genuine understanding. This study introduces SCOPE, an evaluation
framework designed to measure and mitigate such selection bias in a
dataset-independent manner. By repeatedly invoking a null prompt that lacks
semantic content, SCOPE estimates each model's unique position-bias
distribution. It then redistributes the answer slot according to the
inverse-bias distribution, thereby equalizing the lucky-rate, the probability
of selecting the correct answer by chance. Furthermore, it prevents
semantically similar distractors from being placed adjacent to the answer,
thereby blocking near-miss guesses based on superficial proximity cues. Across
multiple benchmark experiments, SCOPE consistently outperformed existing
debiasing methods in terms of stable performance improvements and showed
clearer confidence distributions over correct options. This framework thus
offers a new standard for enhancing the fairness and reliability of LLM
evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments: 34 pages, 1 figure. v2: All "Consequence." statements in
  the Theoretical Analysis section relabeled as "Corollary."; duplicated values
  in Table 20 (previously identical to Table 15) corrected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs
  More Realistic and Less Risky 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.03336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.03336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashutosh Hathidara, Julien Yu, Sebastian Schreiber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly tasked with invoking enterprise
APIs, yet they routinely falter when near-duplicate tools vie for the same user
intent or when required arguments are left underspecified. We introduce
DiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a
disambiguation-centric, three-stage pipeline that (i) synthesizes
persona-driven, multi-turn dialogues in which the assistant must distinguish
among highly similar tools, (ii) performs supervised fine-tuning of open-source
models with reasoning traces across 3B - 70B parameters, and (iii) evaluates
real-world readiness via a dynamic suite that redeploys each model in a live
agentic loop and reports end-to-end goal completion alongside conventional
static metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE
raise tool-invocation success by 27 pp over GPT-4o and by 49 pp over
Claude-3.5-Sonnet, both under optimized prompting. To spur further research, we
release an open corpus of 5000 production-grade enterprise API specifications
paired with rigorously validated, disambiguation-focused dialogues, offering a
practical blueprint for building reliable, enterprise-ready tool-calling
agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Dynamic Clustering-Based Document Compression for
  Retrieval-Augmented-Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.03165v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.03165v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weitao Li, Kaiming Liu, Xiangyu Zhang, Xuanyu Lei, Weizhi Ma, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach
for knowledge injection during large language model (LLM) inference in recent
years. However, due to their limited ability to exploit fine-grained
inter-document relationships, current RAG implementations face challenges in
effectively addressing the retrieved noise and redundancy content, which may
cause error in the generation results. To address these limitations, we propose
an Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG)
that utilizes latent inter-document relationships while simultaneously removing
irrelevant information and redundant content. We validate our approach, built
upon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and
Hallucination-Detection datasets. Experimental results show that our method
achieves consistent performance improvements across various scenarios and
experimental settings, demonstrating strong robustness and applicability. Our
code and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ It's High Time: A <span class="highlight-title">Survey</span> of Temporal Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20243v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20243v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhawna Piryani, Abdelrahman Abdallah, Jamshid Mozafari, Avishek Anand, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time plays a critical role in how information is generated, retrieved, and
interpreted. In this survey, we provide a comprehensive overview of Temporal
Question Answering (TQA), a research area that focuses on answering questions
involving temporal constraints or context. As the amount of time-stamped
content from sources like news articles, web archives, and knowledge bases
increases, systems must address challenges such as detecting temporal intent,
normalizing time expressions, ordering events, and reasoning over evolving or
ambiguous facts. We focus on recent advances in TQA enabled by neural
architectures, especially transformer-based models and Large Language Models
(LLMs), highlighting progress in temporal language modeling,
retrieval-augmented generation (RAG), and temporal reasoning. We also discuss
benchmark datasets and evaluation strategies designed to test temporal
robustness, recency awareness, and generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Xtra<span class="highlight-title">GPT</span>: Context-Aware and Controllable Academic Paper Revision via
  Human-AI Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuo Chen, Andre Lin HuiKai, Jiaying Wu, Junyi Hou, Zining Zhang, Qian Wang, Xidong Wang, Bingsheng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the growing adoption of large language models (LLMs) in academic
workflows, their capabilities remain limited when it comes to supporting
high-quality scientific writing. Most existing systems are designed for
general-purpose scientific text generation and fail to meet the sophisticated
demands of research communication beyond surface-level polishing, such as
conceptual coherence across sections. Furthermore, academic writing is
inherently iterative and revision-driven, a process not well supported by
direct prompting-based paradigms. To address these scenarios, we propose a
human-AI collaboration framework for academic paper revision. We first
introduce a comprehensive dataset of 7,040 research papers from top-tier venues
annotated with over 140,000 instruction-response pairs that reflect realistic,
section-level scientific revisions. Building on the dataset, we develop
XtraGPT, the first suite of open-source LLMs, designed to provide
context-aware, instruction-guided writing assistance, ranging from 1.5B to 14B
parameters. Extensive experiments validate that XtraGPT significantly
outperforms same-scale baselines and approaches the quality of proprietary
systems. Both automated preference assessments and human evaluations confirm
the effectiveness of our models in improving scientific drafts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. The model report is available at
  https://arxiv.org/abs/2505.11336v1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CCSBench: Evaluating Compositional Controllability in LLMs for
  Scientific Document Summarization <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12601v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12601v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixi Ding, Jiaying Wu, Tongyao Zhu, Yanxia Qin, Qian Liu, Min-Yen Kan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To broaden the dissemination of scientific knowledge to diverse audiences, it
is desirable for scientific document summarization systems to simultaneously
control multiple attributes such as length and empirical focus. However,
existing research typically focuses on controlling single attributes, leaving
the compositional control of multiple attributes underexplored. To address this
gap, we introduce CCSBench, the first evaluation benchmark for compositional
controllable summarization in the scientific domain. Our benchmark enables
fine-grained control over both explicit attributes (e.g., length), which are
objective and straightforward, and implicit attributes (e.g., conceptual or
empirical focus), which are more subjective and abstract. We conduct extensive
experiments using various large language models (LLMs) under various settings,
including in-context learning, parameter-efficient fine-tuning, and two-stage
modular methods for balancing control over different attributes. Our findings
reveal significant limitations in LLMs capabilities in balancing trade-offs
between control attributes, especially implicit ones that require deeper
understanding and abstract reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to KDD 2025 SciSoc LLM Workshop: Large Language Models for
  Scientific and Societal Advances</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmoud Ahmed, Junjie Fei, Jian Ding, Eslam Mohamed Bakr, Mohamed Elhoseiny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Part-Aware Point Grounded Description (PaPGD), a
challenging task aimed at advancing 3D multimodal learning for fine-grained,
part-aware segmentation grounding and detailed explanation of 3D objects.
Existing 3D datasets largely focus on either vision-only part segmentation or
vision-language scene segmentation, lacking the fine-grained multimodal
segmentation needed for robotic navigation and interaction in real-world
environments. To address this gap, we present the 3DCoMPaT Grounded
Instructions (3DCoMPaT-GrIn) Dataset, a comprehensive resource that pairs rich
point cloud descriptions with corresponding part-level segmentation masks. This
dataset encompasses extensive samples designed for both PaPGD and fine-grained
single-part grounding tasks. To tackle the inherent challenges of grounding
objects and generating grounded descriptions at the part level, we propose
Kestrel, a part-aware 3D multimodal large language model that integrates an
advanced language model for nuanced language comprehension with multi-level
point feature propagation and query refinement mechanism to enhance spatial
reasoning at the part level. The extensive experiments demonstrate that Kestrel
effectively bridges the gap between part-aware language understanding and 3D
segmentation grounding, paving the way for more robust and interpretable 3D
object comprehension that meets the demands of real-world robotic applications.
Project page at https://feielysia.github.io/Kestrel.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arena-Lite: Efficient and Reliable Large Language Model Evaluation via
  Tournament-Based Direct Comparisons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01281v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01281v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonil Son, Ju-Min Oh, Heegon Jin, Cheolhun Jang, Jeongbeom Jeong, Kuntae Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) expand across domains, LLM judges have become
essential for systems evaluation. Current benchmarks typically compare system
outputs against baselines. This baseline-mediated approach, though convenient,
yields lower reliability than direct comparison between systems. We propose
Arena-Lite which integrates tournament structure on top of head-to-head
comparison. The application of a tournament structure and direct comparison
eliminates the need for baseline outputs, reduces the number of required
comparisons, and allows higher reliability in system rankings. We conducted two
experiments: (1) controlled stochastic modeling and (2) empirical validation
with a real LLM judge. Those experiments collectively demonstrate that
Arena-Lite consistently achieves higher reliability with fewer comparisons,
even with smaller datasets or weaker judges. We release an easy-to-use web
demonstration and code to foster adoption of Arena-Lite, streamlining model
selection across research and industry communities. Arena-Lite demo and code
are available on
\href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages for main body, 19 pages in total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIST: Jailbreaking Black-box Large Language Models via Iterative
  Semantic Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muyang Zheng, Yuanzhi Yao, Changting Lin, Rui Wang, Caihong Kai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite efforts to align large language models (LLMs) with societal and moral
values, these models remain susceptible to jailbreak attacks -- methods
designed to elicit harmful responses. Jailbreaking black-box LLMs is considered
challenging due to the discrete nature of token inputs, restricted access to
the target LLM, and limited query budget. To address the issues above, we
propose an effective method for jailbreaking black-box large language Models
via Iterative Semantic Tuning, named MIST. MIST enables attackers to
iteratively refine prompts that preserve the original semantic intent while
inducing harmful content. Specifically, to balance semantic similarity with
computational efficiency, MIST incorporates two key strategies: sequential
synonym search, and its advanced version -- order-determining optimization. We
conduct extensive experiments on two datasets using two open-source and four
closed-source models. Results show that MIST achieves competitive attack
success rate, relatively low query count, and fair transferability,
outperforming or matching state-of-the-art jailbreak methods. Additionally, we
conduct analysis on computational efficiency to validate the practical
viability of MIST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do MLLMs Capture How Interfaces Guide User Behavior? A Benchmark for
  Multimodal UI/UX Design Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05026v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05026v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehyun Jeon, Min Soo Kim, Jang Han Yoon, Sumin Shim, Yejin Choi, Hanbin Kim, Youngjae Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User interface (UI) design goes beyond visuals, guiding user behavior and
overall user experience (UX). Strategically crafted interfaces, for example,
can boost sign-ups and drive business sales, underscoring the shift toward
UI/UX as a unified design concept. While recent studies have explored UI
quality evaluation using Multimodal Large Language Models (MLLMs), they largely
focus on surface-level features, overlooking behavior-oriented aspects. To fill
this gap, we introduce WiserUI-Bench, a novel benchmark for assessing models'
multimodal understanding of UI/UX design. It includes 300 diverse real-world UI
image pairs, each consisting of two design variants A/B-tested at scale by
actual companies, where one was empirically validated to steer more user
actions than the other. Each pair is accompanied one or more of 684
expert-curated rationales that capture key factors behind each winning design's
effectiveness, spanning diverse cognitive dimensions of UX. Our benchmark
supports two core tasks: (1) selecting the more effective UI/UX design by
predicting the A/B test verified winner and (2) assessing how well a model,
given the winner, can explain its effectiveness in alignment with expert
reasoning. Experiments across several MLLMs show that current models exhibit
limited nuanced reasoning about UI/UX design and its behavioral impact. We
believe our work will foster research in UI/UX understanding and enable broader
applications such as behavior-aware interface optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 25 figures, Our code and dataset:
  https://github.com/jeochris/wiserui-bench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Order Template Prediction for Generative Aspect-Based Sentiment
  Analysis <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11130v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11130v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghyun Jun, Hwanhee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-based sentiment analysis (ABSA) assesses sentiments towards specific
aspects within texts, resulting in detailed sentiment tuples. Previous ABSA
models often use static templates to predict all of the elements in the tuples,
and these models often fail to accurately capture dependencies between
elements. Multi-view prompting method improves the performance of ABSA by
predicting tuples with various templates and then ensembling the results.
However, this method suffers from inefficiencies and out-of-distribution
errors. In this paper, we propose a Dynamic Order Template (DOT) method for
ABSA, which dynamically generates necessary views for each instance based on
instance-level entropy. Ensuring the diverse and relevant view generation, our
proposed method improves F1-scores on ASQP and ACOS datasets while
significantly reducing inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Your AI, Not Your View: The Bias of LLMs in Investment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoyoung Lee, Junhyuk Seo, Suhwan Park, Junhyeong Lee, Wonbin Ahn, Chanyeol Choi, Alejandro Lopez-Lira, Yongjae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In finance, Large Language Models (LLMs) face frequent knowledge conflicts
due to discrepancies between pre-trained parametric knowledge and real-time
market data. These conflicts become particularly problematic when LLMs are
deployed in real-world investment services, where misalignment between a
model's embedded preferences and those of the financial institution can lead to
unreliable recommendations. Yet little research has examined what investment
views LLMs actually hold. We propose an experimental framework to investigate
such conflicts, offering the first quantitative analysis of confirmation bias
in LLM-based investment analysis. Using hypothetical scenarios with balanced
and imbalanced arguments, we extract the latent preferences of models and
measure their persistence. Focusing on sector, size, and momentum, our analysis
reveals distinct, model-specific tendencies. In particular, we observe a
consistent preference for large-cap stocks and contrarian strategies across
most models. These preferences often harden into confirmation bias, with models
clinging to initial judgments despite counter-evidence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leaps Beyond the Seen: Reinforced Reasoning Augmented Generation for
  Clinical Notes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lo Pang-Yun Ting, Chengshuai Zhao, Yu-Hua Zeng, Yuan Jee Lim, Kun-Ta Chuang, Huan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical note generation aims to produce free-text summaries of a patient's
condition and diagnostic process, with discharge instructions being a
representative long-form example. While recent LLM-based methods pre-trained on
general clinical corpora show promise in clinical text generation, they fall
short in producing long-form notes from limited patient information. In this
paper, we propose ReinRAG, a reinforced reasoning augmented generation (RAG)
for long-form discharge instructions based on pre-admission information.
ReinRAG retrieves reasoning paths from a medical knowledge graph to provide
explicit semantic guidance to the LLM. To bridge the information gap, we
propose group-based retriever optimization (GRO) which improves retrieval
quality with group-normalized rewards, encouraging reasoning leaps for deeper
inference by the LLM. Comprehensive experiments on the real-world dataset show
that ReinRAG outperforms baselines in both clinical efficacy and natural
language generation metrics. Further analysis reveals that ReinRAG fills
semantic gaps in sparse input scenarios, and retrieved reasoning paths help
LLMs avoid clinical misinterpretation by focusing on key evidence and following
coherent reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Associative memory inspires improvements for in-context learning using a
  novel attention residual stream architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15113v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15113v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas F Burns, Tomoki Fukai, Christopher J Earls
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate an impressive ability to utilise
information within the context of their input sequences to appropriately
respond to data unseen by the LLM during its training procedure. This ability
is known as in-context learning (ICL). Humans and non-human animals demonstrate
similar abilities, however their neural architectures differ substantially from
LLMs. Despite this, a critical component within LLMs, the attention mechanism,
resembles modern associative memory models, widely used in and influenced by
the computational neuroscience community to model biological memory systems.
Using this connection, we introduce an associative memory model capable of
performing ICL. We use this as inspiration for a novel residual stream
architecture which allows information to directly flow between attention heads.
We test this architecture during training within a two-layer Transformer and
show its ICL abilities manifest more quickly than without this modification. We
then apply our architecture in small language models with 8 million and 1
billion parameters, focusing on attention head values, with results also
indicating improved performance at these larger and more naturalistic scales.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 14 figures, 6 tables; accepted and published in TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thought Manipulation: External Thought Can Be Efficient for Large
  Reasoning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13626v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13626v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yule Liu, Jingyi Zheng, Zhen Sun, Zifan Peng, Wenhan Dong, Zeyang Sha, Shiwen Cui, Weiqiang Wang, Xinlei He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large reasoning models (LRMs) have demonstrated the
effectiveness of scaling test-time computation to enhance reasoning
capabilities on various tasks. However, LRMs often suffer from an
``overthinking'' problem, where the model generates excessively redundant
reasoning steps with limited performance gains. In this work, we empirically
reveal an important characteristic of LRM behaviors that placing external CoTs
generated by smaller models between the thinking token (\texttt{<think>} and
\texttt{</think>}) can effectively manipulate the model to generate fewer
thoughts. Building on this finding, we propose a simple yet efficient pipeline,
\Method, to enable LRMs to bypass unnecessary intermediate steps, thereby
significantly reducing computational costs. We conduct extensive experiments to
evaluate the utility and efficiency of \Method. For instance, when applied to
QwQ-32B on the LiveBench/Code dataset, \Method keeps the original performance
while reducing output token counts by approximately 30\%, with minimal overhead
introduced by the CoT generator. Furthermore, we identify two suboptimal modes,
blindly following flawed external thoughts and unnecessary rethinking, and show
that simple mitigations, such as difficulty-aware fallbacks, can further
improve performance. Overall, \Method offers a practical, general, and
efficient way to optimize LRM inference, making powerful reasoning models more
accessible and scalable for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Generalization vs Fidelity Paradox in Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.15442v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.15442v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhas Kamasetty Ramesh, Ayan Sengupta, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) is a key technique for compressing large language
models into smaller ones while preserving performance. Despite the recent
traction of KD research, its effectiveness for smaller language models (LMs)
and the mechanisms driving knowledge transfer remain underexplored. In this
work, we present the first large-scale empirical and statistical analysis of KD
across models ranging from 0.5B to 7B parameters on 14 complex reasoning tasks
in a zero-shot setting. Our findings reveal that KD can improve the average
performance of smaller models by up to $10\%$, with a peak task specific gain
of $22\%$, while providing only marginal benefits ($\sim 1.3\%$) for larger
models. Surprisingly, teacher performance has a minimal impact on student
outcomes, while teacher task expertise impacts KD effectiveness. A correlation
study indicates that smaller LMs benefit more from KD, whereas larger LMs show
diminished gains. Additionally, we uncover a misalignment between improvements
in student performance and reasoning fidelity, suggesting that while KD
enhances accuracy, it does not always maintain the structured decision-making
processes of the teacher. Our ablation study further highlights the importance
of teacher signals and logit smoothing in influencing students' performance
after distillation. Overall, our study offers a comprehensive empirical and
statistical assessment of KD, highlighting both its benefits and trade-offs
when distilling knowledge from larger to smaller LMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalizing Verifiable Instruction Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentina Pyatkin, Saumya Malik, Victoria Graf, Hamish Ivison, Shengyi Huang, Pradeep Dasigi, Nathan Lambert, Hannaneh Hajishirzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A crucial factor for successful human and AI interaction is the ability of
language models or chatbots to follow human instructions precisely. A common
feature of instructions are output constraints like ``only answer with yes or
no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to
craft a more useful answer. Even today's strongest models struggle with
fulfilling such constraints. We find that most models strongly overfit on a
small set of verifiable constraints from the benchmarks that test these
abilities, a skill called precise instruction following, and are not able to
generalize well to unseen output constraints. We introduce a new benchmark,
IFBench, to evaluate precise instruction following generalization on 58 new,
diverse, and challenging verifiable out-of-domain constraints. In addition, we
perform an extensive analysis of how and on what data models can be trained to
improve precise instruction following generalization. Specifically, we
carefully design constraint verification modules and show that reinforcement
learning with verifiable rewards (RLVR) significantly improves instruction
following. In addition to IFBench, we release 29 additional new hand-annotated
training constraints and verification functions, RLVR training prompts, and
code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Question Answering over Large Semi-structured Tables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13422v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13422v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Wang, Junhao Gan, Jianzhong Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Table Question Answering (TableQA) attracts strong interests due to the
prevalence of web information presented in the form of semi-structured tables.
Despite many efforts, TableQA over large tables remains an open challenge. This
is because large tables may overwhelm models that try to comprehend them in
full to locate question answers. Recent studies reduce input table size by
decomposing tables into smaller, question-relevant sub-tables via generating
programs to parse the tables. However, such solutions are subject to program
generation and execution errors and are difficult to ensure decomposition
quality. To address this issue, we propose TaDRe, a TableQA model that
incorporates both pre- and post-table decomposition refinements to ensure table
decomposition quality, hence achieving highly accurate TableQA results. To
evaluate TaDRe, we construct two new large-table TableQA benchmarks via
LLM-driven table expansion and QA pair generation. Extensive experiments on
both the new and public benchmarks show that TaDRe achieves state-of-the-art
performance on large-table TableQA tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Listening to the Unspoken: Exploring "365" Aspects of Multimodal
  Interview Performance Assessment <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22676v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22676v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Li, Yang Wang, Wenhao Qian, Zhenzhen Hu, Richang Hong, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interview performance assessment is essential for determining candidates'
suitability for professional positions. To ensure holistic and fair
evaluations, we propose a novel and comprehensive framework that explores
``365'' aspects of interview performance by integrating \textit{three}
modalities (video, audio, and text), \textit{six} responses per candidate, and
\textit{five} key evaluation dimensions. The framework employs
modality-specific feature extractors to encode heterogeneous data streams and
subsequently fused via a Shared Compression Multilayer Perceptron. This module
compresses multimodal embeddings into a unified latent space, facilitating
efficient feature interaction. To enhance prediction robustness, we incorporate
a two-level ensemble learning strategy: (1) independent regression heads
predict scores for each response, and (2) predictions are aggregated across
responses using a mean-pooling mechanism to produce final scores for the five
target dimensions. By listening to the unspoken, our approach captures both
explicit and implicit cues from multimodal data, enabling comprehensive and
unbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our
framework secured first place in the AVI Challenge 2025, demonstrating its
effectiveness and robustness in advancing automated and multimodal interview
performance assessment. The full implementation is available at
https://github.com/MSA-LMC/365Aspects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, ACM MM 2025.
  github:https://github.com/MSA-LMC/365Aspects</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenKnowSub: Improving Modularity and Reusability of LLMs through General
  Knowledge Subtraction <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadtaha Bagherifard, Sahar Rajabi, Ali Edalat, Yadollah Yaghoobzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models often struggle with zero-shot generalization, and
several modular approaches have been proposed to address this challenge. Yet,
we hypothesize that a key limitation remains: the entanglement of general
knowledge and task-specific adaptations. To overcome this, we propose a modular
framework that disentangles these components by constructing a library of
task-specific LoRA modules alongside a general-domain LoRA. By subtracting this
general knowledge component from each task-specific module, we obtain residual
modules that focus more exclusively on task-relevant information, a method we
call general knowledge subtraction (GenKnowSub). Leveraging the refined
task-specific modules and the Arrow routing algorithm
\citep{ostapenko2024towards}, we dynamically select and combine modules for new
inputs without additional training. Our studies on the Phi-3 model and standard
Arrow as baselines reveal that using general knowledge LoRAs derived from
diverse languages, including English, French, and German, yields consistent
performance gains in both monolingual and cross-lingual settings across a wide
set of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub
generalizes to weaker LLMs. The complete code and data are available at
https://github.com/saharsamr/Modular-LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 (main conference, short paper), 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech
  Movement Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernd J. Kröger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DYNARTmo, a dynamic articulatory model designed to visualize
speech articulation processes in a two-dimensional midsagittal plane. The model
builds upon the UK-DYNAMO framework and integrates principles of articulatory
underspecification, segmental and gestural control, and coarticulation.
DYNARTmo simulates six key articulators based on ten continuous and six
discrete control parameters, allowing for the generation of both vocalic and
consonantal articulatory configurations. The current implementation is embedded
in a web-based application (SpeechArticulationTrainer) that includes sagittal,
glottal, and palatal views, making it suitable for use in phonetics education
and speech therapy. While this paper focuses on the static modeling aspects,
future work will address dynamic movement generation and integration with
articulatory-acoustic modules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 29 references, 2 figures, supplementary material. V2:
  Discussion of the tongue-palate contact pattern for /t/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CaliDrop: KV Cache Compression with Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.19906v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.19906v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Su, Quantong Qiu, Yuechi Zhou, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) require substantial computational resources
during generation. While the Key-Value (KV) cache significantly accelerates
this process by storing attention intermediates, its memory footprint grows
linearly with sequence length, batch size, and model size, creating a
bottleneck in long-context scenarios. Various KV cache compression techniques,
including token eviction, quantization, and low-rank projection, have been
proposed to mitigate this bottleneck, often complementing each other. This
paper focuses on enhancing token eviction strategies. Token eviction leverages
the observation that the attention patterns are often sparse, allowing for the
removal of less critical KV entries to save memory. However, this reduction
usually comes at the cost of notable accuracy degradation, particularly under
high compression ratios. To address this issue, we propose \textbf{CaliDrop}, a
novel strategy that enhances token eviction through calibration. Our
preliminary experiments show that queries at nearby positions exhibit high
similarity. Building on this observation, CaliDrop performs speculative
calibration on the discarded tokens to mitigate the accuracy loss caused by
token eviction. Extensive experiments demonstrate that CaliDrop significantly
improves the accuracy of existing token eviction methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Reasoning Help Large Language Models Capture Human Annotator
  Disagreement? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Ni, Yu Fan, Vilém Zouhar, Donya Rooein, Alexander Hoyle, Mrinmaya Sachan, Markus Leippold, Dirk Hovy, Elliott Ash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variation in human annotation (i.e., disagreements) is common in NLP, often
reflecting important information like task subjectivity and sample ambiguity.
Modeling this variation is important for applications that are sensitive to
such information. Although RLVR-style reasoning (Reinforcement Learning with
Verifiable Rewards) has improved Large Language Model (LLM) performance on many
tasks, it remains unclear whether such reasoning enables LLMs to capture
informative variation in human annotation. In this work, we evaluate the
influence of different reasoning settings on LLM disagreement modeling. We
systematically evaluate each reasoning setting across model sizes, distribution
expression methods, and steering methods, resulting in 60 experimental setups
across 3 tasks. Surprisingly, our results show that RLVR-style reasoning
degrades performance in disagreement modeling, while naive Chain-of-Thought
(CoT) reasoning improves the performance of RLHF LLMs (RL from human feedback).
These findings underscore the potential risk of replacing human annotators with
reasoning LLMs, especially when disagreements are important.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minhae Oh, Jeonghye Kim, Nakyung Lee, Donggeon Seo, Taeuk Kim, Jungwoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific reasoning requires not only long-chain reasoning processes, but
also knowledge of domain-specific terminologies and adaptation to updated
findings. To deal with these challenges for scientific reasoning, we introduce
RAISE, a step-by-step retrieval-augmented framework which retrieves logically
relevant documents from in-the-wild corpus. RAISE is divided into three steps:
problem decomposition, logical query generation, and logical retrieval. We
observe that RAISE consistently outperforms other baselines on scientific
reasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves
documents that are not only similar in terms of the domain knowledge, but also
documents logically more relevant.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ParetoHqD: Fast Offline Multiobjective Alignment of Large Language
  Models using Pareto High-quality Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.16628v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.16628v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Gu, Handing Wang, Yi Mei, Mengjie Zhang, Yaochu Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models with multiple human expectations and values is
crucial for ensuring that they adequately serve a variety of user needs. To
this end, offline multiobjective alignment algorithms such as the
Rewards-in-Context algorithm have shown strong performance and efficiency.
However, inappropriate preference representations and training with imbalanced
reward scores limit the performance of such algorithms. In this work, we
introduce ParetoHqD that addresses the above issues by representing human
preferences as preference directions in the objective space and regarding data
near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD
follows a two-stage supervised fine-tuning process, where each stage uses an
individual Pareto high-quality training set that best matches its preference
direction. The experimental results have demonstrated the superiority of
ParetoHqD over five baselines on two multiobjective alignment tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and
  Usability in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Gu, Handing Wang, Yi Mei, Mengjie Zhang, Yaochu Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have been extensively used across diverse
domains, including virtual assistants, automated code generation, and
scientific research. However, they remain vulnerable to jailbreak attacks,
which manipulate the models into generating harmful responses despite safety
alignment. Recent studies have shown that current safety-aligned LLMs often
undergo the shallow safety alignment, where the first few tokens largely
determine whether the response will be harmful. Through comprehensive
observations, we find that safety-aligned LLMs and various defense strategies
generate highly similar initial tokens in their refusal responses, which we
define as safety trigger tokens. Building on this insight, we propose
\texttt{D-STT}, a simple yet effective defense algorithm that identifies and
explicitly decodes safety trigger tokens of the given safety-aligned LLM to
trigger the model's learned safety patterns. In this process, the safety
trigger is constrained to a single token, which effectively preserves model
usability by introducing minimum intervention in the decoding process.
Extensive experiments across diverse jailbreak attacks and benign prompts
demonstrate that \ours significantly reduces output harmfulness while
preserving model usability and incurring negligible response time overhead,
outperforming ten baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PRISON: Unmasking the Criminal Potential of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Wu, Geng Hong, Pei Chen, Yueyue Chen, Xudong Pan, Min Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) advance, concerns about their misconduct in
complex social contexts intensify. Existing research overlooked the systematic
understanding and assessment of their criminal capability in realistic
interactions. We propose a unified framework PRISON, to quantify LLMs' criminal
potential across five traits: False Statements, Frame-Up, Psychological
Manipulation, Emotional Disguise, and Moral Disengagement. Using structured
crime scenarios adapted from classic films grounded in reality, we evaluate
both criminal potential and anti-crime ability of LLMs. Results show that
state-of-the-art LLMs frequently exhibit emergent criminal tendencies, such as
proposing misleading statements or evasion tactics, even without explicit
instructions. Moreover, when placed in a detective role, models recognize
deceptive behavior with only 44% accuracy on average, revealing a striking
mismatch between conducting and detecting criminal behavior. These findings
underscore the urgent need for adversarial robustness, behavioral alignment,
and safety mechanisms before broader LLM deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14037v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14037v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Franceschelli, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their growing capabilities, language models still frequently
reproduce content from their training data, generate repetitive text, and favor
common grammatical patterns and vocabulary. A possible cause is the decoding
strategy: the most common strategies either consider only the most probable
tokens, which reduces output diversity, or increase the likelihood of unlikely
tokens, compromising output accuracy and correctness. In this paper, we propose
DiffSampling, a new decoding method that leverages a mathematical analysis of
the token probability distribution to ensure the generation of contextually
appropriate text. In particular, the difference between consecutive, sorted
probabilities can be used to truncate incorrect tokens. In addition, we also
propose two variations of the proposed method that aim to correct the subtle
inconsistencies of common sampling strategies. Experiments involving four
different text-generation tasks demonstrate that our approach consistently
performs at least on par with the existing methods it builds upon in terms of
quality, while potentially improving output diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation
  Extraction with Comprehensive Reasoning Abilities <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00571v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00571v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengcheng Mai, Yuxiang Wang, Ziyu Gong, Hanxiang Wang, Yihua Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document-level relation extraction (Doc-RE) aims to extract relations between
entities across multiple sentences. Therefore, Doc-RE requires more
comprehensive reasoning abilities like humans, involving complex cross-sentence
interactions between entities, contexts, and external general knowledge,
compared to the sentence-level RE. However, most existing Doc-RE methods focus
on optimizing single reasoning ability, but lack the ability to utilize
external knowledge for comprehensive reasoning on long documents. To solve
these problems, a knowledge retrieval augmented method, named KnowRA, was
proposed with comprehensive reasoning to autonomously determine whether to
accept external knowledge to assist DocRE. Firstly, we constructed a document
graph for semantic encoding and integrated the co-reference resolution model to
augment the co-reference reasoning ability. Then, we expanded the document
graph into a document knowledge graph by retrieving the external knowledge base
for common-sense reasoning and a novel knowledge filtration method was
presented to filter out irrelevant knowledge. Finally, we proposed the axis
attention mechanism to build direct and indirect associations with intermediary
entities for achieving cross-sentence logical reasoning. Extensive experiments
conducted on two datasets verified the effectiveness of our method compared to
the state-of-the-art baselines. Our code is available at
https://anonymous.4open.science/r/KnowRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by IJCAI 2025 (CCF A)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language-based Audio Moment Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15672v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15672v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hokuto Munakata, Taichi Nishimura, Shota Nakada, Tatsuya Komatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose and design a new task called audio moment retrieval
(AMR). Unlike conventional language-based audio retrieval tasks that search for
short audio clips from an audio database, AMR aims to predict relevant moments
in untrimmed long audio based on a text query. Given the lack of prior work in
AMR, we first build a dedicated dataset, Clotho-Moment, consisting of
large-scale simulated audio recordings with moment annotations. We then propose
a DETR-based model, named Audio Moment DETR (AM-DETR), as a fundamental
framework for AMR tasks. This model captures temporal dependencies within audio
features, inspired by similar video moment retrieval tasks, thus surpassing
conventional clip-level audio retrieval methods. Additionally, we provide
manually annotated datasets to properly measure the effectiveness and
robustness of our methods on real data. Experimental results show that AM-DETR,
trained with Clotho-Moment, outperforms a baseline model that applies a
clip-level audio retrieval method with a sliding window on all metrics,
particularly improving Recall1@0.7 by 9.00 points. Our datasets and code are
publicly available in
https://h-munakata.github.io/Language-based-Audio-Moment-Retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thinking Outside the (Gray) Box: A Context-Based Score for Assessing
  Value and Originality in Neural Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13207v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13207v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Franceschelli, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the increasing use of large language models for creative tasks, their
outputs often lack diversity. Common solutions, such as sampling at higher
temperatures, can compromise the quality of the results. Dealing with this
trade-off is still an open challenge in designing AI systems for creativity.
Drawing on information theory, we propose a context-based score to
quantitatively evaluate value and originality. This score incentivizes accuracy
and adherence to the request while fostering divergence from the learned
distribution. We show that our score can be used as a reward in a reinforcement
learning framework to fine-tune large language models for maximum performance.
We validate our strategy through experiments considering a variety of creative
tasks, such as poetry generation and math problem solving, demonstrating that
it enhances the value and originality of the generated solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models in Argument Mining: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16383v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16383v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Li, Viktor Schlegel, Yizheng Sun, Riza Batista-Navarro, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Argument Mining (AM), a critical subfield of Natural Language Processing
(NLP), focuses on extracting argumentative structures from text. The advent of
Large Language Models (LLMs) has profoundly transformed AM, enabling advanced
in-context learning, prompt-based generation, and robust cross-domain
adaptability. This survey systematically synthesizes recent advancements in
LLM-driven AM. We provide a concise review of foundational theories and
annotation frameworks, alongside a meticulously curated catalog of datasets. A
key contribution is our comprehensive taxonomy of AM subtasks, elucidating how
contemporary LLM techniques -- such as prompting, chain-of-thought reasoning,
and retrieval augmentation -- have reconfigured their execution. We further
detail current LLM architectures and methodologies, critically assess
evaluation practices, and delineate pivotal challenges including long-context
reasoning, interpretability, and annotation bottlenecks. Conclusively, we
highlight emerging trends and propose a forward-looking research agenda for
LLM-based computational argumentation, aiming to strategically guide
researchers in this rapidly evolving domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work draft</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R1-VL: Learning to Reason with Multimodal Large Language Models via
  Step-wise Group Relative Policy Optimization <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies generally enhance MLLMs' reasoning capabilities via supervised
fine-tuning on high-quality chain-of-thought reasoning data, which often leads
models to merely imitate successful reasoning paths without understanding what
the wrong reasoning paths are. In this work, we aim to enhance the MLLMs'
reasoning ability beyond passively imitating positive reasoning paths. To this
end, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new
online reinforcement learning framework that enables MLLMs to self-improve
reasoning ability via simple, effective and dense step-wise rewarding.
Specifically, StepGRPO introduces two novel rule-based reasoning rewards:
Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity
Reward (StepRVR). StepRAR rewards the reasoning paths that contain necessary
intermediate reasoning steps via a soft key-step matching technique, while
StepRAR rewards reasoning paths that follow a well-structured and logically
consistent reasoning process through a reasoning completeness and logic
evaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series
of MLLMs with outstanding capabilities in step-by-step reasoning. Extensive
experiments over 8 benchmarks demonstrate the superiority of our methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from Negative Samples in Biomedical Generative Entity Linking <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16493v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16493v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanhwi Kim, Hyunjae Kim, Sihyeon Park, Jiwoo Lee, Mujeen Sung, Jaewoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have become widely used in biomedical entity linking
(BioEL) due to their excellent performance and efficient memory usage. However,
these models are usually trained only with positive samples, i.e., entities
that match the input mention's identifier, and do not explicitly learn from
hard negative samples, which are entities that look similar but have different
meanings. To address this limitation, we introduce ANGEL (Learning from
Negative Samples in Biomedical Generative Entity Linking), the first framework
that trains generative BioEL models using negative samples. Specifically, a
generative model is initially trained to generate positive entity names from
the knowledge base for given input entities. Subsequently, both correct and
incorrect outputs are gathered from the model's top-k predictions. Finally, the
model is updated to prioritize the correct predictions through preference
optimization. Our models outperform the previous best baseline models by up to
an average top-1 accuracy of 1.4% on five benchmarks. When incorporating our
framework into pre-training, the performance improvement increases further to
1.7%, demonstrating its effectiveness in both the pre-training and fine-tuning
stages. The code and model weights are available at
https://github.com/dmis-lab/ANGEL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Core Context Aware <span class="highlight-title">Transformer</span>s for Long Context Language Modeling <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12465v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12465v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaofo Chen, Zeng You, Shuhai Zhang, Haokun Li, Yirui Li, Yaowei Wang, Mingkui Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based Large Language Models (LLMs) have exhibited remarkable
success in extensive tasks primarily attributed to self-attention mechanism,
which requires a token to consider all preceding tokens as its context to
compute attention. However, when the context length L becomes very large (e.g.,
128K), the amount of potentially redundant information in the context tends to
increase. The redundant context not only hampers the modeling representation
performance but also incurs unnecessary computational and storage overhead. In
this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for
efficient long-context modeling, comprising two complementary modules: 1)
Globality-aware pooling module groups input tokens and dynamically compresses
each group into one core token based on their significance. In this way, our
method automatically focuses and strengthens core context while diminishing
redundancy during the learning process, leading to effective long-term
dependency modeling. 2) Locality-preserving module incorporates neighboring
tokens to preserve local context for detailed representation. Notably, our
CCA-Attention is able to replace the self-attention module in existing LLMs
with minimal fine-tuning cost. Extensive experimental results show the
superiority of our method in both long-context modeling and computational
efficiency over state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in
  GUI Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12842v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12842v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Wu, Pengzhou Cheng, Zongru Wu, Lingzhong Dong, Zhuosheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphical user interface (GUI) agents have recently emerged as an intriguing
paradigm for human-computer interaction, capable of automatically executing
user instructions to operate intelligent terminal devices. However, when
encountering out-of-distribution (OOD) instructions that violate environmental
constraints or exceed the current capabilities of agents, GUI agents may suffer
task breakdowns or even pose security threats. Therefore, effective OOD
detection for GUI agents is essential. Traditional OOD detection methods
perform suboptimally in this domain due to the complex embedding space and
evolving GUI environments. In this work, we observe that the in-distribution
input semantic space of GUI agents exhibits a clustering pattern with respect
to the distance from the centroid. Based on the finding, we propose GEM, a
novel method based on fitting a Gaussian mixture model over input embedding
distances extracted from the GUI agent that reflect its capability boundary.
Evaluated on eight datasets spanning smartphones, computers, and web browsers,
our method achieves an average accuracy improvement of 23.70\% over the
best-performing baseline while only increasing training time by 4.9\% and
testing time by 6.5\%. We also experimentally demonstrate that GEM can improve
the step-wise success rate by 9.40\% by requesting assistance from the cloud
model when encountering OOD samples. Analysis verifies the generalization
ability of our method through experiments on nine different backbones. The
codes are available at https://github.com/Wuzheng02/GEM-OODforGUIagents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emergent Response Planning in LLMs <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06258v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06258v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Dong, Zhanhui Zhou, Zhixuan Liu, Chao Yang, Chaochao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we argue that large language models (LLMs), though trained to
predict only the next token, exhibit emergent planning behaviors:
$\textbf{their hidden representations encode future outputs beyond the next
token}$. Through simple probing, we demonstrate that LLM prompt representations
encode global attributes of their entire responses, including
$\textit{structure attributes}$ (e.g., response length, reasoning steps),
$\textit{content attributes}$ (e.g., character choices in storywriting,
multiple-choice answers at the end of response), and $\textit{behavior
attributes}$ (e.g., answer confidence, factual consistency). In addition to
identifying response planning, we explore how it scales with model size across
tasks and how it evolves during generation. The findings that LLMs plan ahead
for the future in their hidden representations suggest potential applications
for improving transparency and generation control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICML 2025. Code available at:
  https://github.com/niconi19/Emergent-Response-Planning-in-LLMs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Scaling: Measuring and Predicting the Upper Bound of Knowledge
  Retention in Language Model <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04066v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04066v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changhao Jiang, Ming Zhang, Junjie Ye, Xiaoran Fan, Yifei Cao, Jiajun Sun, Zhiheng Xi, Shihan Dou, Yi Dong, Yujiong Shen, Jingqi Tong, Baoyu Fan, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Qi Zhang, Tao Gui, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The GPT-4 technical report highlights the possibility of predicting model
performance on downstream tasks using only pre-training signals, though
detailed methodologies are absent. Such predictive capabilities are essential
for resource-efficient pre-training and the construction of task-aligned
datasets. In this paper, we aim to predict performance in closed-book question
answering (QA), a vital downstream task that directly reflects a model's
internalized knowledge without the help of external tools. We address three
primary challenges: (1) limited access to and understanding of pre-training
corpora, (2) limitations of current evaluation methods for pre-trained models,
and (3) limitations of frequency-based metrics in predicting model performance.
In response, we conduct large-scale retrieval and semantic analysis across the
pre-training corpora of 21 publicly available and 3 custom-trained large
language models. We then develop a multi-template QA evaluation framework
incorporating paraphrased question variants. Building on these foundations, we
propose Size-dependent Mutual Information (SMI), an information-theoretic
metric that linearly correlates pre-training data characteristics, model size,
and QA accuracy, without requiring additional training. Experimental results
show that SMI outperforms co-occurrence-based baselines, achieving $R^2 > 0.75$
on models with over one billion parameters. Theoretical analysis further
suggests an upper bound of around 80% QA accuracy under optimal pre-training,
reflecting intrinsic memory limitations and motivating the use of retrieval or
few-shot methods in later stages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StableGS: A Floater-Free Framework for 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18458v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18458v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luchao Wang, Qian Ren, Kaimin Liao, Hua Wang, Zhi Chen, Yaohua Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) reconstructions are plagued by stubborn
``floater" artifacts that degrade their geometric and visual fidelity. We are
the first to reveal the root cause: a fundamental conflict in the 3DGS
optimization process where the opacity gradients of floaters vanish when their
blended color reaches a pseudo-equilibrium of canceling errors against the
background, trapping them in a spurious local minimum. To resolve this, we
propose StableGS, a novel framework that decouples geometric regularization
from final appearance rendering. Its core is a Dual Opacity architecture that
creates two separate rendering paths: a ``Geometric Regularization Path" to
bear strong depth-based constraints for structural correctness, and an
``Appearance Refinement Path" to generate high-fidelity details upon this
stable foundation. We complement this with a synergistic set of geometric
constraints: a self-supervised depth consistency loss and an external geometric
prior enabled by our efficient global scale optimization algorithm. Experiments
on multiple benchmarks show StableGS not only eliminates floaters but also
resolves the common blur-artifact trade-off, achieving state-of-the-art
geometric accuracy and visual quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compressing KV Cache for Long-Context LLM Inference with Inter-Layer
  Attention Similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Da Ma, Lu Chen, Situo Zhang, Yuxun Miao, Su Zhu, Zhi Chen, Hongshen Xu, Hanqi Li, Shuai Fan, Lei Pan, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid expansion of context window sizes in Large Language Models~(LLMs)
has enabled them to tackle increasingly complex tasks involving lengthy
documents. However, this progress comes at the cost of a substantial increase
in memory usage during inference, primarily due to the linear growth of the
key-value~(KV) cache. Existing KV cache compression methods often discard less
relevant tokens, which can lead to significant performance degradation when
critical information is lost. In this paper, we propose \textsc{PoD}~(Proximal
tokens over Distant tokens), a novel KV cache compression framework that
allocates memory according to token importance, retaining less important tokens
in a more compact, shared form rather than discarding them entirely. Our
approach is motivated by two key observations: (1) proximal tokens -- those at
the beginning and end of the context -- are significantly more important for
next-token prediction, and (2) attention scores for distant tokens are highly
redundant across consecutive layers. Leveraging these insights, \textsc{PoD}
preserves the full KV cache for proximal tokens, while for distant tokens, it
shares key states across layers. Since attention scores are determined by both
queries and keys, sharing key states enables multiple layers to reuse a single
set of keys for distant tokens, substantially reducing KV cache memory without
discarding essential context. We further introduce a lightweight post-training
adaptation to enable the model to adjust to this new attention-sharing
structure. Extensive experiments on both synthetic~(Needle in a Haystack) and
real-world long-context benchmarks demonstrate that \textsc{PoD} reduces KV
cache memory usage by up to 35\% without compromising performance. Our method
is orthogonal to existing token-selection-based techniques and can be combined
with them for further KV cache compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Evolving Critique Abilities in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, Junyang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their remarkable performance, Large Language Models (LLMs) face a
critical challenge: providing feedback for tasks where human evaluation is
difficult or where LLMs potentially outperform humans. In such scenarios,
leveraging the critique ability of LLMs themselves - identifying and correcting
flaws - shows considerable promise. This paper explores enhancing critique
abilities of LLMs, noting that current approaches rely on human annotations or
more powerful models, leaving the challenge of improving critique abilities
without external supervision unresolved. We introduce SCRIT (Self-evolving
CRITic), a framework that trains LLMs with self-generated data to evolve their
critique abilities. To address the low quality of naively generated data, we
propose a contrastive-critic approach that uses reference solutions during data
synthesis to enhance the model's understanding of key concepts, and
incorporates a self-validation scheme to ensure data quality. The final trained
model operates without any reference solutions at inference time. Implemented
with Qwen2.5-72B-Instruct, a leading LLM, SCRIT demonstrates consistent
improvements across a wide range of benchmarks spanning both mathematical and
scientific reasoning: achieving a 10.0\% relative gain in critique-correction
accuracy and a 19.0\% relative improvement in error identification F1-score.
Our analysis reveals that SCRIT's performance scales positively with data and
model size and enables continuous improvement through multi-round iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TIB-STC: A Large-Scale Structured Tibetan Benchmark for Low-Resource
  Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18288v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18288v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Huang, Fan Gao, Yutong Liu, Nyima Tashi, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Xiao Feng, Hao Wang, Yongbin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancement of large language models (LLMs) has brought transformative
capabilities to NLP, but such progress remains unevenly distributed, especially
for low-resource and culturally rich languages like Tibetan. In this paper, we
present TIB-STC, the first large-scale, expert-curated, and multi-domain
dataset specifically designed to support the development and evaluation of LLMs
for the Tibetan language. Spanning over 11 billion tokens across literature,
religion, medicine, law, and daily communication, TIB-STC preserves traditional
grammar and stylistic richness. To validate its utility, we train a reference
model, Sun-Shine, on TIB-STC through a three-stage pipeline involving
pretraining, supervised fine-tuning, and preference optimization. Evaluation on
TLUE Benchmark for Tibetan-specific tasks, including Ti-MMLU and
Ti-SafetyBench, demonstrates the TIB-STC's effectiveness in enabling robust
instruction-following and culturally aligned generation. We release TIB-STC to
advance research in low-resource language modeling and promote inclusivity in
multilingual NLP. All data are available:
https://github.com/Vicentvankor/sun-shine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ref-Long: Benchmarking the Long-context Referencing Capability of
  Long-context Language Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.09506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.09506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Wu, Gefei Gu, Yanan Zheng, Dit-Yan Yeung, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context language models (LCLMs) have exhibited impressive capabilities
in long-context understanding tasks. Among these, long-context referencing -- a
crucial task that requires LCLMs to attribute items of interest to specific
parts of long-context data -- remains underexplored. To bridge this gap, this
paper proposes Referencing Evaluation for Long-context Language Models
(Ref-Long), a novel benchmark designed to assess the long-context referencing
capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the
indexes of documents that reference a specific key, emphasizing contextual
relationships between the key and the documents over simple retrieval. Based on
the task design, we construct three subsets ranging from synthetic to realistic
scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs
reveal significant shortcomings in long-context referencing, even among
advanced models like GPT-4o. To further investigate these challenges, we
conduct comprehensive analyses, including human evaluations, task format
adjustments, fine-tuning experiments, and error analyses, leading to several
key insights. Our data and code can be found in https://github.
com/wujunjie1998/Ref-Long.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Main Conference. First 2 authors contributed equally.
  Project webpage: https://wujunjie1998.github.io/Ref-Long-website/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What to Keep and What to Drop: Adaptive Table Filtering Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.23463v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.23463v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        WonJune Jang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) for table-based reasoning often struggle with
large tables due to input length limits. We propose ATF (Adaptive Table
Filtering Framework), a modular and question-aware filtering pipeline that
prunes uninformative columns and rows using LLM-generated column descriptions,
clustering, and sparse-dense alignment scores. ATF integrates seamlessly with
existing models (e.g., TAPAS, TAPEX) without retraining. Experiments show that
ATF reduces table cells by 70%, boosting performance on out-of-domain TableQA
tasks while causing slight performance drops on Table Fact Verification, where
full-table context is more critical. These results highlight ATF's ability to
adaptively balance informativeness and minimalism across tasks. Our code
available at:
https://github.com/torijune/ATF-Adaptive-Table-Filtering-Framework
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Raw Data Matters: Enhancing <span class="highlight-title">Prompt</span> Tuning by Internal Augmentation on
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyang Li, Liang Wang, Chao Wang, Siyu Zhou, Jing Jiang, Yan Peng, Guodong Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For CLIP-based prompt tuning, introducing more data as additional knowledge
for enhancing fine-tuning process is proved to be an effective approach.
Existing data amplification strategies for prompt tuning typically rely on
external knowledge (e.g., large language models or pre-structured knowledge
bases), resulting in higher costs for data collection and processing, while
generally ignoring further utilization of features in image modality. To
address this, we propose Augmentation-driven Prompt Tuning (AugPT), a
self-contained distillation-based prompt tuning approach using only internal
augmentation on raw dataset to better exploit known features. Specifically,
AugPT employs self-supervised augmentation on unlabeled images in the training
set, and introduces a novel gating mechanism based on consensus test, reusing
the pre-trained prompt tuning backbone model to spontaneously filter noisy
samples, further enhancing the quality of augmented views. Extensive
experiments validate that AugPT simultaneously enhances model performance and
generalization capability without using appended external knowledge. The code
of AugPT is available at: https://github.com/JREion/AugPT .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures, 15 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedVLThinker: Simple Baselines for Multimodal Medical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoke Huang, Juncheng Wu, Hui Liu, Xianfeng Tang, Yuyin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Reasoning Models (LRMs) have introduced a new paradigm in AI by
enabling models to ``think before responding" via chain-of-thought reasoning.
However, the absence of open and reproducible recipes for building
reasoning-centric medical LMMs hinders community-wide research, analysis, and
comparison. In this paper, we present MedVLThinker, a suite of simple yet
strong baselines. Our fully open recipe consists of: (1) systematic data
curation for both text-only and image-text medical data, filtered according to
varying levels of reasoning difficulty, and (2) two training paradigms:
Supervised Fine-Tuning (SFT) on distilled reasoning traces and Reinforcement
Learning with Verifiable Rewards (RLVR) based on final answer correctness.
Across extensive experiments on the Qwen2.5-VL model family (3B, 7B) and six
medical QA benchmarks, we find that RLVR consistently and significantly
outperforms SFT. Additionally, under the RLVR framework, a key,
counter-intuitive finding is that training on our curated text-only reasoning
data provides a more substantial performance boost than training on multimodal
image-text data. Our best open 7B model, trained using the RLVR recipe on
text-only data, establishes a new state-of-the-art on existing public VQA
benchmarks, surpassing all previous open-source medical LMMs. Furthermore,
scaling our model to 32B achieves performance on par with the proprietary
GPT-4o. We release all curated data, models, and code to provide the community
with a strong, open foundation for future research in multimodal medical
reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page and code: https://ucsc-vlaa.github.io/MedVLThinker/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PMGS: Reconstruction of Projectile Motion across Large Spatiotemporal
  Spans via 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijun Xu, Jingrui Zhang, Yuhan Chen, Dingwen Wang, Lei Yu, Chu He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling complex rigid motion across large spatiotemporal spans remains an
unresolved challenge in dynamic reconstruction. Existing paradigms are mainly
confined to short-term, small-scale deformation and offer limited consideration
for physical consistency. This study proposes PMGS, focusing on reconstructing
Projectile Motion via 3D Gaussian Splatting. The workflow comprises two stages:
1) Target Modeling: achieving object-centralized reconstruction through dynamic
scene decomposition and an improved point density control; 2) Motion Recovery:
restoring full motion sequences by learning per-frame SE(3) poses. We introduce
an acceleration consistency constraint to bridge Newtonian mechanics and pose
estimation, and design a dynamic simulated annealing strategy that adaptively
schedules learning rates based on motion states. Futhermore, we devise a Kalman
fusion scheme to optimize error accumulation from multi-source observations to
mitigate disturbances. Experiments show PMGS's superior performance in
reconstructing high-speed nonlinear rigid motion compared to mainstream dynamic
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Variance in Visual Question Answering Benchmarks <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikitha SR
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have emerged as powerful tools for
visual question answering (VQA), enabling reasoning and contextual
understanding across visual and textual modalities. Despite their advancements,
the evaluation of MLLMs on VQA benchmarks often relies on point estimates,
overlooking the significant variance in performance caused by factors such as
stochastic model outputs, training seed sensitivity, and hyperparameter
configurations. This paper critically examines these issues by analyzing
variance across 14 widely used VQA benchmarks, covering diverse tasks such as
visual reasoning, text understanding, and commonsense reasoning. We
systematically study the impact of training seed, framework non-determinism,
model scale, and extended instruction finetuning on performance variability.
Additionally, we explore Cloze-style evaluation as an alternate assessment
strategy, studying its effectiveness in reducing stochasticity and improving
reliability across benchmarks. Our findings highlight the limitations of
current evaluation practices and advocate for variance-aware methodologies to
foster more robust and reliable development of MLLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICCV 2025 Workshop on What's Next in Multimodal
  Foundational Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReMoMask: Retrieval-Augmented Masked Motion Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengdao Li, Siheng Wang, Zeyu Zhang, Hao Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Motion (T2M) generation aims to synthesize realistic and semantically
aligned human motion sequences from natural language descriptions. However,
current approaches face dual challenges: Generative models (e.g., diffusion
models) suffer from limited diversity, error accumulation, and physical
implausibility, while Retrieval-Augmented Generation (RAG) methods exhibit
diffusion inertia, partial-mode collapse, and asynchronous artifacts. To
address these limitations, we propose ReMoMask, a unified framework integrating
three key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples
negative sample scale from batch size via momentum queues, substantially
improving cross-modal retrieval precision; 2) A Semantic Spatio-temporal
Attention mechanism enforces biomechanical constraints during part-level fusion
to eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates
minor unconditional generation to enhance generalization. Built upon MoMask's
RVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal
steps. Extensive experiments on standard benchmarks demonstrate the
state-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97%
improvement in FID scores on HumanML3D and KIT-ML, respectively, compared to
the previous SOTA method RAG-T2M. Code:
https://github.com/AIGeeksGroup/ReMoMask. Website:
https://aigeeksgroup.github.io/ReMoMask.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable AI Methods for Neuroimaging: Systematic Failures of Common
  Tools, the Need for Domain-Specific Validation, and a Proposal for Safe
  Application 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nys Tjade Siegel, James H. Cole, Mohamad Habes, Stefan Haufe, Kerstin Ritter, Marc-André Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trustworthy interpretation of deep learning models is critical for
neuroimaging applications, yet commonly used Explainable AI (XAI) methods lack
rigorous validation, risking misinterpretation. We performed the first
large-scale, systematic comparison of XAI methods on ~45,000 structural brain
MRIs using a novel XAI validation framework. This framework establishes
verifiable ground truth by constructing prediction tasks with known signal
sources - from localized anatomical features to subject-specific clinical
lesions - without artificially altering input images. Our analysis reveals
systematic failures in two of the most widely used methods: GradCAM
consistently failed to localize predictive features, while Layer-wise Relevance
Propagation generated extensive, artifactual explanations that suggest
incompatibility with neuroimaging data characteristics. Our results indicate
that these failures stem from a domain mismatch, where methods with design
principles tailored to natural images require substantial adaptation for
neuroimaging data. In contrast, the simpler, gradient-based method SmoothGrad,
which makes fewer assumptions about data structure, proved consistently
accurate, suggesting its conceptual simplicity makes it more robust to this
domain shift. These findings highlight the need for domain-specific adaptation
and validation of XAI methods, suggest that interpretations from prior
neuroimaging studies using standard XAI methodology warrant re-evaluation, and
provide urgent guidance for practical application of XAI in neuroimaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RL-U$^2$Net: A Dual-Branch UNet with Reinforcement Learning-Assisted
  Multimodal Feature Fusion for Accurate 3D Whole-Heart Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jierui Qu, Jianchun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate whole-heart segmentation is a critical component in the precise
diagnosis and interventional planning of cardiovascular diseases. Integrating
complementary information from modalities such as computed tomography (CT) and
magnetic resonance imaging (MRI) can significantly enhance segmentation
accuracy and robustness. However, existing multi-modal segmentation methods
face several limitations: severe spatial inconsistency between modalities
hinders effective feature fusion; fusion strategies are often static and lack
adaptability; and the processes of feature alignment and segmentation are
decoupled and inefficient. To address these challenges, we propose a
dual-branch U-Net architecture enhanced by reinforcement learning for feature
alignment, termed RL-U$^2$Net, designed for precise and efficient multi-modal
3D whole-heart segmentation. The model employs a dual-branch U-shaped network
to process CT and MRI patches in parallel, and introduces a novel RL-XAlign
module between the encoders. The module employs a cross-modal attention
mechanism to capture semantic correspondences between modalities and a
reinforcement-learning agent learns an optimal rotation strategy that
consistently aligns anatomical pose and texture features. The aligned features
are then reconstructed through their respective decoders. Finally, an
ensemble-learning-based decision module integrates the predictions from
individual patches to produce the final segmentation result. Experimental
results on the publicly available MM-WHS 2017 dataset demonstrate that the
proposed RL-U$^2$Net outperforms existing state-of-the-art methods, achieving
Dice coefficients of 93.1% on CT and 87.0% on MRI, thereby validating the
effectiveness and superiority of the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Wang, Yongcai Wang, Wanting Li, Yucheng Wang, Maiyue Chen, Kaihui Wang, Zhizhong Su, Xudong Cai, Yeying Jin, Deying Li, Zhaoxin Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth
inputs to provide rich spatial cues for action planning, but these sensors can
be costly or less accessible in real-world deployments. Recent approaches based
on Vision-Language Action (VLA) models achieve strong results with monocular
input, yet they still lag behind methods using panoramic RGB-D information. We
present MonoDream, a lightweight VLA framework that enables monocular agents to
learn a Unified Navigation Representation (UNR). This shared feature
representation jointly aligns navigation-relevant visual semantics (e.g.,
global layout, depth, and future cues) and language-grounded action intent,
enabling more reliable action prediction. MonoDream further introduces Latent
Panoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to
predict latent features of panoramic RGB and depth observations at both current
and future steps based on only monocular input. Experiments on multiple VLN
benchmarks show that MonoDream consistently improves monocular navigation
performance and significantly narrows the gap with panoramic-based agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precision-Aware Video Compression for Reducing Bandwidth Requirements in
  Video Communication for Vehicle Detection-Based Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abyad Enan, Jon C Calhoun, Mashrur Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer vision has become a popular tool in intelligent transportation
systems (ITS), enabling various applications through roadside traffic cameras
that capture video and transmit it in real time to computing devices within the
same network. The efficiency of this video transmission largely depends on the
available bandwidth of the communication system. However, limited bandwidth can
lead to communication bottlenecks, hindering the real-time performance of ITS
applications. To mitigate this issue, lossy video compression techniques can be
used to reduce bandwidth requirements, at the cost of degrading video quality.
This degradation can negatively impact the accuracy of applications that rely
on real-time vehicle detection. Additionally, vehicle detection accuracy is
influenced by environmental factors such as weather and lighting conditions,
suggesting that compression levels should be dynamically adjusted in response
to these variations. In this work, we utilize a framework called
Precision-Aware Video Compression (PAVC), where a roadside video camera
captures footage of vehicles on roadways, compresses videos, and then transmits
them to a processing unit, running a vehicle detection algorithm for
safety-critical applications, such as real-time collision risk assessment. The
system dynamically adjusts the video compression level based on current weather
and lighting conditions to maintain vehicle detection accuracy while minimizing
bandwidth usage. Our results demonstrate that PAVC improves vehicle detection
accuracy by up to 13% and reduces communication bandwidth requirements by up to
8.23x in areas with moderate bandwidth availability. Moreover, in locations
with severely limited bandwidth, PAVC reduces bandwidth requirements by up to
72x while preserving vehicle detection performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the Transportation Research Record:
  Journal of the Transportation Research Board for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Risks of Asphalt Art on the Reliability of
  Surveillance Perception Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Ma, Abyad Enan, Long Cheng, Mashrur Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artistic crosswalks featuring asphalt art, introduced by different
organizations in recent years, aim to enhance the visibility and safety of
pedestrians. However, their visual complexity may interfere with surveillance
systems that rely on vision-based object detection models. In this study, we
investigate the impact of asphalt art on pedestrian detection performance of a
pretrained vision-based object detection model. We construct realistic
crosswalk scenarios by compositing various street art patterns into a fixed
surveillance scene and evaluate the model's performance in detecting
pedestrians on asphalt-arted crosswalks under both benign and adversarial
conditions. A benign case refers to pedestrian crosswalks painted with existing
normal asphalt art, whereas an adversarial case involves digitally crafted or
altered asphalt art perpetrated by an attacker. Our results show that while
simple, color-based designs have minimal effect, complex artistic patterns,
particularly those with high visual salience, can significantly degrade
pedestrian detection performance. Furthermore, we demonstrate that
adversarially crafted asphalt art can be exploited to deliberately obscure real
pedestrians or generate non-existent pedestrian detections. These findings
highlight a potential vulnerability in urban vision-based pedestrian
surveillance systems and underscore the importance of accounting for
environmental visual variations when designing robust pedestrian perception
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>J. Ma and A. Enan are co-first authors; they have contributed
  equally. This work has been submitted to the Transportation Research Record:
  Journal of the Transportation Research Board for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Pixels to Pathology: Restoration Diffusion for
  Diagnostic-Consistent Virtual IHC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingsong Liu, Xiaofeng Deng, Han Li, Azar Kazemi, Christian Grashei, Gesa Wilkens, Xin You, Tanja Groll, Nassir Navab, Carolin Mogler, Peter J. Schüffler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hematoxylin and eosin (H&E) staining is the clinical standard for assessing
tissue morphology, but it lacks molecular-level diagnostic information. In
contrast, immunohistochemistry (IHC) provides crucial insights into biomarker
expression, such as HER2 status for breast cancer grading, but remains costly
and time-consuming, limiting its use in time-sensitive clinical workflows. To
address this gap, virtual staining from H&E to IHC has emerged as a promising
alternative, yet faces two core challenges: (1) Lack of fair evaluation of
synthetic images against misaligned IHC ground truths, and (2) preserving
structural integrity and biological variability during translation. To this
end, we present an end-to-end framework encompassing both generation and
evaluation in this work. We introduce Star-Diff, a structure-aware staining
restoration diffusion model that reformulates virtual staining as an image
restoration task. By combining residual and noise-based generation pathways,
Star-Diff maintains tissue structure while modeling realistic biomarker
variability. To evaluate the diagnostic consistency of the generated IHC
patches, we propose the Semantic Fidelity Score (SFS), a
clinical-grading-task-driven metric that quantifies class-wise semantic
degradation based on biomarker classification accuracy. Unlike pixel-level
metrics such as SSIM and PSNR, SFS remains robust under spatial misalignment
and classifier uncertainty. Experiments on the BCI dataset demonstrate that
Star-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity
and diagnostic relevance. With rapid inference and strong clinical alignment,it
presents a practical solution for applications such as intraoperative virtual
IHC synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Reliable Audio Deepfake Attribution and Model Recognition: A
  Multi-Level Autoencoder-Based Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Di Pierno, Luca Guarnera, Dario Allegra, Sebastiano Battiato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of audio deepfakes poses a growing threat to trust in
digital communications. While detection methods have advanced, attributing
audio deepfakes to their source models remains an underexplored yet crucial
challenge. In this paper we introduce LAVA (Layered Architecture for Voice
Attribution), a hierarchical framework for audio deepfake detection and model
recognition that leverages attention-enhanced latent representations extracted
by a convolutional autoencoder trained solely on fake audio. Two specialized
classifiers operate on these features: Audio Deepfake Attribution (ADA), which
identifies the generation technology, and Audio Deepfake Model Recognition
(ADMR), which recognize the specific generative model instance. To improve
robustness under open-set conditions, we incorporate confidence-based rejection
thresholds. Experiments on ASVspoof2021, FakeOrReal, and CodecFake show strong
performance: the ADA classifier achieves F1-scores over 95% across all
datasets, and the ADMR module reaches 96.31% macro F1 across six classes.
Additional tests on unseen attacks from ASVpoof2019 LA and error propagation
analysis confirm LAVA's robustness and reliability. The framework advances the
field by introducing a supervised approach to deepfake attribution and model
recognition under open-set conditions, validated on public benchmarks and
accompanied by publicly released models and code. Models and code are available
at https://www.github.com/adipiz99/lava-framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Engagement Prediction of Short Videos with Large Multimodal Models <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Sun, Linhan Cao, Yuqin Cao, Weixia Zhang, Wen Wen, Kaiwei Zhang, Zijian Chen, Fangfang Lu, Xiongkuo Min, Guangtao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid proliferation of user-generated content (UGC) on short-form video
platforms has made video engagement prediction increasingly important for
optimizing recommendation systems and guiding content creation. However, this
task remains challenging due to the complex interplay of factors such as
semantic content, visual quality, audio characteristics, and user background.
Prior studies have leveraged various types of features from different
modalities, such as visual quality, semantic content, background sound, etc.,
but often struggle to effectively model their cross-feature and cross-modality
interactions. In this work, we empirically investigate the potential of large
multimodal models (LMMs) for video engagement prediction. We adopt two
representative LMMs: VideoLLaMA2, which integrates audio, visual, and language
modalities, and Qwen2.5-VL, which models only visual and language modalities.
Specifically, VideoLLaMA2 jointly processes key video frames, text-based
metadata, and background sound, while Qwen2.5-VL utilizes only key video frames
and text-based metadata. Trained on the SnapUGC dataset, both models
demonstrate competitive performance against state-of-the-art baselines,
showcasing the effectiveness of LMMs in engagement prediction. Notably,
VideoLLaMA2 consistently outperforms Qwen2.5-VL, highlighting the importance of
audio features in engagement prediction. By ensembling two types of models, our
method achieves first place in the ICCV VQualA 2025 EVQA-SnapUGC Challenge on
short-form video engagement prediction. The code is available at
https://github.com/sunwei925/LMM-EVQA.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The proposed method achieves first place in the ICCV VQualA 2025
  EVQA-SnapUGC Challenge on short-form video engagement prediction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Wu, Fei Teng, Hao Shi, Qi Jiang, Kai Luo, Kaiwei Wang, Kailun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Panoramic cameras, capturing comprehensive 360-degree environmental data, are
suitable for quadruped robots in surrounding perception and interaction with
complex environments. However, the scarcity of high-quality panoramic training
data-caused by inherent kinematic constraints and complex sensor calibration
challenges-fundamentally limits the development of robust perception systems
tailored to these embodied platforms. To address this issue, we propose
QuaDreamer-the first panoramic data generation engine specifically designed for
quadruped robots. QuaDreamer focuses on mimicking the motion paradigm of
quadruped robots to generate highly controllable, realistic panoramic videos,
providing a data source for downstream tasks. Specifically, to effectively
capture the unique vertical vibration characteristics exhibited during
quadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts
controllable vertical signals through frequency-domain feature filtering and
provides high-quality prompts. To facilitate high-quality panoramic video
generation under jitter signal control, we propose a Scene-Object Controller
(SOC) that effectively manages object motion and boosts background jitter
control through the attention mechanism. To address panoramic distortions in
wide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream
architecture that synergizes frequency-texture refinement for local detail
enhancement with spatial-structure correction for global geometric consistency.
We further demonstrate that the generated video sequences can serve as training
data for the quadruped robot's panoramic visual perception model, enhancing the
performance of multi-object tracking in 360-degree scenes. The source code and
model weights will be publicly available at
https://github.com/losehu/QuaDreamer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CoRL 2025. The source code and model weights will be
  publicly available at https://github.com/losehu/QuaDreamer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Transparent Object Grasping: Depth Completion with Monocular
  Depth Estimation and Instance Mask 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaofeng Cheng, Xinkai Gao, Sen Zhang, Chao Zeng, Fusheng Zha, Lining Sun, Chenguang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the optical properties, transparent objects often lead depth cameras
to generate incomplete or invalid depth data, which in turn reduces the
accuracy and reliability of robotic grasping. Existing approaches typically
input the RGB-D image directly into the network to output the complete depth,
expecting the model to implicitly infer the reliability of depth values.
However, while effective in training datasets, such methods often fail to
generalize to real-world scenarios, where complex light interactions lead to
highly variable distributions of valid and invalid depth data. To address this,
we propose ReMake, a novel depth completion framework guided by an instance
mask and monocular depth estimation. By explicitly distinguishing transparent
regions from non-transparent ones, the mask enables the model to concentrate on
learning accurate depth estimation in these areas from RGB-D input during
training. This targeted supervision reduces reliance on implicit reasoning and
improves generalization to real-world scenarios. Additionally, monocular depth
estimation provides depth context between the transparent object and its
surroundings, enhancing depth prediction accuracy. Extensive experiments show
that our method outperforms existing approaches on both benchmark datasets and
real-world scenarios, demonstrating superior accuracy and generalization
capability. Code and videos are available at
https://chengyaofeng.github.io/ReMake.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clinical Expert Uncertainty Guided Generalized Label Smoothing for
  Medical Noisy Label Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunyu Zhang, Lin Gu, Liangchen Liu, Yingke Chen, Bingyang Wang, Jin Yan, Yingying Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many previous studies have proposed extracting image labels from clinical
notes to create large-scale medical image datasets at a low cost. However,
these approaches inherently suffer from label noise due to uncertainty from the
clinical experts. When radiologists and physicians analyze medical images to
make diagnoses, they often include uncertainty-aware notes such as ``maybe'' or
``not excluded''. Unfortunately, current text-mining methods overlook these
nuances, resulting in the creation of noisy labels. Existing methods for
handling noisy labels in medical image analysis, which typically address the
problem through post-processing techniques, have largely ignored the important
issue of expert-driven uncertainty contributing to label noise. To better
incorporate the expert-written uncertainty in clinical notes into medical image
analysis and address the label noise issue, we first examine the impact of
clinical expert uncertainty on label noise. We then propose a clinical expert
uncertainty-aware benchmark, along with a label smoothing method, which
significantly improves performance compared to current state-of-the-art
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianchao Wang, Peng Zhou, Cen Li, Rong Quan, Jie Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) is a powerful and computationally efficient
representation for 3D reconstruction. Despite its strengths, 3DGS often
produces floating artifacts, which are erroneous structures detached from the
actual geometry and significantly degrade visual fidelity. The underlying
mechanisms causing these artifacts, particularly in low-quality initialization
scenarios, have not been fully explored. In this paper, we investigate the
origins of floating artifacts from a frequency-domain perspective and identify
under-optimized Gaussians as the primary source. Based on our analysis, we
propose \textit{Eliminating-Floating-Artifacts} Gaussian Splatting (EFA-GS),
which selectively expands under-optimized Gaussians to prioritize accurate
low-frequency learning. Additionally, we introduce complementary depth-based
and scale-based strategies to dynamically refine Gaussian expansion,
effectively mitigating detail erosion. Extensive experiments on both synthetic
and real-world datasets demonstrate that EFA-GS substantially reduces floating
artifacts while preserving high-frequency details, achieving an improvement of
1.68 dB in PSNR over baseline method on our RWLQ dataset. Furthermore, we
validate the effectiveness of our approach in downstream 3D editing tasks. Our
implementation will be released on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MindShot: Multi-Shot Video Reconstruction from fMRI with LLM Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenwen Zeng, Yonghuang Wu, Yifan Chen, Xuan Xie, Chengqian Zhao, Feiyu Yin, Guoqing Wu, Jinhua Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing dynamic videos from fMRI is important for understanding visual
cognition and enabling vivid brain-computer interfaces. However, current
methods are critically limited to single-shot clips, failing to address the
multi-shot nature of real-world experiences. Multi-shot reconstruction faces
fundamental challenges: fMRI signal mixing across shots, the temporal
resolution mismatch between fMRI and video obscuring rapid scene changes, and
the lack of dedicated multi-shot fMRI-video datasets. To overcome these
limitations, we propose a novel divide-and-decode framework for multi-shot fMRI
video reconstruction. Our core innovations are: (1) A shot boundary predictor
module explicitly decomposing mixed fMRI signals into shot-specific segments.
(2) Generative keyframe captioning using LLMs, which decodes robust textual
descriptions from each segment, overcoming temporal blur by leveraging
high-level semantics. (3) Novel large-scale data synthesis (20k samples) from
existing datasets. Experimental results demonstrate our framework outperforms
state-of-the-art methods in multi-shot reconstruction fidelity. Ablation
studies confirm the critical role of fMRI decomposition and semantic
captioning, with decomposition significantly improving decoded caption CLIP
similarity by 71.8%. This work establishes a new paradigm for multi-shot fMRI
reconstruction, enabling accurate recovery of complex visual narratives through
explicit decomposition and semantic prompting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-grained Multiple Supervisory Network for Multi-modal Manipulation
  Detecting and Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinquan Yu, Wei Lu, Xiangyang Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of Detecting and Grounding Multi-Modal Media Manipulation (DGM$^4$)
is a branch of misinformation detection. Unlike traditional binary
classification, it includes complex subtasks such as forgery content
localization and forgery method classification. Consider that existing methods
are often limited in performance due to neglecting the erroneous interference
caused by unreliable unimodal data and failing to establish comprehensive
forgery supervision for mining fine-grained tampering traces. In this paper, we
present a Fine-grained Multiple Supervisory (FMS) network, which incorporates
modality reliability supervision, unimodal internal supervision and cross-modal
supervision to provide comprehensive guidance for DGM$^4$ detection. For
modality reliability supervision, we propose the Multimodal Decision Supervised
Correction (MDSC) module. It leverages unimodal weak supervision to correct the
multi-modal decision-making process. For unimodal internal supervision, we
propose the Unimodal Forgery Mining Reinforcement (UFMR) module. It amplifies
the disparity between real and fake information within unimodal modality from
both feature-level and sample-level perspectives. For cross-modal supervision,
we propose the Multimodal Forgery Alignment Reasoning (MFAR) module. It
utilizes soft-attention interactions to achieve cross-modal feature perception
from both consistency and inconsistency perspectives, where we also design the
interaction constraints to ensure the interaction quality. Extensive
experiments demonstrate the superior performance of our FMS compared to
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-class Image Anomaly Detection for Practical Applications:
  Requirements and Robust Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehyuk Heo, Pilsung Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in image anomaly detection have extended unsupervised
learning-based models from single-class settings to multi-class frameworks,
aiming to improve efficiency in training time and model storage. When a single
model is trained to handle multiple classes, it often underperforms compared to
class-specific models in terms of per-class detection accuracy. Accordingly,
previous studies have primarily focused on narrowing this performance gap.
However, the way class information is used, or not used, remains a relatively
understudied factor that could influence how detection thresholds are defined
in multi-class image anomaly detection. These thresholds, whether
class-specific or class-agnostic, significantly affect detection outcomes. In
this study, we identify and formalize the requirements that a multi-class image
anomaly detection model must satisfy under different conditions, depending on
whether class labels are available during training and evaluation. We then
re-examine existing methods under these criteria. To meet these challenges, we
propose Hierarchical Coreset (HierCore), a novel framework designed to satisfy
all defined requirements. HierCore operates effectively even without class
labels, leveraging a hierarchical memory bank to estimate class-wise decision
criteria for anomaly detection. We empirically validate the applicability and
robustness of existing methods and HierCore under four distinct scenarios,
determined by the presence or absence of class labels in the training and
evaluation phases. The experimental results demonstrate that HierCore
consistently meets all requirements and maintains strong, stable performance
across all settings, highlighting its practical potential for real-world
multi-class anomaly detection tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with
  Vision Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghuang Wu, Wenwen Zeng, Xuan Xie, Chengqian Zhao, Guoqing Wu, Jinhua Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models like Segment Anything Model (SAM) excel in promptable
segmentation but suffer from an intent gap: they segment only explicitly
prompted objects, failing to generalize to semantically related instances
implicitly desired by users. This limitation is critical in domains with dense
homogeneous objects (e.g., biomedical nuclei segmentation), where sparse visual
prompts typically yield incomplete results, rendering dense annotations
impractical due to prohibitive cost. To bridge this gap, we introduce SAMPO
(Segment Anything Model with Preference Optimization), a novel framework that
teaches visual foundation models to infer high-level categorical intent from
sparse visual interactions. Unlike conventional pixel-level fine-tuning, SAMPO
optimizes models to implicitly capture target-class characteristics through
preference optimization. This approach, which operates without dependency on
language models, enables robust multi-object segmentation even under sparse
prompting and demonstrates superior data efficiency during fine-tuning.
Validated on three medical segmentation tasks, SAMPO achieves state-of-the-art
performance: on challenging tasks like PanNuke-T2, our method, when fine-tuned
with only 10% of the training data, significantly outperforms all existing
methods trained on the full 100% dataset, achieving an improvement of over 9
percentage points compared to the best baseline. Our work establishes a new
paradigm for intent-aware alignment in visual foundation models, removing
dependencies on auxiliary prompt generators or language-model-assisted
preference learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InfoSyncNet: Information Synchronization Temporal Convolutional Network
  for Visual Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junxiao Xue, Xiaozhen Liu, Xuecheng Wu, Fei Yu, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating spoken content from silent videos is crucial for applications in
Assistive Technology (AT) and Augmented Reality (AR). However, accurately
mapping lip movement sequences in videos to words poses significant challenges
due to variability across sequences and the uneven distribution of information
within each sequence. To tackle this, we introduce InfoSyncNet, a non-uniform
sequence modeling network enhanced by tailored data augmentation techniques.
Central to InfoSyncNet is a non-uniform quantization module positioned between
the encoder and decoder, enabling dynamic adjustment to the network's focus and
effectively handling the natural inconsistencies in visual speech data.
Additionally, multiple training strategies are incorporated to enhance the
model's capability to handle variations in lighting and the speaker's
orientation. Comprehensive experiments on the LRW and LRW1000 datasets confirm
the superiority of InfoSyncNet, achieving new state-of-the-art accuracies of
92.0% and 60.7% Top-1 ACC. The code is available for download (see comments).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/liuxiaozhen123/InfoSyncNet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Estimation for Novel Views in Gaussian Splatting from
  Primitive-Based Representations of Error and Visibility 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Gottwald, Edgar Heinert, Matthias Rottmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a novel method for uncertainty estimation (UE) in
Gaussian Splatting. UE is crucial for using Gaussian Splatting in critical
applications such as robotics and medicine. Previous methods typically estimate
the variance of Gaussian primitives and use the rendering process to obtain
pixel-wise uncertainties. Our method establishes primitive representations of
error and visibility of trainings views, which carries meaningful uncertainty
information. This representation is obtained by projection of training error
and visibility onto the primitives. Uncertainties of novel views are obtained
by rendering the primitive representations of uncertainty for those novel
views, yielding uncertainty feature maps. To aggregate these uncertainty
feature maps of novel views, we perform a pixel-wise regression on holdout
data. In our experiments, we analyze the different components of our method,
investigating various combinations of uncertainty feature maps and regression
models. Furthermore, we considered the effect of separating splatting into
foreground and background. Our UEs show high correlations to true errors,
outperforming state-of-the-art methods, especially on foreground objects. The
trained regression models show generalization capabilities to new scenes,
allowing uncertainty estimation without the need for holdout data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Glioblastoma Overall Survival Prediction With Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Lin, iccardo Barbieri, Domenico Aquino, Giuseppe Lauria, Marina Grisoli, Elena De Momi, Alberto Redaelli, Simona Ferrante
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Glioblastoma is one of the most aggressive and common brain tumors, with a
median survival of 10-15 months. Predicting Overall Survival (OS) is critical
for personalizing treatment strategies and aligning clinical decisions with
patient outcomes. In this study, we propose a novel Artificial Intelligence
(AI) approach for OS prediction using Magnetic Resonance Imaging (MRI) images,
exploiting Vision Transformers (ViTs) to extract hidden features directly from
MRI images, eliminating the need of tumor segmentation. Unlike traditional
approaches, our method simplifies the workflow and reduces computational
resource requirements.
  The proposed model was evaluated on the BRATS dataset, reaching an accuracy
of 62.5% on the test set, comparable to the top-performing methods.
Additionally, it demonstrated balanced performance across precision, recall,
and F1 score, overcoming the best model in these metrics. The dataset size
limits the generalization of the ViT which typically requires larger datasets
compared to convolutional neural networks. This limitation in generalization is
observed across all the cited studies. This work highlights the applicability
of ViTs for downsampled medical imaging tasks and establishes a foundation for
OS prediction models that are computationally efficient and do not rely on
segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures, EMBC2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying actionable driver mutations in lung cancer using an
  efficient Asymmetric <span class="highlight-title">Transformer</span> Decoder <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biagio Brattoli, Jack Shi, Jongchan Park, Taebum Lee, Donggeun Yoo, Sergio Pereira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying actionable driver mutations in non-small cell lung cancer (NSCLC)
can impact treatment decisions and significantly improve patient outcomes.
Despite guideline recommendations, broader adoption of genetic testing remains
challenging due to limited availability and lengthy turnaround times. Machine
Learning (ML) methods for Computational Pathology (CPath) offer a potential
solution; however, research often focuses on only one or two common mutations,
limiting the clinical value of these tools and the pool of patients who can
benefit from them. This study evaluates various Multiple Instance Learning
(MIL) techniques to detect six key actionable NSCLC driver mutations: ALK,
BRAF, EGFR, ERBB2, KRAS, and MET ex14. Additionally, we introduce an Asymmetric
Transformer Decoder model that employs queries and key-values of varying
dimensions to maintain a low query dimensionality. This approach efficiently
extracts information from patch embeddings and minimizes overfitting risks,
proving highly adaptable to the MIL setting. Moreover, we present a method to
directly utilize tissue type in the model, addressing a typical MIL limitation
where either all regions or only some specific regions are analyzed, neglecting
biological relevance. Our method outperforms top MIL models by an average of
3%, and over 4% when predicting rare mutations such as ERBB2 and BRAF, moving
ML-based tests closer to being practical alternatives to standard genetic
testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI 2025 Workshop COMPAYL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination
  via Attention Lens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohan Zheng, Zhenguo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have demonstrated remarkable multimodal
comprehension and reasoning capabilities, but they still suffer from severe
object hallucination. Previous studies primarily attribute the flaw to
linguistic prior caused by the scale mismatch between visual encoders and large
language models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon
LLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,
generating descriptions inconsistent with visual cues. However, through an
in-depth investigation of the hallucinated mechanisms, we empirically reveal a
previously overlooked phenomenon: LVLMs may ignore not only visual information
but also textual modality during hallucination, a behavior termed as modality
bias, which indicates that LVLMs struggle to simultaneously attend to both
visual and textual modalities, leading to fragmented understanding of
user-provided instructions. Based on this observation, we propose a simple yet
effective training-free method to mitigate object hallucination. Concretely, we
intervene and adjust the attention weights of textual and visual tokens,
balancing cross-modal compatibility for better alignment with user intentions.
Furthermore, we adopt a contrastive decoding strategy to reduce the LVLM's
overreliance on its parametric knowledge, synergistically enhancing our
attention manipulation. Extensive experiments confirm the widespread presence
of modality bias in LVLMs. Notably, our method effectively mitigates
hallucination across multiple open-source LVLMs and benchmarks, highlighting
its generalizability and efficacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HGTS-Former: Hierarchical HyperGraph <span class="highlight-title">Transformer</span> for Multivariate Time
  Series Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Wang, Hao Si, Fan Zhang, Xiaoya Zhou, Dengdi Sun, Wanli Lyu, Qingquan Yang, Jin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate time series analysis has long been one of the key research
topics in the field of artificial intelligence. However, analyzing complex time
series data remains a challenging and unresolved problem due to its high
dimensionality, dynamic nature, and complex interactions among variables.
Inspired by the strong structural modeling capability of hypergraphs, this
paper proposes a novel hypergraph-based time series transformer backbone
network, termed HGTS-Former, to address the multivariate coupling in time
series data. Specifically, given the multivariate time series signal, we first
normalize and embed each patch into tokens. Then, we adopt the multi-head
self-attention to enhance the temporal representation of each patch. The
hierarchical hypergraphs are constructed to aggregate the temporal patterns
within each channel and fine-grained relations between different variables.
After that, we convert the hyperedge into node features through the EdgeToNode
module and adopt the feed-forward network to further enhance the output
features. Extensive experiments conducted on two multivariate time series tasks
and eight datasets fully validated the effectiveness of our proposed
HGTS-Former. The source code will be released on
https://github.com/Event-AHU/Time_Series_Analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hydra: Accurate Multi-Modal Leaf Wetness Sensing with mm-Wave and Camera
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimeng Liu, Maolin Gan, Huaili Zeng, Li Liu, Younsuk Dong, Zhichao Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leaf Wetness Duration (LWD), the time that water remains on leaf surfaces, is
crucial in the development of plant diseases. Existing LWD detection lacks
standardized measurement techniques, and variations across different plant
characteristics limit its effectiveness. Prior research proposes diverse
approaches, but they fail to measure real natural leaves directly and lack
resilience in various environmental conditions. This reduces the precision and
robustness, revealing a notable practical application and effectiveness gap in
real-world agricultural settings. This paper presents Hydra, an innovative
approach that integrates millimeter-wave (mm-Wave) radar with camera technology
to detect leaf wetness by determining if there is water on the leaf. We can
measure the time to determine the LWD based on this detection. Firstly, we
design a Convolutional Neural Network (CNN) to selectively fuse multiple
mm-Wave depth images with an RGB image to generate multiple feature images.
Then, we develop a transformer-based encoder to capture the inherent connection
among the multiple feature images to generate a feature map, which is further
fed to a classifier for detection. Moreover, we augment the dataset during
training to generalize our model. Implemented using a frequency-modulated
continuous-wave (FMCW) radar within the 76 to 81 GHz band, Hydra's performance
is meticulously evaluated on plants, demonstrating the potential to classify
leaf wetness with up to 96% accuracy across varying scenarios. Deploying Hydra
in the farm, including rainy, dawn, or poorly light nights, it still achieves
an accuracy rate of around 90%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of ACM MobiCom (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikuang Yuluo, Yue Ma, Kuan Shen, Tongtong Jin, Wang Liao, Yangpu Ma, Fuquan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT
reconstruction. However, existing methods rely on the average gradient
magnitude of points within the view, often leading to severe needle-like
artifacts under sparse-view conditions. To address this challenge, we propose
GR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses
needle-like artifacts and improves reconstruction accuracy under sparse-view
conditions. Our framework introduces two key innovations: (1) a Denoised Point
Cloud Initialization Strategy that reduces initialization errors and
accelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that
refines gradient computation using graph-based density differences, improving
splitting accuracy and density representation. Experiments on X-3D and
real-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR
improvements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These
results highlight the applicability of GR-Gaussian for accurate CT
reconstruction under challenging sparse-view conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Generalization of Language-Conditioned Robot Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenglin Cui, Chaoran Zhu, Changjae Oh, Andrea Cavallaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The control of robots for manipulation tasks generally relies on visual
input. Recent advances in vision-language models (VLMs) enable the use of
natural language instructions to condition visual input and control robots in a
wider range of environments. However, existing methods require a large amount
of data to fine-tune VLMs for operating in unseen environments. In this paper,
we present a framework that learns object-arrangement tasks from just a few
demonstrations. We propose a two-stage framework that divides
object-arrangement tasks into a target localization stage, for picking the
object, and a region determination stage for placing the object. We present an
instance-level semantic fusion module that aligns the instance-level image
crops with the text embedding, enabling the model to identify the target
objects defined by the natural language instructions. We validate our method on
both simulation and real-world robotic environments. Our method, fine-tuned
with a few demonstrations, improves generalization capability and demonstrates
zero-shot ability in real-robot manipulation scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages,18 figures,2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $ε$-Softmax: Approximating One-Hot Vectors for Mitigating Label
  Noise <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialiang Wang, Xiong Zhou, Deming Zhai, Junjun Jiang, Xiangyang Ji, Xianming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Noisy labels pose a common challenge for training accurate deep neural
networks. To mitigate label noise, prior studies have proposed various robust
loss functions to achieve noise tolerance in the presence of label noise,
particularly symmetric losses. However, they usually suffer from the
underfitting issue due to the overly strict symmetric condition. In this work,
we propose a simple yet effective approach for relaxing the symmetric
condition, namely $\epsilon$-softmax, which simply modifies the outputs of the
softmax layer to approximate one-hot vectors with a controllable error
$\epsilon$. Essentially, $\epsilon$-softmax not only acts as an alternative for
the softmax layer, but also implicitly plays the crucial role in modifying the
loss function. We prove theoretically that $\epsilon$-softmax can achieve
noise-tolerant learning with controllable excess risk bound for almost any loss
function. Recognizing that $\epsilon$-softmax-enhanced losses may slightly
reduce fitting ability on clean datasets, we further incorporate them with one
symmetric loss, thereby achieving a better trade-off between robustness and
effective learning. Extensive experiments demonstrate the superiority of our
method in mitigating synthetic and real-world label noise. The code is
available at https://github.com/cswjl/eps-softmax.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Object Discovery for Unsupervised Instance Segmentation and
  Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Feng, Hebei Gao, Hong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Cut-Once-and-LEaRn (COLER), a simple approach for unsupervised
instance segmentation and object detection. COLER first uses our developed
CutOnce to generate coarse pseudo labels, then enables the detector to learn
from these masks. CutOnce applies Normalized Cut only once and does not rely on
any clustering methods, but it can generate multiple object masks in an image.
We have designed several novel yet simple modules that not only allow CutOnce
to fully leverage the object discovery capabilities of self-supervised models,
but also free it from reliance on mask post-processing. During training, COLER
achieves strong performance without requiring specially designed loss functions
for pseudo labels, and its performance is further improved through
self-training. COLER is a zero-shot unsupervised model that outperforms
previous state-of-the-art methods on multiple benchmarks.We believe our method
can help advance the field of unsupervised object localization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMART-Ship: A Comprehensive Synchronized Multi-modal Aligned Remote
  Sensing Targets <span class="highlight-title">Dataset</span> and Benchmark for <span class="highlight-title">Bert</span>hed Ships Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen-Chen Fan, Peiyao Guo, Linping Zhang, Kehan Qi, Haolin Huang, Yong-Qiang Mao, Yuxi Suo, Zhizhuo Jiang, Yu Liu, You He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the limitations of satellite orbits and imaging conditions, multi-modal
remote sensing (RS) data is crucial in enabling long-term earth observation.
However, maritime surveillance remains challenging due to the complexity of
multi-scale targets and the dynamic environments. To bridge this critical gap,
we propose a Synchronized Multi-modal Aligned Remote sensing Targets dataset
for berthed ships analysis (SMART-Ship), containing spatiotemporal registered
images with fine-grained annotation for maritime targets from five modalities:
visible-light, synthetic aperture radar (SAR), panchromatic, multi-spectral,
and near-infrared. Specifically, our dataset consists of 1092 multi-modal image
sets, covering 38,838 ships. Each image set is acquired within one week and
registered to ensure spatiotemporal consistency. Ship instances in each set are
annotated with polygonal location information, fine-grained categories,
instance-level identifiers, and change region masks, organized hierarchically
to support diverse multi-modal RS tasks. Furthermore, we define standardized
benchmarks on five fundamental tasks and comprehensively compare representative
methods across the dataset. Thorough experiment evaluations validate that the
proposed SMART-Ship dataset could support various multi-modal RS interpretation
tasks and reveal the promising directions for further exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uni-Layout: Integrating Human Feedback in Unified Layout Generation and
  Evaluation <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Lu, Yanyin Chen, Wei Feng, Jiahao Fan, Fengheng Li, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law, Jian Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Layout generation plays a crucial role in enhancing both user experience and
design efficiency. However, current approaches suffer from task-specific
generation capabilities and perceptually misaligned evaluation metrics, leading
to limited applicability and ineffective measurement. In this paper, we propose
\textit{Uni-Layout}, a novel framework that achieves unified generation,
human-mimicking evaluation and alignment between the two. For universal
generation, we incorporate various layout tasks into a single taxonomy and
develop a unified generator that handles background or element contents
constrained tasks via natural language prompts. To introduce human feedback for
the effective evaluation of layouts, we build \textit{Layout-HF100k}, the first
large-scale human feedback dataset with 100,000 expertly annotated layouts.
Based on \textit{Layout-HF100k}, we introduce a human-mimicking evaluator that
integrates visual and geometric information, employing a Chain-of-Thought
mechanism to conduct qualitative assessments alongside a confidence estimation
module to yield quantitative measurements. For better alignment between the
generator and the evaluator, we integrate them into a cohesive system by
adopting Dynamic-Margin Preference Optimization (DMPO), which dynamically
adjusts margins based on preference strength to better align with human
judgments. Extensive experiments show that \textit{Uni-Layout} significantly
outperforms both task-specific and general-purpose methods. Our code is
publicly available at https://github.com/JD-GenX/Uni-Layout.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRUDI and TITUS: A Multi-Perspective <span class="highlight-title">Dataset</span> and A Three-Stage
  Recognition System for Transportation Unit Identification <span class="chip">BMVC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emre Gülsoylu, André Kelm, Lennart Bengtson, Matthias Hirsch, Christian Wilms, Tim Rolff, Janick Edinger, Simone Frintrop
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying transportation units (TUs) is essential for improving the
efficiency of port logistics. However, progress in this field has been hindered
by the lack of publicly available benchmark datasets that capture the diversity
and dynamics of real-world port environments. To address this gap, we present
the TRUDI dataset-a comprehensive collection comprising 35,034 annotated
instances across five categories: container, tank container, trailer, ID text,
and logo. The images were captured at operational ports using both ground-based
and aerial cameras, under a wide variety of lighting and weather conditions.
For the identification of TUs-which involves reading the 11-digit alphanumeric
ID typically painted on each unit-we introduce TITUS, a dedicated pipeline that
operates in three stages: (1) segmenting the TU instances, (2) detecting the
location of the ID text, and (3) recognising and validating the extracted ID.
Unlike alternative systems, which often require similar scenes, specific camera
angles or gate setups, our evaluation demonstrates that TITUS reliably
identifies TUs from a range of camera perspectives and in varying lighting and
weather conditions. By making the TRUDI dataset publicly available, we provide
a robust benchmark that enables the development and comparison of new
approaches. This contribution supports digital transformation efforts in
multipurpose ports and helps to increase the efficiency of entire logistics
chains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 2 figures, 6 tables. Author version of the paper. Accepted
  for publication in The 36th British Machine Vision Conference (BMVC) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transport-Guided Rectified Flow Inversion: Improved Image Editing Using
  Optimal Transport Theory <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marian Lupascu, Mihai-Sorin Stupariu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective image inversion in rectified flow models - mapping real images to
editable latent representations - is crucial for practical image editing
applications; however, achieving optimal balance between reconstruction
fidelity and editing flexibility remains a fundamental challenge. In this work,
we introduce the Optimal Transport Inversion Pipeline (OTIP), a zero-shot
framework that leverages optimal transport theory to guide the inversion
process in rectified flow models. Our underlying hypothesis is that
incorporating transport-based guidance during the reverse diffusion process can
effectively balance reconstruction accuracy and editing controllability through
principled trajectory optimization. The method computes optimal transport paths
between image and noise distributions while maintaining computational
efficiency. Our approach achieves high-fidelity reconstruction with LPIPS
scores of 0.001 and SSIM of 0.992 on face editing benchmarks, demonstrating
superior preservation of fine-grained details compared to existing methods. We
evaluate the framework across multiple editing tasks, observing 7.8% to 12.9%
improvements in reconstruction loss over RF-Inversion on the LSUN-Bedroom and
LSUN-Church datasets, respectively. For semantic face editing, our method
achieves an 11.2% improvement in identity preservation and a 1.6% enhancement
in perceptual quality, while maintaining computational efficiency comparable to
baseline approaches. Qualitatively, our method produces visually compelling
edits with superior semantic consistency and fine-grained detail preservation
across diverse editing scenarios. Code is available at:
https://github.com/marianlupascu/OT-Inversion
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 24 figures, WACV conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text2Lip: Progressive Lip-Synced Talking Face Generation from Text via
  Viseme-Guided Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Wang, Shengeng Tang, Fei Wang, Lechao Cheng, Dan Guo, Feng Xue, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating semantically coherent and visually accurate talking faces requires
bridging the gap between linguistic meaning and facial articulation. Although
audio-driven methods remain prevalent, their reliance on high-quality paired
audio visual data and the inherent ambiguity in mapping acoustics to lip motion
pose significant challenges in terms of scalability and robustness. To address
these issues, we propose Text2Lip, a viseme-centric framework that constructs
an interpretable phonetic-visual bridge by embedding textual input into
structured viseme sequences. These mid-level units serve as a linguistically
grounded prior for lip motion prediction. Furthermore, we design a progressive
viseme-audio replacement strategy based on curriculum learning, enabling the
model to gradually transition from real audio to pseudo-audio reconstructed
from enhanced viseme features via cross-modal attention. This allows for robust
generation in both audio-present and audio-free scenarios. Finally, a
landmark-guided renderer synthesizes photorealistic facial videos with accurate
lip synchronization. Extensive evaluations show that Text2Lip outperforms
existing approaches in semantic fidelity, visual realism, and modality
robustness, establishing a new paradigm for controllable and flexible talking
face generation. Our project homepage is https://plyon1.github.io/Text2Lip/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward a reliable PWM-based light-emitting diode visual stimulus for
  improved SSVEP response with minimal visual fatigue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Surej Mouli, Ramaswamy Palaniappan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steady state visual evoked response (SSVEP) is widely used in visual-based
diagnosis and applications such as brain computer interfacing due to its high
information transfer rate and the capability to activate commands through
simple gaze control. However, one major impediment in using flashing visual
stimulus to obtain SSVEP is eye fatigue that prevents continued long term use
preventing practical deployment. This combined with the difficulty in
establishing precise pulse-width modulation (PWM) that results in poorer
accuracy warrants the development of appropriate approach to solve these
issues. Various studies have suggested the usage of high frequencies of visual
stimulus to reduce the visual fatigue for the user but this results in poor
response performance. Here, the authors study the use of extremely high
duty-cycles in the stimulus in the hope of solving these constraints.
Electroencephalogram data was recorded with PWM duty-cycles of 50 to 95%
generated by a precise custom-made light-emitting diode hardware and tested ten
subjects responded that increasing duty-cycles had less visual strain for all
the frequency values and the SSVEP exhibited a subject-independent peak
response for duty-cycle of 85%. This could pave the way for increased usage of
SSVEP for practical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at
  T-Junctions Utilizing Road Layout Extraction via Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byeonggyu Park, Hee-Yeun Kim, Byonghyok Choi, Hansang Cho, Byungkwan Kim, Soomok Lee, Mingu Jeon, Seong-Woo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pedestrians Localization in Non-Line-of-Sight (NLoS) regions within urban
environments poses a significant challenge for autonomous driving systems.
While mmWave radar has demonstrated potential for detecting objects in such
scenarios, the 2D radar point cloud (PCD) data is susceptible to distortions
caused by multipath reflections, making accurate spatial inference difficult.
Additionally, although camera images provide high-resolution visual
information, they lack depth perception and cannot directly observe objects in
NLoS regions. In this paper, we propose a novel framework that interprets radar
PCD through road layout inferred from camera for localization of NLoS
pedestrians. The proposed method leverages visual information from the camera
to interpret 2D radar PCD, enabling spatial scene reconstruction. The
effectiveness of the proposed approach is validated through experiments
conducted using a radar-camera system mounted on a real vehicle. The
localization performance is evaluated using a dataset collected in outdoor NLoS
driving environments, demonstrating the practical applicability of the method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Hu, Zijie Xin, Xirong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ad-hoc Video Search (AVS) involves using a textual query to search for
multiple relevant videos in a large collection of unlabeled short videos. The
main challenge of AVS is the visual diversity of relevant videos. A simple
query such as "Find shots of a man and a woman dancing together indoors" can
span a multitude of environments, from brightly lit halls and shadowy bars to
dance scenes in black-and-white animations. It is therefore essential to
retrieve relevant videos as comprehensively as possible. Current solutions for
the AVS task primarily fuse multiple features into one or more common spaces,
yet overlook the need for diverse spaces. To fully exploit the expressive
capability of individual features, we propose LPD, short for Learning Partially
Decorrelated common spaces. LPD incorporates two key innovations:
feature-specific common space construction and the de-correlation loss.
Specifically, LPD learns a separate common space for each video and text
feature, and employs de-correlation loss to diversify the ordering of negative
samples across different spaces. To enhance the consistency of multi-space
convergence, we designed an entropy-based fair multi-space triplet ranking
loss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify
the effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces
highlight its ability to enhance result diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACMMM2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correspondence-Free Fast and Robust Spherical Point Pattern Registration <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anik Sarker, Alan T. Asbeck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing methods for rotation estimation between two spherical
($\mathbb{S}^2$) patterns typically rely on spherical cross-correlation
maximization between two spherical function. However, these approaches exhibit
computational complexities greater than cubic $O(n^3)$ with respect to rotation
space discretization and lack extensive evaluation under significant outlier
contamination. To this end, we propose a rotation estimation algorithm between
two spherical patterns with linear time complexity $O(n)$. Unlike existing
spherical-function-based methods, we explicitly represent spherical patterns as
discrete 3D point sets on the unit sphere, reformulating rotation estimation as
a spherical point-set alignment (i.e., Wahba problem for 3D unit vectors).
Given the geometric nature of our formulation, our spherical pattern alignment
algorithm naturally aligns with the Wahba problem framework for 3D unit
vectors. Specifically, we introduce three novel algorithms: (1) SPMC (Spherical
Pattern Matching by Correlation), (2) FRS (Fast Rotation Search), and (3) a
hybrid approach (SPMC+FRS) that combines the advantages of the previous two
methods. Our experiments demonstrate that in the $\mathbb{S}^2$ domain and in
correspondence-free settings, our algorithms are over 10x faster and over 10x
more accurate than current state-of-the-art methods for the Wahba problem with
outliers. We validate our approach through extensive simulations on a new
dataset of spherical patterns, the ``Robust Vector Alignment Dataset.
"Furthermore, we adapt our methods to two real-world tasks: (i) Point Cloud
Registration (PCR) and (ii) rotation estimation for spherical images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via
  Instruction Editing Data and Long Captions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziteng Wang, Siqi Yang, Limeng Qiao, Lin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of Vision-Language Models (VLMs) like CLIP in aligning
vision and language, their proficiency in detailed, fine-grained visual
comprehension remains a key challenge. We present CLIP-IN, a novel framework
that bolsters CLIP's fine-grained perception through two core innovations.
Firstly, we leverage instruction-editing datasets, originally designed for
image manipulation, as a unique source of hard negative image-text pairs.
Coupled with a symmetric hard negative contrastive loss, this enables the model
to effectively distinguish subtle visual-semantic differences. Secondly,
CLIP-IN incorporates long descriptive captions, utilizing rotary positional
encodings to capture rich semantic context often missed by standard CLIP. Our
experiments demonstrate that CLIP-IN achieves substantial gains on the MMVP
benchmark and various fine-grained visual recognition tasks, without
compromising robust zero-shot performance on broader classification and
retrieval tasks. Critically, integrating CLIP-IN's visual representations into
Multimodal Large Language Models significantly reduces visual hallucinations
and enhances reasoning abilities. This work underscores the considerable
potential of synergizing targeted, instruction-based contrastive learning with
comprehensive descriptive information to elevate the fine-grained understanding
of VLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Qwen-Image Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, Zenan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Qwen-Image, an image generation foundation model in the Qwen
series that achieves significant advances in complex text rendering and precise
image editing. To address the challenges of complex text rendering, we design a
comprehensive data pipeline that includes large-scale data collection,
filtering, annotation, synthesis, and balancing. Moreover, we adopt a
progressive training strategy that starts with non-text-to-text rendering,
evolves from simple to complex textual inputs, and gradually scales up to
paragraph-level descriptions. This curriculum learning approach substantially
enhances the model's native text rendering capabilities. As a result,
Qwen-Image not only performs exceptionally well in alphabetic languages such as
English, but also achieves remarkable progress on more challenging logographic
languages like Chinese. To enhance image editing consistency, we introduce an
improved multi-task training paradigm that incorporates not only traditional
text-to-image (T2I) and text-image-to-image (TI2I) tasks but also
image-to-image (I2I) reconstruction, effectively aligning the latent
representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed
the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and
reconstructive representations, respectively. This dual-encoding mechanism
enables the editing module to strike a balance between preserving semantic
consistency and maintaining visual fidelity. Qwen-Image achieves
state-of-the-art performance, demonstrating its strong capabilities in both
image generation and editing across multiple benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/QwenLM/Qwen-Image</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dream-to-Recon: Monocular 3D Reconstruction with Diffusion-Depth
  Distillation from Single Images <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Wulff, Felix Wimbauer, Dominik Muhle, Daniel Cremers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volumetric scene reconstruction from a single image is crucial for a broad
range of applications like autonomous driving and robotics. Recent volumetric
reconstruction methods achieve impressive results, but generally require
expensive 3D ground truth or multi-view supervision. We propose to leverage
pre-trained 2D diffusion models and depth prediction models to generate
synthetic scene geometry from a single image. This can then be used to distill
a feed-forward scene reconstruction model. Our experiments on the challenging
KITTI-360 and Waymo datasets demonstrate that our method matches or outperforms
state-of-the-art baselines that use multi-view supervision, and offers unique
advantages, for example regarding dynamic scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025. Website: https://philippwulff.github.io/dream-to-recon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot Compositional Action Recognition with Neural Logic Constraints <span class="chip">ACM MM2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gefan Ye, Lin Li, Kexin Li, Jun Xiao, Long chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot compositional action recognition (ZS-CAR) aims to identify unseen
verb-object compositions in the videos by exploiting the learned knowledge of
verb and object primitives during training. Despite compositional learning's
progress in ZS-CAR, two critical challenges persist: 1) Missing compositional
structure constraint, leading to spurious correlations between primitives; 2)
Neglecting semantic hierarchy constraint, leading to semantic ambiguity and
impairing the training process. In this paper, we argue that human-like
symbolic reasoning offers a principled solution to these challenges by
explicitly modeling compositional and hierarchical structured abstraction. To
this end, we propose a logic-driven ZS-CAR framework LogicCAR that integrates
dual symbolic constraints: Explicit Compositional Logic and Hierarchical
Primitive Logic. Specifically, the former models the restrictions within the
compositions, enhancing the compositional reasoning ability of our model. The
latter investigates the semantical dependencies among different primitives,
empowering the models with fine-to-coarse reasoning capacity. By formalizing
these constraints in first-order logic and embedding them into neural network
architectures, LogicCAR systematically bridges the gap between symbolic
abstraction and existing models. Extensive experiments on the Sth-com dataset
demonstrate that our LogicCAR outperforms existing baseline methods, proving
the effectiveness of our logic-driven constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures; Accepted by ACM MM2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Uncertainty Quantification a Viable Alternative to Learned Deferral? <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna M. Wundram, Christian F. Baumgartner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) holds the potential to dramatically improve
patient care. However, it is not infallible, necessitating
human-AI-collaboration to ensure safe implementation. One aspect of AI safety
is the models' ability to defer decisions to a human expert when they are
likely to misclassify autonomously. Recent research has focused on methods that
learn to defer by optimising a surrogate loss function that finds the optimal
trade-off between predicting a class label or deferring. However, during
clinical translation, models often face challenges such as data shift.
Uncertainty quantification methods aim to estimate a model's confidence in its
predictions. However, they may also be used as a deferral strategy which does
not rely on learning from specific training distribution. We hypothesise that
models developed to quantify uncertainty are more robust to out-of-distribution
(OOD) input than learned deferral models that have been trained in a supervised
fashion. To investigate this hypothesis, we constructed an extensive evaluation
study on a large ophthalmology dataset, examining both learned deferral models
and established uncertainty quantification methods, assessing their performance
in- and out-of-distribution. Specifically, we evaluate their ability to
accurately classify glaucoma from fundus images while deferring cases with a
high likelihood of error. We find that uncertainty quantification methods may
be a promising choice for AI deferral.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as an oral presentation at MICCAI UNSURE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Whole-body Representation Learning For Competing Preclinical Disease
  Risk Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitrii Seletkov, Sophie Starck, Ayhan Can Erdur, Yundi Zhang, Daniel Rueckert, Rickmer Braren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable preclinical disease risk assessment is essential to move public
healthcare from reactive treatment to proactive identification and prevention.
However, image-based risk prediction algorithms often consider one condition at
a time and depend on hand-crafted features obtained through segmentation tools.
We propose a whole-body self-supervised representation learning method for the
preclinical disease risk assessment under a competing risk modeling. This
approach outperforms whole-body radiomics in multiple diseases, including
cardiovascular disease (CVD), type 2 diabetes (T2D), chronic obstructive
pulmonary disease (COPD), and chronic kidney disease (CKD). Simulating a
preclinical screening scenario and subsequently combining with cardiac MRI, it
sharpens further the prediction for CVD subgroups: ischemic heart disease
(IHD), hypertensive diseases (HD), and stroke. The results indicate the
translational potential of whole-body representations as a standalone screening
modality and as part of a multi-modal framework within clinical workflows for
early personalized risk stratification. The code is available at
https://github.com/yayapa/WBRLforCR/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Aqeel, Shakiba Sharifi, Marco Cristani, Francesco Setti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  So-called unsupervised anomaly detection is better described as
semi-supervised, as it assumes all training data are nominal. This assumption
simplifies training but requires manual data curation, introducing bias and
limiting adaptability. We propose Confident Meta-learning (CoMet), a novel
training strategy that enables deep anomaly detection models to learn from
uncurated datasets where nominal and anomalous samples coexist, eliminating the
need for explicit filtering. Our approach integrates Soft Confident Learning,
which assigns lower weights to low-confidence samples, and Meta-Learning, which
stabilizes training by regularizing updates based on training validation loss
covariance. This prevents overfitting and enhances robustness to noisy data.
CoMet is model-agnostic and can be applied to any anomaly detection method
trainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2
with two state-of-the-art models demonstrate the effectiveness of our approach,
consistently improving over the baseline methods, remaining insensitive to
anomalies in the training set, and setting a new state-of-the-art across all
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ieee/cvf international conference on computer vision
  (ICCV2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing the Temporal Potential of Stereo Event Cameras for
  Continuous-Time 3D Object Detection <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jae-Young Kang, Hoonhee Cho, Kuk-Jin Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D object detection is essential for autonomous systems, enabling precise
localization and dimension estimation. While LiDAR and RGB cameras are widely
used, their fixed frame rates create perception gaps in high-speed scenarios.
Event cameras, with their asynchronous nature and high temporal resolution,
offer a solution by capturing motion continuously. The recent approach, which
integrates event cameras with conventional sensors for continuous-time
detection, struggles in fast-motion scenarios due to its dependency on
synchronized sensors. We propose a novel stereo 3D object detection framework
that relies solely on event cameras, eliminating the need for conventional 3D
sensors. To compensate for the lack of semantic and geometric information in
event data, we introduce a dual filter mechanism that extracts both.
Additionally, we enhance regression by aligning bounding boxes with
object-centric information. Experiments show that our method outperforms prior
approaches in dynamic environments, demonstrating the potential of event
cameras for robust, continuous-time 3D perception. The code is available at
https://github.com/mickeykang16/Ev-Stereo3D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Edges Matter? Investigating Edge-Enhanced <span class="highlight-title">Pre-Train</span>ing for Medical
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Zaha, Lars Böcking, Simeon Allmendinger, Leopold Müller, Niklas Kühl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation is crucial for disease diagnosis and treatment
planning, yet developing robust segmentation models often requires substantial
computational resources and large datasets. Existing research shows that
pre-trained and finetuned foundation models can boost segmentation performance.
However, questions remain about how particular image preprocessing steps may
influence segmentation performance across different medical imaging modalities.
In particular, edges-abrupt transitions in pixel intensity-are widely
acknowledged as vital cues for object boundaries but have not been
systematically examined in the pre-training of foundation models. We address
this gap by investigating to which extend pre-training with data processed
using computationally efficient edge kernels, such as kirsch, can improve
cross-modality segmentation capabilities of a foundation model. Two versions of
a foundation model are first trained on either raw or edge-enhanced data across
multiple medical imaging modalities, then finetuned on selected raw subsets
tailored to specific medical modalities. After systematic investigation using
the medical domains Dermoscopy, Fundus, Mammography, Microscopy, OCT, US, and
XRay, we discover both increased and reduced segmentation performance across
modalities using edge-focused pre-training, indicating the need for a selective
application of this approach. To guide such selective applications, we propose
a meta-learning strategy. It uses standard deviation and image entropy of the
raw image to choose between a model pre-trained on edge-enhanced or on raw data
for optimal performance. Our experiments show that integrating this
meta-learning layer yields an overall segmentation performance improvement
across diverse medical imaging tasks by 16.42% compared to models pre-trained
on edge-enhanced data only and 19.30% compared to models pre-trained on raw
data only.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, Third International Workshop on Data Engineering
  in Medical Imaging (DEMI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SGAD: Semantic and Geometric-aware Descriptor for Local Feature Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangzeng Liu, Chi Wang, Guanglu Shi, Xiaodong Zhang, Qiguang Miao, Miao Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local feature matching remains a fundamental challenge in computer vision.
Recent Area to Point Matching (A2PM) methods have improved matching accuracy.
However, existing research based on this framework relies on inefficient
pixel-level comparisons and complex graph matching that limit scalability. In
this work, we introduce the Semantic and Geometric-aware Descriptor Network
(SGAD), which fundamentally rethinks area-based matching by generating highly
discriminative area descriptors that enable direct matching without complex
graph optimization. This approach significantly improves both accuracy and
efficiency of area matching. We further improve the performance of area
matching through a novel supervision strategy that decomposes the area matching
task into classification and ranking subtasks. Finally, we introduce the
Hierarchical Containment Redundancy Filter (HCRF) to eliminate overlapping
areas by analyzing containment graphs. SGAD demonstrates remarkable performance
gains, reducing runtime by 60x (0.82s vs. 60.23s) compared to MESA. Extensive
evaluations show consistent improvements across multiple point matchers:
SGAD+LoFTR reduces runtime compared to DKM, while achieving higher accuracy
(0.82s vs. 1.51s, 65.98 vs. 61.11) in outdoor pose estimation, and SGAD+ROMA
delivers +7.39% AUC@5{\deg} in indoor pose estimation, establishing a new
state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Dual-Threshold Contrastive Learning for Ultrasound Image
  Classification and Segmentation <span class="chip">ECAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Zhang, Zhihui Lai, Heng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Confidence-based pseudo-label selection usually generates overly confident
yet incorrect predictions, due to the early misleadingness of model and
overfitting inaccurate pseudo-labels in the learning process, which heavily
degrades the performance of semi-supervised contrastive learning. Moreover,
segmentation and classification tasks are treated independently and the
affinity fails to be fully explored. To address these issues, we propose a
novel semi-supervised dual-threshold contrastive learning strategy for
ultrasound image classification and segmentation, named Hermes. This strategy
combines the strengths of contrastive learning with semi-supervised learning,
where the pseudo-labels assist contrastive learning by providing additional
guidance. Specifically, an inter-task attention and saliency module is also
developed to facilitate information sharing between the segmentation and
classification tasks. Furthermore, an inter-task consistency learning strategy
is designed to align tumor features across both tasks, avoiding negative
transfer for reducing features discrepancy. To solve the lack of publicly
available ultrasound datasets, we have collected the SZ-TUS dataset, a thyroid
ultrasound image dataset. Extensive experiments on two public ultrasound
datasets and one private dataset demonstrate that Hermes consistently
outperforms several state-of-the-art methods across various semi-supervised
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ECAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene
  Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Qian, Haozhi Cao, Tianchen Deng, Shenghai Yuan, Lihua Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular 3D Semantic Scene Completion (SSC) is a challenging yet promising
task that aims to infer dense geometric and semantic descriptions of a scene
from a single image. While recent object-centric paradigms significantly
improve efficiency by leveraging flexible 3D Gaussian primitives, they still
rely heavily on a large number of randomly initialized primitives, which
inevitably leads to 1) inefficient primitive initialization and 2) outlier
primitives that introduce erroneous artifacts. In this paper, we propose
SplatSSC, a novel framework that resolves these limitations with a depth-guided
initialization strategy and a principled Gaussian aggregator. Instead of random
initialization, SplatSSC utilizes a dedicated depth branch composed of a
Group-wise Multi-scale Fusion (GMF) module, which integrates multi-scale image
and depth features to generate a sparse yet representative set of initial
Gaussian primitives. To mitigate noise from outlier primitives, we develop the
Decoupled Gaussian Aggregator (DGA), which enhances robustness by decomposing
geometric and semantic predictions during the Gaussian-to-voxel splatting
process. Complemented with a specialized Probability Scale Loss, our method
achieves state-of-the-art performance on the Occ-ScanNet dataset, outperforming
prior approaches by over 6.3% in IoU and 4.1% in mIoU, while reducing both
latency and memory consumption by more than 9.3%. The code will be released
upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented
  Generation for Pathology VLMs via Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenchuan Zhang, Jingru Guo, Hengzhe Zhang, Penghao Zhang, Jie Chen, Shuwan Zhang, Zhang Zhang, Yuhao Yi, Hong Bu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Vision Language Models (VLMs) have shown strong generalization in
medical imaging, pathology presents unique challenges due to ultra-high
resolution, complex tissue structures, and nuanced clinical semantics. These
factors make pathology VLMs prone to hallucinations, i.e., generating outputs
inconsistent with visual evidence, which undermines clinical trust. Existing
RAG approaches in this domain largely depend on text-based knowledge bases,
limiting their ability to leverage diagnostic visual cues. To address this, we
propose Patho-AgenticRAG, a multimodal RAG framework with a database built on
page-level embeddings from authoritative pathology textbooks. Unlike
traditional text-only retrieval systems, it supports joint text-image search,
enabling direct retrieval of textbook pages that contain both the queried text
and relevant visual cues, thus avoiding the loss of critical image-based
information. Patho-AgenticRAG also supports reasoning, task decomposition, and
multi-turn search interactions, improving accuracy in complex diagnostic
scenarios. Experiments show that Patho-AgenticRAG significantly outperforms
existing multimodal models in complex pathology tasks like multiple-choice
diagnosis and visual question answering. Our project is available at the
Patho-AgenticRAG repository:
https://github.com/Wenchuan-Zhang/Patho-AgenticRAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Semantic Segmentation via Derivative Label Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanbin Fu, Xiaojie Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised semantic segmentation, which leverages a limited set of
labeled images, helps to relieve the heavy annotation burden. While
pseudo-labeling strategies yield promising results, there is still room for
enhancing the reliability of pseudo-labels. Hence, we develop a semi-supervised
framework, namely DerProp, equipped with a novel derivative label propagation
to rectify imperfect pseudo-labels. Our label propagation method imposes
discrete derivative operations on pixel-wise feature vectors as additional
regularization, thereby generating strictly regularized similarity metrics.
Doing so effectively alleviates the ill-posed problem that identical
similarities correspond to different features, through constraining the
solution space. Extensive experiments are conducted to verify the rationality
of our design, and demonstrate our superiority over other methods. Codes are
available at https://github.com/ForawardStar/DerProp/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal
  Entity Linking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Liu, Junwen Li, Kaiwen Li, Tong Ruan, Chao Wang, Xinyan He, Zongyu Wang, Xuezhi Cao, Jingping Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal entity linking plays a crucial role in a wide range of
applications. Recent advances in large language model-based methods have become
the dominant paradigm for this task, effectively leveraging both textual and
visual modalities to enhance performance. Despite their success, these methods
still face two challenges, including unnecessary incorporation of image data in
certain scenarios and the reliance only on a one-time extraction of visual
features, which can undermine their effectiveness and accuracy. To address
these challenges, we propose a novel LLM-based framework for the multimodal
entity linking task, called Intra- and Inter-modal Collaborative Reflections.
This framework prioritizes leveraging text information to address the task.
When text alone is insufficient to link the correct entity through intra- and
inter-modality evaluations, it employs a multi-round iterative strategy that
integrates key visual clues from various aspects of the image to support
reasoning and enhance matching accuracy. Extensive experiments on three widely
used public datasets demonstrate that our framework consistently outperforms
current state-of-the-art methods in the task, achieving improvements of 3.2%,
5.1%, and 1.6%, respectively. Our code is available at
https://github.com/ziyan-xiaoyu/I2CR/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, accepted by ACMMM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forecasting When to Forecast: Accelerating Diffusion Models with
  Confidence-Gated Taylor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoliu Guan, Lielin Jiang, Hanqi Chen, Xu Zhang, Jiaxing Yan, Guanzhong Wang, Yi Liu, Zetao Zhang, Yu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Transformers (DiTs) have demonstrated remarkable performance in
visual generation tasks. However, their low inference speed limits their
deployment in low-resource applications. Recent training-free approaches
exploit the redundancy of features across timesteps by caching and reusing past
representations to accelerate inference. Building on this idea, TaylorSeer
instead uses cached features to predict future ones via Taylor expansion.
However, its module-level prediction across all transformer blocks (e.g.,
attention or feedforward modules) requires storing fine-grained intermediate
features, leading to notable memory and computation overhead. Moreover, it
adopts a fixed caching schedule without considering the varying accuracy of
predictions across timesteps, which can lead to degraded outputs when
prediction fails. To address these limitations, we propose a novel approach to
better leverage Taylor-based acceleration. First, we shift the Taylor
prediction target from the module level to the last block level, significantly
reducing the number of cached features. Furthermore, observing strong
sequential dependencies among Transformer blocks, we propose to use the error
between the Taylor-estimated and actual outputs of the first block as an
indicator of prediction reliability. If the error is small, we trust the Taylor
prediction for the last block; otherwise, we fall back to full computation,
thereby enabling a dynamic caching mechanism. Empirical results show that our
method achieves a better balance between speed and quality, achieving a 3.17x
acceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible
quality drop. The Project Page is
\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Event-based Fast Intensity Reconstruction Scheme for UAV Real-time
  Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Dong, Yiwei Zhang, Yangjie Cui, Jinwu Xiang, Daochun Li, Zhan Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras offer significant advantages, including a wide dynamic range,
high temporal resolution, and immunity to motion blur, making them highly
promising for addressing challenging visual conditions. Extracting and
utilizing effective information from asynchronous event streams is essential
for the onboard implementation of event cameras. In this paper, we propose a
streamlined event-based intensity reconstruction scheme, event-based single
integration (ESI), to address such implementation challenges. This method
guarantees the portability of conventional frame-based vision methods to
event-based scenarios and maintains the intrinsic advantages of event cameras.
The ESI approach reconstructs intensity images by performing a single
integration of the event streams combined with an enhanced decay algorithm.
Such a method enables real-time intensity reconstruction at a high frame rate,
typically 100 FPS. Furthermore, the relatively low computation load of ESI fits
onboard implementation suitably, such as in UAV-based visual tracking
scenarios. Extensive experiments have been conducted to evaluate the
performance comparison of ESI and state-of-the-art algorithms. Compared to
state-of-the-art algorithms, ESI demonstrates remarkable runtime efficiency
improvements, superior reconstruction quality, and a high frame rate. As a
result, ESI enhances UAV onboard perception significantly under visual
adversary surroundings. In-flight tests, ESI demonstrates effective performance
for UAV onboard visual tracking under extremely low illumination
conditions(2-10lux), whereas other comparative algorithms fail due to
insufficient frame rate, poor image quality, or limited real-time performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A supplementary video is available at https://youtu.be/tLzXjXVRkVg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Welcome New Doctor: Continual Learning with Expert Consultation and
  Autoregressive Inference for Whole Slide Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Doanh Cao Bui, Jin Tae Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole Slide Image (WSI) analysis, with its ability to reveal detailed tissue
structures in magnified views, plays a crucial role in cancer diagnosis and
prognosis. Due to their giga-sized nature, WSIs require substantial storage and
computational resources for processing and training predictive models. With the
rapid increase in WSIs used in clinics and hospitals, there is a growing need
for a continual learning system that can efficiently process and adapt existing
models to new tasks without retraining or fine-tuning on previous tasks. Such a
system must balance resource efficiency with high performance. In this study,
we introduce COSFormer, a Transformer-based continual learning framework
tailored for multi-task WSI analysis. COSFormer is designed to learn
sequentially from new tasks wile avoiding the need to revisit full historical
datasets. We evaluate COSFormer on a sequence of seven WSI datasets covering
seven organs and six WSI-related tasks under both class-incremental and
task-incremental settings. The results demonstrate COSFormer's superior
generalizability and effectiveness compared to existing continual learning
frameworks, establishing it as a robust solution for continual WSI analysis in
clinical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMIC: Content-Adaptive Mamba for Learned Image Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunuo Chen, Zezheng Lyu, Bing He, Hongwei Hu, Qi Wang, Yuan Tian, Li Song, Wenjun Zhang, Guo Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Learned image compression (LIC) leverages Mamba-style state-space
models (SSMs) for global receptive fields with linear complexity. However,
vanilla Mamba is content-agnostic, relying on fixed and predefined selective
scans, which restricts its ability to dynamically and fully exploit content
dependencies. We introduce Content-Adaptive Mamba (CAM), a dynamic SSM that
addresses two critical limitations. First, it employs content-aware token
reorganization, clustering and reordering tokens based on content similarity to
prioritize proximity in feature space over Euclidean space. Second, it
integrates global priors into SSM via a prompt dictionary, effectively
mitigating the strict causality and long-range decay in the token interactions
of Mamba. These innovations enable CAM to better capture global dependencies
while preserving computational efficiency. Leveraging CAM, our Content-Adaptive
Mamba-based LIC model (CMIC) achieves state-of-the-art rate-distortion
performance, surpassing VTM-21.0 by -15.91\%, -21.34\%, and -17.58\% BD-rate on
Kodak, Tecnick, and CLIC benchmarks, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Moment Matching-Based Method for Sparse and Noisy Point Cloud
  Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyi Li, Han Zhang, Ziliang Wang, Yukai Yang, Weidong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud registration is a key step in robotic perception tasks, such as
Simultaneous Localization and Mapping (SLAM). It is especially challenging in
conditions with sparse points and heavy noise. Traditional registration
methods, such as Iterative Closest Point (ICP) and Normal Distributions
Transform (NDT), often have difficulties in achieving a robust and accurate
alignment under these conditions. In this paper, we propose a registration
framework based on moment matching. In particular, the point clouds are
regarded as i.i.d. samples drawn from the same distribution observed in the
source and target frames. We then match the generalized Gaussian Radial Basis
moments calculated from the point clouds to estimate the rigid transformation
between two frames. Moreover, such method does not require explicit
point-to-point correspondences among the point clouds. We further show the
consistency of the proposed method. Experiments on synthetic and real-world
datasets show that our approach achieves higher accuracy and robustness than
existing methods. In addition, we integrate our framework into a 4D Radar SLAM
system. The proposed method significantly improves the localization performance
and achieves results comparable to LiDAR-based systems. These findings
demonstrate the potential of moment matching technique for robust point cloud
registration in sparse and noisy scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Failure Cases Are Better Learned But Boundary Says Sorry: Facilitating
  Smooth Perception Change for Accuracy-Robustness Trade-Off in Adversarial
  Training <span class="chip">ICCV'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyun Wang, Li Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial Training (AT) is one of the most effective methods to train
robust Deep Neural Networks (DNNs). However, AT creates an inherent trade-off
between clean accuracy and adversarial robustness, which is commonly attributed
to the more complicated decision boundary caused by the insufficient learning
of hard adversarial samples. In this work, we reveal a counterintuitive fact
for the first time: From the perspective of perception consistency, hard
adversarial samples that can still attack the robust model after AT are already
learned better than those successfully defended. Thus, different from previous
views, we argue that it is rather the over-sufficient learning of hard
adversarial samples that degrades the decision boundary and contributes to the
trade-off problem. Specifically, the excessive pursuit of perception
consistency would force the model to view the perturbations as noise and ignore
the information within them, which should have been utilized to induce a
smoother perception transition towards the decision boundary to support its
establishment to an appropriate location. In response, we define a new AT
objective named Robust Perception, encouraging the model perception to change
smoothly with input perturbations, based on which we propose a novel Robust
Perception Adversarial Training (RPAT) method, effectively mitigating the
current accuracy-robustness trade-off. Experiments on CIFAR-10, CIFAR-100, and
Tiny-ImageNet with ResNet-18, PreActResNet-18, and WideResNet-34-10 demonstrate
the effectiveness of our method beyond four common baselines and 12
state-of-the-art (SOTA) works. The code is available at
https://github.com/FlaAI/RPAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE/CVF International Conference on Computer Vision (ICCV'25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-Time Model Adaptation for Quantized Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeshuai Deng, Guohao Chen, Shuaicheng Niu, Hui Luo, Shuhai Zhang, Yifan Yang, Renjie Chen, Wei Luo, Mingkui Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantizing deep models prior to deployment is a widely adopted technique to
speed up inference for various real-time applications, such as autonomous
driving. However, quantized models often suffer from severe performance
degradation in dynamic environments with potential domain shifts and this
degradation is significantly more pronounced compared with their full-precision
counterparts, as shown by our theoretical and empirical illustrations. To
address the domain shift problem, test-time adaptation (TTA) has emerged as an
effective solution by enabling models to learn adaptively from test data.
Unfortunately, existing TTA methods are often impractical for quantized models
as they typically rely on gradient backpropagation--an operation that is
unsupported on quantized models due to vanishing gradients, as well as memory
and latency constraints. In this paper, we focus on TTA for quantized models to
improve their robustness and generalization ability efficiently. We propose a
continual zeroth-order adaptation (ZOA) framework that enables efficient model
adaptation using only two forward passes, eliminating the computational burden
of existing methods. Moreover, we propose a domain knowledge management scheme
to store and reuse different domain knowledge with negligible memory
consumption, reducing the interference of different domain knowledge and
fostering the knowledge accumulation during long-term adaptation. Experimental
results on three classical architectures, including quantized transformer-based
and CNN-based models, demonstrate the superiority of our methods for quantized
model adaptation. On the quantized W6A6 ViT-B model, our ZOA is able to achieve
a 5.0\% improvement over the state-of-the-art FOA on ImageNet-C dataset. The
source code is available at https://github.com/DengZeshuai/ZOA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly Supervised Multimodal Temporal Forgery Localization via Multitask
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Xu, Wei Lu, Xiangyang Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The spread of Deepfake videos has caused a trust crisis and impaired social
stability. Although numerous approaches have been proposed to address the
challenges of Deepfake detection and localization, there is still a lack of
systematic research on the weakly supervised multimodal fine-grained temporal
forgery localization (WS-MTFL). In this paper, we propose a novel weakly
supervised multimodal temporal forgery localization via multitask learning
(WMMT), which addresses the WS-MTFL under the multitask learning paradigm. WMMT
achieves multimodal fine-grained Deepfake detection and temporal partial
forgery localization using merely video-level annotations. Specifically, visual
and audio modality detection are formulated as two binary classification tasks.
The multitask learning paradigm is introduced to integrate these tasks into a
multimodal task. Furthermore, WMMT utilizes a Mixture-of-Experts structure to
adaptively select appropriate features and localization head, achieving
excellent flexibility and localization precision in WS-MTFL. A feature
enhancement module with temporal property preserving attention mechanism is
proposed to identify the intra- and inter-modality feature deviation and
construct comprehensive video features. To further explore the temporal
information for weakly supervised learning, an extensible deviation perceiving
loss has been proposed, which aims to enlarge the deviation of adjacent
segments of the forged samples and reduce the deviation of genuine samples.
Extensive experiments demonstrate the effectiveness of multitask learning for
WS-MTFL, and the WMMT achieves comparable results to fully supervised
approaches in several evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,4 figures. arXiv admin note: text overlap with
  arXiv:2507.16596</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep classification algorithm for De-identification of DICOM medical
  images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bufano Michele, Kotter Elmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background : De-identification of DICOM (Digital Imaging and Communi-cations
in Medicine) files is an essential component of medical image research.
Personal Identifiable Information (PII) and/or Personal Health Identifying
Information (PHI) need to be hidden or removed due to legal reasons. According
to the Health Insurance Portability and Accountability Act (HIPAA) and privacy
rules, also full-face photographic images and any compa-rable images are direct
identifiers and are considered protected health information that also need to
be de-identified. Objective : The study aimed to implement a method that permit
to de-identify the PII and PHI information present in the header and burned on
the pixel data of DICOM. Methods : To execute the de-identification, we
implemented an algorithm based on the safe harbor method, defined by HIPAA. Our
algorithm uses input customizable parameter to classify and then possibly
de-identify individual DICOM tags. Results : The most sensible information,
like names, history, personal data and institution were successfully
recognized. Conclusions : We developed a python algorithm that is able to
classify infor-mation present in a DICOM file. The flexibility provided by the
use of customi-zable input parameters, which allow the user to customize the
entire process de-pending on the case (e.g., the language), makes the entire
program very promis-ing for both everyday use and research purposes. Our code
is available at https://github.com/rtdicomexplorer/deep_deidentification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GaussianCross: Cross-modal <span class="highlight-title">Self-supervised</span> 3D Representation Learning
  via Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yao, Yi Wang, Yi Zhang, Moyun Liu, Lap-Pui Chau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The significance of informative and robust point representations has been
widely acknowledged for 3D scene understanding. Despite existing
self-supervised pre-training counterparts demonstrating promising performance,
the model collapse and structural information deficiency remain prevalent due
to insufficient point discrimination difficulty, yielding unreliable
expressions and suboptimal performance. In this paper, we present
GaussianCross, a novel cross-modal self-supervised 3D representation learning
architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques
to address current challenges. GaussianCross seamlessly converts
scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian
representation without missing details, enabling stable and generalizable
pre-training. Subsequently, a tri-attribute adaptive distillation splatting
module is incorporated to construct a 3D feature field, facilitating synergetic
feature capturing of appearance, geometry, and semantic cues to maintain
cross-modal consistency. To validate GaussianCross, we perform extensive
evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In
particular, GaussianCross shows a prominent parameter and data efficiency,
achieving superior performance through linear probing (<0.1% parameters) and
limited data training (1% of scenes) compared to state-of-the-art methods.
Furthermore, GaussianCross demonstrates strong generalization capabilities,
improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on
ScanNet200 semantic and instance segmentation tasks, respectively, supporting
the effectiveness of our approach. The code, weights, and visualizations are
publicly available at
\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, accepted by MM'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ After the Party: Navigating the Mapping From Color to Ambient Lighting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florin-Alexandru Vasluianu, Tim Seizinger, Zongwei Wu, Radu Timofte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Illumination in practical scenarios is inherently complex, involving colored
light sources, occlusions, and diverse material interactions that produce
intricate reflectance and shading effects. However, existing methods often
oversimplify this challenge by assuming a single light source or uniform,
white-balanced lighting, leaving many of these complexities unaddressed.In this
paper, we introduce CL3AN, the first large-scale, high-resolution dataset of
its kind designed to facilitate the restoration of images captured under
multiple Colored Light sources to their Ambient-Normalized counterparts.
Through benchmarking, we find that leading approaches often produce artifacts,
such as illumination inconsistencies, texture leakage, and color distortion,
primarily due to their limited ability to precisely disentangle illumination
from reflectance. Motivated by this insight, we achieve such a desired
decomposition through a novel learning framework that leverages explicit
chromaticity and luminance components guidance, drawing inspiration from the
principles of the Retinex model. Extensive evaluations on existing benchmarks
and our dataset demonstrate the effectiveness of our approach, showcasing
enhanced robustness under non-homogeneous color lighting and material-specific
reflectance variations, all while maintaining a highly competitive
computational cost. The benchmark, codes, and models are available at
www.github.com/fvasluianu97/RLN2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>an 8-pages manuscript, 9 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subject or Style: Adaptive and Training-Free Mixture of LoRAs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Chen Zhang, Yu-Jie Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning models via Low-Rank Adaptation (LoRA) demonstrates remarkable
performance in subject-driven or style-driven generation tasks. Studies have
explored combinations of different LoRAs to jointly generate learned styles and
content. However, current methods struggle to balance the original subject and
style, and often require additional training. Recently, K-LoRA proposed a
training-free LoRA fusion method. But it involves multiple hyperparameters,
making it difficult to adapt to all styles and subjects. In this paper, we
propose EST-LoRA, a training-free adaptive LoRA fusion method. It
comprehensively considers three critical factors: \underline{E}nergy of matrix,
\underline{S}tyle discrepancy scores and \underline{T}ime steps. Analogous to
the Mixture of Experts (MoE) architecture, the model adaptively selects between
subject LoRA and style LoRA within each attention layer. This integrated
selection mechanism ensures balanced contributions from both components during
the generation process. Experimental results show that EST-LoRA outperforms
state-of-the-art methods in both qualitative and quantitative evaluations and
achieves faster generation speed compared to other efficient fusion approaches.
Our code is publicly available at:
https://anonymous.4open.science/r/EST-LoRA-F318.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Category-Level Object Detection and Pose Estimation from RGB
  Images using 3D Prototypes <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Fischer, Xiaojie Zhang, Eddy Ilg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing objects in images is a fundamental problem in computer vision.
Although detecting objects in 2D images is common, many applications require
determining their pose in 3D space. Traditional category-level methods rely on
RGB-D inputs, which may not always be available, or employ two-stage approaches
that use separate models and representations for detection and pose estimation.
For the first time, we introduce a unified model that integrates detection and
pose estimation into a single framework for RGB images by leveraging neural
mesh models with learned features and multi-model RANSAC. Our approach achieves
state-of-the-art results for RGB category-level pose estimation on REAL275,
improving on the current state-of-the-art by 22.9% averaged across all
scale-agnostic metrics. Finally, we demonstrate that our unified method
exhibits greater robustness compared to single-stage baselines. Our code and
models are available at
https://github.com/Fischer-Tom/unified-detection-and-pose-estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamPainter: Image Background Inpainting for E-commerce Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijie Zhao, Jing Cheng, Yaoyao Wu, Hao Xu, Shaohui Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although diffusion-based image genenation has been widely explored and
applied, background generation tasks in e-commerce scenarios still face
significant challenges. The first challenge is to ensure that the generated
products are consistent with the given product inputs while maintaining a
reasonable spatial arrangement, harmonious shadows, and reflections between
foreground products and backgrounds. Existing inpainting methods fail to
address this due to the lack of domain-specific data. The second challenge
involves the limitation of relying solely on text prompts for image control, as
effective integrating visual information to achieve precise control in
inpainting tasks remains underexplored. To address these challenges, we
introduce DreamEcom-400K, a high-quality e-commerce dataset containing accurate
product instance masks, background reference images, text prompts, and
aesthetically pleasing product images. Based on this dataset, we propose
DreamPainter, a novel framework that not only utilizes text prompts for control
but also flexibly incorporates reference image information as an additional
control signal. Extensive experiments demonstrate that our approach
significantly outperforms state-of-the-art methods, maintaining high product
consistency while effectively integrating both text prompt and reference image
information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Chambolle-Pock based algorithms for Convoltional sparse
  representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Liu, Junjing Li, Yang Chen, Haowei Tang, Pengcheng Zhang, Tianling Lyu, Zhiguo Gui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently convolutional sparse representation (CSR), as a sparse
representation technique, has attracted increasing attention in the field of
image processing, due to its good characteristic of translate-invariance. The
content of CSR usually consists of convolutional sparse coding (CSC) and
convolutional dictionary learning (CDL), and many studies focus on how to solve
the corresponding optimization problems. At present, the most efficient
optimization scheme for CSC is based on the alternating direction method of
multipliers (ADMM). However, the ADMM-based approach involves a penalty
parameter that needs to be carefully selected, and improper parameter selection
may result in either no convergence or very slow convergence. In this paper, a
novel fast and efficient method using Chambolle-Pock(CP) framework is proposed,
which does not require extra manual selection parameters in solving processing,
and has faster convergence speed. Furthermore, we propose an anisotropic total
variation penalty of the coefficient maps for CSC and apply the CP algorithm to
solve it. In addition, we also apply the CP framework to solve the
corresponding CDL problem. Experiments show that for noise-free image the
proposed CSC algorithms can achieve rival results of the latest ADMM-based
approach, while outperforms in removing noise from Gaussian noise pollution
image.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AttriCtrl: Fine-Grained Control of Aesthetic Attribute Intensity in
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Die Chen, Zhongjie Duan, Zhiwen Li, Cen Chen, Daoyuan Chen, Yaliang Li, Yinda Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in text-to-image diffusion models have significantly
enhanced both the visual fidelity and semantic controllability of generated
images. However, fine-grained control over aesthetic attributes remains
challenging, especially when users require continuous and intensity-specific
adjustments. Existing approaches often rely on vague textual prompts, which are
inherently ambiguous in expressing both the aesthetic semantics and the desired
intensity, or depend on costly human preference data for alignment, limiting
their scalability and practicality. To address these limitations, we propose
AttriCtrl, a plug-and-play framework for precise and continuous control of
aesthetic attributes. Specifically, we quantify abstract aesthetics by
leveraging semantic similarity from pre-trained vision-language models, and
employ a lightweight value encoder that maps scalar intensities in $[0,1]$ to
learnable embeddings within diffusion-based generation. This design enables
intuitive and customizable aesthetic manipulation, with minimal training
overhead and seamless integration into existing generation pipelines. Extensive
experiments demonstrate that AttriCtrl achieves accurate control over
individual attributes as well as flexible multi-attribute composition.
Moreover, it is fully compatible with popular open-source controllable
generation frameworks, showcasing strong integration capability and practical
utility across diverse generation scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AURORA: Augmented Understanding via Structured Reasoning and
  Reinforcement Learning for Reference Audio-Visual Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Luo, Nian Liu, Fahad Shahbaz Khan, Junwei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reference Audio-Visual Segmentation (Ref-AVS) tasks challenge models to
precisely locate sounding objects by integrating visual, auditory, and textual
cues. Existing methods often lack genuine semantic understanding, tending to
memorize fixed reasoning patterns. Furthermore, jointly training for reasoning
and segmentation can compromise pixel-level precision. To address these issues,
we introduce AURORA, a novel framework designed to enhance genuine reasoning
and language comprehension in reference audio-visual segmentation. We employ a
structured Chain-of-Thought (CoT) prompting mechanism to guide the model
through a step-by-step reasoning process and introduce a novel segmentation
feature distillation loss to effectively integrate these reasoning abilities
without sacrificing segmentation performance. To further cultivate the model's
genuine reasoning capabilities, we devise a further two-stage training
strategy: first, a ``corrective reflective-style training" stage utilizes
self-correction to enhance the quality of reasoning paths, followed by
reinforcement learning via Group Reward Policy Optimization (GRPO) to bolster
robustness in challenging scenarios. Experiments demonstrate that AURORA
achieves state-of-the-art performance on Ref-AVS benchmarks and generalizes
effectively to unreferenced segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScrewSplat: An End-to-End Method for Articulated Object Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungyeon Kim, Junsu Ha, Young Hun Kim, Yonghyeon Lee, Frank C. Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Articulated object recognition -- the task of identifying both the geometry
and kinematic joints of objects with movable parts -- is essential for enabling
robots to interact with everyday objects such as doors and laptops. However,
existing approaches often rely on strong assumptions, such as a known number of
articulated parts; require additional inputs, such as depth images; or involve
complex intermediate steps that can introduce potential errors -- limiting
their practicality in real-world settings. In this paper, we introduce
ScrewSplat, a simple end-to-end method that operates solely on RGB
observations. Our approach begins by randomly initializing screw axes, which
are then iteratively optimized to recover the object's underlying kinematic
structure. By integrating with Gaussian Splatting, we simultaneously
reconstruct the 3D geometry and segment the object into rigid, movable parts.
We demonstrate that our method achieves state-of-the-art recognition accuracy
across a diverse set of articulated objects, and further enables zero-shot,
text-guided manipulation using the recovered kinematic model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 12 figures, Conference on Robot Learning (CoRL) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TrackletGait: A Robust Framework for Gait Recognition in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoxiong Zhang, Jinkai Zheng, Shangdong Zhu, Chenggang Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gait recognition aims to identify individuals based on their body shape and
walking patterns. Though much progress has been achieved driven by deep
learning, gait recognition in real-world surveillance scenarios remains quite
challenging to current methods. Conventional approaches, which rely on periodic
gait cycles and controlled environments, struggle with the non-periodic and
occluded silhouette sequences encountered in the wild. In this paper, we
propose a novel framework, TrackletGait, designed to address these challenges
in the wild. We propose Random Tracklet Sampling, a generalization of existing
sampling methods, which strikes a balance between robustness and representation
in capturing diverse walking patterns. Next, we introduce Haar Wavelet-based
Downsampling to preserve information during spatial downsampling. Finally, we
present a Hardness Exclusion Triplet Loss, designed to exclude low-quality
silhouettes by discarding hard triplet samples. TrackletGait achieves
state-of-the-art results, with 77.8 and 80.4 rank-1 accuracy on the Gait3D and
GREW datasets, respectively, while using only 10.3M backbone parameters.
Extensive experiments are also conducted to further investigate the factors
affecting gait recognition in the wild.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AID4AD: Aerial Image Data for Automated Driving Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Lengerer, Mathias Pechinger, Klaus Bogenberger, Carsten Markgraf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates the integration of spatially aligned aerial imagery
into perception tasks for automated vehicles (AVs). As a central contribution,
we present AID4AD, a publicly available dataset that augments the nuScenes
dataset with high-resolution aerial imagery precisely aligned to its local
coordinate system. The alignment is performed using SLAM-based point cloud maps
provided by nuScenes, establishing a direct link between aerial data and
nuScenes local coordinate system. To ensure spatial fidelity, we propose an
alignment workflow that corrects for localization and projection distortions. A
manual quality control process further refines the dataset by identifying a set
of high-quality alignments, which we publish as ground truth to support future
research on automated registration. We demonstrate the practical value of
AID4AD in two representative tasks: in online map construction, aerial imagery
serves as a complementary input that improves the mapping process; in motion
prediction, it functions as a structured environmental representation that
replaces high-definition maps. Experiments show that aerial imagery leads to a
15-23% improvement in map construction accuracy and a 2% gain in trajectory
prediction performance. These results highlight the potential of aerial imagery
as a scalable and adaptable source of environmental context in automated
vehicle systems, particularly in scenarios where high-definition maps are
unavailable, outdated, or costly to maintain. AID4AD, along with evaluation
code and pretrained models, is publicly released to foster further research in
this direction: https://github.com/DriverlessMobility/AID4AD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Free-MoRef: Instantly Multiplexing Context Perception Capabilities of
  Video-MLLMs within Single Inference <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuo Wang, Quanlong Zheng, Junlin Xie, Yanhao Zhang, Jinguo Luo, Haonan Lu, Liang Lin, Fan Zhou, Guanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Multimodal Large Language Models~(Video-MLLM) have achieved remarkable
advancements in video understanding tasks. However, constrained by the context
length limitation in the underlying LLMs, existing Video-MLLMs typically
exhibit suboptimal performance on long video scenarios. To understand extended
input frames, common solutions span token compression and streaming inference
techniques, which sacrifice feature granularity or inference efficiency.
Differently, to efficiently achieve comprehensive understanding of longer frame
inputs, we draw ideas from MoE and propose a training-free approach
\textbf{Free-MoRef}, which instantly multiplexes the context perception
capabilities of Video-MLLMs within one inference pass. Specifically, Free-MoRef
reconstructs the vision tokens into several short sequences as
multi-references. Subsequently, we introduce MoRef-attention, which gathers
clues from the multi-reference chunks in parallel to summarize unified query
activations. After the shadow layers in LLMs, a reference fusion step is
derived to compose a final mixed reasoning sequence with key tokens from
parallel chunks, which compensates the cross-reference vision interactions that
are neglected in MoRef-attention. By splitting and fusing the long vision token
sequences, Free-MoRef achieves improved performance under much lower computing
costs in reasoning multiplexed context length, demonstrating strong efficiency
and effectiveness. Experiments on VideoMME, MLVU, LongVideoBench show that
Free-MoRef achieves full perception of 2$\times$ to 8$\times$ longer input
frames without compression on a single A100 GPU while keeping instant
responses, thereby bringing significant performance gains, even surpassing
dedicatedly trained long-video-MLLMs. Codes are available at
https://github.com/wkfdb/Free-MoRef
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published in ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Neural Quality Metric for BRDF Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behnaz Kavoosighafi, Rafal K. Mantiuk, Saghi Hajisharif, Ehsan Miandji, Jonas Unger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately evaluating the quality of bidirectional reflectance distribution
function (BRDF) models is essential for photo-realistic rendering. Traditional
BRDF-space metrics often employ numerical error measures that fail to capture
perceptual differences evident in rendered images. In this paper, we introduce
the first perceptually informed neural quality metric for BRDF evaluation that
operates directly in BRDF space, eliminating the need for rendering during
quality assessment. Our metric is implemented as a compact multi-layer
perceptron (MLP), trained on a dataset of measured BRDFs supplemented with
synthetically generated data and labelled using a perceptually validated
image-space metric. The network takes as input paired samples of reference and
approximated BRDFs and predicts their perceptual quality in terms of
just-objectionable-difference (JOD) scores. We show that our neural metric
achieves significantly higher correlation with human judgments than existing
BRDF-space metrics. While its performance as a loss function for BRDF fitting
remains limited, the proposed metric offers a perceptually grounded alternative
for evaluating BRDF models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic
  Urban Scenes Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuru Xiao, Zihan Lin, Chao Lu, Deming Zhai, Kui Jiang, Wenbo Zhao, Wei Zhang, Junjun Jiang, Huanran Wang, Xianming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic urban scene modeling is a rapidly evolving area with broad
applications. While current approaches leveraging neural radiance fields or
Gaussian Splatting have achieved fine-grained reconstruction and high-fidelity
novel view synthesis, they still face significant limitations. These often stem
from a dependence on pre-calibrated object tracks or difficulties in accurately
modeling fast-moving objects from undersampled capture, particularly due to
challenges in handling temporal discontinuities. To overcome these issues, we
propose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our
key insight is to distill robust, temporally consistent priors from a test-time
adapted video diffusion model. To ensure precise pose alignment and effective
integration of this denoised content, we introduce two core innovations: a
joint timestamp optimization strategy that refines interpolated frame poses,
and an uncertainty distillation method that adaptively extracts target content
while preserving well-reconstructed regions. Extensive experiments demonstrate
that our method significantly enhances dynamic modeling, especially for
fast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view
synthesis over baseline approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond RGB and Events: Enhancing Object Detection under Adverse Lighting
  with Monocular Normal Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingjie Liu, Hanqing Liu, Chuang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate object detection under adverse lighting conditions is critical for
real-world applications such as autonomous driving. Although neuromorphic event
cameras have been introduced to handle these scenarios, adverse lighting often
induces distracting reflections from tunnel walls or road surfaces, which
frequently lead to false obstacle detections. However, neither RGB nor event
data alone is robust enough to address these complexities, and mitigating these
issues without additional sensors remains underexplored. To overcome these
challenges, we propose leveraging normal maps, directly predicted from
monocular RGB images, as robust geometric cues to suppress false positives and
enhance detection accuracy. We introduce NRE-Net, a novel multi-modal detection
framework that effectively fuses three complementary modalities: monocularly
predicted surface normal maps, RGB images, and event streams. To optimize the
fusion process, our framework incorporates two key modules: the Adaptive
Dual-stream Fusion Module (ADFM), which integrates RGB and normal map features,
and the Event-modality Aware Fusion Module (EAFM), which adapts to the high
dynamic range characteristics of event data. Extensive evaluations on the
DSEC-Det-sub and PKU-DAVIS-SOD datasets demonstrate that NRE-Net significantly
outperforms state-of-the-art methods. Our approach achieves mAP50 improvements
of 7.9% and 6.1% over frame-based approaches (e.g., YOLOX), while surpassing
the fusion-based SFNet by 2.7% on the DSEC-Det-sub dataset and SODFormer by
7.1% on the PKU-DAVIS-SOD dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeflareMamba: Hierarchical Vision Mamba for Contextually Consistent Lens
  Flare Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihang Huang, Yuanfei Huang, Junhui Lin, Hua Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lens flare removal remains an information confusion challenge in the
underlying image background and the optical flares, due to the complex optical
interactions between light sources and camera lens. While recent solutions have
shown promise in decoupling the flare corruption from image, they often fail to
maintain contextual consistency, leading to incomplete and inconsistent flare
removal. To eliminate this limitation, we propose DeflareMamba, which leverages
the efficient sequence modeling capabilities of state space models while
maintains the ability to capture local-global dependencies. Particularly, we
design a hierarchical framework that establishes long-range pixel correlations
through varied stride sampling patterns, and utilize local-enhanced state space
models that simultaneously preserves local details. To the best of our
knowledge, this is the first work that introduces state space models to the
flare removal task. Extensive experiments demonstrate that our method
effectively removes various types of flare artifacts, including scattering and
reflective flares, while maintaining the natural appearance of non-flare
regions. Further downstream applications demonstrate the capacity of our method
to improve visual object recognition and cross-modal semantic understanding.
Code is available at https://github.com/BNU-ERC-ITEA/DeflareMamba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACMMM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tackling Ill-posedness of Reversible Image Conversion with Well-posed
  Invertible Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanfei Huang, Hua Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reversible image conversion (RIC) suffers from ill-posedness issues due to
its forward conversion process being considered an underdetermined system.
Despite employing invertible neural networks (INN), existing RIC methods
intrinsically remain ill-posed as inevitably introducing uncertainty by
incorporating randomly sampled variables. To tackle the ill-posedness dilemma,
we focus on developing a reliable approximate left inverse for the
underdetermined system by constructing an overdetermined system with a non-zero
Gram determinant, thus ensuring a well-posed solution. Based on this principle,
we propose a well-posed invertible $1\times1$ convolution (WIC), which
eliminates the reliance on random variable sampling and enables the development
of well-posed invertible networks. Furthermore, we design two innovative
networks, WIN-Na\"ive and WIN, with the latter incorporating advanced
skip-connections to enhance long-term memory. Our methods are evaluated across
diverse RIC tasks, including reversible image hiding, image rescaling, and
image decolorization, consistently achieving state-of-the-art performance.
Extensive experiments validate the effectiveness of our approach, demonstrating
its ability to overcome the bottlenecks of existing RIC solutions and setting a
new benchmark in the field. Codes are available in
https://github.com/BNU-ERC-ITEA/WIN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoLoRA: Automatic LoRA Retrieval and Fine-Grained Gated Fusion for
  Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwen Li, Zhongjie Duan, Die Chen, Cen Chen, Daoyuan Chen, Yaliang Li, Yingda Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in photorealistic image generation through
large-scale models like FLUX and Stable Diffusion v3, the practical deployment
of these architectures remains constrained by their inherent intractability to
parameter fine-tuning. While low-rank adaptation (LoRA) have demonstrated
efficacy in enabling model customization with minimal parameter overhead, the
effective utilization of distributed open-source LoRA modules faces three
critical challenges: sparse metadata annotation, the requirement for zero-shot
adaptation capabilities, and suboptimal fusion strategies for multi-LoRA fusion
strategies. To address these limitations, we introduce a novel framework that
enables semantic-driven LoRA retrieval and dynamic aggregation through two key
components: (1) weight encoding-base LoRA retriever that establishes a shared
semantic space between LoRA parameter matrices and text prompts, eliminating
dependence on original training data, and (2) fine-grained gated fusion
mechanism that computes context-specific fusion weights across network layers
and diffusion timesteps to optimally integrate multiple LoRA modules during
generation. Our approach achieves significant improvement in image generation
perfermance, thereby facilitating scalable and data-efficient enhancement of
foundational models. This work establishes a critical bridge between the
fragmented landscape of community-developed LoRAs and practical deployment
requirements, enabling collaborative model evolution through standardized
adapter integration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Immersive Human-X Interaction: A Real-Time Framework for
  Physically Plausible Motion Synthesis <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyang Ji, Ye Shi, Zichen Jin, Kangyi Chen, Lan Xu, Yuexin Ma, Jingyi Yu, Jingya Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time synthesis of physically plausible human interactions remains a
critical challenge for immersive VR/AR systems and humanoid robotics. While
existing methods demonstrate progress in kinematic motion generation, they
often fail to address the fundamental tension between real-time responsiveness,
physical feasibility, and safety requirements in dynamic human-machine
interactions. We introduce Human-X, a novel framework designed to enable
immersive and physically plausible human interactions across diverse entities,
including human-avatar, human-humanoid, and human-robot systems. Unlike
existing approaches that focus on post-hoc alignment or simplified physics, our
method jointly predicts actions and reactions in real-time using an
auto-regressive reaction diffusion planner, ensuring seamless synchronization
and context-aware responses. To enhance physical realism and safety, we
integrate an actor-aware motion tracking policy trained with reinforcement
learning, which dynamically adapts to interaction partners' movements while
avoiding artifacts like foot sliding and penetration. Extensive experiments on
the Inter-X and InterHuman datasets demonstrate significant improvements in
motion quality, interaction continuity, and physical plausibility over
state-of-the-art methods. Our framework is validated in real-world
applications, including virtual reality interface for human-robot interaction,
showcasing its potential for advancing human-robot collaboration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REACT-KD: Region-Aware Cross-modal Topological Knowledge Distillation
  for Interpretable Medical Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhao Chen, Hexiao Ding, Yufeng Jiang, Jing Lan, Ka Chun Li, Gerald W. Y. Cheng, Sam Ng, Chi Lai Ho, Jing Cai, Liang-ting Lin, Jung Sun Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable and interpretable tumor classification from clinical imaging remains
a core challenge due to heterogeneous modality quality, limited annotations,
and the lack of structured anatomical guidance. We introduce REACT-KD, a
Region-Aware Cross-modal Topological Knowledge Distillation framework that
transfers rich supervision from high-fidelity multi-modal sources into a
lightweight CT-based student model. The framework uses a dual teacher design:
one branch captures structure-function relationships using dual-tracer PET/CT,
and the other models dose-aware features through synthetically degraded
low-dose CT data. These branches jointly guide the student model through two
complementary objectives. The first focuses on semantic alignment via logits
distillation, while the second models anatomical topology using region graph
distillation. A shared CBAM-3D module is employed to maintain consistent
attention across modalities. To improve reliability for deployment, REACT-KD
introduces modality dropout during training, allowing inference under partial
or noisy inputs. The staging task for hepatocellular carcinoma (HCC) is
conducted as a case study. REACT-KD achieves an average AUC of 93.4% on an
internal PET/CT cohort and maintains 76.6% to 81.5% AUC across varying dose
levels in external CT testing. Decision curve analysis shows that REACT-KD
consistently provides the highest clinical benefit across decision thresholds,
supporting its potential in real-world diagnostics. Code is available at
https://github.com/Kinetics-JOJO/REACT-KD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLM4D: Towards Spatiotemporal Awareness in Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Zhou, Alexander Vilesov, Xuehai He, Ziyu Wan, Shuwang Zhang, Aditya Nagachandra, Di Chang, Dongdong Chen, Xin Eric Wang, Achuta Kadambi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision language models (VLMs) have shown remarkable capabilities in
integrating linguistic and visual reasoning but remain fundamentally limited in
understanding dynamic spatiotemporal interactions. Humans effortlessly track
and reason about object movements, rotations, and perspective shifts-abilities
essential for robust dynamic real-world understanding yet notably lacking in
current VLMs. In this paper, we introduce VLM4D, the first benchmark
specifically designed to evaluate the spatiotemporal reasoning capabilities of
VLMs. Our benchmark comprises diverse real-world and synthetic videos
accompanied by carefully curated question-answer pairs emphasizing
translational and rotational motions, perspective awareness, and motion
continuity. Through comprehensive evaluations of state-of-the-art open and
closed-source VLMs, we identify significant performance gaps compared to human
baselines, highlighting fundamental deficiencies in existing models. Extensive
analysis reveals that VLMs struggle particularly with integrating multiple
visual cues and maintaining temporal coherence. We further explore promising
directions, such as leveraging 4D feature field reconstruction and targeted
spatiotemporal supervised fine-tuning, demonstrating their effectiveness in
enhancing spatiotemporal comprehension. Our work aims to encourage deeper
exploration into improving VLMs' spatial and temporal grounding, paving the way
towards more capable and reliable visual intelligence for dynamic environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S-RRG-Bench: Structured Radiology Report Generation with Fine-Grained
  Evaluation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lingqiao Liu, Lei Wang, Luping Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiology report generation (RRG) for diagnostic images, such as chest
X-rays, plays a pivotal role in both clinical practice and AI. Traditional
free-text reports suffer from redundancy and inconsistent language,
complicating the extraction of critical clinical details. Structured radiology
report generation (S-RRG) offers a promising solution by organizing information
into standardized, concise formats. However, existing approaches often rely on
classification or visual question answering (VQA) pipelines that require
predefined label sets and produce only fragmented outputs. Template-based
approaches, which generate reports by replacing keywords within fixed sentence
patterns, further compromise expressiveness and often omit clinically important
details. In this work, we present a novel approach to S-RRG that includes
dataset construction, model training, and the introduction of a new evaluation
framework. We first create a robust chest X-ray dataset (MIMIC-STRUC) that
includes disease names, severity levels, probabilities, and anatomical
locations, ensuring that the dataset is both clinically relevant and
well-structured. We train an LLM-based model to generate standardized,
high-quality reports. To assess the generated reports, we propose a specialized
evaluation metric (S-Score) that not only measures disease prediction accuracy
but also evaluates the precision of disease-specific details, thus offering a
clinically meaningful metric for report quality that focuses on elements
critical to clinical decision-making and demonstrates a stronger alignment with
human assessments. Our approach highlights the effectiveness of structured
reports and the importance of a tailored evaluation metric for S-RRG, providing
a more clinically relevant measure of report quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YOLOv1 to YOLOv11: A Comprehensive <span class="highlight-title">Survey</span> of Real-Time Object Detection
  Innovations and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manikanta Kotthapalli, Deepika Ravipati, Reshma Bhatia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past decade, object detection has advanced significantly, with the
YOLO (You Only Look Once) family of models transforming the landscape of
real-time vision applications through unified, end-to-end detection frameworks.
From YOLOv1's pioneering regression-based detection to the latest YOLOv9, each
version has systematically enhanced the balance between speed, accuracy, and
deployment efficiency through continuous architectural and algorithmic
advancements.. Beyond core object detection, modern YOLO architectures have
expanded to support tasks such as instance segmentation, pose estimation,
object tracking, and domain-specific applications including medical imaging and
industrial automation. This paper offers a comprehensive review of the YOLO
family, highlighting architectural innovations, performance benchmarks,
extended capabilities, and real-world use cases. We critically analyze the
evolution of YOLO models and discuss emerging research directions that extend
their impact across diverse computer vision domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StarPose: 3D Human Pose Estimation via Spatial-Temporal Autoregressive
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxin Yang, Weihong Chen, Xuemiao Xu, Cheng Xu, Peng Xiao, Cuifeng Sun, Shaoyu Huang, Shengfeng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular 3D human pose estimation remains a challenging task due to inherent
depth ambiguities and occlusions. Compared to traditional methods based on
Transformers or Convolutional Neural Networks (CNNs), recent diffusion-based
approaches have shown superior performance, leveraging their probabilistic
nature and high-fidelity generation capabilities. However, these methods often
fail to account for the spatial and temporal correlations across predicted
frames, resulting in limited temporal consistency and inferior accuracy in
predicted 3D pose sequences. To address these shortcomings, this paper proposes
StarPose, an autoregressive diffusion framework that effectively incorporates
historical 3D pose predictions and spatial-temporal physical guidance to
significantly enhance both the accuracy and temporal coherence of pose
predictions. Unlike existing approaches, StarPose models the 2D-to-3D pose
mapping as an autoregressive diffusion process. By synergically integrating
previously predicted 3D poses with 2D pose inputs via a Historical Pose
Integration Module (HPIM), the framework generates rich and informative
historical pose embeddings that guide subsequent denoising steps, ensuring
temporally consistent predictions. In addition, a fully plug-and-play
Spatial-Temporal Physical Guidance (STPG) mechanism is tailored to refine the
denoising process in an iterative manner, which further enforces spatial
anatomical plausibility and temporal motion dynamics, rendering robust and
realistic pose estimates. Extensive experiments on benchmark datasets
demonstrate that StarPose outperforms state-of-the-art methods, achieving
superior accuracy and temporal consistency in 3D human pose estimation. Code is
available at https://github.com/wileychan/StarPose.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HCF: Hierarchical Cascade Framework for Distributed Multi-Stage Image
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Cai, Taegun An, Chengjun Jin, Sung Il Choi, JuHyun Park, Changhee Joo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed multi-stage image compression -- where visual content traverses
multiple processing nodes under varying quality requirements -- poses
challenges. Progressive methods enable bitstream truncation but underutilize
available compute resources; successive compression repeats costly pixel-domain
operations and suffers cumulative quality loss and inefficiency;
fixed-parameter models lack post-encoding flexibility. In this work, we
developed the Hierarchical Cascade Framework (HCF) that achieves high
rate-distortion performance and better computational efficiency through direct
latent-space transformations across network nodes in distributed multi-stage
image compression system. Under HCF, we introduced policy-driven quantization
control to optimize rate-distortion trade-offs, and established the edge
quantization principle through differential entropy analysis. The configuration
based on this principle demonstrates up to 0.6dB PSNR gains over other
configurations. When comprehensively evaluated on the Kodak, CLIC, and
CLIC2020-mobile datasets, HCF outperforms successive-compression methods by up
to 5.56% BD-Rate in PSNR on CLIC, while saving up to 97.8% FLOPs, 96.5% GPU
memory, and 90.0% execution time. It also outperforms state-of-the-art
progressive compression methods by up to 12.64% BD-Rate on Kodak and enables
retraining-free cross-quality adaptation with 7.13-10.87% BD-Rate reductions on
CLIC2020-mobile.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapillary Vistas Validation for Fine-Grained Traffic Signs: A Benchmark
  Revealing Vision-Language Model Limitations <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sparsh Garg, Abhishek Aich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obtaining high-quality fine-grained annotations for traffic signs is critical
for accurate and safe decision-making in autonomous driving. Widely used
datasets, such as Mapillary, often provide only coarse-grained labels - without
distinguishing semantically important types such as stop signs or speed limit
signs. To this end, we present a new validation set for traffic signs derived
from the Mapillary dataset called Mapillary Vistas Validation for Traffic Signs
(MVV), where we decompose composite traffic signs into granular, semantically
meaningful categories. The dataset includes pixel-level instance masks and has
been manually annotated by expert annotators to ensure label fidelity. Further,
we benchmark several state-of-the-art VLMs against the self-supervised DINOv2
model on this dataset and show that DINOv2 consistently outperforms all VLM
baselines-not only on traffic sign recognition, but also on heavily represented
categories like vehicles and humans. Our analysis reveals significant
limitations in current vision-language models for fine-grained visual
understanding and establishes DINOv2 as a strong baseline for dense semantic
matching in autonomous driving scenarios. This dataset and evaluation framework
pave the way for more reliable, interpretable, and scalable perception systems.
  Code and data are available at: https://github.com/nec-labs-ma/relabeling
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2025 Workshop (4th DataCV Workshop and Challenge)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Diffusion Model with Anatomical-Dose Dual Constraints for
  End-to-End Multi-Tumor Dose Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Xie, Haiqin Hu, Lijuan Ding, Qing Li, Yue Sun, Tao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiotherapy treatment planning often relies on time-consuming,
trial-and-error adjustments that heavily depend on the expertise of
specialists, while existing deep learning methods face limitations in
generalization, prediction accuracy, and clinical applicability. To tackle
these challenges, we propose ADDiff-Dose, an Anatomical-Dose Dual Constraints
Conditional Diffusion Model for end-to-end multi-tumor dose prediction. The
model employs LightweightVAE3D to compress high-dimensional CT data and
integrates multimodal inputs, including target and organ-at-risk (OAR) masks
and beam parameters, within a progressive noise addition and denoising
framework. It incorporates conditional features via a multi-head attention
mechanism and utilizes a composite loss function combining MSE, conditional
terms, and KL divergence to ensure both dosimetric accuracy and compliance with
clinical constraints. Evaluation on a large-scale public dataset (2,877 cases)
and three external institutional cohorts (450 cases in total) demonstrates that
ADDiff-Dose significantly outperforms traditional baselines, achieving an MAE
of 0.101-0.154 (compared to 0.316 for UNet and 0.169 for GAN models), a DICE
coefficient of 0.927 (a 6.8% improvement), and limiting spinal cord maximum
dose error to within 0.1 Gy. The average plan generation time per case is
reduced to 22 seconds. Ablation studies confirm that the structural encoder
enhances compliance with clinical dose constraints by 28.5%. To our knowledge,
this is the first study to introduce a conditional diffusion model framework
for radiotherapy dose prediction, offering a generalizable and efficient
solution for automated treatment planning across diverse tumor sites, with the
potential to substantially reduce planning time and improve clinical workflow
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ActAlign: Zero-Shot Fine-Grained Video Classification via
  Language-Guided Sequence Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.22967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.22967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Aghdam, Vincent Tao Hu, Björn Ommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the task of zero-shot video classification for extremely
fine-grained actions (e.g., Windmill Dunk in basketball), where no video
examples or temporal annotations are available for unseen classes. While
image-language models (e.g., CLIP, SigLIP) show strong open-set recognition,
they lack temporal modeling needed for video understanding. We propose
ActAlign, a truly zero-shot, training-free method that formulates video
classification as a sequence alignment problem, preserving the generalization
strength of pretrained image-language models. For each class, a large language
model (LLM) generates an ordered sequence of sub-actions, which we align with
video frames using Dynamic Time Warping (DTW) in a shared embedding space.
Without any video-text supervision or fine-tuning, ActAlign achieves 30.5%
accuracy on ActionAtlas--the most diverse benchmark of fine-grained actions
across multiple sports--where human performance is only 61.6%. ActAlign
outperforms billion-parameter video-language models while using 8x fewer
parameters. Our approach is model-agnostic and domain-general, demonstrating
that structured language priors combined with classical alignment methods can
unlock the open-set recognition potential of image-language models for
fine-grained video understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint manuscript - Project page:
  https://amir-aghdam.github.io/act-align/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Subjective Camera 0.1: Bridging Human Cognition and Visual
  Reconstruction through Sequence-Aware Sketch-Guided Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.23711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.23711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyang Chen, Dongfang Sun, Caoyuan Ma, Shiqin Wang, Kewei Zhang, Zheng Wang, Zhixiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the concept of a subjective camera to reconstruct meaningful
moments that physical cameras fail to capture. We propose Subjective Camera
0.1, a framework for reconstructing real-world scenes from readily accessible
subjective readouts, i.e., textual descriptions and progressively drawn rough
sketches. Built on optimization-based alignment of diffusion models, our
approach avoids large-scale paired training data and mitigates generalization
issues. To address the challenge of integrating multiple abstract concepts in
real-world scenarios, we design a Sequence-Aware Sketch-Guided Diffusion
framework with three loss terms for concept-wise sequential optimization,
following the natural order of subjective readouts. Experiments on two datasets
demonstrate that our method achieves state-of-the-art performance in image
quality as well as spatial and semantic alignment with target scenes. User
studies with 40 participants further confirm that our approach is consistently
preferred.Our project page is at: subjective-camera.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Image Capture for Computer Vision-Powered Taxonomic
  Identification and Trait Recognition of Biodiversity Specimens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alyson East, Elizabeth G. Campolongo, Luke Meyers, S M Rayeed, Samuel Stevens, Iuliia Zarubiieva, Isadora E. Fluck, Jennifer C. Girón, Maximiliane Jousse, Scott Lowe, Kayla I Perry, Isabelle Betancourt, Noah Charney, Evan Donoso, Nathan Fox, Kim J. Landsbergen, Ekaterina Nepovinnykh, Michelle Ramirez, Parkash Singh, Khum Thapa-Magar, Matthew Thompson, Evan Waite, Tanya Berger-Wolf, Hilmar Lapp, Paula Mabee, Charles Stewart, Graham Taylor, Sydne Record
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  1) Biological collections house millions of specimens with digital images
increasingly available through open-access platforms. However, most imaging
protocols were developed for human interpretation without considering automated
analysis requirements. As computer vision applications revolutionize taxonomic
identification and trait extraction, a critical gap exists between current
digitization practices and computational analysis needs. This review provides
the first comprehensive practical framework for optimizing biological specimen
imaging for computer vision applications. 2) Through interdisciplinary
collaboration between taxonomists, collection managers, ecologists, and
computer scientists, we synthesized evidence-based recommendations addressing
fundamental computer vision concepts and practical imaging considerations. We
provide immediately actionable implementation guidance while identifying
critical areas requiring community standards development. 3) Our framework
encompasses ten interconnected considerations for optimizing image capture for
computer vision-powered taxonomic identification and trait extraction. We
translate these into practical implementation checklists, equipment selection
guidelines, and a roadmap for community standards development including
filename conventions, pixel density requirements, and cross-institutional
protocols. 4)By bridging biological and computational disciplines, this
approach unlocks automated analysis potential for millions of existing
specimens and guides future digitization efforts toward unprecedented
analytical capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A novel approach to navigate the taxonomic hierarchy to address the
  Open-World Scenarios in Medicinal Plant Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17289v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17289v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumen Sinha, Tanisha Rana, Susmita Ghosh, Rahul Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we propose a novel approach for plant hierarchical taxonomy
classification by posing the problem as an open class problem. It is observed
that existing methods for medicinal plant classification often fail to perform
hierarchical classification and accurately identifying unknown species,
limiting their effectiveness in comprehensive plant taxonomy classification.
Thus we address the problem of unknown species classification by assigning it
best hierarchical labels. We propose a novel method, which integrates
DenseNet121, Multi-Scale Self-Attention (MSSA) and cascaded classifiers for
hierarchical classification. The approach systematically categorizes medicinal
plants at multiple taxonomic levels, from phylum to species, ensuring detailed
and precise classification. Using multi scale space attention, the model
captures both local and global contextual information from the images,
improving the distinction between similar species and the identification of new
ones. It uses attention scores to focus on important features across multiple
scales. The proposed method provides a solution for hierarchical
classification, showcasing superior performance in identifying both known and
unknown species. The model was tested on two state-of-art datasets with and
without background artifacts and so that it can be deployed to tackle real word
application. We used unknown species for testing our model. For unknown species
the model achieved an average accuracy of 83.36%, 78.30%, 60.34% and 43.32% for
predicting correct phylum, class, order and family respectively. Our proposed
model size is almost four times less than the existing state of the art methods
making it easily deploy able in real world application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We want to do some modifications and add more experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DRC: Enhancing Personalized Image Generation via Disentangled
  Representation Composition <span class="chip">ACM MM'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyan Xu, Wuqiang Zheng, Wenjie Wang, Fengbin Zhu, Xinting Hu, Yang Zhang, Fuli Feng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized image generation has emerged as a promising direction in
multimodal content creation. It aims to synthesize images tailored to
individual style preferences (e.g., color schemes, character appearances,
layout) and semantic intentions (e.g., emotion, action, scene contexts) by
leveraging user-interacted history images and multimodal instructions. Despite
notable progress, existing methods -- whether based on diffusion models, large
language models, or Large Multimodal Models (LMMs) -- struggle to accurately
capture and fuse user style preferences and semantic intentions. In particular,
the state-of-the-art LMM-based method suffers from the entanglement of visual
features, leading to Guidance Collapse, where the generated images fail to
preserve user-preferred styles or reflect the specified semantics.
  To address these limitations, we introduce DRC, a novel personalized image
generation framework that enhances LMMs through Disentangled Representation
Composition. DRC explicitly extracts user style preferences and semantic
intentions from history images and the reference image, respectively, to form
user-specific latent instructions that guide image generation within LMMs.
Specifically, it involves two critical learning stages: 1) Disentanglement
learning, which employs a dual-tower disentangler to explicitly separate style
and semantic features, optimized via a reconstruction-driven paradigm with
difficulty-aware importance sampling; and 2) Personalized modeling, which
applies semantic-preserving augmentations to effectively adapt the disentangled
representations for robust personalized generation. Extensive experiments on
two benchmarks demonstrate that DRC shows competitive performance while
effectively mitigating the guidance collapse issue, underscoring the importance
of disentangled representation learning for controllable and effective
personalized image generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in ACM MM'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UrbanSense:A Framework for Quantitative Analysis of Urban Streetscapes
  leveraging Vision Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10342v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10342v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yin, Jing Zhong, Peilin Li, Ruolin Pan, Pengyu Zeng, Miao Zhang, Shuai Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban cultures and architectural styles vary significantly across cities due
to geographical, chronological, historical, and socio-political factors.
Understanding these differences is essential for anticipating how cities may
evolve in the future. As representative cases of historical continuity and
modern innovation in China, Beijing and Shenzhen offer valuable perspectives
for exploring the transformation of urban streetscapes. However, conventional
approaches to urban cultural studies often rely on expert interpretation and
historical documentation, which are difficult to standardize across different
contexts. To address this, we propose a multimodal research framework based on
vision-language models, enabling automated and scalable analysis of urban
streetscape style differences. This approach enhances the objectivity and
data-driven nature of urban form research. The contributions of this study are
as follows: First, we construct UrbanDiffBench, a curated dataset of urban
streetscapes containing architectural images from different periods and
regions. Second, we develop UrbanSense, the first vision-language-model-based
framework for urban streetscape analysis, enabling the quantitative generation
and comparison of urban style representations. Third, experimental results show
that Over 80% of generated descriptions pass the t-test (p less than 0.05).
High Phi scores (0.912 for cities, 0.833 for periods) from subjective
evaluations confirm the method's ability to capture subtle stylistic
differences. These results highlight the method's potential to quantify and
interpret urban style evolution, offering a scientifically grounded lens for
future design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by
  Reinforcement Learning from Visual Map Feed Back 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18661v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18661v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixing Zhang, Yang Zhang, Tongyu Zhu, Leilei Sun, Weifeng Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Next Location Prediction is a fundamental task in the study of human
mobility, with wide-ranging applications in transportation planning, urban
governance, and epidemic forecasting. In practice, when humans attempt to
predict the next location in a trajectory, they often visualize the trajectory
on a map and reason based on road connectivity and movement trends. However,
the vast majority of existing next-location prediction models do not reason
over maps \textbf{in the way that humans do}. Fortunately, the recent
development of Vision-Language Models (VLMs) has demonstrated strong
capabilities in visual perception and even visual reasoning. This opens up a
new possibility: by rendering both the road network and trajectory onto an
image and leveraging the reasoning abilities of VLMs, we can enable models to
perform trajectory inference in a human-like manner. To explore this idea, we
first propose a method called Vision-Guided Location Search (VGLS), which
evaluates whether a general-purpose VLM is capable of trajectory-based
reasoning without modifying any of its internal parameters. Based on insights
from the VGLS results, we further propose our main approach: VLMLocPredictor,
which is composed of two stages: In the first stage, we design two Supervised
Fine-Tuning (SFT) tasks that help the VLM understand road network and
trajectory structures and acquire basic reasoning ability on such visual
inputs. In the second stage, we introduce Reinforcement Learning from Visual
Map Feedback, enabling the model to self-improve its next-location prediction
ability through interaction with the environment. Experiments conducted on
datasets from four different cities show that our method achieves
state-of-the-art (SOTA) performance and exhibits superior cross-city
generalization compared to other LLM-based approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SDMatte: Grafting Diffusion Models for Interactive Matting <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longfei Huang, Yu Liang, Hao Zhang, Jinwei Chen, Wei Dong, Lunde Chen, Wanyu Liu, Bo Li, Peng-Tao Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent interactive matting methods have shown satisfactory performance in
capturing the primary regions of objects, but they fall short in extracting
fine-grained details in edge regions. Diffusion models trained on billions of
image-text pairs, demonstrate exceptional capability in modeling highly complex
data distributions and synthesizing realistic texture details, while exhibiting
robust text-driven interaction capabilities, making them an attractive solution
for interactive matting. To this end, we propose SDMatte, a diffusion-driven
interactive matting model, with three key contributions. First, we exploit the
powerful priors of diffusion models and transform the text-driven interaction
capability into visual prompt-driven interaction capability to enable
interactive matting. Second, we integrate coordinate embeddings of visual
prompts and opacity embeddings of target objects into U-Net, enhancing
SDMatte's sensitivity to spatial position information and opacity information.
Third, we propose a masked self-attention mechanism that enables the model to
focus on areas specified by visual prompts, leading to better performance.
Extensive experiments on multiple datasets demonstrate the superior performance
of our method, validating its effectiveness in interactive matting. Our code
and model are available at https://github.com/vivoCameraResearch/SDMatte.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2025, 11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SEAL: Semantic Aware Image Watermarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12172v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12172v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kasra Arabi, R. Teal Witter, Chinmay Hegde, Niv Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have rapidly evolved to generate realistic outputs.
However, their synthetic outputs increasingly challenge the clear distinction
between natural and AI-generated content, necessitating robust watermarking
techniques. Watermarks are typically expected to preserve the integrity of the
target image, withstand removal attempts, and prevent unauthorized replication
onto unrelated images. To address this need, recent methods embed persistent
watermarks into images produced by diffusion models using the initial noise.
Yet, to do so, they either distort the distribution of generated images or rely
on searching through a long dictionary of used keys for detection.
  In this paper, we propose a novel watermarking method that embeds semantic
information about the generated image directly into the watermark, enabling a
distortion-free watermark that can be verified without requiring a database of
key patterns. Instead, the key pattern can be inferred from the semantic
embedding of the image using locality-sensitive hashing. Furthermore,
conditioning the watermark detection on the original image content improves
robustness against forgery attacks. To demonstrate that, we consider two
largely overlooked attack strategies: (i) an attacker extracting the initial
noise and generating a novel image with the same pattern; (ii) an attacker
inserting an unrelated (potentially harmful) object into a watermarked image,
possibly while preserving the watermark. We empirically validate our method's
increased robustness to these attacks. Taken together, our results suggest that
content-aware watermarks can mitigate risks arising from image-generative
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UDC-VIT: A Real-World Video <span class="highlight-title">Dataset</span> for Under-Display Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18545v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18545v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyusu Ahn, JiSoo Kim, Sangik Lee, HyunGyu Lee, Byeonghyun Ko, Chanwoo Park, Jaejin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Even though an Under-Display Camera (UDC) is an advanced imaging system, the
display panel significantly degrades captured images or videos, introducing low
transmittance, blur, noise, and flare issues. Tackling such issues is
challenging because of the complex degradation of UDCs, including diverse flare
patterns. However, no dataset contains videos of real-world UDC degradation. In
this paper, we propose a real-world UDC video dataset called UDC-VIT. Unlike
existing datasets, UDC-VIT exclusively includes human motions for facial
recognition. We propose a video-capturing system to acquire clean and
UDC-degraded videos of the same scene simultaneously. Then, we align a pair of
captured videos frame by frame, using discrete Fourier transform (DFT). We
compare UDC-VIT with six representative UDC still image datasets and two
existing UDC video datasets. Using six deep-learning models, we compare UDC-VIT
and an existing synthetic UDC video dataset. The results indicate the
ineffectiveness of models trained on earlier synthetic UDC video datasets, as
they do not reflect the actual characteristics of UDC-degraded videos. We also
demonstrate the importance of effective UDC restoration by evaluating face
recognition accuracy concerning PSNR, SSIM, and LPIPS scores. UDC-VIT is
available at our official GitHub repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond the Visible: Multispectral Vision-Language Learning for Earth
  Observation <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.15969v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.15969v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clive Tinashe Marimo, Benedikt Blumenstiel, Maximilian Nitsche, Johannes Jakubik, Thomas Brunschwiler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models for Earth observation (EO) typically rely on the
visual spectrum of data as the only model input, thus failing to leverage the
rich spectral information available in the multispectral channels recorded by
satellites. Therefore, we introduce Llama3-MS-CLIP, the first vision-language
model pre-trained with contrastive learning on a large-scale multispectral
dataset and report on the performance gains due to the extended spectral range.
Furthermore, we present the largest-to-date image-caption dataset for
multispectral data, consisting of one million Sentinel-2 samples and
corresponding textual descriptions generated using Llama3-LLaVA-Next and
Overture Maps data. We develop a scalable captioning pipeline, which is
validated by domain experts. We evaluate Llama3-MS-CLIP on multispectral
zero-shot image classification and retrieval using three datasets of varying
complexity. Our results demonstrate that Llama3-MS-CLIP significantly
outperforms other RGB-based approaches, improving classification accuracy by
+6.77% on average and retrieval performance by +4.63% mAP compared to the
second-best model. Our results emphasize the relevance of multispectral
vision-language learning. The image-caption dataset, code, and model weights
are available at https://github.com/IBM/MS-CLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Machine Learning and Knowledge Discovery in Databases. Research Track
  - European Conference, ECML PKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TACO: Taming Diffusion for in-the-wild Video Amodal Completion <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruijie Lu, Yixin Chen, Yu Liu, Jiaxiang Tang, Junfeng Ni, Diwen Wan, Gang Zeng, Siyuan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can infer complete shapes and appearances of objects from limited
visual cues, relying on extensive prior knowledge of the physical world.
However, completing partially observable objects while ensuring consistency
across video frames remains challenging for existing models, especially for
unstructured, in-the-wild videos. This paper tackles the task of Video Amodal
Completion (VAC), which aims to generate the complete object consistently
throughout the video given a visual prompt specifying the object of interest.
Leveraging the rich, consistent manifolds learned by pre-trained video
diffusion models, we propose a conditional diffusion model, TACO, that
repurposes these manifolds for VAC. To enable its effective and robust
generalization to challenging in-the-wild scenarios, we curate a large-scale
synthetic dataset with multiple difficulty levels by systematically imposing
occlusions onto un-occluded videos. Building on this, we devise a progressive
fine-tuning paradigm that starts with simpler recovery tasks and gradually
advances to more complex ones. We demonstrate TACO's versatility on a wide
range of in-the-wild videos from Internet, as well as on diverse, unseen
datasets commonly used in autonomous driving, robotic manipulation, and scene
understanding. Moreover, we show that TACO can be effectively applied to
various downstream tasks like object reconstruction and pose estimation,
highlighting its potential to facilitate physical world understanding and
reasoning. Our project page is available at https://jason-aplp.github.io/TACO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025.Project page: https://jason-aplp.github.io/TACO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrast-Invariant <span class="highlight-title">Self-supervised</span> Segmentation for Quantitative
  Placental MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.24739v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.24739v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinliu Zhong, Ruiying Liu, Emily S. Nichols, Xuzhe Zhang, Andrew F. Laine, Emma G. Duerden, Yun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate placental segmentation is essential for quantitative analysis of the
placenta. However, this task is particularly challenging in T2*-weighted
placental imaging due to: (1) weak and inconsistent boundary contrast across
individual echoes; (2) the absence of manual ground truth annotations for all
echo times; and (3) motion artifacts across echoes caused by fetal and maternal
movement. In this work, we propose a contrast-augmented segmentation framework
that leverages complementary information across multi-echo T2*-weighted MRI to
learn robust, contrast-invariant representations. Our method integrates: (i)
masked autoencoding (MAE) for self-supervised pretraining on unlabeled
multi-echo slices; (ii) masked pseudo-labeling (MPL) for unsupervised domain
adaptation across echo times; and (iii) global-local collaboration to align
fine-grained features with global anatomical context. We further introduce a
semantic matching loss to encourage representation consistency across echoes of
the same subject. Experiments on a clinical multi-echo placental MRI dataset
demonstrate that our approach generalizes effectively across echo times and
outperforms both single-echo and naive fusion baselines. To our knowledge, this
is the first work to systematically exploit multi-echo T2*-weighted MRI for
placental segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prototype Embedding Optimization for Human-Object Interaction Detection
  in Livestreaming <span class="chip">SP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Menghui Zhang, Jing Zhang, Lin Chen, Li Zhuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Livestreaming often involves interactions between streamers and objects,
which is critical for understanding and regulating web content. While
human-object interaction (HOI) detection has made some progress in
general-purpose video downstream tasks, when applied to recognize the
interaction behaviors between a streamer and different objects in
livestreaming, it tends to focuses too much on the objects and neglects their
interactions with the streamer, which leads to object bias. To solve this
issue, we propose a prototype embedding optimization for human-object
interaction detection (PeO-HOI). First, the livestreaming is preprocessed using
object detection and tracking techniques to extract features of the
human-object (HO) pairs. Then, prototype embedding optimization is adopted to
mitigate the effect of object bias on HOI. Finally, after modelling the
spatio-temporal context between HO pairs, the HOI detection results are
obtained by the prediction head. The experimental results show that the
detection accuracy of the proposed PeO-HOI method has detection accuracies of
37.19%@full, 51.42%@non-rare, 26.20%@rare on the publicly available dataset
VidHOI, 45.13%@full, 62.78%@non-rare and 30.37%@rare on the self-built dataset
BJUT-HOI, which effectively improves the HOI detection performance in
livestreaming.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE MMSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DIP: Unsupervised Dense In-Context Post-training of Visual
  Representations <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophia Sirko-Galouchenko, Spyros Gidaris, Antonin Vobecky, Andrei Bursuc, Nicolas Thome
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce DIP, a novel unsupervised post-training method designed to
enhance dense image representations in large-scale pretrained vision encoders
for in-context scene understanding. Unlike prior approaches that rely on
complex self-distillation architectures, our method trains the vision encoder
using pseudo-tasks that explicitly simulate downstream in-context scenarios,
inspired by meta-learning principles. To enable post-training on unlabeled
data, we propose an automatic mechanism for generating in-context tasks that
combines a pretrained diffusion model and the vision encoder itself. DIP is
simple, unsupervised, and computationally efficient, requiring less than 9
hours on a single A100 GPU. By learning dense representations through pseudo
in-context tasks, it achieves strong performance across a wide variety of
downstream real-world in-context scene understanding tasks. It outperforms both
the initial vision encoder and prior methods, offering a practical and
effective solution for improving dense representations. Code available here:
https://github.com/sirkosophia/DIP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Neurons to Computation: Biological Reservoir Computing for Pattern
  Recognition <span class="chip">ICONIP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludovico Iannello, Luca Ciampi, Gabriele Lagani, Fabrizio Tonelli, Eleonora Crocco, Lucio Maria Calcagnile, Angelo Di Garbo, Federico Cremisi, Giuseppe Amato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a paradigm for reservoir computing (RC) that
leverages a pool of cultured biological neurons as the reservoir substrate,
creating a biological reservoir computing (BRC). This system operates similarly
to an echo state network (ESN), with the key distinction that the neural
activity is generated by a network of cultured neurons, rather than being
modeled by traditional artificial computational units. The neuronal activity is
recorded using a multi-electrode array (MEA), which enables high-throughput
recording of neural signals. In our approach, inputs are introduced into the
network through a subset of the MEA electrodes, while the remaining electrodes
capture the resulting neural activity. This generates a nonlinear mapping of
the input data to a high-dimensional biological feature space, where
distinguishing between data becomes more efficient and straightforward,
allowing a simple linear classifier to perform pattern recognition tasks
effectively. To evaluate the performance of our proposed system, we present an
experimental study that includes various input patterns, such as positional
codes, bars with different orientations, and a digit recognition task. The
results demonstrate the feasibility of using biological neural networks to
perform tasks traditionally handled by artificial neural networks, paving the
way for further exploration of biologically-inspired computing systems, with
potential applications in neuromorphic engineering and bio-hybrid computing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICONIP 2025 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attack Anything: Blind DNNs via Universal Background Adversarial Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00029v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00029v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Lian, Shaohui Mei, Xiaofei Wang, Yi Wang, Lefan Wang, Yingjie Lu, Mingyang Ma, Lap-Pui Chau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been widely substantiated that deep neural networks (DNNs) are
susceptible and vulnerable to adversarial perturbations. Existing studies
mainly focus on performing attacks by corrupting targeted objects (physical
attack) or images (digital attack), which is intuitively acceptable and
understandable in terms of the attack's effectiveness. In contrast, our focus
lies in conducting background adversarial attacks in both digital and physical
domains, without causing any disruptions to the targeted objects themselves.
Specifically, an effective background adversarial attack framework is proposed
to attack anything, by which the attack efficacy generalizes well between
diverse objects, models, and tasks. Technically, we approach the background
adversarial attack as an iterative optimization problem, analogous to the
process of DNN learning. Besides, we offer a theoretical demonstration of its
convergence under a set of mild but sufficient conditions. To strengthen the
attack efficacy and transferability, we propose a new ensemble strategy
tailored for adversarial perturbations and introduce an improved smooth
constraint for the seamless connection of integrated perturbations. We conduct
comprehensive and rigorous experiments in both digital and physical domains
across various objects, models, and tasks, demonstrating the effectiveness of
attacking anything of the proposed method. The findings of this research
substantiate the significant discrepancy between human and machine vision on
the value of background variations, which play a far more critical role than
previously recognized, necessitating a reevaluation of the robustness and
reliability of DNNs. The code will be publicly available at
https://github.com/JiaweiLian/Attack_Anything
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 10K is Enough: An Ultra-Lightweight Binarized Network for Infrared
  Small-Target Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02662v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02662v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biqiao Xin, Qianchen Mao, Bingshu Wang, Jiangbin Zheng, Yong Zhao, C. L. Philip Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread deployment of Infrared Small-Target Detection (IRSTD)
algorithms on edge devices necessitates the exploration of model compression
techniques. Binarized neural networks (BNNs) are distinguished by their
exceptional efficiency in model compression. However, the small size of
infrared targets introduces stringent precision requirements for the IRSTD
task, while the inherent precision loss during binarization presents a
significant challenge. To address this, we propose the Binarized Infrared
Small-Target Detection Network (BiisNet), which preserves the core operations
of binarized convolutions while integrating full-precision features into the
network's information flow. Specifically, we propose the Dot Binary
Convolution, which retains fine-grained semantic information in feature maps
while still leveraging the binarized convolution operations. In addition, we
introduce a smooth and adaptive Dynamic Softsign function, which provides more
comprehensive and progressively finer gradient during backpropagation,
enhancing model stability and promoting an optimal weight distribution.
Experimental results demonstrate that BiisNet not only significantly
outperforms other binary architectures but also has strong competitiveness
among state-of-the-art full-precision models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We found the paper has insufficient workload after review. No
  substitute manuscript can be ready soon. To ensure academic quality, we
  withdraw it and plan to resubmit when improved</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the evaluators: Towards human-aligned metrics for missing
  markers reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14334v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14334v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taras Kucherenko, Derek Peristy, Judith Bütepage
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animation data is often obtained through optical motion capture systems,
which utilize a multitude of cameras to establish the position of optical
markers. However, system errors or occlusions can result in missing markers,
the manual cleaning of which can be time-consuming. This has sparked interest
in machine learning-based solutions for missing marker reconstruction in the
academic community. Most academic papers utilize a simplistic mean square error
as the main metric. In this paper, we show that this metric does not correlate
with subjective perception of the fill quality. Additionally, we introduce and
evaluate a set of better-correlated metrics that can drive progress in the
field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the ACM International Conference on Multimedia 2025 (ACM
  MM'25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic brain tumor segmentation in 2D intra-operative ultrasound
  images using magnetic resonance imaging tumor annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14017v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14017v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathilde Faanes, Ragnhild Holden Helland, Ole Solheim, Sébastien Muller, Ingerid Reinertsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic segmentation of brain tumors in intra-operative ultrasound (iUS)
images could facilitate localization of tumor tissue during resection surgery.
The lack of large annotated datasets limits the current models performances. In
this paper, we investigated the use of tumor annotations in magnetic resonance
imaging (MRI) scans, which are more accessible than annotations in iUS images,
for training of deep learning models for iUS brain tumor segmentation. We used
180 annotated MRI scans with corresponding unannotated iUS images, and 29
annotated iUS images. Image registration was performed to transfer the MRI
annotations to the corresponding iUS images before training the nnU-Net model
with different configurations of the data and label origins. The results showed
no significant difference in Dice score for a model trained with only MRI
annotated tumors compared to models trained with only iUS annotations and both,
and to expert annotations, indicating that MRI tumor annotations can be used as
a substitute for iUS tumor annotations to train a deep learning model for
automatic brain tumor segmentation in iUS images. The best model obtained an
average Dice score of $0.62\pm0.31$, compared to $0.67\pm0.25$ for an expert
neurosurgeon, where the performance on larger tumors were similar, but lower
for the models on smaller tumors. In addition, the results showed that removing
smaller tumors from the training sets improved the results. The main models are
available here:
https://github.com/mathildefaanes/us_brain_tumor_segmentation/tree/main
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14, 5figures. This work has been submitted to the IEEE for possible
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MathPhys-Guided Coarse-to-Fine Anomaly Synthesis with SQE-Driven
  Bi-Level Optimization for Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.12970v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.12970v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Qian, Bingke Zhu, Yingying Chen, Ming Tang, Jinqiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, industrial anomaly detection suffers from two bottlenecks: (i) the
rarity of real-world defect images and (ii) the opacity of sample quality when
synthetic data are used. Existing synthetic strategies (e.g., cut-and-paste)
overlook the underlying physical causes of defects, leading to inconsistent,
low-fidelity anomalies that hamper model generalization to real-world
complexities. In this paper, we introduce a novel and lightweight pipeline that
generates synthetic anomalies through Math-Phys model guidance, refines them
via a Coarse-to-Fine approach and employs a bi-level optimization strategy with
a Synthesis Quality Estimator (SQE). By combining physical modeling of the
three most typical physics-driven defect mechanisms: Fracture Line (FL),
Pitting Loss (PL), and Plastic Warpage (PW), our method produces realistic
defect masks, which are subsequently enhanced in two phases. The first stage
(npcF) enforces a PDE-based consistency to achieve a globally coherent anomaly
structure, while the second stage (npcF++) further improves local fidelity.
Additionally, we leverage SQE-driven weighting, ensuring that high-quality
synthetic samples receive greater emphasis during training. To validate our
method, we conduct experiments on three anomaly detection benchmarks: MVTec AD,
VisA, and BTAD. Across these datasets, our method achieves state-of-the-art
results in both image- and pixel-AUROC, confirming the effectiveness of our
MaPhC2F dataset and BiSQAD method. All code will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GausSim: Foreseeing Reality by Gaussian Simulator for Elastic Objects <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17804v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17804v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidi Shao, Mu Huang, Chen Change Loy, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce GausSim, a novel neural network-based simulator designed to
capture the dynamic behaviors of real-world elastic objects represented through
Gaussian kernels. We leverage continuum mechanics and treat each kernel as a
Center of Mass System (CMS) that represents continuous piece of matter,
accounting for realistic deformations without idealized assumptions. To improve
computational efficiency and fidelity, we employ a hierarchical structure that
further organizes kernels into CMSs with explicit formulations, enabling a
coarse-to-fine simulation approach. This structure significantly reduces
computational overhead while preserving detailed dynamics. In addition, GausSim
incorporates explicit physics constraints, such as mass and momentum
conservation, ensuring interpretable results and robust, physically plausible
simulations. To validate our approach, we present a new dataset, READY,
containing multi-view videos of real-world elastic deformations. Experimental
results demonstrate that GausSim achieves superior performance compared to
existing physics-driven baselines, offering a practical and accurate solution
for simulating complex dynamic behaviors. Code and model are available at our
project page: https://www.mmlab-ntu.com/project/gausim/index.html .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV2025. Project page:
  https://www.mmlab-ntu.com/project/gausim/index.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmoud Ahmed, Junjie Fei, Jian Ding, Eslam Mohamed Bakr, Mohamed Elhoseiny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Part-Aware Point Grounded Description (PaPGD), a
challenging task aimed at advancing 3D multimodal learning for fine-grained,
part-aware segmentation grounding and detailed explanation of 3D objects.
Existing 3D datasets largely focus on either vision-only part segmentation or
vision-language scene segmentation, lacking the fine-grained multimodal
segmentation needed for robotic navigation and interaction in real-world
environments. To address this gap, we present the 3DCoMPaT Grounded
Instructions (3DCoMPaT-GrIn) Dataset, a comprehensive resource that pairs rich
point cloud descriptions with corresponding part-level segmentation masks. This
dataset encompasses extensive samples designed for both PaPGD and fine-grained
single-part grounding tasks. To tackle the inherent challenges of grounding
objects and generating grounded descriptions at the part level, we propose
Kestrel, a part-aware 3D multimodal large language model that integrates an
advanced language model for nuanced language comprehension with multi-level
point feature propagation and query refinement mechanism to enhance spatial
reasoning at the part level. The extensive experiments demonstrate that Kestrel
effectively bridges the gap between part-aware language understanding and 3D
segmentation grounding, paving the way for more robust and interpretable 3D
object comprehension that meets the demands of real-world robotic applications.
Project page at https://feielysia.github.io/Kestrel.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty-Aware Knowledge Distillation for Compact and Efficient 6DoF
  Pose Estimation <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nassim Ali Ousalah, Anis Kacem, Enjie Ghorbel, Emmanuel Koumandakis, Djamila Aouada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compact and efficient 6DoF object pose estimation is crucial in applications
such as robotics, augmented reality, and space autonomous navigation systems,
where lightweight models are critical for real-time accurate performance. This
paper introduces a novel uncertainty-aware end-to-end Knowledge Distillation
(KD) framework focused on keypoint-based 6DoF pose estimation. Keypoints
predicted by a large teacher model exhibit varying levels of uncertainty that
can be exploited within the distillation process to enhance the accuracy of the
student model while ensuring its compactness. To this end, we propose a
distillation strategy that aligns the student and teacher predictions by
adjusting the knowledge transfer based on the uncertainty associated with each
teacher keypoint prediction. Additionally, the proposed KD leverages this
uncertainty-aware alignment of keypoints to transfer the knowledge at key
locations of their respective feature maps. Experiments on the widely-used
LINEMOD benchmark demonstrate the effectiveness of our method, achieving
superior 6DoF object pose estimation with lightweight models compared to
state-of-the-art approaches. Further validation on the SPEED+ dataset for
spacecraft pose estimation highlights the robustness of our approach under
diverse 6DoF pose estimation scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Deepfake Detectors in the Wild <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viacheslav Pirogov, Maksim Artemev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deepfakes powered by advanced machine learning models present a significant
and evolving threat to identity verification and the authenticity of digital
media. Although numerous detectors have been developed to address this problem,
their effectiveness has yet to be tested when applied to real-world data. In
this work we evaluate modern deepfake detectors, introducing a novel testing
procedure designed to mimic real-world scenarios for deepfake detection. Using
state-of-the-art deepfake generation methods, we create a comprehensive dataset
containing more than 500,000 high-quality deepfake images. Our analysis shows
that detecting deepfakes still remains a challenging task. The evaluation shows
that in fewer than half of the deepfake detectors tested achieved an AUC score
greater than 60%, with the lowest being 50%. We demonstrate that basic image
manipulations, such as JPEG compression or image enhancement, can significantly
reduce model performance. All code and data are publicly available at
https://github.com/SumSubstance/Deepfake-Detectors-in-the-Wild.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the ICML 2025 Workshop 'DataWorld: Unifying Data Curation
  Frameworks Across Domains'</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing ImageNet <span class="highlight-title">Pre-train</span>ing with Digital Pathology Foundation Models
  for Whole Slide Image-Based Survival Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17446v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17446v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kleanthis Marios Papadopoulos, Tania Stathaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The abundance of information present in Whole Slide Images (WSIs) renders
them an essential tool for survival analysis. Several Multiple Instance
Learning frameworks proposed for this task utilize a ResNet50 backbone
pre-trained on natural images. By leveraging recenetly released
histopathological foundation models such as UNI and Hibou, the predictive
prowess of existing MIL networks can be enhanced. Furthermore, deploying an
ensemble of digital pathology foundation models yields higher baseline
accuracy, although the benefits appear to diminish with more complex MIL
architectures. Our code will be made publicly available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted (Oral) at the 6th International Conference on Computer
  Vision and Information Technology (CVIT 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frequency Regulation for Exposure Bias Mitigation in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.10072v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.10072v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Yu, Kun Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models exhibit impressive generative capabilities but are
significantly impacted by exposure bias. In this paper, we make a key
observation: the energy of predicted noisy samples in the reverse process
continuously declines compared to perturbed samples in the forward process.
Building on this, we identify two important findings: 1) The reduction in
energy follows distinct patterns in the low-frequency and high-frequency
subbands; 2) The subband energy of reverse-process reconstructed samples is
consistently lower than that of forward-process ones, and both are lower than
the original data samples. Based on the first finding, we introduce a dynamic
frequency regulation mechanism utilizing wavelet transforms, which separately
adjusts the low- and high-frequency subbands. Leveraging the second insight, we
derive the rigorous mathematical form of exposure bias. It is worth noting
that, our method is training-free and plug-and-play, significantly improving
the generative quality of various diffusion models and frameworks with
negligible computational cost. The source code is available at
https://github.com/kunzhan/wpp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Multimedia 2025 accepted!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is It Really You? Exploring Biometric Verification Scenarios in
  Photorealistic Talking-Head Avatar Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Pedrouzo-Rodriguez, Pedro Delgado-DeRobles, Luis F. Gomez, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photorealistic talking-head avatars are becoming increasingly common in
virtual meetings, gaming, and social platforms. These avatars allow for more
immersive communication, but they also introduce serious security risks. One
emerging threat is impersonation: an attacker can steal a user's avatar,
preserving his appearance and voice, making it nearly impossible to detect its
fraudulent usage by sight or sound alone. In this paper, we explore the
challenge of biometric verification in such avatar-mediated scenarios. Our main
question is whether an individual's facial motion patterns can serve as
reliable behavioral biometrics to verify their identity when the avatar's
visual appearance is a facsimile of its owner. To answer this question, we
introduce a new dataset of realistic avatar videos created using a
state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and
impostor avatar videos. We also propose a lightweight, explainable
spatio-temporal Graph Convolutional Network architecture with temporal
attention pooling, that uses only facial landmarks to model dynamic facial
gestures. Experimental results demonstrate that facial motion cues enable
meaningful identity verification with AUC values approaching 80%. The proposed
benchmark and biometric system are available for the research community in
order to bring attention to the urgent need for more advanced behavioral
biometric defenses in avatar-based communication systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the IEEE International Joint Conference on Biometrics
  (IJCB 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ All-in-One Transferring Image Compression from Human Perception to
  Multi-Machine Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.12997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.12997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiancheng Zhao, Xiang Ji, Yinqiang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiently transferring Learned Image Compression (LIC) model from human
perception to machine perception is an emerging challenge in vision-centric
representation learning. Existing approaches typically adapt LIC to downstream
tasks in a single-task manner, which is inefficient, lacks task interaction,
and results in multiple task-specific bitstreams. In this paper, we propose a
multi-task adaptation framework that enables transferring a pre-trained base
codec to multiple machine vision tasks through a unified model and a single
training process. To achieve this, we design an asymmetric adaptation
architecture consisting of a task-agnostic encoder adaptation and task-specific
decoder adaptation. Furthermore, we introduce two feature propagation modules
to facilitate inter-task and inter-scale feature represenation learning.
Experiments on PASCAL-Context and NYUD-V2 dataset demonstrate that our method
outperforms both Fully Fine-Tuned and other Parameter Efficient Fine-Tuned
(PEFT) baselines. Code will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-Independent Machine Learning Approach for Nanometric Axial
  Localization and Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14754v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14754v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Alexandrov, Giovanni Acampora, Giovanni De Lellis, Antonia Di Crescenzo, Chiara Errico, Daria Morozova, Valeri Tioukov, Autilia Vittiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately tracking particles and determining their coordinate along the
optical axis is a major challenge in optical microscopy, especially when
extremely high precision is needed. In this study, we introduce a deep learning
approach using convolutional neural networks (CNNs) that can determine axial
coordinates from dual-focal-plane images without relying on predefined models.
Our method achieves an axial localization precision of 40 nanometers-six times
better than traditional single-focal-plane techniques. The model's simple
design and strong performance make it suitable for a wide range of uses,
including dark matter detection, proton therapy for cancer, and radiation
protection in space. It also shows promise in fields like biological imaging,
materials science, and environmental monitoring. This work highlights how
machine learning can turn complex image data into reliable, precise
information, offering a flexible and powerful tool for many scientific
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast
  Lesion Segmentation in Longitudinal DCE-MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00496v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00496v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Kamran, Maria Bernathova, Raoul Varga, Christian F. Singer, Zsuzsanna Bago-Horvath, Thomas Helbich, Georg Langs, Philipp Seeböck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced
MRI (DCE-MRI) is critical for early cancer detection, especially in high-risk
patients. While recent deep learning methods have advanced lesion segmentation,
they primarily target large lesions and neglect valuable longitudinal and
clinical information routinely used by radiologists. In real-world screening,
detecting subtle or emerging lesions requires radiologists to compare across
timepoints and consider previous radiology assessments, such as the BI-RADS
score. We propose LesiOnTime, a novel 3D segmentation approach that mimics
clinical diagnostic workflows by jointly leveraging longitudinal imaging and
BIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)
block that dynamically integrates information from previous and current scans;
and (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent
space alignment for scans with similar radiological assessments, thus embedding
domain knowledge into the training process. Evaluated on a curated in-house
longitudinal dataset of high-risk patients with DCE-MRI, our approach
outperforms state-of-the-art single-timepoint and longitudinal baselines by 5%
in terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute
complementary performance gains. These results highlight the importance of
incorporating temporal and clinical context for reliable early lesion
segmentation in real-world breast cancer screening. Our code is publicly
available at https://github.com/cirmuw/LesiOnTime
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D-Judge: How Far Are We? Evaluating the Discrepancies Between
  AI-synthesized Images and Natural Images through Multimodal Guidance <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17632v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17632v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renyang Liu, Ziyu Lyu, Wei Zhou, See-Kiong Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving field of Artificial Intelligence Generated Content
(AIGC), a central challenge is distinguishing AI-synthesized images from
natural images. Despite the impressive capabilities of advanced AI generative
models in producing visually compelling content, significant discrepancies
remain when compared to natural images. To systematically investigate and
quantify these differences, we construct a large-scale multimodal dataset named
DANI, comprising 5,000 natural images and over 440,000 AI-generated image
(AIGI) samples produced by nine representative models using both unimodal and
multimodal prompts, including Text-to-Image (T2I), Image-to-Image (I2I), and
Text and Image-to-Image (TI2I). We then introduce D-Judge, a benchmark designed
to answer the critical question: how far are AI-generated images from truly
realistic images? Our fine-grained evaluation framework assesses DANI across
five key dimensions: naive visual quality, semantic alignment, aesthetic
appeal, downstream task applicability, and coordinated human validation.
Extensive experiments reveal substantial discrepancies across these dimensions,
highlighting the importance of aligning quantitative metrics with human
judgment to achieve a comprehensive understanding of AI-generated image
quality. The code and dataset are publicly available at:
https://github.com/ryliu68/DJudge and
https://huggingface.co/datasets/Renyang/DANI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAGI: Semantically Aligned and Uncertainty Guided AI Image Inpainting <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06593v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06593v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paschalis Giakoumoglou, Dimitrios Karageorgiou, Symeon Papadopoulos, Panagiotis C. Petrantonakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in generative AI have made text-guided image inpainting -
adding, removing, or altering image regions using textual prompts - widely
accessible. However, generating semantically correct photorealistic imagery,
typically requires carefully-crafted prompts and iterative refinement by
evaluating the realism of the generated content - tasks commonly performed by
humans. To automate the generative process, we propose Semantically Aligned and
Uncertainty Guided AI Image Inpainting (SAGI), a model-agnostic pipeline, to
sample prompts from a distribution that closely aligns with human perception
and to evaluate the generated content and discard instances that deviate from
such a distribution, which we approximate using pretrained large language
models and vision-language models. By applying this pipeline on multiple
state-of-the-art inpainting models, we create the SAGI Dataset (SAGI-D),
currently the largest and most diverse dataset of AI-generated inpaintings,
comprising over 95k inpainted images and a human-evaluated subset. Our
experiments show that semantic alignment significantly improves image quality
and aesthetics, while uncertainty guidance effectively identifies realistic
manipulations - human ability to distinguish inpainted images from real ones
drops from 74% to 35% in terms of accuracy, after applying our pipeline.
Moreover, using SAGI-D for training several image forensic approaches increases
in-domain detection performance on average by 37.4% and out-of-domain
generalization by 26.1% in terms of IoU, also demonstrating its utility in
countering malicious exploitation of generative AI. Code and dataset are
available at https://mever-team.github.io/SAGI/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Survivability of Backdoor Attacks on Unconstrained Face Recognition
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01607v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01607v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Le Roux, Yannick Teglia, Teddy Furon, Philippe Loubet-Moundi, Eric Bourbao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread use of deep learning face recognition raises several security
concerns. Although prior works point at existing vulnerabilities, DNN backdoor
attacks against real-life, unconstrained systems dealing with images captured
in the wild remain a blind spot of the literature. This paper conducts the
first system-level study of backdoors in deep learning-based face recognition
systems. This paper yields four contributions by exploring the feasibility of
DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the
first time two backdoor attacks on the face detection task: face generation and
face landmark shift attacks. We then show that face feature extractors trained
with large margin losses also fall victim to backdoor attacks. Combining our
models, we then show using 20 possible pipeline configurations and 15 attack
cases that a single backdoor enables an attacker to bypass the entire function
of a system. Finally, we provide stakeholders with several best practices and
countermeasures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiashuo Yu, Yue Wu, Meng Chu, Zhifei Ren, Zizheng Huang, Pei Chu, Ruijie Zhang, Yinan He, Qirui Li, Songze Li, Zhenxiang Li, Zhongying Tu, Conghui He, Yu Qiao, Yali Wang, Yi Wang, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present VRBench, the first long narrative video benchmark crafted for
evaluating large models' multi-step reasoning capabilities, addressing
limitations in existing evaluations that overlook temporal reasoning and
procedural validity. It comprises 960 long videos (with an average duration of
1.6 hours), along with 8,243 human-labeled multi-step question-answering pairs
and 25,106 reasoning steps with timestamps. These videos are curated via a
multi-stage filtering process including expert inter-rater reviewing to
prioritize plot coherence. We develop a human-AI collaborative framework that
generates coherent reasoning chains, each requiring multiple temporally
grounded steps, spanning seven types (e.g., event attribution, implicit
inference). VRBench designs a multi-phase evaluation pipeline that assesses
models at both the outcome and process levels. Apart from the MCQs for the
final results, we propose a progress-level LLM-guided scoring metric to
evaluate the quality of the reasoning chain from multiple dimensions
comprehensively. Through extensive evaluations of 12 LLMs and 19 VLMs on
VRBench, we undertake a thorough analysis and provide valuable insights that
advance the field of multi-step reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AudioGen-Omni: A Unified Multimodal Diffusion <span class="highlight-title">Transformer</span> for
  Video-Synchronized Audio, Speech, and Song Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00733v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00733v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Wang, Jun Wang, Feng Deng, Chen Zhang, Di Zhang, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present AudioGen-Omni - a unified approach based on multimodal diffusion
transformers (MMDit), capable of generating high-fidelity audio, speech, and
songs coherently synchronized with the input video. AudioGen-Omni introduces a
novel joint training paradigm that seamlessly integrates large-scale
video-text-audio corpora, enabling a model capable of generating semantically
rich, acoustically diverse audio conditioned on multimodal inputs and adaptable
to a wide range of audio generation tasks. AudioGen-Omni employs a unified
lyrics-transcription encoder that encodes graphemes and phonemes from both sung
and spoken inputs into dense frame-level representations. Dense frame-level
representations are fused using an AdaLN-based joint attention mechanism
enhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein
RoPE is selectively applied to temporally structured modalities to ensure
precise and robust cross-modal alignment. By unfreezing all modalities and
masking missing inputs, AudioGen-Omni mitigates the semantic constraints of
text-frozen paradigms, enabling effective cross-modal conditioning. This joint
training approach enhances audio quality, semantic alignment, and lip-sync
accuracy, while also achieving state-of-the-art results on
Text-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8
seconds of audio, it offers substantial improvements in both efficiency and
generality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Polymorph: Energy-Efficient Multi-Label Classification for Video Streams
  on Embedded Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saeid Ghafouri, Mohsen Fayyaz, Xiangchen Li, Deepu John, Bo Ji, Dimitrios Nikolopoulos, Hans Vandierendonck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time multi-label video classification on embedded devices is constrained
by limited compute and energy budgets. Yet, video streams exhibit structural
properties such as label sparsity, temporal continuity, and label co-occurrence
that can be leveraged for more efficient inference. We introduce Polymorph, a
context-aware framework that activates a minimal set of lightweight Low Rank
Adapters (LoRA) per frame. Each adapter specializes in a subset of classes
derived from co-occurrence patterns and is implemented as a LoRA weight over a
shared backbone. At runtime, Polymorph dynamically selects and composes only
the adapters needed to cover the active labels, avoiding full-model switching
and weight merging. This modular strategy improves scalability while reducing
latency and energy overhead. Polymorph achieves 40% lower energy consumption
and improves mAP by 9 points over strong baselines on the TAO dataset.
Polymorph is open source at https://github.com/inference-serving/polymorph/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hypergraph Mamba for Efficient Whole Slide Image Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17457v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17457v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxuan Lu, Yuhui Lin, Junyan Shi, Fang Yan, Dongzhan Zhou, Yue Gao, Xiaosong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole Slide Images (WSIs) in histopathology pose a significant challenge for
extensive medical image analysis due to their ultra-high resolution, massive
scale, and intricate spatial relationships. Although existing Multiple Instance
Learning (MIL) approaches like Graph Neural Networks (GNNs) and Transformers
demonstrate strong instance-level modeling capabilities, they encounter
constraints regarding scalability and computational expenses. To overcome these
limitations, we introduce the WSI-HGMamba, a novel framework that unifies the
high-order relational modeling capabilities of the Hypergraph Neural Networks
(HGNNs) with the linear-time sequential modeling efficiency of the State Space
Models. At the core of our design is the HGMamba block, which integrates
message passing, hypergraph scanning & flattening, and bidirectional state
space modeling (Bi-SSM), enabling the model to retain both relational and
contextual cues while remaining computationally efficient. Compared to
Transformer and Graph Transformer counterparts, WSI-HGMamba achieves superior
performance with up to 7* reduction in FLOPs. Extensive experiments on multiple
public and private WSI benchmarks demonstrate that our method provides a
scalable, accurate, and efficient solution for slide-level understanding,
making it a promising backbone for next-generation pathology AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Hyper-Graph Convolution Network for Skeleton-based Human Action
  Recognition with Virtual Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14796v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14796v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youwei Zhou, Tianyang Xu, Cong Wu, Xiaojun Wu, Josef Kittler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The shared topology of human skeletons motivated the recent investigation of
graph convolutional network (GCN) solutions for action recognition. However,
most of the existing GCNs rely on the binary connection of two neighboring
vertices (joints) formed by an edge (bone), overlooking the potential of
constructing multi-vertex convolution structures. Although some studies have
attempted to utilize hyper-graphs to represent the topology, they rely on a
fixed construction strategy, which limits their adaptivity in uncovering the
intricate latent relationships within the action. In this paper, we address
this oversight and explore the merits of an adaptive hyper-graph convolutional
network (Hyper-GCN) to achieve the aggregation of rich semantic information
conveyed by skeleton vertices. In particular, our Hyper-GCN adaptively
optimises the hyper-graphs during training, revealing the action-driven
multi-vertex relations. Besides, virtual connections are often designed to
support efficient feature aggregation, implicitly extending the spectrum of
dependencies within the skeleton. By injecting virtual connections into
hyper-graphs, the semantic clues of diverse action categories can be
highlighted. The results of experiments conducted on the NTU-60, NTU-120, and
NW-UCLA datasets demonstrate the merits of our Hyper-GCN, compared to the
state-of-the-art methods. The code is available at
https://github.com/6UOOON9/Hyper-GCN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wi-CBR: Salient-aware Adaptive WiFi Sensing for Cross-domain Behavior
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruobei Zhang, Shengeng Tang, Huan Yan, Xiang Zhang, Jiabao Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge in WiFi-based cross-domain Behavior Recognition lies in the
significant interference of domain-specific signals on gesture variation.
However, previous methods alleviate this interference by mapping the phase from
multiple domains into a common feature space. If the Doppler Frequency Shift
(DFS) signal is used to dynamically supplement the phase features to achieve
better generalization, enabling model to not only explore a wider feature space
but also avoid potential degradation of gesture semantic information.
Specifically, we propose a novel Salient-aware Adaptive WiFi Sensing for
Cross-domain Behavior Recognition (Wi-CBR}, which constructs a dual-branch
self-attention module that captures temporal features from phase information
reflecting dynamic path length variations, while extracting spatial features
from DFS correlated with motion velocity. Moreover, we design a Saliency
Guidance Module that employs group attention mechanisms to mine critical
activity features, and utilizes gating mechanisms to optimize information
entropy, facilitating feature fusion and enabling effective interaction between
salient and non-salient behavior characteristics. Extensive experiments on two
large-scale public datasets (Widar3.0 and XRF55) demonstrate the superior
performance of our method in both in-domain and cross-domain scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multimodal Deviation Perceiving Framework for Weakly-Supervised
  Temporal Forgery Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16596v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16596v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Xu, Junyan Wu, Wei Lu, Xiangyang Luo, Qian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current researches on Deepfake forensics often treat detection as a
classification task or temporal forgery localization problem, which are usually
restrictive, time-consuming, and challenging to scale for large datasets. To
resolve these issues, we present a multimodal deviation perceiving framework
for weakly-supervised temporal forgery localization (MDP), which aims to
identify temporal partial forged segments using only video-level annotations.
The MDP proposes a novel multimodal interaction mechanism (MI) and an
extensible deviation perceiving loss to perceive multimodal deviation, which
achieves the refined start and end timestamps localization of forged segments.
Specifically, MI introduces a temporal property preserving cross-modal
attention to measure the relevance between the visual and audio modalities in
the probabilistic embedding space. It could identify the inter-modality
deviation and construct comprehensive video features for temporal forgery
localization. To explore further temporal deviation for weakly-supervised
learning, an extensible deviation perceiving loss has been proposed, aiming at
enlarging the deviation of adjacent segments of the forged samples and reducing
that of genuine samples. Extensive experiments demonstrate the effectiveness of
the proposed framework and achieve comparable results to fully-supervised
approaches in several evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures,conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic-Dark SLAM: RGB-Thermal Cooperative Robot Vision Strategy for
  Multi-Person Tracking in Both Well-Lit and Low-Light Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12768v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12768v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatsuro Sakai, Kanji Tanaka, Yuki Minase, Jonathan Tay Yu Liang, Muhammad Adil Luqman, Daiki Iwata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In robot vision, thermal cameras hold great potential for recognizing humans
even in complete darkness. However, their application to multi-person tracking
(MPT) has been limited due to data scarcity and the inherent difficulty of
distinguishing individuals. In this study, we propose a cooperative MPT system
that utilizes co-located RGB and thermal cameras, where pseudo-annotations
(bounding boxes and person IDs) are used to train both RGB and thermal
trackers. Evaluation experiments demonstrate that the thermal tracker performs
robustly in both bright and dark environments. Moreover, the results suggest
that a tracker-switching strategy -- guided by a binary brightness classifier
-- is more effective for information integration than a tracker-fusion
approach. As an application example, we present an image change pattern
recognition (ICPR) method, the ``human-as-landmark,'' which combines two key
properties: the thermal recognizability of humans in dark environments and the
rich landmark characteristics -- appearance, geometry, and semantics -- of
static objects (occluders). Whereas conventional SLAM focuses on mapping static
landmarks in well-lit environments, the present study takes a first step toward
a new Human-Only SLAM paradigm, ``Dynamic-Dark SLAM,'' which aims to map even
dynamic landmarks in complete darkness. Additionally, this study demonstrates
that knowledge transfer between thermal and depth modalities enables reliable
person tracking using low-resolution 3D LiDAR data without RGB input,
contributing an important advance toward cross-robot SLAM systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 11 figures, technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HumanDiT: Pose-Guided Diffusion <span class="highlight-title">Transformer</span> for Long-form Human Motion
  Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04847v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04847v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qijun Gan, Yi Ren, Chen Zhang, Zhenhui Ye, Pan Xie, Xiang Yin, Zehuan Yuan, Bingyue Peng, Jianke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion video generation has advanced significantly, while existing
methods still struggle with accurately rendering detailed body parts like hands
and faces, especially in long sequences and intricate motions. Current
approaches also rely on fixed resolution and struggle to maintain visual
consistency. To address these limitations, we propose HumanDiT, a pose-guided
Diffusion Transformer (DiT)-based framework trained on a large and wild dataset
containing 14,000 hours of high-quality video to produce high-fidelity videos
with fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT,
supports numerous video resolutions and variable sequence lengths, facilitating
learning for long-sequence video generation; (ii) we introduce a prefix-latent
reference strategy to maintain personalized characteristics across extended
sequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to
generate subsequent pose sequences, facilitating video continuation from static
images or existing videos. It also utilizes a Pose Adapter to enable pose
transfer with given sequences. Extensive experiments demonstrate its superior
performance in generating long-form, pose-accurate videos across diverse
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://agnjason.github.io/HumanDiT-page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniLDiff: Unlocking the Power of Diffusion Priors for All-in-One Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Cheng, Liangtai Zhou, Dian Chen, Ni Tang, Xiaotong Luo, Yanyun Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  All-in-One Image Restoration (AiOIR) has emerged as a promising yet
challenging research direction. To address the core challenges of diverse
degradation modeling and detail preservation, we propose UniLDiff, a unified
framework enhanced with degradation- and detail-aware mechanisms, unlocking the
power of diffusion priors for robust image restoration. Specifically, we
introduce a Degradation-Aware Feature Fusion (DAFF) to dynamically inject
low-quality features into each denoising step via decoupled fusion and adaptive
modulation, enabling implicit modeling of diverse and compound degradations.
Furthermore, we design a Detail-Aware Expert Module (DAEM) in the decoder to
enhance texture and fine-structure recovery through expert routing. Extensive
experiments across multi-task and mixed degradation settings demonstrate that
our method consistently achieves state-of-the-art performance, highlighting the
practical potential of diffusion priors for unified image restoration. Our code
will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WhoFi: Deep Person Re-Identification via Wi-Fi Channel Signal Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.12869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.12869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danilo Avola, Emad Emam, Dario Montagnini, Daniele Pannone, Amedeo Ranaldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person Re-Identification is a key and challenging task in video surveillance.
While traditional methods rely on visual data, issues like poor lighting,
occlusion, and suboptimal angles often hinder performance. To address these
challenges, we introduce WhoFi, a novel pipeline that utilizes Wi-Fi signals
for person re-identification. Biometric features are extracted from Channel
State Information (CSI) and processed through a modular Deep Neural Network
(DNN) featuring a Transformer-based encoder. The network is trained using an
in-batch negative loss function to learn robust and generalizable biometric
signatures. Experiments on the NTU-Fi dataset show that our approach achieves
competitive results compared to state-of-the-art methods, confirming its
effectiveness in identifying individuals via Wi-Fi signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GS-ID: Illumination Decomposition on Gaussian Splatting via Adaptive
  Light Aggregation and Diffusion-Guided Material Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Du, Zhihao Liang, Yulin Shen, Zeyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian Splatting (GS) has emerged as an effective representation for
photorealistic rendering, but the underlying geometry, material, and lighting
remain entangled, hindering scene editing. Existing GS-based methods struggle
to disentangle these components under non-Lambertian conditions, especially in
the presence of specularities and shadows. We propose \textbf{GS-ID}, an
end-to-end framework for illumination decomposition that integrates adaptive
light aggregation with diffusion-based material priors. In addition to a
learnable environment map for ambient illumination, we model spatially-varying
local lighting using anisotropic spherical Gaussian mixtures (SGMs) that are
jointly optimized with scene content. To better capture cast shadows, we
associate each splat with a learnable unit vector that encodes shadow
directions from multiple light sources, further improving material and lighting
estimation. By combining SGMs with intrinsic priors from diffusion models,
GS-ID significantly reduces ambiguity in light-material-geometry interactions
and achieves state-of-the-art performance on inverse rendering and relighting
benchmarks. Experiments also demonstrate the effectiveness of GS-ID for
downstream applications such as relighting and scene composition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Discriminability of <span class="highlight-title">Self-Supervised</span> Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeen Song, Wenwen Qiang, Changwen Zheng, Fuchun Sun, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has recently shown notable success in various
visual tasks. However, in terms of discriminability, SSL is still not on par
with supervised learning (SL). This paper identifies a key issue, the
``crowding problem," where features from different classes are not
well-separated, and there is high intra-class variance. In contrast, SL ensures
clear class separation. Our analysis reveals that SSL objectives do not
adequately constrain the relationships between samples and their augmentations,
leading to poorer performance in complex tasks. We further establish a
theoretical framework that connects SSL objectives to cross-entropy risk
bounds, explaining how reducing intra-class variance and increasing inter-class
separation can improve generalization. To address this, we propose the Dynamic
Semantic Adjuster (DSA), a learnable regulator that enhances feature
aggregation and separation while being robust to outliers. Comprehensive
experiments conducted on diverse benchmark datasets validate that DSA leads to
substantial gains in SSL performance, narrowing the performance gap with SL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Gesture: Co-Speech Gesture Video Generation through
  Context-aware Gesture Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pinxin Liu, Pengfei Zhang, Hyeongwoo Kim, Pablo Garrido, Ari Sharpio, Kyle Olszewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Co-speech gesture generation is crucial for creating lifelike avatars and
enhancing human-computer interactions by synchronizing gestures with speech.
Despite recent advancements, existing methods struggle with accurately
identifying the rhythmic or semantic triggers from audio for generating
contextualized gesture patterns and achieving pixel-level realism. To address
these challenges, we introduce Contextual Gesture, a framework that improves
co-speech gesture video generation through three innovative components: (1) a
chronological speech-gesture alignment that temporally connects two modalities,
(2) a contextualized gesture tokenization that incorporate speech context into
motion pattern representation through distillation, and (3) a structure-aware
refinement module that employs edge connection to link gesture keypoints to
improve video generation. Our extensive experiments demonstrate that Contextual
Gesture not only produces realistic and speech-aligned gesture videos but also
supports long-sequence generation and video gesture editing applications, shown
in Fig.1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACMMM 2025. Project Page:
  https://andypinxinliu.github.io/Contextual-Gesture/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NSegment : Label-specific Deformations for Remote Sensing Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.19634v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.19634v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yechan Kim, DongHo Yoon, SooYeon Kim, Moongu Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Labeling errors in remote sensing (RS) image segmentation datasets often
remain implicit and subtle due to ambiguous class boundaries, mixed pixels,
shadows, complex terrain features, and subjective annotator bias. Furthermore,
the scarcity of annotated RS data due to the high cost of labeling complicates
training noise-robust models. While sophisticated mechanisms such as label
selection or noise correction might address the issue mentioned above, they
tend to increase training time and add implementation complexity. In this
paper, we propose NSegment-a simple yet effective data augmentation solution to
mitigate this issue. Unlike traditional methods, it applies elastic
transformations only to segmentation labels, varying deformation intensity per
sample in each training epoch to address annotation inconsistencies.
Experimental results demonstrate that our approach improves the performance of
RS image segmentation over various state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE Geoscience and Remote Sensing Letters (GRSL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PriorFusion: Unified Integration of Priors for Robust Road Perception in
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuewei Tang, Mengmeng Yang, Tuopu Wen, Peijin Jia, Le Cui, Mingshang Luo, Kehua Sheng, Bo Zhang, Diange Yang, Kun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing interest in autonomous driving, there is an increasing
demand for accurate and reliable road perception technologies. In complex
environments without high-definition map support, autonomous vehicles must
independently interpret their surroundings to ensure safe and robust
decision-making. However, these scenarios pose significant challenges due to
the large number, complex geometries, and frequent occlusions of road elements.
A key limitation of existing approaches lies in their insufficient exploitation
of the structured priors inherently present in road elements, resulting in
irregular, inaccurate predictions. To address this, we propose PriorFusion, a
unified framework that effectively integrates semantic, geometric, and
generative priors to enhance road element perception. We introduce an
instance-aware attention mechanism guided by shape-prior features, then
construct a data-driven shape template space that encodes low-dimensional
representations of road elements, enabling clustering to generate anchor points
as reference priors. We design a diffusion-based framework that leverages these
prior anchors to generate accurate and complete predictions. Experiments on
large-scale autonomous driving datasets demonstrate that our method
significantly improves perception accuracy, particularly under challenging
conditions. Visualization results further confirm that our approach produces
more accurate, regular, and coherent predictions of road elements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable
  and Intelligent Embodied Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06669v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06669v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         AgiBot-World-Contributors, Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, Shu Jiang, Yuxin Jiang, Cheng Jing, Hongyang Li, Jialu Li, Chiming Liu, Yi Liu, Yuxiang Lu, Jianlan Luo, Ping Luo, Yao Mu, Yuehan Niu, Yixuan Pan, Jiangmiao Pang, Yu Qiao, Guanghui Ren, Cheng Ruan, Jiaqi Shan, Yongjian Shen, Chengshi Shi, Mingkang Shi, Modi Shi, Chonghao Sima, Jianheng Song, Huijie Wang, Wenhao Wang, Dafeng Wei, Chengen Xie, Guo Xu, Junchi Yan, Cunbiao Yang, Lei Yang, Shukai Yang, Maoqing Yao, Jia Zeng, Chi Zhang, Qinglin Zhang, Bin Zhao, Chengyue Zhao, Jiaqi Zhao, Jianchao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore how scalable robot data can address real-world challenges for
generalized robotic manipulation. Introducing AgiBot World, a large-scale
platform comprising over 1 million trajectories across 217 tasks in five
deployment scenarios, we achieve an order-of-magnitude increase in data scale
compared to existing datasets. Accelerated by a standardized collection
pipeline with human-in-the-loop verification, AgiBot World guarantees
high-quality and diverse data distribution. It is extensible from grippers to
dexterous hands and visuo-tactile sensors for fine-grained skill acquisition.
Building on top of data, we introduce Genie Operator-1 (GO-1), a novel
generalist policy that leverages latent action representations to maximize data
utilization, demonstrating predictable performance scaling with increased data
volume. Policies pre-trained on our dataset achieve an average performance
improvement of 30% over those trained on Open X-Embodiment, both in in-domain
and out-of-distribution scenarios. GO-1 exhibits exceptional capability in
real-world dexterous and long-horizon tasks, achieving over 60% success rate on
complex tasks and outperforming prior RDT approach by 32%. By open-sourcing the
dataset, tools, and models, we aim to democratize access to large-scale,
high-quality robot data, advancing the pursuit of scalable and general-purpose
intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://agibot-world.com/. Github repo:
  https://github.com/OpenDriveLab/AgiBot-World. The author list is ordered
  alphabetically by surname, with detailed contributions provided in the
  appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junzhe Lu, Jing Lin, Hongkun Dou, Ailing Zeng, Yue Deng, Xian Liu, Zhongang Cai, Lei Yang, Yulun Zhang, Haoqian Wang, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DPoser-X, a diffusion-based prior model for 3D whole-body human
poses. Building a versatile and robust full-body human pose prior remains
challenging due to the inherent complexity of articulated human poses and the
scarcity of high-quality whole-body pose datasets. To address these
limitations, we introduce a Diffusion model as body Pose prior (DPoser) and
extend it to DPoser-X for expressive whole-body human pose modeling. Our
approach unifies various pose-centric tasks as inverse problems, solving them
through variational diffusion sampling. To enhance performance on downstream
applications, we introduce a novel truncated timestep scheduling method
specifically designed for pose data characteristics. We also propose a masked
training mechanism that effectively combines whole-body and part-specific
datasets, enabling our model to capture interdependencies between body parts
while avoiding overfitting to specific actions. Extensive experiments
demonstrate DPoser-X's robustness and versatility across multiple benchmarks
for body, hand, face, and full-body pose modeling. Our model consistently
outperforms state-of-the-art alternatives, establishing a new benchmark for
whole-body human pose prior modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025 (oral); Code released: https://github.com/moonbow721/DPoser</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R1-VL: Learning to Reason with Multimodal Large Language Models via
  Step-wise Group Relative Policy Optimization <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies generally enhance MLLMs' reasoning capabilities via supervised
fine-tuning on high-quality chain-of-thought reasoning data, which often leads
models to merely imitate successful reasoning paths without understanding what
the wrong reasoning paths are. In this work, we aim to enhance the MLLMs'
reasoning ability beyond passively imitating positive reasoning paths. To this
end, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new
online reinforcement learning framework that enables MLLMs to self-improve
reasoning ability via simple, effective and dense step-wise rewarding.
Specifically, StepGRPO introduces two novel rule-based reasoning rewards:
Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity
Reward (StepRVR). StepRAR rewards the reasoning paths that contain necessary
intermediate reasoning steps via a soft key-step matching technique, while
StepRAR rewards reasoning paths that follow a well-structured and logically
consistent reasoning process through a reasoning completeness and logic
evaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series
of MLLMs with outstanding capabilities in step-by-step reasoning. Extensive
experiments over 8 benchmarks demonstrate the superiority of our methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preserving Topological and Geometric Embeddings for Point Cloud Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.19121v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.19121v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyue Zhou, Zelong Tan, Hongxiao Wang, Ya-li Li, Shengjin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering point clouds involves the sequential process of sampling and
restoration, yet existing methods struggle to effectively leverage both
topological and geometric attributes. To address this, we propose an end-to-end
architecture named \textbf{TopGeoFormer}, which maintains these critical
properties throughout the sampling and restoration phases. First, we revisit
traditional feature extraction techniques to yield topological embedding using
a continuous mapping of relative relationships between neighboring points, and
integrate it in both phases for preserving the structure of the original space.
Second, we propose the \textbf{InterTwining Attention} to fully merge
topological and geometric embeddings, which queries shape with local awareness
in both phases to form a learnable 3D shape context facilitated with
point-wise, point-shape-wise, and intra-shape features. Third, we introduce a
full geometry loss and a topological constraint loss to optimize the embeddings
in both Euclidean and topological spaces. The geometry loss uses inconsistent
matching between coarse-to-fine generations and targets for reconstructing
better geometric details, and the constraint loss limits embedding variances
for better approximation of the topological space. In experiments, we
comprehensively analyze the circumstances using the conventional and
learning-based sampling/upsampling/recovery algorithms. The quantitative and
qualitative results demonstrate that our method significantly outperforms
existing sampling and recovery methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VSA: Faster Video Diffusion with Trainable Sparse Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13389v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13389v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D
attention, even though most of the attention mass concentrates on a small
subset of positions. We turn this observation into VSA, a trainable,
hardware-efficient sparse attention that replaces full attention at \emph{both}
training and inference. In VSA, a lightweight coarse stage pools tokens into
tiles and identifies high-weight \emph{critical tokens}; a fine stage computes
token-level attention only inside those tiles subjecting to block computing
layout to ensure hard efficiency. This leads to a single differentiable kernel
that trains end-to-end, requires no post-hoc profiling, and sustains 85\% of
FlashAttention3 MFU. We perform a large sweep of ablation studies and
scaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA
reaches a Pareto point that cuts training FLOPS by 2.53$\times$ with no drop in
diffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention
time by 6$\times$ and lowers end-to-end generation time from 31s to 18s with
comparable quality. These results establish trainable sparse attention as a
practical alternative to full attention and a key enabler for further scaling
of video diffusion models. Code will be available at
https://github.com/hao-ai-lab/FastVideo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Style Content Decomposition-based Data Augmentation for Domain
  Generalizable Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20619v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20619v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Shen, Peng Cao, Jinzhu Yang, Osmar R. Zaiane, Zhaolin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to domain shifts across diverse medical imaging modalities, learned
segmentation models often suffer significant performance degradation during
deployment. These domain shifts, typically caused by variations in imaging
systems, generally comprise two principal components: 1) \textbf{"style"
shifts}, referring to global disparities in image properties such as
illumination, contrast, and color; and 2) \textbf{"content" shifts}, which
involve local discrepancies in anatomical structures. To address domain shifts
in medical image segmentation, a core challenge arises: how can we decouple the
factors within images that determine their "style" and "content" components? To
this end, we first propose a linear style-content decomposition method that
factorizes an image into style codes and content maps, explicitly modeling the
"style" and "content" components. Building on this, we introduce a
\textbf{Sty}le-\textbf{Con}tent decomposition-based data \textbf{a}ugmentation
algorithm (StyCona), which leverages this decomposition strategy to guide
augmentation of both the global style and local content of source-domain
images, enabling the training of a well-generalized model for
domain-generalizable medical image segmentation. StyCona is a simple yet
effective plug-and-play module that substantially improves model generalization
without requiring additional training parameters or modifications to
segmentation model architectures. Experiments on cardiac magnetic resonance
imaging and fundus photography segmentation tasks, with single and multiple
target domains respectively, demonstrate the effectiveness of StyCona and its
superiority over state-of-the-art domain generalization methods. The code will
be released at https://github.com/Senyh/StyCona.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text Embedding Knows How to Quantize Text-Guided Diffusion Models <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.10340v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.10340v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjae Lee, Myungjun Son, Dongjea Kang, Seung-Won Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of diffusion models in image generation tasks such as
text-to-image, the enormous computational complexity of diffusion models limits
their use in resource-constrained environments. To address this, network
quantization has emerged as a promising solution for designing efficient
diffusion models. However, existing diffusion model quantization methods do not
consider input conditions, such as text prompts, as an essential source of
information for quantization. In this paper, we propose a novel quantization
method dubbed Quantization of Language-to-Image diffusion models using text
Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit
precision for every layer at each time step. In addition, QLIP can be
seamlessly integrated into existing quantization methods to enhance
quantization efficiency. Our extensive experiments demonstrate the
effectiveness of QLIP in reducing computational complexity and improving the
quality of the generated images across various datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pairwise Matching of Intermediate Representations for Fine-grained
  Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.22881v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.22881v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lauren Shrack, Timm Haucke, Antoine Salaün, Arjun Subramonian, Sara Beery
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The differences between images belonging to fine-grained categories are often
subtle and highly localized, and existing explainability techniques for deep
learning models are often too diffuse to provide useful and interpretable
explanations. We propose a new explainability method (PAIR-X) that leverages
both intermediate model activations and backpropagated relevance scores to
generate fine-grained, highly-localized pairwise visual explanations. We use
animal and building re-identification (re-ID) as a primary case study of our
method, and we demonstrate qualitatively improved results over a diverse set of
explainability baselines on 35 public re-ID datasets. In interviews, animal
re-ID experts found PAIR-X to be a meaningful improvement over existing
baselines for deep model explainability, and suggested that its visualizations
would be directly applicable to their work. We also propose a novel
quantitative evaluation metric for our method, and demonstrate that PAIR-X
visualizations appear more plausible for correct image matches than incorrect
ones even when the model similarity score for the pairs is the same. By
improving interpretability, PAIR-X enables humans to better distinguish correct
and incorrect matches. Our code is available at:
https://github.com/pairx-explains/pairx
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">22</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hubness Reduction with Dual Bank Sinkhorn Normalization for Cross-Modal
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxin Pan, Haishuai Wang, Fangyu Wu, Peng Zhang, Jiajun Bu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The past decade has witnessed rapid advancements in cross-modal retrieval,
with significant progress made in accurately measuring the similarity between
cross-modal pairs. However, the persistent hubness problem, a phenomenon where
a small number of targets frequently appear as nearest neighbors to numerous
queries, continues to hinder the precision of similarity measurements. Despite
several proposed methods to reduce hubness, their underlying mechanisms remain
poorly understood. To bridge this gap, we analyze the widely-adopted Inverted
Softmax approach and demonstrate its effectiveness in balancing target
probabilities during retrieval. Building on these insights, we propose a
probability-balancing framework for more effective hubness reduction. We
contend that balancing target probabilities alone is inadequate and, therefore,
extend the framework to balance both query and target probabilities by
introducing Sinkhorn Normalization (SN). Notably, we extend SN to scenarios
where the true query distribution is unknown, showing that current methods,
which rely solely on a query bank to estimate target hubness, produce
suboptimal results due to a significant distributional gap between the query
bank and targets. To mitigate this issue, we introduce Dual Bank Sinkhorn
Normalization (DBSN), incorporating a corresponding target bank alongside the
query bank to narrow this distributional gap. Our comprehensive evaluation
across various cross-modal retrieval tasks, including image-text retrieval,
video-text retrieval, and audio-text retrieval, demonstrates consistent
performance improvements, validating the effectiveness of both SN and DBSN. All
codes are publicly available at https://github.com/ppanzx/DBSN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACMMM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decomposed Reasoning with Reinforcement Learning for Relevance
  Assessment in UGC Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaowei Yuan, Lei Jin, Haoxin Zhang, Yan Gao, Yi Wu, Yao Hu, Ziyang Huang, Jun Zhao, Kang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) plays a critical role in user-generated
content (UGC) platforms, but its effectiveness depends heavily on accurate
relevance assessment of query-document pairs. Despite recent advances in
applying large language models (LLMs) to relevance modeling, UGC platforms
present unique challenges: 1) ambiguous user intent due to sparse user feedback
in RAG scenarios, and 2) substantial noise introduced by informal and
unstructured language. To address these issues, we propose the Reinforced
Reasoning Model for Relevance Assessment (R3A), which introduces a decomposed
reasoning framework over queries and candidate documents before scoring. R3A
first leverages auxiliary high-ranked documents within the platform to infer
latent query intent. It then performs verbatim fragment extraction to justify
relevance decisions, thereby reducing errors caused by noisy UGC. Based on a
reinforcement learning framework, R3A is optimized to mitigate distortions
arising from ambiguous queries and unstructured content. Experimental results
show that R3A significantly outperforms existing baseline methods in terms of
relevance accuracy, across both offline benchmarks and online experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions
  in IDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Cipollone, Egor Bogomolov, Arie van Deursen, Maliheh Izadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Token-level code completion is one of the most critical features in modern
Integrated Development Environments (IDEs). It assists developers by suggesting
relevant identifiers and APIs during coding. While completions are typically
derived from static analysis, their usefulness depends heavily on how they are
ranked, as correct predictions buried deep in the list are rarely seen by
users. Most current systems rely on hand-crafted heuristics or lightweight
machine learning models trained on user logs, which can be further improved to
capture context information and generalize across projects and coding styles.
In this work, we propose a new scoring approach to ranking static completions
using language models in a lightweight and model-agnostic way. Our method
organizes all valid completions into a prefix tree and performs a single greedy
decoding pass to collect token-level scores across the tree. This enables a
precise token-aware ranking without needing beam search, prompt engineering, or
model adaptations. The approach is fast, architecture-agnostic, and compatible
with already deployed models for code completion. These findings highlight a
practical and effective pathway for integrating language models into already
existing tools within IDEs, and ultimately providing smarter and more
responsive developer assistance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Forgetting and Spatio-Temporal Periodic Interest Modeling for
  Local-Life Service Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyu Hu, Hao Guo, Yuan Tian, Erpeng Xue, Jianyang Wang, Xianyang Qi, Hongxiang Lin, Lei Wang, Sheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of the booming digital economy, recommendation systems, as a
key link connecting users and numerous services, face challenges in modeling
user behavior sequences on local-life service platforms, including the sparsity
of long sequences and strong spatio-temporal dependence. Such challenges can be
addressed by drawing an analogy to the forgetting process in human memory. This
is because users' responses to recommended content follow the recency effect
and the cyclicality of memory. By exploring this, this paper introduces the
forgetting curve and proposes Spatio-Temporal periodic Interest Modeling (STIM)
with long sequences for local-life service recommendation. STIM integrates
three key components: a dynamic masking module based on the forgetting curve,
which is used to extract both recent spatiotemporal features and periodic
spatiotemporal features; a query-based mixture of experts (MoE) approach that
can adaptively activate expert networks under different dynamic masks, enabling
the collaborative modeling of time, location, and items; and a hierarchical
multi-interest network unit, which captures multi-interest representations by
modeling the hierarchical interactions between the shallow and deep semantics
of users' recent behaviors. By introducing the STIM method, we conducted online
A/B tests and achieved a 1.54\% improvement in gross transaction volume (GTV).
In addition, extended offline experiments also showed improvements. STIM has
been deployed in a large-scale local-life service recommendation system,
serving hundreds of millions of daily active users in core application
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Chunks and Graphs: Retrieval-Augmented Generation through
  Triplet-Driven Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengbo Gong, Xianfeng Tang, Carl Yang, Wei jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is critical for reducing hallucinations
and incorporating external knowledge into Large Language Models (LLMs).
However, advanced RAG systems face a trade-off between performance and
efficiency. Multi-round RAG approaches achieve strong reasoning but incur
excessive LLM calls and token costs, while Graph RAG methods suffer from
computationally expensive, error-prone graph construction and retrieval
redundancy. To address these challenges, we propose T$^2$RAG, a novel framework
that operates on a simple, graph-free knowledge base of atomic triplets.
T$^2$RAG leverages an LLM to decompose questions into searchable triplets with
placeholders, which it then iteratively resolves by retrieving evidence from
the triplet database. Empirical results show that T$^2$RAG significantly
outperforms state-of-the-art multi-round and Graph RAG methods, achieving an
average performance gain of up to 11\% across six datasets while reducing
retrieval costs by up to 45\%. Our code is available at
https://github.com/rockcor/T2RAG
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Embedding in the Graph Fractional Fourier Transform Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changjie Sheng, Zhichao Zhang, Wei Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spectral graph embedding plays a critical role in graph representation
learning by generating low-dimensional vector representations from graph
spectral information. However, the embedding space of traditional spectral
embedding methods often exhibit limited expressiveness, failing to exhaustively
capture latent structural features across alternative transform domains. To
address this issue, we use the graph fractional Fourier transform to extend the
existing state-of-the-art generalized frequency filtering embedding (GEFFE)
into fractional domains, giving birth to the generalized fractional filtering
embedding (GEFRFE), which enhances embedding informativeness via the graph
fractional domain. The GEFRFE leverages graph fractional domain filtering and a
nonlinear composition of eigenvector components derived from a fractionalized
graph Laplacian. To dynamically determine the fractional order, two parallel
strategies are introduced: search-based optimization and a ResNet18-based
adaptive learning. Extensive experiments on six benchmark datasets demonstrate
that the GEFRFE captures richer structural features and significantly enhance
classification performance. Notably, the proposed method retains computational
complexity comparable to GEFFE approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uni-Layout: Integrating Human Feedback in Unified Layout Generation and
  Evaluation <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Lu, Yanyin Chen, Wei Feng, Jiahao Fan, Fengheng Li, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law, Jian Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Layout generation plays a crucial role in enhancing both user experience and
design efficiency. However, current approaches suffer from task-specific
generation capabilities and perceptually misaligned evaluation metrics, leading
to limited applicability and ineffective measurement. In this paper, we propose
\textit{Uni-Layout}, a novel framework that achieves unified generation,
human-mimicking evaluation and alignment between the two. For universal
generation, we incorporate various layout tasks into a single taxonomy and
develop a unified generator that handles background or element contents
constrained tasks via natural language prompts. To introduce human feedback for
the effective evaluation of layouts, we build \textit{Layout-HF100k}, the first
large-scale human feedback dataset with 100,000 expertly annotated layouts.
Based on \textit{Layout-HF100k}, we introduce a human-mimicking evaluator that
integrates visual and geometric information, employing a Chain-of-Thought
mechanism to conduct qualitative assessments alongside a confidence estimation
module to yield quantitative measurements. For better alignment between the
generator and the evaluator, we integrate them into a cohesive system by
adopting Dynamic-Margin Preference Optimization (DMPO), which dynamically
adjusts margins based on preference strength to better align with human
judgments. Extensive experiments show that \textit{Uni-Layout} significantly
outperforms both task-specific and general-purpose methods. Our code is
publicly available at https://github.com/JD-GenX/Uni-Layout.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agentic Personalized Fashion Recommendation in the Age of Generative AI:
  Challenges, Opportunities, and Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yashar Deldjoo, Nima Rafiee, Mahdyar Ravanbakhsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fashion recommender systems (FaRS) face distinct challenges due to rapid
trend shifts, nuanced user preferences, intricate item-item compatibility, and
the complex interplay among consumers, brands, and influencers. Traditional
recommendation approaches, largely static and retrieval-focused, struggle to
effectively capture these dynamic elements, leading to decreased user
satisfaction and elevated return rates. This paper synthesizes both academic
and industrial viewpoints to map the distinctive output space and stakeholder
ecosystem of modern FaRS, identifying the complex interplay among users,
brands, platforms, and influencers, and highlighting the unique data and
modeling challenges that arise.
  We outline a research agenda for industrial FaRS, centered on five
representative scenarios spanning static queries, outfit composition, and
multi-turn dialogue, and argue that mixed-modality refinement-the ability to
combine image-based references (anchors) with nuanced textual constraints-is a
particularly critical task for real-world deployment. To this end, we propose
an Agentic Mixed-Modality Refinement (AMMR) pipeline, which fuses multimodal
encoders with agentic LLM planners and dynamic retrieval, bridging the gap
between expressive user intent and fast-changing fashion inventories. Our work
shows that moving beyond static retrieval toward adaptive, generative, and
stakeholder-aware systems is essential to satisfy the evolving expectations of
fashion consumers and brands.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Hu, Zijie Xin, Xirong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ad-hoc Video Search (AVS) involves using a textual query to search for
multiple relevant videos in a large collection of unlabeled short videos. The
main challenge of AVS is the visual diversity of relevant videos. A simple
query such as "Find shots of a man and a woman dancing together indoors" can
span a multitude of environments, from brightly lit halls and shadowy bars to
dance scenes in black-and-white animations. It is therefore essential to
retrieve relevant videos as comprehensively as possible. Current solutions for
the AVS task primarily fuse multiple features into one or more common spaces,
yet overlook the need for diverse spaces. To fully exploit the expressive
capability of individual features, we propose LPD, short for Learning Partially
Decorrelated common spaces. LPD incorporates two key innovations:
feature-specific common space construction and the de-correlation loss.
Specifically, LPD learns a separate common space for each video and text
feature, and employs de-correlation loss to diversify the ordering of negative
samples across different spaces. To enhance the consistency of multi-space
convergence, we designed an entropy-based fair multi-space triplet ranking
loss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify
the effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces
highlight its ability to enhance result diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACMMM2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding User Preferences for Interaction Styles in Conversational
  Recommender Systems: The Predictive Role of System Qualities, User
  Experience, and Traits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raj Mahmud, Shlomo Berkovsky, Mukesh Prasad, A. Baki Kocaballi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Recommender Systems (CRSs) deliver personalised
recommendations through multi-turn natural language dialogue and increasingly
support both task-oriented and exploratory interactions. Yet, the factors
shaping user interaction preferences remain underexplored. In this
within-subjects study (\(N = 139\)), participants experienced two scripted CRS
dialogues, rated their experiences, and indicated the importance of eight
system qualities. Logistic regression revealed that preference for the
exploratory interaction was predicted by enjoyment, usefulness, novelty, and
conversational quality. Unexpectedly, perceived effectiveness was also
associated with exploratory preference. Clustering uncovered five latent user
profiles with distinct dialogue style preferences. Moderation analyses
indicated that age, gender, and control preference significantly influenced
these choices. These findings integrate affective, cognitive, and trait-level
predictors into CRS user modelling and inform autonomy-sensitive,
value-adaptive dialogue design. The proposed predictive and adaptive framework
applies broadly to conversational AI systems seeking to align dynamically with
evolving user needs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at OZCHI 2025. 21 pages, 9 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research Knowledge Graphs in NFDI4DataScience: Key Activities,
  Achievements, and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanishka Silva, Marcel R. Ackermann, Heike Fliegl, Genet-Asefa Gesese, Fidan Limani, Philipp Mayr, Peter Mutschke, Allard Oelen, Muhammad Asif Suryani, Sharmila Upadhyaya, Benjamin Zapilko, Harald Sack, Stefan Dietze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As research in Artificial Intelligence and Data Science continues to grow in
volume and complexity, it becomes increasingly difficult to ensure
transparency, reproducibility, and discoverability. To address these
challenges, as research artifacts should be understandable and usable by
machines, the NFDI4DataScience consortium is developing and providing Research
Knowledge Graphs (RKGs). Building upon earlier works, this paper presents
recent progress in creating semantically rich RKGs using standardized
ontologies, shared vocabularies, and automated Information Extraction
techniques. Key achievements include the development of the NFDI4DS ontology,
metadata standards, tools, and services designed to support the FAIR
principles, as well as community-led projects and various implementations of
RKGs. Together, these efforts aim to capture and connect the complex
relationships between datasets, models, software, and scientific publications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Voronoi Diagram Encoded Hashing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Xu, Kai Ming Ting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of learning to hash (L2H) is to derive data-dependent hash functions
from a given data distribution in order to map data from the input space to a
binary coding space. Despite the success of L2H, two observations have cast
doubt on the source of the power of L2H, i.e., learning. First, a recent study
shows that even using a version of locality sensitive hashing functions without
learning achieves binary representations that have comparable accuracy as those
of L2H, but with less time cost. Second, existing L2H methods are constrained
to three types of hash functions: thresholding, hyperspheres, and hyperplanes
only. In this paper, we unveil the potential of Voronoi diagrams in hashing.
Voronoi diagram is a suitable candidate because of its three properties. This
discovery has led us to propose a simple and efficient no-learning binary
hashing method, called Voronoi Diagram Encoded Hashing (VDeH), which constructs
a set of hash functions through a data-dependent similarity measure and
produces independent binary bits through encoded hashing. We demonstrate
through experiments on several benchmark datasets that VDeH achieves superior
performance and lower computational cost compared to existing state-of-the-art
methods under the same bit length.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal
  Entity Linking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Liu, Junwen Li, Kaiwen Li, Tong Ruan, Chao Wang, Xinyan He, Zongyu Wang, Xuezhi Cao, Jingping Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal entity linking plays a crucial role in a wide range of
applications. Recent advances in large language model-based methods have become
the dominant paradigm for this task, effectively leveraging both textual and
visual modalities to enhance performance. Despite their success, these methods
still face two challenges, including unnecessary incorporation of image data in
certain scenarios and the reliance only on a one-time extraction of visual
features, which can undermine their effectiveness and accuracy. To address
these challenges, we propose a novel LLM-based framework for the multimodal
entity linking task, called Intra- and Inter-modal Collaborative Reflections.
This framework prioritizes leveraging text information to address the task.
When text alone is insufficient to link the correct entity through intra- and
inter-modality evaluations, it employs a multi-round iterative strategy that
integrates key visual clues from various aspects of the image to support
reasoning and enhance matching accuracy. Extensive experiments on three widely
used public datasets demonstrate that our framework consistently outperforms
current state-of-the-art methods in the task, achieving improvements of 3.2%,
5.1%, and 1.6%, respectively. Our code is available at
https://github.com/ziyan-xiaoyu/I2CR/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, accepted by ACMMM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Generation to Consumption: Personalized List Value Estimation for
  Re-ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaike Zhang, Xiaobei Wang, Xiaoyu Liu, Shuchang Liu, Hailan Yang, Xiang Li, Fei Sun, Qi Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Re-ranking is critical in recommender systems for optimizing the order of
recommendation lists, thus improving user satisfaction and platform revenue.
Most existing methods follow a generator-evaluator paradigm, where the
evaluator estimates the overall value of each candidate list. However, they
often ignore the fact that users may exit before consuming the full list,
leading to a mismatch between estimated generation value and actual consumption
value. To bridge this gap, we propose CAVE, a personalized Consumption-Aware
list Value Estimation framework. CAVE formulates the list value as the
expectation over sub-list values, weighted by user-specific exit probabilities
at each position. The exit probability is decomposed into an interest-driven
component and a stochastic component, the latter modeled via a Weibull
distribution to capture random external factors such as fatigue. By jointly
modeling sub-list values and user exit behavior, CAVE yields a more faithful
estimate of actual list consumption value. We further contribute three
large-scale real-world list-wise benchmarks from the Kuaishou platform, varying
in size and user activity patterns. Extensive experiments on these benchmarks,
two Amazon datasets, and online A/B testing on Kuaishou show that CAVE
consistently outperforms strong baselines, highlighting the benefit of
explicitly modeling user exits in re-ranking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FinCPRG: A Bidirectional Generation Pipeline for Hierarchical Queries
  and Rich Relevance in Financial Chinese Passage Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Xu, Beilin Chu, Qinhong Lin, Yixiao Zhong, Fufang Wen, Jiaqi Liu, Binjie Fei, Yu Li, Zhongliang Yang, Linna Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, large language models (LLMs) have demonstrated significant
potential in constructing passage retrieval datasets. However, existing methods
still face limitations in expressing cross-doc query needs and controlling
annotation quality. To address these issues, this paper proposes a
bidirectional generation pipeline, which aims to generate 3-level hierarchical
queries for both intra-doc and cross-doc scenarios and mine additional
relevance labels on top of direct mapping annotation. The pipeline introduces
two query generation methods: bottom-up from single-doc text and top-down from
multi-doc titles. The bottom-up method uses LLMs to disassemble and generate
structured queries at both sentence-level and passage-level simultaneously from
intra-doc passages. The top-down approach incorporates three key financial
elements--industry, topic, and time--to divide report titles into clusters and
prompts LLMs to generate topic-level queries from each cluster. For relevance
annotation, our pipeline not only relies on direct mapping annotation from the
generation relationship but also implements an indirect positives mining method
to enrich the relevant query-passage pairs. Using this pipeline, we constructed
a Financial Passage Retrieval Generated dataset (FinCPRG) from almost 1.3k
Chinese financial research reports, which includes hierarchical queries and
rich relevance labels. Through evaluations of mined relevance labels,
benchmarking and training experiments, we assessed the quality of FinCPRG and
validated its effectiveness as a passage retrieval dataset for both training
and benchmarking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating User Experience in Conversational Recommender Systems: A
  Systematic <span class="highlight-title">Review</span> Across Classical and LLM-Powered Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raj Mahmud, Yufeng Wu, Abdullah Bin Sawad, Shlomo Berkovsky, Mukesh Prasad, A. Baki Kocaballi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Recommender Systems (CRSs) are receiving growing research
attention across domains, yet their user experience (UX) evaluation remains
limited. Existing reviews largely overlook empirical UX studies, particularly
in adaptive and large language model (LLM)-based CRSs. To address this gap, we
conducted a systematic review following PRISMA guidelines, synthesising 23
empirical studies published between 2017 and 2025. We analysed how UX has been
conceptualised, measured, and shaped by domain, adaptivity, and LLM.
  Our findings reveal persistent limitations: post hoc surveys dominate,
turn-level affective UX constructs are rarely assessed, and adaptive behaviours
are seldom linked to UX outcomes. LLM-based CRSs introduce further challenges,
including epistemic opacity and verbosity, yet evaluations infrequently address
these issues. We contribute a structured synthesis of UX metrics, a comparative
analysis of adaptive and nonadaptive systems, and a forward-looking agenda for
LLM-aware UX evaluation. These findings support the development of more
transparent, engaging, and user-centred CRS evaluation practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at OZCHI 2025. 23 pages, 1 figure, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why Generate When You Can Transform? Unleashing Generative Attention for
  Dynamic Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuli Liu, Wenjun Kong, Cheng Luo, Weizhi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommendation (SR) focuses on personalizing user experiences by
predicting future preferences based on historical interactions. Transformer
models, with their attention mechanisms, have become the dominant architecture
in SR tasks due to their ability to capture dependencies in user behavior
sequences. However, traditional attention mechanisms, where attention weights
are computed through query-key transformations, are inherently linear and
deterministic. This fixed approach limits their ability to account for the
dynamic and non-linear nature of user preferences, leading to challenges in
capturing evolving interests and subtle behavioral patterns. Given that
generative models excel at capturing non-linearity and probabilistic
variability, we argue that generating attention distributions offers a more
flexible and expressive alternative compared to traditional attention
mechanisms. To support this claim, we present a theoretical proof demonstrating
that generative attention mechanisms offer greater expressiveness and
stochasticity than traditional deterministic approaches. Building upon this
theoretical foundation, we introduce two generative attention models for SR,
each grounded in the principles of Variational Autoencoders (VAE) and Diffusion
Models (DMs), respectively. These models are designed specifically to generate
adaptive attention distributions that better align with variable user
preferences. Extensive experiments on real-world datasets show our models
significantly outperform state-of-the-art in both accuracy and diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACMMM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Position Bias in Large Language Model Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Bito, Yongli Ren, Estrid He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are being increasingly explored as
general-purpose tools for recommendation tasks, enabling zero-shot and
instruction-following capabilities without the need for task-specific training.
While the research community is enthusiastically embracing LLMs, there are
important caveats to directly adapting them for recommendation tasks. In this
paper, we show that LLM-based recommendation models suffer from position bias,
where the order of candidate items in a prompt can disproportionately influence
the recommendations produced by LLMs. First, we analyse the position bias of
LLM-based recommendations on real-world datasets, where results uncover
systemic biases of LLMs with high sensitivity to input orders. Furthermore, we
introduce a new prompting strategy to mitigate the position bias of LLM
recommendation models called Ranking via Iterative SElection (RISE). We compare
our proposed method against various baselines on key benchmark datasets.
Experiment results show that our method reduces sensitivity to input ordering
and improves stability without requiring model fine-tuning or post-processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controllable and Stealthy Shilling Attacks via Dispersive Latent
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shutong Qiao, Wei Yuan, Junliang Yu, Tong Chen, Quoc Viet Hung Nguyen, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RSs) are now fundamental to various online platforms,
but their dependence on user-contributed data leaves them vulnerable to
shilling attacks that can manipulate item rankings by injecting fake users.
Although widely studied, most existing attack models fail to meet two critical
objectives simultaneously: achieving strong adversarial promotion of target
items while maintaining realistic behavior to evade detection. As a result, the
true severity of shilling threats that manage to reconcile the two objectives
remains underappreciated. To expose this overlooked vulnerability, we present
DLDA, a diffusion-based attack framework that can generate highly effective yet
indistinguishable fake users by enabling fine-grained control over target
promotion. Specifically, DLDA operates in a pre-aligned collaborative embedding
space, where it employs a conditional latent diffusion process to iteratively
synthesize fake user profiles with precise target item control. To evade
detection, DLDA introduces a dispersive regularization mechanism that promotes
variability and realism in generated behavioral patterns. Extensive experiments
on three real-world datasets and five popular RS models demonstrate that,
compared to prior attacks, DLDA consistently achieves stronger item promotion
while remaining harder to detect. These results highlight that modern RSs are
more vulnerable than previously recognized, underscoring the urgent need for
more robust defenses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DRC: Enhancing Personalized Image Generation via Disentangled
  Representation Composition <span class="chip">ACM MM'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyan Xu, Wuqiang Zheng, Wenjie Wang, Fengbin Zhu, Xinting Hu, Yang Zhang, Fuli Feng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized image generation has emerged as a promising direction in
multimodal content creation. It aims to synthesize images tailored to
individual style preferences (e.g., color schemes, character appearances,
layout) and semantic intentions (e.g., emotion, action, scene contexts) by
leveraging user-interacted history images and multimodal instructions. Despite
notable progress, existing methods -- whether based on diffusion models, large
language models, or Large Multimodal Models (LMMs) -- struggle to accurately
capture and fuse user style preferences and semantic intentions. In particular,
the state-of-the-art LMM-based method suffers from the entanglement of visual
features, leading to Guidance Collapse, where the generated images fail to
preserve user-preferred styles or reflect the specified semantics.
  To address these limitations, we introduce DRC, a novel personalized image
generation framework that enhances LMMs through Disentangled Representation
Composition. DRC explicitly extracts user style preferences and semantic
intentions from history images and the reference image, respectively, to form
user-specific latent instructions that guide image generation within LMMs.
Specifically, it involves two critical learning stages: 1) Disentanglement
learning, which employs a dual-tower disentangler to explicitly separate style
and semantic features, optimized via a reconstruction-driven paradigm with
difficulty-aware importance sampling; and 2) Personalized modeling, which
applies semantic-preserving augmentations to effectively adapt the disentangled
representations for robust personalized generation. Extensive experiments on
two benchmarks demonstrate that DRC shows competitive performance while
effectively mitigating the guidance collapse issue, underscoring the importance
of disentangled representation learning for controllable and effective
personalized image generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in ACM MM'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ It's High Time: A <span class="highlight-title">Survey</span> of Temporal Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20243v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20243v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhawna Piryani, Abdelrahman Abdallah, Jamshid Mozafari, Avishek Anand, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time plays a critical role in how information is generated, retrieved, and
interpreted. In this survey, we provide a comprehensive overview of Temporal
Question Answering (TQA), a research area that focuses on answering questions
involving temporal constraints or context. As the amount of time-stamped
content from sources like news articles, web archives, and knowledge bases
increases, systems must address challenges such as detecting temporal intent,
normalizing time expressions, ordering events, and reasoning over evolving or
ambiguous facts. We focus on recent advances in TQA enabled by neural
architectures, especially transformer-based models and Large Language Models
(LLMs), highlighting progress in temporal language modeling,
retrieval-augmented generation (RAG), and temporal reasoning. We also discuss
benchmark datasets and evaluation strategies designed to test temporal
robustness, recency awareness, and generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "Beyond the past": Leveraging Audio and Human Memory for Sequential
  Music Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viet-Anh Tran, Bruno Sguerra, Gabriel Meseguer-Brocal, Lea Briand, Manuel Moussallam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On music streaming services, listening sessions are often composed of a
balance of familiar and new tracks. Recently, sequential recommender systems
have adopted cognitive-informed approaches, such as Adaptive Control of
Thought-Rational (ACT-R), to successfully improve the prediction of the most
relevant tracks for the next user session. However, one limitation of using a
model inspired by human memory (or the past), is that it struggles to recommend
new tracks that users have not previously listened to. To bridge this gap, here
we propose a model that leverages audio information to predict in advance the
ACT-R-like activation of new tracks and incorporates them into the
recommendation scoring process. We demonstrate the empirical effectiveness of
the proposed model using proprietary data, which we publicly release along with
the model's source code to foster future research in this field.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LOST: Low-rank and Sparse <span class="highlight-title">Pre-train</span>ing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxi Li, Lu Yin, Li Shen, Jinjin Xu, Liwu Xu, Tianjin Huang, Wenwu Wang, Shiwei Liu, Xilu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have achieved remarkable performance
across a wide range of tasks, their massive scale incurs prohibitive
computational and memory costs for pre-training from scratch. Recent studies
have investigated the use of low-rank parameterization as a means of reducing
model size and training cost. In this context, sparsity is often employed as a
complementary technique to recover important information lost in low-rank
compression by capturing salient features in the residual space. However,
existing approaches typically combine low-rank and sparse components in a
simplistic or ad hoc manner, often resulting in undesirable performance
degradation compared to full-rank training. In this paper, we propose
\textbf{LO}w-rank and \textbf{S}parse pre-\textbf{T}raining (\textbf{LOST}) for
LLMs, a novel method that ingeniously integrates low-rank and sparse structures
to enable effective training of LLMs from scratch under strict efficiency
constraints. LOST applies singular value decomposition to weight matrices,
preserving the dominant low-rank components, while allocating the remaining
singular values to construct channel-wise sparse components to complement the
expressiveness of low-rank training. We evaluate LOST on LLM pretraining
ranging from 60M to 7B parameters. Our experiments show that LOST achieves
competitive or superior performance compared to full-rank models, while
significantly reducing both memory and compute overhead. Moreover, Code is
available at
\href{https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models}{LOST
Repo}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAK: Emergent Audio Effects from Minimal Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Austin Rockman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate that a single 3x3 convolutional kernel can produce emergent
audio effects when trained on 200 samples from a personalized corpus. We
achieve this through two key techniques: (1) Conditioning Aware Kernels (CAK),
where output = input + (learned_pattern x control), with a soft-gate mechanism
supporting identity preservation at zero control; and (2) AuGAN (Audit GAN),
which reframes adversarial training from "is this real?" to "did you apply the
requested value?" Rather than learning to generate or detect forgeries, our
networks cooperate to verify control application, discovering unique
transformations. The learned kernel exhibits a diagonal structure creating
frequency-dependent temporal shifts that are capable of producing musical
effects based on input characteristics. Our results show the potential of
adversarial training to discover audio transformations from minimal data,
enabling new approaches to effect design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, code and other resources at
  https://github.com/gloame-ai/cak-audio/tree/main/cak-audio</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastCSP: Accelerated Molecular Crystal Structure Prediction with
  Universal Model for Atoms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vahe Gharakhanyan, Yi Yang, Luis Barroso-Luque, Muhammed Shuaibi, Daniel S. Levine, Kyle Michel, Viachaslau Bernat, Misko Dzamba, Xiang Fu, Meng Gao, Xingyu Liu, Keian Noori, Lafe J. Purvis, Tingling Rao, Brandon M. Wood, Ammar Rizvi, Matt Uyttendaele, Andrew J. Ouderkirk, Chiara Daraio, C. Lawrence Zitnick, Arman Boromand, Noa Marom, Zachary W. Ulissi, Anuroop Sriram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crystal Structure Prediction (CSP) of molecular crystals plays a central role
in applications, such as pharmaceuticals and organic electronics. CSP is
challenging and computationally expensive due to the need to explore a large
search space with sufficient accuracy to capture energy differences of a few
kJ/mol between polymorphs. Dispersion-inclusive density functional theory (DFT)
provides the required accuracy but its computational cost is impractical for a
large number of putative structures. We introduce FastCSP, an open-source,
high-throughput CSP workflow based on machine learning interatomic potentials
(MLIPs). FastCSP combines random structure generation using Genarris 3.0 with
geometry relaxation and free energy calculations powered entirely by the
Universal Model for Atoms (UMA) MLIP. We benchmark FastCSP on a curated set of
28 mostly rigid molecules, demonstrating that our workflow consistently
generates known experimental structures and ranks them within 5 kJ/mol per
molecule of the global minimum. Our results demonstrate that universal MLIPs
can be used across diverse compounds without requiring system-specific tuning.
Moreover, the speed and accuracy afforded by UMA eliminate the need for
classical force fields in the early stages of CSP and for final re-ranking with
DFT. The open-source release of the entire FastCSP workflow significantly
lowers the barrier to accessing CSP. CSP results for a single system can be
obtained within hours on tens of modern GPUs, making high-throughput crystal
structure prediction feasible for a broad range of scientific applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, 19 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instance-Optimal Uniformity Testing and Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Blanc, Clément L. Canonne, Erik Waingarten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the uniformity testing task, an algorithm is provided with samples from an
unknown probability distribution over a (known) finite domain, and must decide
whether it is the uniform distribution, or, alternatively, if its total
variation distance from uniform exceeds some input distance parameter. This
question has received a significant amount of interest and its complexity is,
by now, fully settled. Yet, we argue that it fails to capture many scenarios of
interest, and that its very definition as a gap problem in terms of a
prespecified distance may lead to suboptimal performance.
  To address these shortcomings, we introduce the problem of uniformity
tracking, whereby an algorithm is required to detect deviations from uniformity
(however they may manifest themselves) using as few samples as possible, and be
competitive against an optimal algorithm knowing the distribution profile in
hindsight. Our main contribution is a
$\operatorname{polylog}(\operatorname{opt})$-competitive uniformity tracking
algorithm. We obtain this result by leveraging new structural results on
Poisson mixtures, which we believe to be of independent interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>FOCS 2025, to appear</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Actionable Counterfactual Explanations Using Bayesian Networks and Path
  Planning with Applications to Environmental Quality Improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrique Valero-Leal, Pedro Larrañaga, Concha Bielza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual explanations study what should have changed in order to get an
alternative result, enabling end-users to understand machine learning
mechanisms with counterexamples. Actionability is defined as the ability to
transform the original case to be explained into a counterfactual one. We
develop a method for actionable counterfactual explanations that, unlike
predecessors, does not directly leverage training data. Rather, data is only
used to learn a density estimator, creating a search landscape in which to
apply path planning algorithms to solve the problem and masking the endogenous
data, which can be sensitive or private. We put special focus on estimating the
data density using Bayesian networks, demonstrating how their enhanced
interpretability is useful in high-stakes scenarios in which fairness is
raising concern. Using a synthetic benchmark comprised of 15 datasets, our
proposal finds more actionable and simpler counterfactuals than the current
state-of-the-art algorithms. We also test our algorithm with a real-world
Environmental Protection Agency dataset, facilitating a more efficient and
equitable study of policies to improve the quality of life in United States of
America counties. Our proposal captures the interaction of variables, ensuring
equity in decisions, as policies to improve certain domains of study (air,
water quality, etc.) can be detrimental in others. In particular, the
sociodemographic domain is often involved, where we find important variables
related to the ongoing housing crisis that can potentially have a severe
negative impact on communities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tensor Dynamic Mode Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqin He, Mengqi Hu, Yifei Lou, Can Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic mode decomposition (DMD) has become a powerful data-driven method for
analyzing the spatiotemporal dynamics of complex, high-dimensional systems.
However, conventional DMD methods are limited to matrix-based formulations,
which might be inefficient or inadequate for modeling inherently
multidimensional data including images, videos, and higher-order networks. In
this letter, we propose tensor dynamic mode decomposition (TDMD), a novel
extension of DMD to third-order tensors based on the recently developed
T-product framework. By incorporating tensor factorization techniques, TDMD
achieves more efficient computation and better preservation of spatial and
temporal structures in multiway data for tasks such as state reconstruction and
dynamic component separation, compared to standard DMD with data flattening. We
demonstrate the effectiveness of TDMD on both synthetic and real-world
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoML-Med: A Framework for Automated Machine Learning in Medical
  Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Francia, Maurizio Leone, Giorgio Leonardi, Stefania Montani, Marzio Pennisi, Manuel Striani, Sandra D'Alfonso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical datasets are typically affected by issues such as missing values,
class imbalance, a heterogeneous feature types, and a high number of features
versus a relatively small number of samples, preventing machine learning models
from obtaining proper results in classification and regression tasks. This
paper introduces AutoML-Med, an Automated Machine Learning tool specifically
designed to address these challenges, minimizing user intervention and
identifying the optimal combination of preprocessing techniques and predictive
models. AutoML-Med's architecture incorporates Latin Hypercube Sampling (LHS)
for exploring preprocessing methods, trains models using selected metrics, and
utilizes Partial Rank Correlation Coefficient (PRCC) for fine-tuned
optimization of the most influential preprocessing steps. Experimental results
demonstrate AutoML-Med's effectiveness in two different clinical settings,
achieving higher balanced accuracy and sensitivity, which are crucial for
identifying at-risk patients, compared to other state-of-the-art tools.
AutoML-Med's ability to improve prediction results, especially in medical
datasets with sparse data and class imbalance, highlights its potential to
streamline Machine Learning applications in healthcare.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, preprint for conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous
  Healthcare Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Zhu, Yifan Qi, Zixiang Wang, Lei Gu, Dehao Sui, Haoran Hu, Xichen Zhang, Ziyi He, Liantao Ma, Lequan Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The efficacy of AI agents in healthcare research is hindered by their
reliance on static, predefined strategies. This creates a critical limitation:
agents can become better tool-users but cannot learn to become better strategic
planners, a crucial skill for complex domains like healthcare. We introduce
HealthFlow, a self-evolving AI agent that overcomes this limitation through a
novel meta-level evolution mechanism. HealthFlow autonomously refines its own
high-level problem-solving policies by distilling procedural successes and
failures into a durable, strategic knowledge base. To anchor our research and
facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark
featuring complex, realistic health data analysis tasks derived from
peer-reviewed clinical research. Our comprehensive experiments demonstrate that
HealthFlow's self-evolving approach significantly outperforms state-of-the-art
agent frameworks. This work marks a necessary shift from building better
tool-users to designing smarter, self-evolving task-managers, paving the way
for more autonomous and effective AI for scientific discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/yhzhu99/HealthFlow</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepKoopFormer: A Koopman Enhanced <span class="highlight-title">Transformer</span> Based Architecture for
  Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Forootani, Mohammad Khosravi, Masoud Barati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting plays a vital role across scientific, industrial, and
environmental domains, especially when dealing with high-dimensional and
nonlinear systems. While Transformer-based models have recently achieved
state-of-the-art performance in long-range forecasting, they often suffer from
interpretability issues and instability in the presence of noise or dynamical
uncertainty. In this work, we propose DeepKoopFormer, a principled forecasting
framework that combines the representational power of Transformers with the
theoretical rigor of Koopman operator theory. Our model features a modular
encoder-propagator-decoder structure, where temporal dynamics are learned via a
spectrally constrained, linear Koopman operator in a latent space. We impose
structural guarantees-such as bounded spectral radius, Lyapunov based energy
regularization, and orthogonal parameterization to ensure stability and
interpretability. Comprehensive evaluations are conducted on both synthetic
dynamical systems, real-world climate dataset (wind speed and surface
pressure), financial time series (cryptocurrency), and electricity generation
dataset using the Python package that is prepared for this purpose. Across all
experiments, DeepKoopFormer consistently outperforms standard LSTM and baseline
Transformer models in terms of accuracy, robustness to noise, and long-term
forecasting stability. These results establish DeepKoopFormer as a flexible,
interpretable, and robust framework for forecasting in high dimensional and
dynamical settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entity Representation Learning Through Onsite-Offsite Graph for
  Pinterset Ads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayin Jin, Zhimeng Pan, Yang Tang, Jiarui Feng, Kungang Li, Chongyuan Xiang, Jiacheng Li, Runze Su, Siping Ji, Han Sun, Ling Leng, Prathibha Deshikachar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNN) have been extensively applied to industry
recommendation systems, as seen in models like GraphSage\cite{GraphSage},
TwHIM\cite{TwHIM}, LiGNN\cite{LiGNN} etc. In these works, graphs were
constructed based on users' activities on the platforms, and various graph
models were developed to effectively learn node embeddings. In addition to
users' onsite activities, their offsite conversions are crucial for Ads models
to capture their shopping interest. To better leverage offsite conversion data
and explore the connection between onsite and offsite activities, we
constructed a large-scale heterogeneous graph based on users' onsite ad
interactions and opt-in offsite conversion activities. Furthermore, we
introduced TransRA (TransR\cite{TransR} with Anchors), a novel Knowledge Graph
Embedding (KGE) model, to more efficiently integrate graph embeddings into Ads
ranking models. However, our Ads ranking models initially struggled to directly
incorporate Knowledge Graph Embeddings (KGE), and only modest gains were
observed during offline experiments. To address this challenge, we employed the
Large ID Embedding Table technique and innovated an attention based KGE
finetuning approach within the Ads ranking models. As a result, we observed a
significant AUC lift in Click-Through Rate (CTR) and Conversion Rate (CVR)
prediction models. Moreover, this framework has been deployed in Pinterest's
Ads Engagement Model and contributed to $2.69\%$ CTR lift and $1.34\%$ CPC
reduction. We believe the techniques presented in this paper can be leveraged
by other large-scale industrial models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trustworthy scientific inference for inverse problems with generative
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Carzon, Luca Masserano, Joshua D. Ingram, Alex Shen, Antonio Carlos Herling Ribeiro Junior, Tommaso Dorigo, Michele Doro, Joshua S. Speagle, Rafael Izbicki, Ann B. Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative artificial intelligence (AI) excels at producing complex data
structures (text, images, videos) by learning patterns from training examples.
Across scientific disciplines, researchers are now applying generative models
to ``inverse problems'' to infer hidden parameters from observed data. While
these methods can handle intractable models and large-scale studies, they can
also produce biased or overconfident conclusions. We present a solution with
Frequentist-Bayes (FreB), a mathematically rigorous protocol that reshapes
AI-generated probability distributions into confidence regions that
consistently include true parameters with the expected probability, while
achieving minimum size when training and target data align. We demonstrate
FreB's effectiveness by tackling diverse case studies in the physical sciences:
identifying unknown sources under dataset shift, reconciling competing
theoretical models, and mitigating selection bias and systematics in
observational studies. By providing validity guarantees with interpretable
diagnostics, FreB enables trustworthy scientific inference across fields where
direct likelihood evaluation remains impossible or prohibitively expensive.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Riemannian Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Wang, Tongxin Li, Chris Ding, Jicong Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph data often exhibits complex geometric heterogeneity, where structures
with varying local curvature, such as tree-like hierarchies and dense
communities, coexist within a single network. Existing geometric GNNs, which
embed graphs into single fixed-curvature manifolds or discrete product spaces,
struggle to capture this diversity. We introduce Adaptive Riemannian Graph
Neural Networks (ARGNN), a novel framework that learns a continuous and
anisotropic Riemannian metric tensor field over the graph. It allows each node
to determine its optimal local geometry, enabling the model to fluidly adapt to
the graph's structural landscape. Our core innovation is an efficient
parameterization of the node-wise metric tensor, specializing to a learnable
diagonal form that captures directional geometric information while maintaining
computational tractability. To ensure geometric regularity and stable training,
we integrate a Ricci flow-inspired regularization that smooths the learned
manifold. Theoretically, we establish the rigorous geometric evolution
convergence guarantee for ARGNN and provide a continuous generalization that
unifies prior fixed or mixed-curvature GNNs. Empirically, our method
demonstrates superior performance on both homophilic and heterophilic benchmark
datasets with the ability to capture diverse structures adaptively. Moreover,
the learned geometries both offer interpretable insights into the underlying
graph structure and empirically corroborate our theoretical analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis
  in Low-Data Regimes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyi Liu, Yujia Zheng, Yongqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of machine learning on tabular data in specialized domains is
severely limited by data scarcity. While generative models offer a solution,
traditional methods falter in low-data regimes, and recent Large Language
Models (LLMs) often ignore the explicit dependency structure of tabular data,
leading to low-fidelity synthetics. To address these limitations, we introduce
StructSynth, a novel framework that integrates the generative power of LLMs
with robust structural control. StructSynth employs a two-stage architecture.
First, it performs explicit structure discovery to learn a Directed Acyclic
Graph (DAG) from the available data. Second, this learned structure serves as a
high-fidelity blueprint to steer the LLM's generation process, forcing it to
adhere to the learned feature dependencies and thereby ensuring the generated
data respects the underlying structure by design. Our extensive experiments
demonstrate that StructSynth produces synthetic data with significantly higher
structural integrity and downstream utility than state-of-the-art methods. It
proves especially effective in challenging low-data scenarios, successfully
navigating the trade-off between privacy preservation and statistical fidelity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-Efficient Routed Fine-Tuning: Mixture-of-Experts Demands
  Mixture of Adaptation Modules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Liu, Yunpu Ma, Yuetian Lu, Shuo Chen, Zifeng Ding, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) benefits from a dynamic routing mechanism among
their specialized experts, which existing Parameter- Efficient Fine-Tuning
(PEFT) strategies fail to leverage. This motivates us to investigate whether
adaptation modules themselves should incorporate routing mechanisms to align
with MoE's multi-expert architecture. We analyze dynamics of core components
when applying PEFT to MoE language models and examine how different routing
strategies affect adaptation effectiveness. Extensive experiments adapting
OLMoE-1B-7B and Mixtral-8x7B on various commonsense and math reasoning tasks
validate the performance and efficiency of our routed approach. We identify the
optimal configurations for different scenarios and provide empirical analyses
with practical insights to facilitate better PEFT and MoE applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is a preprint under review. arXiv admin note: text overlap
  with arXiv:2411.08212</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAMA: Enhancing Mathematical Reasoning in Large Language Models with
  Causal Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Zan, Keli Zhang, Ruichu Cai, Lujia Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated strong performance across a
wide range of tasks, yet they still struggle with complex mathematical
reasoning, a challenge fundamentally rooted in deep structural dependencies. To
address this challenge, we propose \textbf{CA}usal \textbf{MA}thematician
(\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit,
reusable mathematical structure. In the learning stage, CAMA first constructs
the \textbf{M}athematical \textbf{C}ausal \textbf{G}raph (\textbf{MCG}), a
high-level representation of solution strategies, by combining LLM priors with
causal discovery algorithms applied to a corpus of question-solution pairs. The
resulting MCG encodes essential knowledge points and their causal dependencies.
To better align the graph with downstream reasoning tasks, CAMA further refines
the MCG through iterative feedback derived from a selected subset of the
question-solution pairs. In the reasoning stage, given a new question, CAMA
dynamically extracts a task-relevant subgraph from the MCG, conditioned on both
the question content and the LLM's intermediate reasoning trace. This subgraph,
which encodes the most pertinent knowledge points and their causal
dependencies, is then injected back into the LLM to guide its reasoning
process. Empirical results on real-world datasets show that CAMA significantly
improves LLM performance on challenging mathematical problems. Furthermore, our
experiments demonstrate that structured guidance consistently outperforms
unstructured alternatives, and that incorporating asymmetric causal
relationships yields greater improvements than using symmetric associations
alone.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EHSAN: Leveraging Chat<span class="highlight-title">GPT</span> in a Hybrid Framework for Arabic Aspect-Based
  Sentiment Analysis in Healthcare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eman Alamoudi, Ellis Solaiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arabic-language patient feedback remains under-analysed because dialect
diversity and scarce aspect-level sentiment labels hinder automated assessment.
To address this gap, we introduce EHSAN, a data-centric hybrid pipeline that
merges ChatGPT pseudo-labelling with targeted human review to build the first
explainable Arabic aspect-based sentiment dataset for healthcare. Each sentence
is annotated with an aspect and sentiment label (positive, negative, or
neutral), forming a pioneering Arabic dataset aligned with healthcare themes,
with ChatGPT-generated rationales provided for each label to enhance
transparency. To evaluate the impact of annotation quality on model
performance, we created three versions of the training data: a fully supervised
set with all labels reviewed by humans, a semi-supervised set with 50% human
review, and an unsupervised set with only machine-generated labels. We
fine-tuned two transformer models on these datasets for both aspect and
sentiment classification. Experimental results show that our Arabic-specific
model achieved high accuracy even with minimal human supervision, reflecting
only a minor performance drop when using ChatGPT-only labels. Reducing the
number of aspect classes notably improved classification metrics across the
board. These findings demonstrate an effective, scalable approach to Arabic
aspect-based sentiment analysis (SA) in healthcare, combining large language
model annotation with human expertise to produce a robust and explainable
dataset. Future directions include generalisation across hospitals, prompt
refinement, and interpretable data-driven modelling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Feature Selection based on Rule-based Learning for Explainable
  Classification with Uncertainty Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Fumanal-Idocin, Raquel Fernandez-Peralta, Javier Andreu-Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic feature selection (DFS) offers a compelling alternative to
traditional, static feature selection by adapting the selected features to each
individual sample. Unlike classical methods that apply a uniform feature set,
DFS customizes feature selection per sample, providing insight into the
decision-making process for each case. DFS is especially significant in
settings where decision transparency is key, i.e., clinical decisions; however,
existing methods use opaque models, which hinder their applicability in
real-life scenarios. This paper introduces a novel approach leveraging a
rule-based system as a base classifier for the DFS process, which enhances
decision interpretability compared to neural estimators. We also show how this
method provides a quantitative measure of uncertainty for each feature query
and can make the feature selection process computationally lighter by
constraining the feature search space. We also discuss when greedy selection of
conditional mutual information is equivalent to selecting features that
minimize the difference with respect to the global model predictions. Finally,
we demonstrate the competitive performance of our rule-based DFS approach
against established and state-of-the-art greedy and RL methods, which are
mostly considered opaque, compared to our explainable rule-based system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable AI Methods for Neuroimaging: Systematic Failures of Common
  Tools, the Need for Domain-Specific Validation, and a Proposal for Safe
  Application 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nys Tjade Siegel, James H. Cole, Mohamad Habes, Stefan Haufe, Kerstin Ritter, Marc-André Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trustworthy interpretation of deep learning models is critical for
neuroimaging applications, yet commonly used Explainable AI (XAI) methods lack
rigorous validation, risking misinterpretation. We performed the first
large-scale, systematic comparison of XAI methods on ~45,000 structural brain
MRIs using a novel XAI validation framework. This framework establishes
verifiable ground truth by constructing prediction tasks with known signal
sources - from localized anatomical features to subject-specific clinical
lesions - without artificially altering input images. Our analysis reveals
systematic failures in two of the most widely used methods: GradCAM
consistently failed to localize predictive features, while Layer-wise Relevance
Propagation generated extensive, artifactual explanations that suggest
incompatibility with neuroimaging data characteristics. Our results indicate
that these failures stem from a domain mismatch, where methods with design
principles tailored to natural images require substantial adaptation for
neuroimaging data. In contrast, the simpler, gradient-based method SmoothGrad,
which makes fewer assumptions about data structure, proved consistently
accurate, suggesting its conceptual simplicity makes it more robust to this
domain shift. These findings highlight the need for domain-specific adaptation
and validation of XAI methods, suggest that interpretations from prior
neuroimaging studies using standard XAI methodology warrant re-evaluation, and
provide urgent guidance for practical application of XAI in neuroimaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Noori, Pratik Devkota, Somya Mohanty, Prashanti Manda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated annotation of clinical text with standardized medical concepts is
critical for enabling structured data extraction and decision support. SNOMED
CT provides a rich ontology for labeling clinical entities, but manual
annotation is labor-intensive and impractical at scale. This study introduces a
neural sequence labeling approach for SNOMED CT concept recognition using a
Bidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text
with domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences
into overlapping 19-token chunks enriched with contextual, syntactic, and
morphological features. The Bi-GRU model assigns IOB tags to identify concept
spans and achieves strong performance with a 90 percent F1-score on the
validation set. These results surpass traditional rule-based systems and match
or exceed existing neural models. Qualitative analysis shows effective handling
of ambiguous terms and misspellings. Our findings highlight that lightweight
RNN-based architectures can deliver high-quality clinical concept annotation
with significantly lower computational cost than transformer-based models,
making them well-suited for real-world deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSI Obfuscation: Single-Antenna Transmitters Can Not Hide from
  Adversarial Multi-Antenna Radio Localization Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phillip Stephan, Florian Euchner, Stephan ten Brink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of modern telecommunication systems to locate users and objects
in the radio environment raises justified privacy concerns. To prevent
unauthorized localization, single-antenna transmitters can obfuscate the signal
by convolving it with a randomized sequence prior to transmission, which alters
the channel state information (CSI) estimated at the receiver. However, this
strategy is only effective against CSI-based localization systems deploying
single-antenna receivers. Inspired by the concept of blind multichannel
identification, we propose a simple CSI recovery method for multi-antenna
receivers to extract channel features that ensure reliable user localization
regardless of the transmitted signal. We comparatively evaluate the impact of
signal obfuscation and the proposed recovery method on the localization
performance of CSI fingerprinting, channel charting, and classical
triangulation using real-world channel measurements. This work aims to
demonstrate the necessity for further efforts to protect the location privacy
of users from adversarial radio-based localization systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What are you sinking? A geometric approach on attention sink 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valeria Ruscio, Umberto Nanni, Fabrizio Silvestri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention sink (AS) is a consistent pattern in transformer attention maps
where certain tokens (often special tokens or positional anchors)
disproportionately attract attention from other tokens. We show that in
transformers, AS is not an architectural artifact, but it is the manifestation
of a fundamental geometric principle: the establishment of reference frames
that anchor representational spaces. We analyze several architectures and
identify three distinct reference frame types, centralized, distributed, and
bidirectional, that correlate with the attention sink phenomenon. We show that
they emerge during the earliest stages of training as optimal solutions to the
problem of establishing stable coordinate systems in high-dimensional spaces.
We show the influence of architecture components, particularly position
encoding implementations, on the specific type of reference frame. This
perspective transforms our understanding of transformer attention mechanisms
and provides insights for both architecture design and the relationship with
AS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solved in Unit Domain: JacobiNet for Differentiable Coordinate
  Transformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Chen, Jianchuan Yang, Junjie Zhang, Runnan Yang, Xu Liu, Hong Wang, Ziyu Ren, Wenqi Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-Informed Neural Networks (PINNs) are effective for solving PDEs by
incorporating physical laws into the learning process. However, they face
challenges with irregular boundaries, leading to instability and slow
convergence due to inconsistent normalization, inaccurate boundary enforcement,
and imbalanced loss terms. A common solution is to map the domain to a regular
space, but traditional methods rely on case-specific meshes and simple
geometries, limiting their compatibility with modern frameworks. To overcome
these limitations, we introduce JacobiNet, a neural network-based coordinate
transformation method that learns continuous, differentiable mappings from
supervised point pairs. Utilizing lightweight MLPs, JacobiNet allows for direct
Jacobian computation via autograd and integrates seamlessly with downstream
PINNs, enabling end-to-end differentiable PDE solving without the need for
meshing or explicit Jacobian computation. JacobiNet effectively addresses
normalization challenges, facilitates hard constraints of boundary conditions,
and mitigates the long-standing imbalance among loss terms. It demonstrates
significant improvements, reducing the relative L2 error from 0.287-0.637 to
0.013-0.039, achieving an average accuracy improvement of 18.3*. In vessel-like
domains, it enables rapid mapping for unseen geometries, improving prediction
accuracy by 3.65* and achieving over 10* speedup, showcasing its
generalization, accuracy, and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to CMAME, revision in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Communication and Computation Efficient Split Federated Learning in
  O-RAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunxian Gu, Chaoqun You, Bangbang Ren, Deke Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The hierarchical architecture of Open Radio Access Network (O-RAN) has
enabled a new Federated Learning (FL) paradigm that trains models using data
from non- and near-real-time (near-RT) Radio Intelligent Controllers (RICs).
However, the ever-increasing model size leads to longer training time,
jeopardizing the deadline requirements for both non-RT and near-RT RICs. To
address this issue, split federated learning (SFL) offers an approach by
offloading partial model layers from near-RT-RIC to high-performance
non-RT-RIC. Nonetheless, its deployment presents two challenges: (i) Frequent
data/gradient transfers between near-RT-RIC and non-RT-RIC in SFL incur
significant communication cost in O-RAN. (ii) Proper allocation of
computational and communication resources in O-RAN is vital to satisfying the
deadline and affects SFL convergence. Therefore, we propose SplitMe, an SFL
framework that exploits mutual learning to alternately and independently train
the near-RT-RIC's model and the non-RT-RIC's inverse model, eliminating
frequent transfers. The ''inverse'' of the inverse model is derived via a
zeroth-order technique to integrate the final model. Then, we solve a joint
optimization problem for SplitMe to minimize overall resource costs with
deadline-aware selection of near-RT-RICs and adaptive local updates. Our
numerical results demonstrate that SplitMe remarkably outperforms FL frameworks
like SFL, FedAvg and O-RANFed regarding costs and convergence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextual Graph <span class="highlight-title">Transformer</span>: A Small Language Model for Enhanced
  Engineering Document Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karan Reddy, Mayukha Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard transformer-based language models, while powerful for general text,
often struggle with the fine-grained syntax and entity relationships in complex
technical, engineering documents. To address this, we propose the Contextual
Graph Transformer (CGT), a hybrid neural architecture that combines Graph
Neural Networks (GNNs) and Transformers for domain-specific question answering.
CGT constructs a dynamic graph over input tokens using sequential, skip-gram,
and semantic similarity edges, which is processed by GATv2Conv layers for local
structure learning. These enriched embeddings are then passed to a Transformer
encoder to capture global dependencies. Unlike generic large models, technical
domains often require specialized language models with stronger
contextualization and structure awareness. CGT offers a parameter-efficient
solution for such use cases. Integrated into a Retrieval-Augmented Generation
(RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7%
higher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from
CGTs ability to jointly model structural token interactions and long-range
semantic coherence. The model is trained from scratch using a two-phase
approach: pretraining on general text followed by fine-tuning on
domain-specific manuals. This highlights CGTs adaptability to technical
language, enabling better grounding, entity tracking, and retrieval-augmented
responses in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic
  Representations in LLaMA 3.2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Merullo, Arjun Khurana, Oliver McLaughlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models demonstrate proficiency on phonetic tasks, such as
rhyming, without explicit phonetic or auditory grounding. In this work, we
investigate how \verb|Llama-3.2-1B-Instruct| represents token-level phonetic
information. Our results suggest that Llama uses a rich internal model of
phonemes to complete phonetic tasks. We provide evidence for high-level
organization of phoneme representations in its latent space. In doing so, we
also identify a ``phoneme mover head" which promotes phonetic information
during rhyming tasks. We visualize the output space of this head and find that,
while notable differences exist, Llama learns a model of vowels similar to the
standard IPA vowel chart for humans, despite receiving no direct supervision to
do so.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causality and Interpretability for Electrical Distribution System faults 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karthik Peddi, Sai Ram Aditya Parisineni, Hemanth Macharla, Mayukha Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal analysis helps us understand variables that are responsible for system
failures. This improves fault detection and makes system more reliable. In this
work, we present a new method that combines causal inference with machine
learning to classify faults in electrical distribution systems (EDS) using
graph-based models. We first build causal graphs using transfer entropy (TE).
Each fault case is represented as a graph, where the nodes are features such as
voltage and current, and the edges demonstrate how these features influence
each other. Then, the graphs are classified using machine learning and
GraphSAGE where the model learns from both the node values and the structure of
the graph to predict the type of fault. To make the predictions understandable,
we further developed an integrated approach using GNNExplainer and Captums
Integrated Gradients to highlight the nodes (features) that influences the most
on the final prediction. This gives us clear insights into the possible causes
of the fault. Our experiments show high accuracy: 99.44% on the EDS fault
dataset, which is better than state of art models. By combining causal graphs
with machine learning, our method not only predicts faults accurately but also
helps understand their root causes. This makes it a strong and practical tool
for improving system reliability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnalogCoder-Pro: Unifying Analog Circuit Generation and Optimization via
  Multi-modal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Lai, Souradip Poddar, Sungyoung Lee, Guojin Chen, Mengkang Hu, Bei Yu, Ping Luo, David Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances in analog design automation, analog front-end design still
heavily depends on expert intuition and iterative simulations, underscoring
critical gaps in fully automated optimization for performance-critical
applications. Recently, the rapid development of Large Language Models (LLMs)
has brought new promise to analog design automation. However, existing work
remains in its early stages, and holistic joint optimization for practical
end-to-end solutions remains largely unexplored. We propose AnalogCoder-Pro, a
unified multimodal LLM-based framework that integrates generative capabilities
and optimization techniques to jointly explore circuit topologies and optimize
device sizing, automatically generating performance-specific, fully sized
schematic netlists. AnalogCoder-Pro employs rejection sampling for fine-tuning
LLMs on high-quality synthesized circuit data and introduces a multimodal
diagnosis and repair workflow based on functional specifications and waveform
images. By leveraging LLMs to interpret generated circuit netlists,
AnalogCoder-Pro automates the extraction of critical design parameters and the
formulation of parameter spaces, establishing an end-to-end workflow for
simultaneous topology generation and device sizing optimization. Extensive
experiments demonstrate that these orthogonal approaches significantly improve
the success rate of analog circuit design and enhance circuit performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PoeTone: A Framework for Constrained Generation of Structured Chinese
  Songci with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Qu, Shuzhou Yuan, Michael Färber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a systematic investigation into the constrained
generation capabilities of large language models (LLMs) in producing Songci, a
classical Chinese poetry form characterized by strict structural, tonal, and
rhyme constraints defined by Cipai templates. We first develop a comprehensive,
multi-faceted evaluation framework that includes: (i) a formal conformity
score, (ii) automated quality assessment using LLMs, (iii) human evaluation,
and (iv) classification-based probing tasks. Using this framework, we evaluate
the generative performance of 18 LLMs, including 3 proprietary models and 15
open-source models across four families, under five prompting strategies:
zero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.
Finally, we propose a Generate-Critic architecture in which the evaluation
framework functions as an automated critic. Leveraging the critic's feedback as
a reward signal, we fine-tune three lightweight open-source LLMs via supervised
fine-tuning (SFT), resulting in improvements of up to 5.88% in formal
conformity. Our findings offer new insights into the generative strengths and
limitations of LLMs in producing culturally significant and formally
constrained literary texts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Distributional Dependent Performance of Classical and Neural Routing
  Solvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniela Thyssens, Tim Dernedde, Wilson Sentanoe, Lars Schmidt-Thieme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Combinatorial Optimization aims to learn to solve a class of
combinatorial problems through data-driven methods and notably through
employing neural networks by learning the underlying distribution of problem
instances. While, so far neural methods struggle to outperform highly
engineered problem specific meta-heuristics, this work explores a novel
approach to formulate the distribution of problem instances to learn from and,
more importantly, plant a structure in the sampled problem instances. In
application to routing problems, we generate large problem instances that
represent custom base problem instance distributions from which training
instances are sampled. The test instances to evaluate the methods on the
routing task consist of unseen problems sampled from the underlying large
problem instance. We evaluate representative NCO methods and specialized
Operation Research meta heuristics on this novel task and demonstrate that the
performance gap between neural routing solvers and highly specialized
meta-heuristics decreases when learning from sub-samples drawn from a fixed
base node distribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clinical Expert Uncertainty Guided Generalized Label Smoothing for
  Medical Noisy Label Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunyu Zhang, Lin Gu, Liangchen Liu, Yingke Chen, Bingyang Wang, Jin Yan, Yingying Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many previous studies have proposed extracting image labels from clinical
notes to create large-scale medical image datasets at a low cost. However,
these approaches inherently suffer from label noise due to uncertainty from the
clinical experts. When radiologists and physicians analyze medical images to
make diagnoses, they often include uncertainty-aware notes such as ``maybe'' or
``not excluded''. Unfortunately, current text-mining methods overlook these
nuances, resulting in the creation of noisy labels. Existing methods for
handling noisy labels in medical image analysis, which typically address the
problem through post-processing techniques, have largely ignored the important
issue of expert-driven uncertainty contributing to label noise. To better
incorporate the expert-written uncertainty in clinical notes into medical image
analysis and address the label noise issue, we first examine the impact of
clinical expert uncertainty on label noise. We then propose a clinical expert
uncertainty-aware benchmark, along with a label smoothing method, which
significantly improves performance compared to current state-of-the-art
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Graph Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuming Ai, Xunkai Li, Jiaqi Chao, Bowen Fan, Zhengyu Wu, Yinlin Zhu, Rong-Hua Li, Guoren Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The demand for data privacy has led to the development of frameworks like
Federated Graph Learning (FGL), which facilitate decentralized model training.
However, a significant operational challenge in such systems is adhering to the
right to be forgotten. This principle necessitates robust mechanisms for two
distinct types of data removal: the selective erasure of specific entities and
their associated knowledge from local subgraphs and the wholesale removal of a
user's entire dataset and influence. Existing methods often struggle to fully
address both unlearning requirements, frequently resulting in incomplete data
removal or the persistence of residual knowledge within the system. This work
introduces a unified framework, conceived to provide a comprehensive solution
to these challenges. The proposed framework employs a bifurcated strategy
tailored to the specific unlearning request. For fine-grained Meta Unlearning,
it uses prototype gradients to direct the initial local forgetting process,
which is then refined by generating adversarial graphs to eliminate any
remaining data traces among affected clients. In the case of complete client
unlearning, the framework utilizes adversarial graph generation exclusively to
purge the departed client's contributions from the remaining network. Extensive
experiments on multiple benchmark datasets validate the proposed approach. The
framework achieves substantial improvements in model prediction accuracy across
both client and meta-unlearning scenarios when compared to existing methods.
Furthermore, additional studies confirm its utility as a plug-in module, where
it materially enhances the predictive capabilities and unlearning effectiveness
of other established methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Using Machine Learning as a Shape Quality Metric for Liver Point
  Cloud Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khoa Tuan Nguyen, Gaeun Oh, Ho-min Park, Francesca Tozzi, Wouter Willaert, Joris Vankerschaver, Niki Rashidian, Wesley De Neve
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While 3D medical shape generative models such as diffusion models have shown
promise in synthesizing diverse and anatomically plausible structures, the
absence of ground truth makes quality evaluation challenging. Existing
evaluation metrics commonly measure distributional distances between training
and generated sets, while the medical field requires assessing quality at the
individual level for each generated shape, which demands labor-intensive expert
review.
  In this paper, we investigate the use of classical machine learning (ML)
methods and PointNet as an alternative, interpretable approach for assessing
the quality of generated liver shapes. We sample point clouds from the surfaces
of the generated liver shapes, extract handcrafted geometric features, and
train a group of supervised ML and PointNet models to classify liver shapes as
good or bad. These trained models are then used as proxy discriminators to
assess the quality of synthetic liver shapes produced by generative models.
  Our results show that ML-based shape classifiers provide not only
interpretable feedback but also complementary insights compared to expert
evaluation. This suggests that ML classifiers can serve as lightweight,
task-relevant quality metrics in 3D organ shape generation, supporting more
transparent and clinically aligned evaluation protocols in medical shape
modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient and Adaptive Next Edit Suggestion Framework with Zero Human
  Instructions in IDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinfang Chen, Siyang Xiao, Xianying Zhu, Junhong Xie, Ming Liang, Dajun Chen, Wei Jiang, Yong Li, Peng Di
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code editing, including modifying, refactoring, and maintaining existing
code, is the most frequent task in software development and has garnered
significant attention from AI-powered tools. However, existing solutions that
translate explicit natural language instructions into code edits face critical
limitations, such as heavy reliance on human instruction input and high
latency, which hinder their effective integration into a developer's workflow.
We observe that developers' habitual behaviors and coding objectives are often
reflected in their historical editing patterns, making this data key to
addressing existing limitations. To leverage these insights, we propose NES
(Next Edit Suggestion), an LLM-driven code editing framework that delivers an
instruction-free and low-latency experience. Built on a dual-model architecture
and trained with our high-quality SFT and DAPO datasets, NES enhances
productivity by understanding developer intent while optimizing inference to
minimize latency. NES is a scalable, industry-ready solution with a continuous
Tab key interaction workflow, seamlessly adopted by a FinTech company with over
20,000 developers. Evaluations on real-world datasets show NES achieves 75.6%
and 81.6% accuracy in two tasks of predicting next edit locations, alongside
91.36% ES and 27.7% EMR for intent-aligned edits, outperforming SOTA models.
Our open-sourced SFT and DAPO datasets have been demonstrated to enhance the
performance of open-source CodeLLMs. The demonstration of NES is available at
https://youtu.be/yGoyYOe6fbY.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computationally efficient Gauss-Newton reinforcement learning for model
  predictive control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dean Brandner, Sebastien Gros, Sergio Lucia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model predictive control (MPC) is widely used in process control due to its
interpretability and ability to handle constraints. As a parametric policy in
reinforcement learning (RL), MPC offers strong initial performance and low data
requirements compared to black-box policies like neural networks. However, most
RL methods rely on first-order updates, which scale well to large parameter
spaces but converge at most linearly, making them inefficient when each policy
update requires solving an optimal control problem, as is the case with MPC.
While MPC policies are typically sparsely parameterized and thus amenable to
second-order approaches, existing second-order methods demand second-order
policy derivatives, which can be computationally and memory-wise intractable.
  This work introduces a Gauss-Newton approximation of the deterministic policy
Hessian that eliminates the need for second-order policy derivatives, enabling
superlinear convergence with minimal computational overhead. To further improve
robustness, we propose a momentum-based Hessian averaging scheme for stable
training under noisy estimates. We demonstrate the effectiveness of the
approach on a nonlinear continuously stirred tank reactor (CSTR), showing
faster convergence and improved data efficiency over state-of-the-art
first-order methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, submitted to Elsevier</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Large Language Models for End-to-End Affective Computing:
  Benchmarking and Boosting with Generative Knowledge <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miaosen Luo, Jiesen Long, Zequn Li, Yunying Yang, Yuncheng Jiang, Sijie Mai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Affective Computing (MAC) aims to recognize and interpret human
emotions by integrating information from diverse modalities such as text,
video, and audio. Recent advancements in Multimodal Large Language Models
(MLLMs) have significantly reshaped the landscape of MAC by offering a unified
framework for processing and aligning cross-modal information. However,
practical challenges remain, including performance variability across complex
MAC tasks and insufficient understanding of how architectural designs and data
characteristics impact affective analysis. To address these gaps, we conduct a
systematic benchmark evaluation of state-of-the-art open-source MLLMs capable
of concurrently processing audio, visual, and textual modalities across
multiple established MAC datasets. Our evaluation not only compares the
performance of these MLLMs but also provides actionable insights into model
optimization by analyzing the influence of model architectures and dataset
properties. Furthermore, we propose a novel hybrid strategy that combines
generative knowledge prompting with supervised fine-tuning to enhance MLLMs'
affective computing capabilities. Experimental results demonstrate that this
integrated approach significantly improves performance across various MAC
tasks, offering a promising avenue for future research and development in this
field. Our code is released on https://github.com/LuoMSen/MLLM-MAC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyu Li, Zhi Jin, Yuanpeng He, Dongming Jin, Yichi Zhang, Haoran Duan, Nyima Tash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since knowledge graphs (KG) will continue to evolve in real scenarios,
traditional KGE models are only suitable for static knowledge graphs.
Therefore, continual knowledge graph embedding (CKGE) has attracted the
attention of researchers. Currently, a key challenge facing CKGE is that the
model is prone to "catastrophic forgetting", resulting in the loss of
previously learned knowledge. In order to effectively alleviate this problem,
we propose a new CKGE model BAKE. First, we note that the Bayesian posterior
update principle provides a natural continual learning strategy that is
insensitive to data order and can theoretically effectively resist the
forgetting of previous knowledge during data evolution. Different from the
existing CKGE method, BAKE regards each batch of new data as a Bayesian update
of the model prior. Under this framework, as long as the posterior distribution
of the model is maintained, the model can better preserve the knowledge of
early snapshots even after evolving through multiple time snapshots. Secondly,
we propose a continual clustering method for CKGE, which further directly
combats knowledge forgetting by constraining the evolution difference (or
change amplitude) between new and old knowledge between different snapshots. We
conduct extensive experiments on BAKE on multiple datasets, and the results
show that BAKE significantly outperforms existing baseline models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superior resilience to poisoning and amenability to unlearning in
  quantum machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Qin Chen, Shi-Xin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reliability of artificial intelligence hinges on the integrity of its
training data, a foundation often compromised by noise and corruption. Here,
through a comparative study of classical and quantum neural networks on both
classical and quantum data, we reveal a fundamental difference in their
response to data corruption. We find that classical models exhibit brittle
memorization, leading to a failure in generalization. In contrast, quantum
models demonstrate remarkable resilience, which is underscored by a phase
transition-like response to increasing label noise, revealing a critical point
beyond which the model's performance changes qualitatively. We further
establish and investigate the field of quantum machine unlearning, the process
of efficiently forcing a trained model to forget corrupting influences. We show
that the brittle nature of the classical model forms rigid, stubborn memories
of erroneous data, making efficient unlearning challenging, while the quantum
model is significantly more amenable to efficient forgetting with approximate
unlearning methods. Our findings establish that quantum machine learning can
possess a dual advantage of intrinsic resilience and efficient adaptability,
providing a promising paradigm for the trustworthy and robust artificial
intelligence of the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures with references and supplemental materials</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emergence of Fair Leaders via Mediators in Multi-Agent Reinforcement
  Learning <span class="chip">ECAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Dodwadmath, Setareh Maghsudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stackelberg games and their resulting equilibria have received increasing
attention in the multi-agent reinforcement learning literature. Each stage of a
traditional Stackelberg game involves a leader(s) acting first, followed by the
followers. In situations where the roles of leader(s) and followers can be
interchanged, the designated role can have considerable advantages, for
example, in first-mover advantage settings. Then the question arises: Who
should be the leader and when? A bias in the leader selection process can lead
to unfair outcomes. This problem is aggravated if the agents are
self-interested and care only about their goals and rewards. We formally define
this leader selection problem and show its relation to fairness in agents'
returns. Furthermore, we propose a multi-agent reinforcement learning framework
that maximizes fairness by integrating mediators. Mediators have previously
been used in the simultaneous action setting with varying levels of control,
such as directly performing agents' actions or just recommending them. Our
framework integrates mediators in the Stackelberg setting with minimal control
(leader selection). We show that the presence of mediators leads to
self-interested agents taking fair actions, resulting in higher overall
fairness in agents' returns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Role of <span class="highlight-title">Review</span> Process Failures in Affective State Estimation: An
  Empirical Investigation of DEAP <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nazmun N Khan, Taylor Sweet, Chase A Harvey, Calder Knapp, Dean J. Krusienski, David E Thompson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reliability of affective state estimation using EEG data is in question,
given the variability in reported performance and the lack of standardized
evaluation protocols. To investigate this, we reviewed 101 studies, focusing on
the widely used DEAP dataset for emotion recognition. Our analysis revealed
widespread methodological issues that include data leakage from improper
segmentation, biased feature selection, flawed hyperparameter optimization,
neglect of class imbalance, and insufficient methodological reporting. Notably,
we found that nearly 87% of the reviewed papers contained one or more of these
errors. Moreover, through experimental analysis, we observed that such
methodological flaws can inflate the classification accuracy by up to 46%.
These findings reveal fundamental gaps in standardized evaluation practices and
highlight critical deficiencies in the peer review process for machine learning
applications in neuroscience, emphasizing the urgent need for stricter
methodological standards and evaluation protocols.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 4 figures, This is a preprint version of the manuscript. It
  is intended for submission to a peer-reviewed journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASMR: Angular Support for Malfunctioning Client Resilience in Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mirko Konstantin, Moritz Fuchs, Anirban Mukhopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) allows the training of deep neural networks in a
distributed and privacy-preserving manner. However, this concept suffers from
malfunctioning updates sent by the attending clients that cause global model
performance degradation. Reasons for this malfunctioning might be technical
issues, disadvantageous training data, or malicious attacks. Most of the
current defense mechanisms are meant to require impractical prerequisites like
knowledge about the number of malfunctioning updates, which makes them
unsuitable for real-world applications. To counteract these problems, we
introduce a novel method called Angular Support for Malfunctioning Client
Resilience (ASMR), that dynamically excludes malfunctioning clients based on
their angular distance. Our novel method does not require any hyperparameters
or knowledge about the number of malfunctioning clients. Our experiments
showcase the detection capabilities of ASMR in an image classification task on
a histopathological dataset, while also presenting findings on the significance
of dynamically adapting decision boundaries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HGTS-Former: Hierarchical HyperGraph <span class="highlight-title">Transformer</span> for Multivariate Time
  Series Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Wang, Hao Si, Fan Zhang, Xiaoya Zhou, Dengdi Sun, Wanli Lyu, Qingquan Yang, Jin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate time series analysis has long been one of the key research
topics in the field of artificial intelligence. However, analyzing complex time
series data remains a challenging and unresolved problem due to its high
dimensionality, dynamic nature, and complex interactions among variables.
Inspired by the strong structural modeling capability of hypergraphs, this
paper proposes a novel hypergraph-based time series transformer backbone
network, termed HGTS-Former, to address the multivariate coupling in time
series data. Specifically, given the multivariate time series signal, we first
normalize and embed each patch into tokens. Then, we adopt the multi-head
self-attention to enhance the temporal representation of each patch. The
hierarchical hypergraphs are constructed to aggregate the temporal patterns
within each channel and fine-grained relations between different variables.
After that, we convert the hyperedge into node features through the EdgeToNode
module and adopt the feed-forward network to further enhance the output
features. Extensive experiments conducted on two multivariate time series tasks
and eight datasets fully validated the effectiveness of our proposed
HGTS-Former. The source code will be released on
https://github.com/Event-AHU/Time_Series_Analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $ε$-Softmax: Approximating One-Hot Vectors for Mitigating Label
  Noise <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialiang Wang, Xiong Zhou, Deming Zhai, Junjun Jiang, Xiangyang Ji, Xianming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Noisy labels pose a common challenge for training accurate deep neural
networks. To mitigate label noise, prior studies have proposed various robust
loss functions to achieve noise tolerance in the presence of label noise,
particularly symmetric losses. However, they usually suffer from the
underfitting issue due to the overly strict symmetric condition. In this work,
we propose a simple yet effective approach for relaxing the symmetric
condition, namely $\epsilon$-softmax, which simply modifies the outputs of the
softmax layer to approximate one-hot vectors with a controllable error
$\epsilon$. Essentially, $\epsilon$-softmax not only acts as an alternative for
the softmax layer, but also implicitly plays the crucial role in modifying the
loss function. We prove theoretically that $\epsilon$-softmax can achieve
noise-tolerant learning with controllable excess risk bound for almost any loss
function. Recognizing that $\epsilon$-softmax-enhanced losses may slightly
reduce fitting ability on clean datasets, we further incorporate them with one
symmetric loss, thereby achieving a better trade-off between robustness and
effective learning. Extensive experiments demonstrate the superiority of our
method in mitigating synthetic and real-world label noise. The code is
available at https://github.com/cswjl/eps-softmax.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Embedding in the Graph Fractional Fourier Transform Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changjie Sheng, Zhichao Zhang, Wei Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spectral graph embedding plays a critical role in graph representation
learning by generating low-dimensional vector representations from graph
spectral information. However, the embedding space of traditional spectral
embedding methods often exhibit limited expressiveness, failing to exhaustively
capture latent structural features across alternative transform domains. To
address this issue, we use the graph fractional Fourier transform to extend the
existing state-of-the-art generalized frequency filtering embedding (GEFFE)
into fractional domains, giving birth to the generalized fractional filtering
embedding (GEFRFE), which enhances embedding informativeness via the graph
fractional domain. The GEFRFE leverages graph fractional domain filtering and a
nonlinear composition of eigenvector components derived from a fractionalized
graph Laplacian. To dynamically determine the fractional order, two parallel
strategies are introduced: search-based optimization and a ResNet18-based
adaptive learning. Extensive experiments on six benchmark datasets demonstrate
that the GEFRFE captures richer structural features and significantly enhance
classification performance. Notably, the proposed method retains computational
complexity comparable to GEFFE approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Manually Designed Pruning Policies with Second-Level Performance
  Prediction: A Pruning Framework for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuxin Ma, Yunhe Cui, Yongbin Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-uniform structured network pruning methods can effectively reduce Large
Language Model (LLM) size by eliminating redundant channels or layers, offering
lower performance degradation than uniform strategies. However, existing
non-uniform methods rely heavily on manually designed pruning policies (e.g.,
layer importance and scaling factors), and therefore cannot efficiently adapt
to scenarios with dynamic pruning ratio requirements. Additionly, a critical
bottleneck -- the time-consuming evaluation of pruning policies -- further
limits the feasibility of iteratively and dynamically finding optimal pruning
policies. To address these limitations, we propose PPF (Predictive Pruning
Framework), a novel pruning framework for LLMs that eliminates manual design
dependencies via second-level performance prediction. PPF not only supports
real-time pruning decisions under dynamic pruning ratios but is also applicable
to static pruning scenarios. It employs an agent for producing adaptive and
real-time pruning actions, while a lightweight performance predictor that can
evaluate a pruning policy in seconds, significantly speeding up the iterative
optimization process. Experiments on Llama2-7B and Llama3-8B show that PPF can
generate dynamic/static pruning policies and it reduces perplexity by up to
33.4% (dynamic pruning) and 84.78% (static pruning) over existing methods,
outperforming manually designed pruning policies. The performance predictor
achieves second-level performance prediction with high accuracy (prediction
error < 0.0011). It reduces the mean evaluation latency from minute-level (1
minute and 38.02 seconds of test-set evaluation methods) to second-level (1.52
second), achieving over 64 times speedup. Our code will be available at
https://github.com/Ma-zx/PPF .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uni-Layout: Integrating Human Feedback in Unified Layout Generation and
  Evaluation <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Lu, Yanyin Chen, Wei Feng, Jiahao Fan, Fengheng Li, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law, Jian Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Layout generation plays a crucial role in enhancing both user experience and
design efficiency. However, current approaches suffer from task-specific
generation capabilities and perceptually misaligned evaluation metrics, leading
to limited applicability and ineffective measurement. In this paper, we propose
\textit{Uni-Layout}, a novel framework that achieves unified generation,
human-mimicking evaluation and alignment between the two. For universal
generation, we incorporate various layout tasks into a single taxonomy and
develop a unified generator that handles background or element contents
constrained tasks via natural language prompts. To introduce human feedback for
the effective evaluation of layouts, we build \textit{Layout-HF100k}, the first
large-scale human feedback dataset with 100,000 expertly annotated layouts.
Based on \textit{Layout-HF100k}, we introduce a human-mimicking evaluator that
integrates visual and geometric information, employing a Chain-of-Thought
mechanism to conduct qualitative assessments alongside a confidence estimation
module to yield quantitative measurements. For better alignment between the
generator and the evaluator, we integrate them into a cohesive system by
adopting Dynamic-Margin Preference Optimization (DMPO), which dynamically
adjusts margins based on preference strength to better align with human
judgments. Extensive experiments show that \textit{Uni-Layout} significantly
outperforms both task-specific and general-purpose methods. Our code is
publicly available at https://github.com/JD-GenX/Uni-Layout.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Model Guided Reinforcement Learning in Quantitative Trading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Darmanin, Vince Vella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithmic trading requires short-term decisions aligned with long-term
financial goals. While reinforcement learning (RL) has been explored for such
tactical decisions, its adoption remains limited by myopic behavior and opaque
policy rationale. In contrast, large language models (LLMs) have recently
demonstrated strategic reasoning and multi-modal financial signal
interpretation when guided by well-designed prompts.
  We propose a hybrid system where LLMs generate high-level trading strategies
to guide RL agents in their actions. We evaluate (i) the rationale of
LLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and
Maximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results
show improved return and risk metrics over standard RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages (4 pages appendix and references), 6 figures, preprint under
  review for FLLM 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Sliced Fused Gromov-Wasserstein Distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Piening, Robert Beinert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Gromov--Wasserstein (GW) distance and its fused extension (FGW) are
powerful tools for comparing heterogeneous data. Their computation is, however,
challenging since both distances are based on non-convex, quadratic optimal
transport (OT) problems. Leveraging 1D OT, a sliced version of GW has been
proposed to lower the computational burden. Unfortunately, this sliced version
is restricted to Euclidean geometry and loses invariance to isometries,
strongly limiting its application in practice. To overcome these issues, we
propose a novel slicing technique for GW as well as for FGW that is based on an
appropriate lower bound, hierarchical OT, and suitable quadrature rules for the
underlying 1D OT problems. Our novel sliced FGW significantly reduces the
numerical effort while remaining invariant to isometric transformations and
allowing the comparison of arbitrary geometries. We show that our new distance
actually defines a pseudo-metric for structured spaces that bounds FGW from
below and study its interpolation properties between sliced Wasserstein and GW.
Since we avoid the underlying quadratic program, our sliced distance is
numerically more robust and reliable than the original GW and FGW distance;
especially in the context of shape retrieval and graph isomorphism testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting COPD Through Speech Analysis: A <span class="highlight-title">Dataset</span> of Danish Speech and
  Machine Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cuno Sankey-Olsen, Rasmus Hvass Olesen, Tobias Oliver Eberhard, Andreas Triantafyllopoulos, Björn Schuller, Ilhan Aslan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chronic Obstructive Pulmonary Disease (COPD) is a serious and debilitating
disease affecting millions around the world. Its early detection using
non-invasive means could enable preventive interventions that improve quality
of life and patient outcomes, with speech recently shown to be a valuable
biomarker. Yet, its validity across different linguistic groups remains to be
seen. To that end, audio data were collected from 96 Danish participants
conducting three speech tasks (reading, coughing, sustained vowels). Half of
the participants were diagnosed with different levels of COPD and the other
half formed a healthy control group. Subsequently, we investigated different
baseline models using openSMILE features and learnt x-vector embeddings. We
obtained a best accuracy of 67% using openSMILE features and logistic
regression. Our findings support the potential of speech-based analysis as a
non-invasive, remote, and scalable screening tool as part of future COPD
healthcare solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting and measuring respiratory events in horses during exercise
  with a microphone: deep learning vs. standard signal processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeanne I. M. Parmentier, Rhana M. Aarts, Elin Hernlund, Marie Rhodin, Berend Jan van der Zwaag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring respiration parameters such as respiratory rate could be
beneficial to understand the impact of training on equine health and
performance and ultimately improve equine welfare. In this work, we compare
deep learning-based methods to an adapted signal processing method to
automatically detect cyclic respiratory events and extract the dynamic
respiratory rate from microphone recordings during high intensity exercise in
Standardbred trotters. Our deep learning models are able to detect exhalation
sounds (median F1 score of 0.94) in noisy microphone signals and show promising
results on unlabelled signals at lower exercising intensity, where the
exhalation sounds are less recognisable. Temporal convolutional networks were
better at detecting exhalation events and estimating dynamic respiratory rates
(median F1: 0.94, Mean Absolute Error (MAE) $\pm$ Confidence Intervals (CI):
1.44$\pm$1.04 bpm, Limits Of Agreements (LOA): 0.63$\pm$7.06 bpm) than long
short-term memory networks (median F1: 0.90, MAE$\pm$CI: 3.11$\pm$1.58 bpm) and
signal processing methods (MAE$\pm$CI: 2.36$\pm$1.11 bpm). This work is the
first to automatically detect equine respiratory sounds and automatically
compute dynamic respiratory rates in exercising horses. In the future, our
models will be validated on lower exercising intensity sounds and different
microphone placements will be evaluated in order to find the best combination
for regular monitoring.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MicroMix: Efficient Mixed-Precision Quantization with Microscaling
  Formats for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyuan Liu, Haoqian Meng, Yilun Luo, Peng Zhang, Xindian Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization significantly accelerates inference in large language models
(LLMs) by replacing original high-precision matrices with low-precision
counterparts. Recent advances in weight-activation quantization have primarily
focused on mapping both weights and activations to the INT4 format. Although
the new FP4 Tensor Cores in NVIDIA's Blackwell architecture offer up to 4x
speedup over FP16, existing INT4-based kernels fail to fully exploit this
capability due to mismatched data formats. To bridge this gap, we propose
MicroMix, a co-designed mixed-precision quantization algorithm and matrix
multiplication kernel based on Microscaling (MX) data formats. Tailored for the
Blackwell architecture, the MicroMix kernel supports arbitrary combinations of
MXFP4, MXFP6, and MXFP8 channels, and produces BFloat16 outputs. To achieve a
favorable trade-off between accuracy and efficiency for each linear layer, we
introduce quantization thresholds that identify activation elements where
lower-precision formats (MXFP4 or MXFP6) incur excessive quantization error.
Our algorithm selectively allocates higher-precision channels to preserve
accuracy while maintaining compute efficiency. MicroMix achieves competitive or
superior performance across diverse downstream tasks, including zero-shot and
few-shot learning, language modeling, code generation, and mathematical
reasoning. On both consumer-grade (RTX 5070Ti laptop) and server-grade (RTX
5090) GPUs, our kernel delivers at least 20% faster execution than
TensorRT-FP8. Furthermore, when applied to various Llama and Qwen models,
MicroMix consistently improves prefill latency and memory efficiency across a
range of batch sizes compared to TensorRT baselines. Our code is available at
https://github.com/lwy2020/MicroMix.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Posterior Sampling of Probabilistic Word Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Väinö Yrjänäinen, Isac Boström, Måns Magnusson, Johan Jonasson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantifying uncertainty in word embeddings is crucial for reliable inference
from textual data. However, existing Bayesian methods such as Hamiltonian Monte
Carlo (HMC) and mean-field variational inference (MFVI) are either
computationally infeasible for large data or rely on restrictive assumptions.
  We propose a scalable Gibbs sampler using Polya-Gamma augmentation as well as
Laplace approximation and compare them with MFVI and HMC for word embeddings.
In addition, we address non-identifiability in word embeddings. Our Gibbs
sampler and HMC correctly estimate uncertainties, while MFVI does not, and
Laplace approximation only does so on large sample sizes, as expected. Applying
the Gibbs sampler to the US Congress and the Movielens datasets, we demonstrate
the feasibility on larger real data. Finally, as a result of having draws from
the full posterior, we show that the posterior mean of word embeddings improves
over maximum a posteriori (MAP) estimates in terms of hold-out likelihood,
especially for smaller sampling sizes, further strengthening the need for
posterior sampling of word embeddings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BOOST: Bayesian Optimization with Optimal Kernel and Acquisition
  Function Selection Technique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joon-Hyun Park, Mujin Cheon, Dong-Yeun Koh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of Bayesian optimization (BO), a highly sample-efficient
method for expensive black-box problems, is critically governed by the
selection of its hyperparameters, including the kernel and acquisition
functions. This presents a challenge: an inappropriate combination of these can
lead to poor performance and wasted evaluations. While individual improvements
to kernel functions (e.g., tree-based kernels, deep kernel learning) and
acquisition functions (e.g., multi-step lookahead, tree-based planning) have
been explored, the joint and autonomous selection of the best pair of these
fundamental hyperparameters has been overlooked. This forces practitioners to
rely on heuristics or costly manual training. We propose a simple yet effective
framework, BOOST (Bayesian Optimization with Optimal Kernel and Acquisition
Function Selection Technique), that automates this selection. BOOST utilizes a
lightweight, offline evaluation stage to predict the performance of various
kernel-acquisition function pairs and identify the most suitable configuration
before expensive evaluations. BOOST partitions data-in-hand into two subsets: a
reference subset and a query subset, and it prepares all possible
kernel-acquisition pairs from the user's chosen candidates. For each
configuration, BOOST conducts internal BO runs using the reference subset,
evaluating how effectively each pair guides the search toward the optimum in
the unknown query subset, thereby identifying the configuration with the best
retrospective performance for future optimization. Experiments on both
synthetic benchmark functions and real-world hyperparameter optimization tasks
demonstrate that BOOST consistently outperforms standard BO approaches with
fixed hyperparameters, highlighting its effectiveness and robustness in diverse
problem landscapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Compression Based Classification Framework Using Symbolic Dynamics of
  Chaotic Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parth Naik, Harikrishnan N B
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel classification framework grounded in symbolic dynamics and
data compression using chaotic maps. The core idea is to model each class by
generating symbolic sequences from thresholded real-valued training data, which
are then evolved through a one-dimensional chaotic map. For each class, we
compute the transition probabilities of symbolic patterns (e.g., `00', `01',
`10', and `11' for the second return map) and aggregate these statistics to
form a class-specific probabilistic model. During testing phase, the test data
are thresholded and symbolized, and then encoded using the class-wise symbolic
statistics via back iteration, a dynamical reconstruction technique. The
predicted label corresponds to the class yielding the shortest compressed
representation, signifying the most efficient symbolic encoding under its
respective chaotic model. This approach fuses concepts from dynamical systems,
symbolic representations, and compression-based learning. We evaluate the
proposed method: \emph{ChaosComp} on both synthetic and real-world datasets,
demonstrating competitive performance compared to traditional machine learning
algorithms (e.g., macro F1-scores for the proposed method on Breast Cancer
Wisconsin = 0.9531, Seeds = 0.9475, Iris = 0.8317 etc.). Rather than aiming for
state-of-the-art performance, the goal of this research is to reinterpret the
classification problem through the lens of dynamical systems and compression,
which are foundational perspectives in learning theory and information
processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert
  Redundancy Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhuang Xu, Xu Han, Yuanchi Zhang, Yixuan Wang, Yijun Liu, Shiyu Ji, Qingfu Zhu, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are
distinguished by their strong performance scaling with increasing parameters
across a wide range of tasks, yet they also suffer from substantial
computational and storage overheads. Notably, the performance gains of MoE
models do not scale proportionally with the growth in expert parameters. While
prior works attempt to reduce parameters via expert-level pruning, merging, or
decomposition, they still suffer from challenges in both performance and
computational efficiency. In this paper, we address these challenges by
introducing micro-expert as a finer-grained compression unit that spans across
matrices. We first establish a more fundamental perspective, viewing MoE layers
as mixtures of micro-experts, and present CAMERA, a lightweight and
training-free framework for identifying micro-expert redundancy. Our analysis
uncovers significant variance in micro-expert contributions during decoding.
Based on this insight, we further propose CAMERA-P, a structured micro-expert
pruning framework, and CAMERA-Q, a mixed-precision quantization idea designed
for micro-experts. Extensive experiments on nine downstream tasks show that
CAMERA-P consistently outperforms strong baselines under pruning ratios ranging
from 20% to 60%. Furthermore, CAMERA-Q achieves superior results under
aggressive 2-bit quantization, surpassing existing matrix- and channel-level
ideas. Notably, our method enables complete micro-expert analysis of
Qwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NMS: Efficient Edge DNN Training via Near-Memory Sampling on Manifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boran Zhao, Haiduo Huang, Qiwei Dang, Wenzhe Zhao, Tian Xia, Pengju Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep neural networks (DNNs) on edge devices has attracted increasing
attention due to its potential to address challenges related to domain
adaptation and privacy preservation. However, DNNs typically rely on large
datasets for training, which results in substantial energy consumption, making
the training in edge devices impractical. Some dataset compression methods have
been proposed to solve this challenge. For instance, the coreset selection and
dataset distillation reduce the training cost by selecting and generating
representative samples respectively. Nevertheless, these methods have two
significant defects: (1) The necessary of leveraging a DNN model to evaluate
the quality of representative samples, which inevitably introduces inductive
bias of DNN, resulting in a severe generalization issue; (2) All training
images require multiple accesses to the DDR via long-distance PCB connections,
leading to substantial energy overhead. To address these issues, inspired by
the nonlinear manifold stationary of the human brain, we firstly propose a
DNN-free sample-selecting algorithm, called DE-SNE, to improve the
generalization issue. Secondly, we innovatively utilize the near-memory
computing technique to implement DE-SNE, thus only a small fraction of images
need to access the DDR via long-distance PCB. It significantly reduces DDR
energy consumption. As a result, we build a novel expedited DNN training system
with a more efficient in-place Near-Memory Sampling characteristic for edge
devices, dubbed NMS. As far as we know, our NMS is the first DNN-free
near-memory sampling technique that can effectively alleviate generalization
issues and significantly reduce DDR energy caused by dataset access. The
experimental results show that our NMS outperforms the current state-of-the-art
(SOTA) approaches, namely DQ, DQAS, and NeSSA, in model accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Whole-body Representation Learning For Competing Preclinical Disease
  Risk Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitrii Seletkov, Sophie Starck, Ayhan Can Erdur, Yundi Zhang, Daniel Rueckert, Rickmer Braren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable preclinical disease risk assessment is essential to move public
healthcare from reactive treatment to proactive identification and prevention.
However, image-based risk prediction algorithms often consider one condition at
a time and depend on hand-crafted features obtained through segmentation tools.
We propose a whole-body self-supervised representation learning method for the
preclinical disease risk assessment under a competing risk modeling. This
approach outperforms whole-body radiomics in multiple diseases, including
cardiovascular disease (CVD), type 2 diabetes (T2D), chronic obstructive
pulmonary disease (COPD), and chronic kidney disease (CKD). Simulating a
preclinical screening scenario and subsequently combining with cardiac MRI, it
sharpens further the prediction for CVD subgroups: ischemic heart disease
(IHD), hypertensive diseases (HD), and stroke. The results indicate the
translational potential of whole-body representations as a standalone screening
modality and as part of a multi-modal framework within clinical workflows for
early personalized risk stratification. The code is available at
https://github.com/yayapa/WBRLforCR/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative
  Credit Assignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guofu Xie, Yunsheng Shi, Hongtao Tian, Ting Yao, Xiao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Verifiable Rewards (RLVR) has improved the
reasoning abilities of Large Language Models (LLMs) by using rule-based binary
feedback, helping to mitigate reward hacking. However, current RLVR methods
typically treat whole responses as single actions, assigning the same reward to
every token. This coarse-grained feedback hampers precise credit assignment,
making it hard for models to identify which reasoning steps lead to success or
failure, and often results in suboptimal policies and inefficient learning.
Methods like PPO provide credit assignment through value estimation, but often
yield inaccurate and unverifiable signals due to limited sampling. On the other
hand, methods using Process Reward Models can provide step-by-step judgments
for each reasoning step, but they require high-quality process supervision
labels and are time-consuming when applied in online reinforcement learning
(RL). To overcome these limitations, we introduce a simple but efficient method
Credit Assignment Policy Optimization (CAPO). Given a reasoning response
rollout from the policy model, CAPO directly leverages an off-the-shelf,
general-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to
generate all step-wise critique by one pass, thereby providing verifiable
token-level rewards to refine the tokens that were originally assigned
identical rule-based rewards. This enables more fine-grained credit assignment
in an effective way. Furthermore, to enhance the accuracy and robustness of
CAPO, we employ voting mechanisms that scale with the number of generated
critiques. Extensive experiments using different backbones like Llama and Qwen
models and in different sizes show that CAPO consistently outperforms
supervised learning-based and RL-based fine-tuning methods across six
challenging mathematical benchmarks and three out-of-domain benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pre-Tactical Flight-Delay and Turnaround Forecasting with Synthetic
  Aviation Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulmajid Murad, Massimiliano Ruocco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Access to comprehensive flight operations data remains severely restricted in
aviation due to commercial sensitivity and competitive considerations,
hindering the development of predictive models for operational planning. This
paper investigates whether synthetic data can effectively replace real
operational data for training machine learning models in pre-tactical aviation
scenarios-predictions made hours to days before operations using only scheduled
flight information. We evaluate four state-of-the-art synthetic data generators
on three prediction tasks: aircraft turnaround time, departure delays, and
arrival delays. Using a Train on Synthetic, Test on Real (TSTR) methodology on
over 1.7 million European flight records, we first validate synthetic data
quality through fidelity assessments, then assess both predictive performance
and the preservation of operational relationships. Our results show that
advanced neural network architectures, specifically transformer-based
generators, can retain 94-97% of real-data predictive performance while
maintaining feature importance patterns informative for operational
decision-making. Our analysis reveals that even with real data, prediction
accuracy is inherently limited when only scheduled information is
available-establishing realistic baselines for pre-tactical forecasting. These
findings suggest that high-quality synthetic data can enable broader access to
aviation analytics capabilities while preserving commercial confidentiality,
though stakeholders must maintain realistic expectations about pre-tactical
prediction accuracy given the stochastic nature of flight operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Aqeel, Shakiba Sharifi, Marco Cristani, Francesco Setti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  So-called unsupervised anomaly detection is better described as
semi-supervised, as it assumes all training data are nominal. This assumption
simplifies training but requires manual data curation, introducing bias and
limiting adaptability. We propose Confident Meta-learning (CoMet), a novel
training strategy that enables deep anomaly detection models to learn from
uncurated datasets where nominal and anomalous samples coexist, eliminating the
need for explicit filtering. Our approach integrates Soft Confident Learning,
which assigns lower weights to low-confidence samples, and Meta-Learning, which
stabilizes training by regularizing updates based on training validation loss
covariance. This prevents overfitting and enhances robustness to noisy data.
CoMet is model-agnostic and can be applied to any anomaly detection method
trainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2
with two state-of-the-art models demonstrate the effectiveness of our approach,
consistently improving over the baseline methods, remaining insensitive to
anomalies in the training set, and setting a new state-of-the-art across all
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ieee/cvf international conference on computer vision
  (ICCV2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI
  Research and Deployment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Zhang, Yilei Zhao, Chuqiao Zong, Xinrun Wang, Bo An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial AI holds great promise for transforming modern finance, with the
potential to support a wide range of tasks such as market forecasting,
portfolio management, quantitative trading, and automated analysis. However,
existing platforms remain limited in task coverage, lack robust multimodal data
integration, and offer insufficient support for the training and deployment of
large language models (LLMs). In response to these limitations, we present
FinWorld, an all-in-one open-source platform that provides end-to-end support
for the entire financial AI workflow, from data acquisition to experimentation
and deployment. FinWorld distinguishes itself through native integration of
heterogeneous financial data, unified support for diverse AI paradigms, and
advanced agent automation, enabling seamless development and deployment.
Leveraging data from 2 representative markets, 4 stock pools, and over 800
million financial data points, we conduct comprehensive experiments on 4 key
financial AI tasks. These experiments systematically evaluate deep learning and
reinforcement learning algorithms, with particular emphasis on RL-based
finetuning for LLMs and LLM Agents. The empirical results demonstrate that
FinWorld significantly enhances reproducibility, supports transparent
benchmarking, and streamlines deployment, thereby providing a strong foundation
for future research and real-world applications. Code is available at
Github~\footnote{https://github.com/DVampire/FinWorld}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flexible Automatic Identification and Removal (FAIR)-Pruner: An
  Efficient Neural Network Pruning Method <span class="chip">AAAI 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenqing Lin, Mostafa Hussien, Chengyao Yu, Mohamed Cheriet, Osama Abdelrahman, Ruixing Ming
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network pruning is a critical compression technique that facilitates
the deployment of large-scale neural networks on resource-constrained edge
devices, typically by identifying and eliminating redundant or insignificant
parameters to reduce computational and memory overhead. This paper proposes the
Flexible Automatic Identification and Removal (FAIR)-Pruner, a novel method for
neural network structured pruning. Specifically, FAIR-Pruner first evaluates
the importance of each unit (e.g., neuron or channel) through the Utilization
Score quantified by the Wasserstein distance. To reflect the performance
degradation after unit removal, it then introduces the Reconstruction Error,
which is computed via the Taylor expansion of the loss function. Finally,
FAIR-Pruner identifies superfluous units with negligible impact on model
performance by controlling the proposed Tolerance of Difference, which measures
differences between unimportant units and those that cause performance
degradation. A major advantage of FAIR-Pruner lies in its capacity to
automatically determine the layer-wise pruning rates, which yields a more
efficient subnetwork structure compared to applying a uniform pruning rate.
Another advantage of the FAIR-Pruner is its great one-shot performance without
post-pruning fine-tuning. Furthermore, with utilization scores and
reconstruction errors, users can flexibly obtain pruned models under different
pruning ratios. Comprehensive experimental validation on diverse benchmark
datasets (e.g., ImageNet) and various neural network architectures (e.g., VGG)
demonstrates that FAIR-Pruner achieves significant model compression while
maintaining high accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to AAAI 2026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Enhanced Focal Loss Function to Mitigate Class Imbalance in Auto
  Insurance Fraud Detection with Explainable AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francis Boabang, Samuel Asante Gyamerah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In insurance fraud prediction, handling class imbalance remains a critical
challenge. This paper presents a novel multistage focal loss function designed
to enhance the performance of machine learning models in such imbalanced
settings by helping to escape local minima and converge to a good solution.
Building upon the foundation of the standard focal loss, our proposed approach
introduces a dynamic, multi-stage convex and nonconvex mechanism that
progressively adjusts the focus on hard-to-classify samples across training
epochs. This strategic refinement facilitates more stable learning and improved
discrimination between fraudulent and legitimate cases. Through extensive
experimentation on a real-world insurance dataset, our method achieved better
performance than the traditional focal loss, as measured by accuracy,
precision, F1-score, recall and Area Under the Curve (AUC) metrics on the auto
insurance dataset. These results demonstrate the efficacy of the multistage
focal loss in boosting model robustness and predictive accuracy in highly
skewed classification tasks, offering significant implications for fraud
detection systems in the insurance industry. An explainable model is included
to interpret the results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Edges Matter? Investigating Edge-Enhanced <span class="highlight-title">Pre-Train</span>ing for Medical
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Zaha, Lars Böcking, Simeon Allmendinger, Leopold Müller, Niklas Kühl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation is crucial for disease diagnosis and treatment
planning, yet developing robust segmentation models often requires substantial
computational resources and large datasets. Existing research shows that
pre-trained and finetuned foundation models can boost segmentation performance.
However, questions remain about how particular image preprocessing steps may
influence segmentation performance across different medical imaging modalities.
In particular, edges-abrupt transitions in pixel intensity-are widely
acknowledged as vital cues for object boundaries but have not been
systematically examined in the pre-training of foundation models. We address
this gap by investigating to which extend pre-training with data processed
using computationally efficient edge kernels, such as kirsch, can improve
cross-modality segmentation capabilities of a foundation model. Two versions of
a foundation model are first trained on either raw or edge-enhanced data across
multiple medical imaging modalities, then finetuned on selected raw subsets
tailored to specific medical modalities. After systematic investigation using
the medical domains Dermoscopy, Fundus, Mammography, Microscopy, OCT, US, and
XRay, we discover both increased and reduced segmentation performance across
modalities using edge-focused pre-training, indicating the need for a selective
application of this approach. To guide such selective applications, we propose
a meta-learning strategy. It uses standard deviation and image entropy of the
raw image to choose between a model pre-trained on edge-enhanced or on raw data
for optimal performance. Our experiments show that integrating this
meta-learning layer yields an overall segmentation performance improvement
across diverse medical imaging tasks by 16.42% compared to models pre-trained
on edge-enhanced data only and 19.30% compared to models pre-trained on raw
data only.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, Third International Workshop on Data Engineering
  in Medical Imaging (DEMI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CellForge: Agentic Design of Virtual Cell Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangru Tang, Zhuoyun Yu, Jiapeng Chen, Yan Cui, Daniel Shao, Weixu Wang, Fang Wu, Yuchen Zhuang, Wenqi Shi, Zhi Huang, Arman Cohan, Xihong Lin, Fabian Theis, Smita Krishnaswamy, Mark Gerstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual cell modeling represents an emerging frontier at the intersection of
artificial intelligence and biology, aiming to predict quantities such as
responses to diverse perturbations quantitatively. However, autonomously
building computational models for virtual cells is challenging due to the
complexity of biological systems, the heterogeneity of data modalities, and the
need for domain-specific expertise across multiple disciplines. Here, we
introduce CellForge, an agentic system that leverages a multi-agent framework
that transforms presented biological datasets and research objectives directly
into optimized computational models for virtual cells. More specifically, given
only raw single-cell multi-omics data and task descriptions as input, CellForge
outputs both an optimized model architecture and executable code for training
virtual cell models and inference. The framework integrates three core modules:
Task Analysis for presented dataset characterization and relevant literature
retrieval, Method Design, where specialized agents collaboratively develop
optimized modeling strategies, and Experiment Execution for automated
generation of code. The agents in the Design module are separated into experts
with differing perspectives and a central moderator, and have to
collaboratively exchange solutions until they achieve a reasonable consensus.
We demonstrate CellForge's capabilities in single-cell perturbation prediction,
using six diverse datasets that encompass gene knockouts, drug treatments, and
cytokine stimulations across multiple modalities. CellForge consistently
outperforms task-specific state-of-the-art methods. Overall, CellForge
demonstrates how iterative interaction between LLM agents with differing
perspectives provides better solutions than directly addressing a modeling
challenge. Our code is publicly available at
https://github.com/gersteinlab/CellForge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Generative Models with the New Physics Learning Machine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuele Grossi, Marco Letizia, Riccardo Torre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of generative models for scientific research calls for the
development of new methods to evaluate their fidelity. A natural framework for
addressing this problem is two-sample hypothesis testing, namely the task of
determining whether two data sets are drawn from the same distribution. In
large-scale and high-dimensional regimes, machine learning offers a set of
tools to push beyond the limitations of standard statistical techniques. In
this work, we put this claim to the test by comparing a recent proposal from
the high-energy physics literature, the New Physics Learning Machine, to
perform a classification-based two-sample test against a number of alternative
approaches, following the framework presented in Grossi et al. (2025). We
highlight the efficiency tradeoffs of the method and the computational costs
that come from adopting learning-based approaches. Finally, we discuss the
advantages of the different methods for different use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v1: 14 pages, 7 figures, 8 tables, additional material on GitHub
  referenced in the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mCardiacDx: Radar-Driven Contactless Monitoring and Diagnosis of
  Arrhythmia 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Kumar, Noppanat Wadlom, Jaeheon Kwak, Si-Hyuck Kang, Insik Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arrhythmia is a common cardiac condition that can precipitate severe
complications without timely intervention. While continuous monitoring is
essential for timely diagnosis, conventional approaches such as
electrocardiogram and wearable devices are constrained by their reliance on
specialized medical expertise and patient discomfort from their contact nature.
Existing contactless monitoring, primarily designed for healthy subjects, face
significant challenges when analyzing reflected signals from arrhythmia
patients due to disrupted spatial stability and temporal consistency.
  In this paper, we introduce mCardiacDx, a radar-driven contactless system
that accurately analyzes reflected signals and reconstructs heart pulse
waveforms for arrhythmia monitoring and diagnosis. The key contributions of our
work include a novel precise target localization (PTL) technique that locates
reflected signals despite spatial disruptions, and an encoder-decoder model
that transforms these signals into HPWs, addressing temporal inconsistencies.
Our evaluation on a large dataset of healthy subjects and arrhythmia patients
shows that both mCardiacDx and PTL outperform state-of-the-art approach in
arrhythmia monitoring and diagnosis, also demonstrating improved performance in
healthy subjects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 27 images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skeleton-Guided Learning for Shortest Path Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiantian Liu, Xiao Li, Huan Li, Hua Lu, Christian S. Jensen, Jianliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shortest path search is a core operation in graph-based applications, yet
existing methods face important limitations. Classical algorithms such as
Dijkstra's and A* become inefficient as graphs grow more complex, while
index-based techniques often require substantial preprocessing and storage.
Recent learning-based approaches typically focus on spatial graphs and rely on
context-specific features like geographic coordinates, limiting their general
applicability. We propose a versatile learning-based framework for shortest
path search on generic graphs, without requiring domain-specific features. At
the core of our approach is the construction of a skeleton graph that captures
multi-level distance and hop information in a compact form. A Skeleton Graph
Neural Network (SGNN) operates on this structure to learn node embeddings and
predict distances and hop lengths between node pairs. These predictions support
LSearch, a guided search algorithm that uses model-driven pruning to reduce the
search space while preserving accuracy. To handle larger graphs, we introduce a
hierarchical training strategy that partitions the graph into subgraphs with
individually trained SGNNs. This structure enables HLSearch, an extension of
our method for efficient path search across graph partitions. Experiments on
five diverse real-world graphs demonstrate that our framework achieves strong
performance across graph types, offering a flexible and effective solution for
learning-based shortest path search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ByteGen: A Tokenizer-Free Generative Model for Orderbook Events in Byte
  Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Li, Zhi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative modeling of high-frequency limit order book (LOB) dynamics is a
critical yet unsolved challenge in quantitative finance, essential for robust
market simulation and strategy backtesting. Existing approaches are often
constrained by simplifying stochastic assumptions or, in the case of modern
deep learning models like Transformers, rely on tokenization schemes that
affect the high-precision, numerical nature of financial data through
discretization and binning. To address these limitations, we introduce ByteGen,
a novel generative model that operates directly on the raw byte streams of LOB
events. Our approach treats the problem as an autoregressive next-byte
prediction task, for which we design a compact and efficient 32-byte packed
binary format to represent market messages without information loss. The core
novelty of our work is the complete elimination of feature engineering and
tokenization, enabling the model to learn market dynamics from its most
fundamental representation. We achieve this by adapting the H-Net architecture,
a hybrid Mamba-Transformer model that uses a dynamic chunking mechanism to
discover the inherent structure of market messages without predefined rules.
Our primary contributions are: 1) the first end-to-end, byte-level framework
for LOB modeling; 2) an efficient packed data representation; and 3) a
comprehensive evaluation on high-frequency data. Trained on over 34 million
events from CME Bitcoin futures, ByteGen successfully reproduces key stylized
facts of financial markets, generating realistic price distributions,
heavy-tailed returns, and bursty event timing. Our findings demonstrate that
learning directly from byte space is a promising and highly flexible paradigm
for modeling complex financial systems, achieving competitive performance on
standard market quality metrics without the biases of tokenization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 3 tables, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pigeon-SL: Robust Split Learning Framework for Edge Intelligence under
  Malicious Clients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangjun Park, Tony Q. S. Quek, Hyowoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in split learning (SL) have established it as a promising
framework for privacy-preserving, communication-efficient distributed learning
at the network edge. However, SL's sequential update process is vulnerable to
even a single malicious client, which can significantly degrade model accuracy.
To address this, we introduce Pigeon-SL, a novel scheme grounded in the
pigeonhole principle that guarantees at least one entirely honest cluster among
M clients, even when up to N of them are adversarial. In each global round, the
access point partitions the clients into N+1 clusters, trains each cluster
independently via vanilla SL, and evaluates their validation losses on a shared
dataset. Only the cluster with the lowest loss advances, thereby isolating and
discarding malicious updates. We further enhance training and communication
efficiency with Pigeon-SL+, which repeats training on the selected cluster to
match the update throughput of standard SL. We validate the robustness and
effectiveness of our approach under three representative attack models -- label
flipping, activation and gradient manipulation -- demonstrating significant
improvements in accuracy and resilience over baseline SL methods in future
intelligent wireless networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through
  Chunked Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongchi Huang, Zhirui Fang, Tianle Zhang, Yihang Li, Lin Zhao, Chunhe Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language-Action (VLA) models demonstrate significant potential for
developing generalized policies in real-world robotic control. This progress
inspires researchers to explore fine-tuning these models with Reinforcement
Learning (RL). However, fine-tuning VLA models with RL still faces challenges
related to sample efficiency, compatibility with action chunking, and training
stability. To address these challenges, we explore the fine-tuning of VLA
models through offline reinforcement learning incorporating action chunking. In
this work, we propose Chunked RL, a novel reinforcement learning framework
specifically designed for VLA models. Within this framework, we extend temporal
difference (TD) learning to incorporate action chunking, a prominent
characteristic of VLA models. Building upon this framework, we propose CO-RFT,
an algorithm aimed at fine-tuning VLA models using a limited set of
demonstrations (30 to 60 samples). Specifically, we first conduct imitation
learning (IL) with full parameter fine-tuning to initialize both the backbone
and the policy. Subsequently, we implement offline RL with action chunking to
optimize the pretrained policy. Our empirical results in real-world
environments demonstrate that CO-RFT outperforms previous supervised methods,
achieving a 57% improvement in success rate and a 22.3% reduction in cycle
time. Moreover, our method exhibits robust positional generalization
capabilities, attaining a success rate of 44.3% in previously unseen positions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Policy Pareto Front Tracking Based Online and Offline
  Multi-Objective Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Zhao, Yueling Che, Kaichen Liu, Jian Li, Junmei Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective reinforcement learning (MORL) plays a pivotal role in
addressing multi-criteria decision-making problems in the real world. The
multi-policy (MP) based methods are widely used to obtain high-quality Pareto
front approximation for the MORL problems. However, traditional MP methods only
rely on the online reinforcement learning (RL) and adopt the evolutionary
framework with a large policy population. This may lead to sample inefficiency
and/or overwhelmed agent-environment interactions in practice. By forsaking the
evolutionary framework, we propose the novel Multi-policy Pareto Front Tracking
(MPFT) framework without maintaining any policy population, where both online
and offline MORL algorithms can be applied. The proposed MPFT framework
includes four stages: Stage 1 approximates all the Pareto-vertex policies,
whose mapping to the objective space fall on the vertices of the Pareto front.
Stage 2 designs the new Pareto tracking mechanism to track the Pareto front,
starting from each of the Pareto-vertex policies. Stage 3 identifies the sparse
regions in the tracked Pareto front, and introduces a new objective weight
adjustment method to fill the sparse regions. Finally, by combining all the
policies tracked in Stages 2 and 3, Stage 4 approximates the Pareto front.
Experiments are conducted on seven different continuous-action robotic control
tasks with both online and offline MORL algorithms, and demonstrate the
superior hypervolume performance of our proposed MPFT approach over the
state-of-the-art benchmarks, with significantly reduced agent-environment
interactions and hardware requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 10 figures, conference paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LeanK: Learnable K Cache Channel Pruning for Efficient Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) enable long-context tasks but face efficiency
challenges due to the growing key-value (KV) cache. We propose LeanK, a
learning-based method that prunes unimportant key (K) cache channels by
leveraging static channel sparsity. With a novel two-stage training process,
LeanK learns channel-wise static mask that could satisfy specific sparsity
ratio and hardware alignment requirement. LeanK reduces GPU memory and
accelerates decoding without sacrificing accuracy. Experiments demonstrate up
to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel
enables 1.3x speedup for attention computation. We also provide insights into
model channels and attention heads during long-context inference by analyzing
the learned importance distribution. Our code is available at
https://aka.ms/LeanK.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WhiSQA: Non-Intrusive Speech Quality Prediction Using Whisper Encoder
  Features <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Close, Kris Hong, Thomas Hain, Stefan Goetze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been significant research effort developing neural-network-based
predictors of SQ in recent years. While a primary objective has been to develop
non-intrusive, i.e.~reference-free, metrics to assess the performance of SE
systems, recent work has also investigated the direct inference of neural SQ
predictors within the loss function of downstream speech tasks. To aid in the
training of SQ predictors, several large datasets of audio with corresponding
human labels of quality have been created. Recent work in this area has shown
that speech representations derived from large unsupervised or semi-supervised
foundational speech models are useful input feature representations for neural
SQ prediction. In this work, a novel and robust SQ predictor is proposed based
on feature representations extracted from an ASR model, found to be a powerful
input feature for the SQ prediction task. The proposed system achieves higher
correlation with human MOS ratings than recent approaches on all NISQA test
sets and shows significantly better domain adaption compared to the commonly
used DNSMOS metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SPECOM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balancing Information Accuracy and Response Timeliness in Networked LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yigit Turkmen, Baturalp Buyukates, Melih Bastopcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have transformed many
fields including scientific discovery, content generation, biomedical text
mining, and educational technology. However, the substantial requirements for
training data, computational resources, and energy consumption pose significant
challenges for their practical deployment. A promising alternative is to
leverage smaller, specialized language models and aggregate their outputs to
improve overall response quality. In this work, we investigate a networked LLM
system composed of multiple users, a central task processor, and clusters of
topic-specialized LLMs. Each user submits categorical binary (true/false)
queries, which are routed by the task processor to a selected cluster of $m$
LLMs. After gathering individual responses, the processor returns a final
aggregated answer to the user. We characterize both the information accuracy
and response timeliness in this setting, and formulate a joint optimization
problem to balance these two competing objectives. Our extensive simulations
demonstrate that the aggregated responses consistently achieve higher accuracy
than those of individual LLMs. Notably, this improvement is more significant
when the participating LLMs exhibit similar standalone performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, Yuwei Fu, Jing Su, Ge Zhang, Wenhao Huang, Mingxuan Wang, Lin Yan, Xiaoying Jia, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Yonghui Wu, Hao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Seed Diffusion Preview, a large-scale language model based on
discrete-state diffusion, offering remarkably fast inference speed. Thanks to
non-sequential, parallel generation, discrete diffusion models provide a
notable speedup to mitigate the inherent latency of token-by-token decoding, as
demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion
Preview achieves an inference speed of 2,146 token/s over H20 GPUs while
maintaining competitive performance across a sweep of standard code evaluation
benchmarks, significantly faster than contemporary Mercury and Gemini
Diffusion, establishing new state of the art on the speed-quality Pareto
frontier for code models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Demo is available at https://studio.seed.ai/exp/seed_diffusion/;
  Project page is https://seed.bytedance.com/seed_diffusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAAD: Context-Aware Adaptive Decoding for Truthful Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manh Nguyen, Sunil Gupta, Hung Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring truthfulness in large language models remains a critical challenge
for reliable text generation. While supervised fine-tuning and reinforcement
learning with human feedback have shown promise, they require substantial
amount of annotated data and computational resources, limiting scalability. In
contrast, decoding-time interventions offer lightweight alternatives without
model retraining. However, existing decoding strategies often face issues like
prompt sensitivity, limited generalization, or dependence on internal model
states. We propose a context-aware adaptive decoding method that leverages a
compact reference grounding space, built from as few as 10 annotated examples
and comprising pairs of context embeddings and next token logits from truthful
responses, to enable retrieval-based logit shaping during inference. At each
decoding step, our method retrieves top-N semantically similar contexts and
aggregates their associated next token logits to modify the LLM's logits.
Across three open-ended question-answering benchmarks, our approach achieves a
2.8 percent average improvement on TruthfulQA and further outperforms existing
baselines on both Biographies and WikiQA. Experimental results also demonstrate
cross-task generalization, with TruthfulQA-derived grounding enhancing
biography generation. Our model-agnostic, scalable, and efficient method
requires only a single generation pass, highlighting the potential of
context-aware decoding for factual reliability in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Treatment-DML: Causal Estimation for Multi-Dimensional Continuous
  Treatments with Monotonicity Constraints in Personal Loan Risk Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexin Zhao, Bo Wang, Cuiying Zhao, Tongyao Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizing credit limits, interest rates, and loan terms is crucial for
managing borrower risk and lifetime value (LTV) in personal loan platform.
However, counterfactual estimation of these continuous, multi-dimensional
treatments faces significant challenges: randomized trials are often prohibited
by risk controls and long repayment cycles, forcing reliance on biased
observational data. Existing causal methods primarily handle binary/discrete
treatments and struggle with continuous, multi-dimensional settings.
Furthermore, financial domain knowledge mandates provably monotonic
treatment-outcome relationships (e.g., risk increases with credit limit).To
address these gaps, we propose Multi-Treatment-DML, a novel framework
leveraging Double Machine Learning (DML) to: (i) debias observational data for
causal effect estimation; (ii) handle arbitrary-dimensional continuous
treatments; and (iii) enforce monotonic constraints between treatments and
outcomes, guaranteeing adherence to domain requirements.Extensive experiments
on public benchmarks and real-world industrial datasets demonstrate the
effectiveness of our approach. Furthermore, online A/B testing conducted on a
realworld personal loan platform, confirms the practical superiority of
Multi-Treatment-DML in real-world loan operations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ User Trajectory Prediction Unifying Global and Local Temporal
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Hao, Bin Chong, Ronghua Ji, Chen Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory prediction is essential for formulating proactive strategies that
anticipate user mobility and support advance preparation. Therefore, how to
reduce the forecasting error in user trajectory prediction within an acceptable
inference time arises as an interesting issue. However, trajectory data
contains both global and local temporal information, complicating the
extraction of the complete temporal pattern. Moreover, user behavior occurs
over different time scales, increasing the difficulty of capturing behavioral
patterns. To address these challenges, a trajectory prediction model based on
multilayer perceptron (MLP), multi-scale convolutional neural network (MSCNN),
and cross-attention (CA) is proposed. Specifically, MLP is used to extract the
global temporal information of each feature. In parallel, MSCNN is employed to
extract the local temporal information by modeling interactions among features
within a local temporal range. Convolutional kernels with different sizes are
used in MSCNN to capture temporal information at multiple resolutions,
enhancing the model's adaptability to different behavioral patterns. Finally,
CA is applied to fuse the global and local temporal information. Experimental
results show that our model reduces mean squared error (MSE) by 5.04% and mean
absolute error (MAE) by 4.35% compared with ModernTCN in 12-step prediction,
while maintaining similar inference time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIGDreamer: Privileged Information Guided World Models for Safe
  Partially Observable Reinforcement Learning <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongchi Huang, Jiaqi Wang, Yang Li, Chunhe Xia, Tianle Zhang, Kaige Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial observability presents a significant challenge for safe reinforcement
learning, as it impedes the identification of potential risks and rewards.
Leveraging specific types of privileged information during training to mitigate
the effects of partial observability has yielded notable empirical successes.
In this paper, we propose Asymmetric Constrained Partially Observable Markov
Decision Processes (ACPOMDPs) to theoretically examine the advantages of
incorporating privileged information. Building upon ACPOMDPs, we propose the
Privileged Information Guided Dreamer, a model-based safe reinforcement
learning approach that leverages privileged information to enhance the agent's
safety and performance through privileged representation alignment and an
asymmetric actor-critic structure. Our empirical results demonstrate that our
approach significantly outperforms existing methods in terms of safety and
task-centric performance. Meanwhile, compared to alternative privileged
model-based reinforcement learning methods, our approach exhibits superior
performance and ease of training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Detection of Planted Subgraphs in Semi-Random Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dor Elimelech, Wasim Huleihel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detection of planted subgraphs in Erd\"os-R\'enyi random graphs has been
extensively studied, leading to a rich body of results characterizing both
statistical and computational thresholds. However, most prior work assumes a
purely random generative model, making the resulting algorithms potentially
fragile in the face of real-world perturbations. In this work, we initiate the
study of semi-random models for the planted subgraph detection problem, wherein
an adversary is allowed to remove edges outside the planted subgraph before the
graph is revealed to the statistician. Crucially, the statistician remains
unaware of which edges have been removed, introducing fundamental challenges to
the inference task. We establish fundamental statistical limits for detection
under this semi-random model, revealing a sharp dichotomy. Specifically, for
planted subgraphs with strongly sub-logarithmic maximum density detection
becomes information-theoretically impossible in the presence of an adversary,
despite being possible in the classical random model. In stark contrast, for
subgraphs with super-logarithmic density, the statistical limits remain
essentially unchanged; we prove that the optimal (albeit computationally
intractable) likelihood ratio test remains robust. Beyond these statistical
boundaries, we design a new computationally efficient and robust detection
algorithm, and provide rigorous statistical guarantees for its performance. Our
results establish the first robust framework for planted subgraph detection and
open new directions in the study of semi-random models,
computational-statistical trade-offs, and robustness in graph inference
problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large-Scale Model Enabled Semantic Communication Based on Robust
  Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuiyuan DIng, Caili Guo, Yang Yang, Zhongtian Du, Walid Saad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale models (LSMs) can be an effective framework for semantic
representation and understanding, thereby providing a suitable tool for
designing semantic communication (SC) systems. However, their direct deployment
is often hindered by high computational complexity and resource requirements.
In this paper, a novel robust knowledge distillation based semantic
communication (RKD-SC) framework is proposed to enable efficient and
\textcolor{black}{channel-noise-robust} LSM-powered SC. The framework addresses
two key challenges: determining optimal compact model architectures and
effectively transferring knowledge while maintaining robustness against channel
noise. First, a knowledge distillation-based lightweight differentiable
architecture search (KDL-DARTS) algorithm is proposed. This algorithm
integrates knowledge distillation loss and a complexity penalty into the neural
architecture search process to identify high-performance, lightweight semantic
encoder architectures. Second, a novel two-stage robust knowledge distillation
(RKD) algorithm is developed to transfer semantic capabilities from an LSM
(teacher) to a compact encoder (student) and subsequently enhance system
robustness. To further improve resilience to channel impairments, a
channel-aware transformer (CAT) block is introduced as the channel codec,
trained under diverse channel conditions with variable-length outputs.
Extensive simulations on image classification tasks demonstrate that the RKD-SC
framework significantly reduces model parameters while preserving a high degree
of the teacher model's performance and exhibiting superior robustness compared
to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fitness aligned structural modeling enables scalable virtual screening
  with AuroBind 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongyue Zhang, Jiahua Rao, Jie Zhong, Weiqiang Bai, Dongxue Wang, Shaobo Ning, Lifeng Qiao, Sheng Xu, Runze Ma, Will Hua, Jack Xiaoyu Chen, Odin Zhang, Wei Lu, Hanyi Feng, He Yang, Xinchao Shi, Rui Li, Wanli Ouyang, Xinzhu Ma, Jiahao Wang, Jixian Zhang, Jia Duan, Siqi Sun, Jian Zhang, Shuangjia Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most human proteins remain undrugged, over 96% of human proteins remain
unexploited by approved therapeutics. While structure-based virtual screening
promises to expand the druggable proteome, existing methods lack atomic-level
precision and fail to predict binding fitness, limiting translational impact.
We present AuroBind, a scalable virtual screening framework that fine-tunes a
custom atomic-level structural model on million-scale chemogenomic data.
AuroBind integrates direct preference optimization, self-distillation from
high-confidence complexes, and a teacher-student acceleration strategy to
jointly predict ligand-bound structures and binding fitness. The proposed
models outperform state-of-the-art models on structural and functional
benchmarks while enabling 100,000-fold faster screening across ultra-large
compound libraries. In a prospective screen across ten disease-relevant
targets, AuroBind achieved experimental hit rates of 7-69%, with top compounds
reaching sub-nanomolar to picomolar potency. For the orphan GPCRs GPR151 and
GPR160, AuroBind identified both agonists and antagonists with success rates of
16-30%, and functional assays confirmed GPR160 modulation in liver and prostate
cancer models. AuroBind offers a generalizable framework for structure-function
learning and high-throughput molecular screening, bridging the gap between
structure prediction and therapeutic discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>54 pages, 13 figures, code available at
  https://github.com/GENTEL-lab/AuroBind</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedLAD: A Linear Algebra Based Data Poisoning Defence for Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Xiong, Hai Dong, Nasrin Sohrabi, Zahir Tari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sybil attacks pose a significant threat to federated learning, as malicious
nodes can collaborate and gain a majority, thereby overwhelming the system.
Therefore, it is essential to develop countermeasures that ensure the security
of federated learning environments. We present a novel defence method against
targeted data poisoning, which is one of the types of Sybil attacks, called
Linear Algebra-based Detection (FedLAD). Unlike existing approaches, such as
clustering and robust training, which struggle in situations where malicious
nodes dominate, FedLAD models the federated learning aggregation process as a
linear problem, transforming it into a linear algebra optimisation challenge.
This method identifies potential attacks by extracting the independent linear
combinations from the original linear combinations, effectively filtering out
redundant and malicious elements. Extensive experimental evaluations
demonstrate the effectiveness of FedLAD compared to five well-established
defence methods: Sherpa, CONTRA, Median, Trimmed Mean, and Krum. Using tasks
from both image classification and natural language processing, our experiments
confirm that FedLAD is robust and not dependent on specific application
settings. The results indicate that FedLAD effectively protects federated
learning systems across a broad spectrum of malicious node ratios. Compared to
baseline defence methods, FedLAD maintains a low attack success rate for
malicious nodes when their ratio ranges from 0.2 to 0.8. Additionally, it
preserves high model accuracy when the malicious node ratio is between 0.2 and
0.5. These findings underscore FedLAD's potential to enhance both the
reliability and performance of federated learning systems in the face of data
poisoning attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Complexity of Extreme Climate Events on the New Zealand's Kiwifruit
  Industry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyuan Zheng, Victor W. Chu, Zhidong Li, Evan Webster, Ashley Rootsey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate change has intensified the frequency and severity of extreme weather
events, presenting unprecedented challenges to the agricultural industry
worldwide. In this investigation, we focus on kiwifruit farming in New Zealand.
We propose to examine the impacts of climate-induced extreme events,
specifically frost, drought, extreme rainfall, and heatwave, on kiwifruit
harvest yields. These four events were selected due to their significant
impacts on crop productivity and their prevalence as recorded by climate
monitoring institutions in the country. We employed Isolation Forest, an
unsupervised anomaly detection method, to analyse climate history and recorded
extreme events, alongside with kiwifruit yields. Our analysis reveals
considerable variability in how different types of extreme event affect
kiwifruit yields underscoring notable discrepancies between climatic extremes
and individual farm's yield outcomes. Additionally, our study highlights
critical limitations of current anomaly detection approaches, particularly in
accurately identifying events such as frost. These findings emphasise the need
for integrating supplementary features like farm management strategies with
climate adaptation practices. Our further investigation will employ ensemble
methods that consolidate nearby farms' yield data and regional climate station
features to reduce variance, thereby enhancing the accuracy and reliability of
extreme event detection and the formulation of response strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print v0.8 2025-08-04</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill
  in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tai An, Ruwu Cai, Yanzhe Zhang, Yang Liu, Hao Chen, Pengcheng Xie, Sheng Chang, Yiwu Yao, Gongyi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of large language models (LLMs), N:M sparsity has emerged as a
structured compression technique critical for accelerating inference. While
prior work has primarily focused on weight sparsity, it often suffers from
significant accuracy degradation. Activation sparsity, though promising, is
typically training-dependent and faces challenges in generalization. To address
these limitations, we introduce Amber Pruner, a training-free N:M activation
sparsity method designed specifically for the prefill stage, targeting the
acceleration of linear projection layers in LLMs. Extensive experiments across
multiple models and sparsity ratios (2:4, 4:8, and 8:16) demonstrate that Amber
Pruner can effectively sparsify and accelerate more than 55% of linear
computations without requiring model retraining. To further enhance generality
and efficiency, we propose Outstanding-sparse, a unified framework that
integrates Amber Pruner with post-training W8A8 quantization. Our approach
preserves strong performance across a range of downstream tasks, with notable
advantages in generative tasks. This work pioneers a new frontier in activation
sparsity, providing foundational insights that are poised to guide the
co-evolution of algorithms and architectures in the design of next-generation
AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Learning Dynamics Through Structured Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saleh Nikooroo, Thomas Engel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While modern deep networks have demonstrated remarkable versatility, their
training dynamics remain poorly understood--often driven more by empirical
tweaks than architectural insight. This paper investigates how internal
structural choices shape the behavior of learning systems. Building on prior
efforts that introduced simple architectural constraints, we explore the
broader implications of structure for convergence, generalization, and
adaptation. Our approach centers on a family of enriched transformation layers
that incorporate constrained pathways and adaptive corrections. We analyze how
these structures influence gradient flow, spectral sensitivity, and fixed-point
behavior--uncovering mechanisms that contribute to training stability and
representational regularity. Theoretical analysis is paired with empirical
studies on synthetic and structured tasks, demonstrating improved robustness,
smoother optimization, and scalable depth behavior. Rather than prescribing
fixed templates, we emphasize principles of tractable design that can steer
learning behavior in interpretable ways. Our findings support a growing view
that architectural design is not merely a matter of performance tuning, but a
critical axis for shaping learning dynamics in scalable and trustworthy neural
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trainable Dynamic Mask Sparse Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingze Shi, Yifan Wu, Bingheng Wu, Yiran Peng, Liangdong Wang, Guang Liu, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large language models, the demand for modeling long contexts is constantly
increasing, but the quadratic complexity of the standard self-attention
mechanism often becomes a bottleneck. Although existing sparse attention
mechanisms have improved efficiency, they may still encounter issues such as
static patterns or information loss. We introduce a trainable dynamic mask
sparse attention mechanism, Dynamic Mask Attention, which effectively utilizes
content-aware and position-aware sparsity. DMA achieves this through two key
innovations: First, it dynamically generates content-aware sparse masks from
value representations, enabling the model to identify and focus on critical
information adaptively. Second, it implements position-aware sparse attention
computation that effectively skips unnecessary calculation regions. This
dual-sparsity design allows the model to significantly reduce the computational
complexity of important information while retaining complete information,
achieving an excellent balance between information fidelity and computational
efficiency. We have verified the performance of DMA through comprehensive
experiments. Comparative studies show that DMA outperforms multi-head
attention, sliding window attention, multi-head latent attention, and native
sparse attention in terms of perplexity under Chinchilla Scaling Law settings.
Moreover, in challenging multi-query associative recall tasks, DMA also
demonstrates superior performance and efficiency compared to these methods.
Crucially, in the evaluation of a 1.7B parameter model, DMA significantly
outperforms multi-head attention in both standard benchmark performance and the
challenging needle-in-a-haystack task. These experimental results highlight its
capability to balance model efficiency and long-context modeling ability
effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Essence: Delving into Annotator Prototype Learning for
  Multi-Class Annotation Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ju Chen, Jun Feng, Shenyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-class classification annotations have significantly advanced AI
applications, with truth inference serving as a critical technique for
aggregating noisy and biased annotations. Existing state-of-the-art methods
typically model each annotator's expertise using a confusion matrix. However,
these methods suffer from two widely recognized issues: 1) when most annotators
label only a few tasks, or when classes are imbalanced, the estimated confusion
matrices are unreliable, and 2) a single confusion matrix often remains
inadequate for capturing each annotator's full expertise patterns across all
tasks. To address these issues, we propose a novel confusion-matrix-based
method, PTBCC (ProtoType learning-driven Bayesian Classifier Combination), to
introduce a reliable and richer annotator estimation by prototype learning.
Specifically, we assume that there exists a set $S$ of prototype confusion
matrices, which capture the inherent expertise patterns of all annotators.
Rather than a single confusion matrix, the expertise per annotator is extended
as a Dirichlet prior distribution over these prototypes. This prototype
learning-driven mechanism circumvents the data sparsity and class imbalance
issues, ensuring a richer and more flexible characterization of annotators.
Extensive experiments on 11 real-world datasets demonstrate that PTBCC achieves
up to a 15% accuracy improvement in the best case, and a 3% higher average
accuracy while reducing computational cost by over 90%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Conflict Prediction for Large Truck Merging in Mixed Traffic
  at Work Zone Lane Closures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abyad Enan, Abdullah Al Mamun, Gurcan Comert, Debbie Aisiana Indah, Judith Mwakalonge, Amy W. Apon, Mashrur Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large trucks substantially contribute to work zone-related crashes, primarily
due to their large size and blind spots. When approaching a work zone, large
trucks often need to merge into an adjacent lane because of lane closures
caused by construction activities. This study aims to enhance the safety of
large truck merging maneuvers in work zones by evaluating the risk associated
with merging conflicts and establishing a decision-making strategy for merging
based on this risk assessment. To predict the risk of large trucks merging into
a mixed traffic stream within a work zone, a Long Short-Term Memory (LSTM)
neural network is employed. For a large truck intending to merge, it is
critical that the immediate downstream vehicle in the target lane maintains a
minimum safe gap to facilitate a safe merging process. Once a conflict-free
merging opportunity is predicted, large trucks are instructed to merge in
response to the lane closure. Our LSTM-based conflict prediction method is
compared against baseline approaches, which include probabilistic risk-based
merging, 50th percentile gap-based merging, and 85th percentile gap-based
merging strategies. The results demonstrate that our method yields a lower
conflict risk, as indicated by reduced Time Exposed Time-to-Collision (TET) and
Time Integrated Time-to-Collision (TIT) values relative to the baseline models.
Furthermore, the findings indicate that large trucks that use our method can
perform early merging while still in motion, as opposed to coming to a complete
stop at the end of the current lane prior to closure, which is commonly
observed with the baseline approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the Transportation Research Record:
  Journal of the Transportation Research Board for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instance-Dependent Continuous-Time Reinforcement Learning via Maximum
  Likelihood Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runze Zhao, Yue Yu, Ruhan Wang, Chunfeng Huang, Dongruo Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous-time reinforcement learning (CTRL) provides a natural framework
for sequential decision-making in dynamic environments where interactions
evolve continuously over time. While CTRL has shown growing empirical success,
its ability to adapt to varying levels of problem difficulty remains poorly
understood. In this work, we investigate the instance-dependent behavior of
CTRL and introduce a simple, model-based algorithm built on maximum likelihood
estimation (MLE) with a general function approximator. Unlike existing
approaches that estimate system dynamics directly, our method estimates the
state marginal density to guide learning. We establish instance-dependent
performance guarantees by deriving a regret bound that scales with the total
reward variance and measurement resolution. Notably, the regret becomes
independent of the specific measurement strategy when the observation frequency
adapts appropriately to the problem's complexity. To further improve
performance, our algorithm incorporates a randomized measurement schedule that
enhances sample efficiency without increasing measurement cost. These results
highlight a new direction for designing CTRL algorithms that automatically
adjust their learning behavior based on the underlying difficulty of the
environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 3 figures, 1 table. The first two authors contributed
  equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRINN: Contrastive Reinforcement Learning for Approximate Nearest
  Neighbor Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoya Li, Xiaofei Sun, Albert Wang, Chris Shum, Jiwei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate nearest-neighbor search (ANNS) algorithms have become
increasingly critical for recent AI applications, particularly in
retrieval-augmented generation (RAG) and agent-based LLM applications. In this
paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS
optimization as a reinforcement learning problem where execution speed serves
as the reward signal. This approach enables the automatic generation of
progressively faster ANNS implementations while maintaining accuracy
constraints. Our experimental evaluation demonstrates CRINN's effectiveness
across six widely-used NNS benchmark datasets. When compared against
state-of-the-art open-source ANNS algorithms, CRINN achieves best performance
on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and
GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean
and GloVe-25-angular). The implications of CRINN's success reach well beyond
ANNS optimization: It validates that LLMs augmented with reinforcement learning
can function as an effective tool for automating sophisticated algorithmic
optimizations that demand specialized knowledge and labor-intensive manual
refinement.Code can be found at https://github.com/deepreinforce-ai/CRINN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Geometry of Machine Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pawel Gajer, Jacques Ravel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a mathematical framework for analyzing machine learning
  models through the geometry of their induced partitions. By representing
  partitions as Riemannian simplicial complexes, we capture not only adjacency
  relationships but also geometric properties including cell volumes, volumes
of
  faces where cells meet, and dihedral angles between adjacent cells. For
neural
  networks, we introduce a differential forms approach that tracks geometric
  structure through layers via pullback operations, making computations
  tractable by focusing on data-containing cells. The framework enables
  geometric regularization that directly penalizes problematic spatial
  configurations and provides new tools for model refinement through extended
  Laplacians and simplicial splines. We also explore how data distribution
  induces effective geometric curvature in model partitions, developing
discrete
  curvature measures for vertices that quantify local geometric complexity and
  statistical Ricci curvature for edges that captures pairwise relationships
  between cells. While focused on mathematical foundations, this geometric
  perspective offers new approaches to model interpretation, regularization,
and
  diagnostic tools for understanding learning dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>61 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DART-Eval: A Comprehensive DNA Language Model Evaluation Benchmark on
  Regulatory DNA <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Patel, Arpita Singhal, Austin Wang, Anusri Pampari, Maya Kasowski, Anshul Kundaje
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in self-supervised models for natural language, vision, and
protein sequences have inspired the development of large genomic DNA language
models (DNALMs). These models aim to learn generalizable representations of
diverse DNA elements, potentially enabling various genomic prediction,
interpretation and design tasks. Despite their potential, existing benchmarks
do not adequately assess the capabilities of DNALMs on key downstream
applications involving an important class of non-coding DNA elements critical
for regulating gene activity. In this study, we introduce DART-Eval, a suite of
representative benchmarks specifically focused on regulatory DNA to evaluate
model performance across zero-shot, probed, and fine-tuned scenarios against
contemporary ab initio models as baselines. Our benchmarks target biologically
meaningful downstream tasks such as functional sequence feature discovery,
predicting cell-type specific regulatory activity, and counterfactual
prediction of the impacts of genetic variants. We find that current DNALMs
exhibit inconsistent performance and do not offer compelling gains over
alternative baseline models for most tasks, while requiring significantly more
computational resources. We discuss potentially promising modeling, data
curation, and evaluation strategies for the next generation of DNALMs. Our code
is available at https://github.com/kundajelab/DART-Eval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS Datasets and Benchmarks 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ActAlign: Zero-Shot Fine-Grained Video Classification via
  Language-Guided Sequence Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.22967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.22967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Aghdam, Vincent Tao Hu, Björn Ommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the task of zero-shot video classification for extremely
fine-grained actions (e.g., Windmill Dunk in basketball), where no video
examples or temporal annotations are available for unseen classes. While
image-language models (e.g., CLIP, SigLIP) show strong open-set recognition,
they lack temporal modeling needed for video understanding. We propose
ActAlign, a truly zero-shot, training-free method that formulates video
classification as a sequence alignment problem, preserving the generalization
strength of pretrained image-language models. For each class, a large language
model (LLM) generates an ordered sequence of sub-actions, which we align with
video frames using Dynamic Time Warping (DTW) in a shared embedding space.
Without any video-text supervision or fine-tuning, ActAlign achieves 30.5%
accuracy on ActionAtlas--the most diverse benchmark of fine-grained actions
across multiple sports--where human performance is only 61.6%. ActAlign
outperforms billion-parameter video-language models while using 8x fewer
parameters. Our approach is model-agnostic and domain-general, demonstrating
that structured language priors combined with classical alignment methods can
unlock the open-set recognition potential of image-language models for
fine-grained video understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint manuscript - Project page:
  https://amir-aghdam.github.io/act-align/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gandalf the Red: Adaptive Security for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07927v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07927v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Pfister, Václav Volhejn, Manuel Knott, Santiago Arias, Julia Bazińska, Mykhailo Bichurin, Alan Commike, Janet Darling, Peter Dienes, Matthew Fiedler, David Haber, Matthias Kraft, Marco Lancini, Max Mathys, Damián Pascual-Ortiz, Jakub Podolak, Adrià Romero-López, Kyriacos Shiarlis, Andreas Signer, Zsolt Terek, Athanasios Theocharis, Daniel Timbrell, Samuel Trautwein, Samuel Watts, Yun-Han Wu, Mateo Rojas-Carulla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current evaluations of defenses against prompt attacks in large language
model (LLM) applications often overlook two critical factors: the dynamic
nature of adversarial behavior and the usability penalties imposed on
legitimate users by restrictive defenses. We propose D-SEC (Dynamic Security
Utility Threat Model), which explicitly separates attackers from legitimate
users, models multi-step interactions, and expresses the security-utility in an
optimizable form. We further address the shortcomings in existing evaluations
by introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed
to generate realistic, adaptive attack. Using Gandalf, we collect and release a
dataset of 279k prompt attacks. Complemented by benign user data, our analysis
reveals the interplay between security and utility, showing that defenses
integrated in the LLM (e.g., system prompts) can degrade usability even without
blocking requests. We demonstrate that restricted application domains,
defense-in-depth, and adaptive defenses are effective strategies for building
secure and useful LLM applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Niklas Pfister, V\'aclav Volhejn and Manuel Knott contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial flows: A gradient flow characterization of adversarial
  attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05376v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05376v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Weigand, Tim Roith, Martin Burger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A popular method to perform adversarial attacks on neuronal networks is the
so-called fast gradient sign method and its iterative variant. In this paper,
we interpret this method as an explicit Euler discretization of a differential
inclusion, where we also show convergence of the discretization to the
associated gradient flow. To do so, we consider the concept of p-curves of
maximal slope in the case $p=\infty$. We prove existence of $\infty$-curves of
maximum slope and derive an alternative characterization via differential
inclusions. Furthermore, we also consider Wasserstein gradient flows for
potential energies, where we show that curves in the Wasserstein space can be
characterized by a representing measure on the space of curves in the
underlying Banach space, which fulfill the differential inclusion. The
application of our theory to the finite-dimensional setting is twofold: On the
one hand, we show that a whole class of normalized gradient descent methods (in
particular signed gradient descent) converge, up to subsequences, to the flow,
when sending the step size to zero. On the other hand, in the distributional
setting, we show that the inner optimization task of adversarial training
objective can be characterized via $\infty$-curves of maximum slope on an
appropriate optimal transport space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Refined Policy Distillation: From VLA Generalists to RL Experts <span class="chip">IROS 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.05833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.05833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Jülg, Wolfram Burgard, Florian Walter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language-Action Models (VLAs) have demonstrated remarkable
generalization capabilities in real-world experiments. However, their success
rates are often not on par with expert policies, and they require fine-tuning
when the setup changes. In this work, we introduce Refined Policy Distillation
(RPD), a novel Reinforcement Learning (RL)-based policy refinement method that
bridges this performance gap through a combination of on-policy RL with
behavioral cloning. The core idea of RPD is to distill and refine VLAs into
compact, high-performing expert policies by guiding the student policy during
RL exploration using the actions of a teacher VLA, resulting in increased
sample efficiency and faster convergence. We complement our method by
fine-tuned versions of Octo and OpenVLA for ManiSkill3 to evaluate RPD in
simulation. While this is a key requirement for applying RL, it also yields new
insights beyond existing studies on VLA performance in real-world settings. Our
experimental results across various manipulation tasks show that RPD enables
the RL student to learn expert policies that outperform the VLA teacher in both
dense and sparse reward settings, while also achieving faster convergence than
the RL baseline. Our approach is even robust to changes in camera perspective
and can generalize to task variations that the underlying VLA cannot solve. Our
code, dataset, VLA checkpoints, and videos are available at
https://refined-policy-distillation.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for publication at IROS 2026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Audio-Visual Speech Enhancement Using <span class="highlight-title">Pre-train</span>ed Visual
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21448v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21448v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. Aleksandra Ma, Sile Yin, Li-Chia Yang, Shuo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech enhancement in audio-only settings remains challenging, particularly
in the presence of interfering speakers. This paper presents a simple yet
effective real-time audio-visual speech enhancement (AVSE) system, RAVEN, which
isolates and enhances the on-screen target speaker while suppressing
interfering speakers and background noise. We investigate how visual embeddings
learned from audio-visual speech recognition (AVSR) and active speaker
detection (ASD) contribute to AVSE across different SNR conditions and numbers
of interfering speakers. Our results show concatenating embeddings from AVSR
and ASD models provides the greatest improvement in low-SNR, multi-speaker
environments, while AVSR embeddings alone perform best in noise-only scenarios.
In addition, we develop a real-time streaming system that operates on a
computer CPU and we provide a video demonstration and code repository. To our
knowledge, this is the first open-source implementation of a real-time AVSE
system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted into Interspeech 2025; corrected author name typo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs
  More Realistic and Less Risky 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.03336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.03336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashutosh Hathidara, Julien Yu, Sebastian Schreiber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly tasked with invoking enterprise
APIs, yet they routinely falter when near-duplicate tools vie for the same user
intent or when required arguments are left underspecified. We introduce
DiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a
disambiguation-centric, three-stage pipeline that (i) synthesizes
persona-driven, multi-turn dialogues in which the assistant must distinguish
among highly similar tools, (ii) performs supervised fine-tuning of open-source
models with reasoning traces across 3B - 70B parameters, and (iii) evaluates
real-world readiness via a dynamic suite that redeploys each model in a live
agentic loop and reports end-to-end goal completion alongside conventional
static metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE
raise tool-invocation success by 27 pp over GPT-4o and by 49 pp over
Claude-3.5-Sonnet, both under optimized prompting. To spur further research, we
release an open corpus of 5000 production-grade enterprise API specifications
paired with rigorously validated, disambiguation-focused dialogues, offering a
practical blueprint for building reliable, enterprise-ready tool-calling
agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DHO$_2$: Accelerating Distributed Hybrid Order Optimization via Model
  Parallelism and ADMM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.00982v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.00982v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunxian Gu, Chaoqun You, Bangbang Ren, Lailong Luo, Junxu Xia, Deke Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling deep neural network (DNN) training to more devices can reduce
time-to-solution. However, it is impractical for users with limited computing
resources. FOSI, as a hybrid order optimizer, converges faster than
conventional optimizers by taking advantage of both gradient information and
curvature information when updating the DNN model. Therefore, it provides a new
chance for accelerating DNN training in the resource-constrained setting. In
this paper, we explore its distributed design, namely DHO$_2$, including
distributed calculation of curvature information and model update with partial
curvature information to accelerate DNN training with a low memory burden. To
further reduce the training time, we design a novel strategy to parallelize the
calculation of curvature information and the model update on different devices.
Experimentally, our distributed design can achieve an approximate linear
reduction of memory burden on each device with the increase of the device
number. Meanwhile, it achieves $1.4\times\sim2.1\times$ speedup in the total
training time compared with other distributed designs based on conventional
first- and second-order optimizers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Structure Sharing Empowers Multi-task Heterogeneous GNNs
  for Customer Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22089v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22089v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyue Feng, Shuxin Zhong, Jinquan Hang, Wenjun Lyu, Yuequn Zhang, Guang Yang, Haotian Wang, Desheng Zhang, Guang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customer expansion, i.e., growing a business existing customer base by
acquiring new customers, is critical for scaling operations and sustaining the
long-term profitability of logistics companies. Although state-of-the-art works
model this task as a single-node classification problem under a heterogeneous
graph learning framework and achieve good performance, they struggle with
extremely positive label sparsity issues in our scenario. Multi-task learning
(MTL) offers a promising solution by introducing a correlated, label-rich task
to enhance the label-sparse task prediction through knowledge sharing. However,
existing MTL methods result in performance degradation because they fail to
discriminate task-shared and task-specific structural patterns across tasks.
This issue arises from their limited consideration of the inherently complex
structure learning process of heterogeneous graph neural networks, which
involves the multi-layer aggregation of multi-type relations. To address the
challenge, we propose a Structure-Aware Hierarchical Information Sharing
Framework (SrucHIS), which explicitly regulates structural information sharing
across tasks in logistics customer expansion. SrucHIS breaks down the structure
learning phase into multiple stages and introduces sharing mechanisms at each
stage, effectively mitigating the influence of task-specific structural
patterns during each stage. We evaluate StrucHIS on both private and public
datasets, achieving a 51.41% average precision improvement on the private
dataset and a 10.52% macro F1 gain on the public dataset. StrucHIS is further
deployed at one of the largest logistics companies in China and demonstrates a
41.67% improvement in the success contract-signing rate over existing
strategies, generating over 453K new orders within just two months.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SEAL: Semantic Aware Image Watermarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12172v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12172v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kasra Arabi, R. Teal Witter, Chinmay Hegde, Niv Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have rapidly evolved to generate realistic outputs.
However, their synthetic outputs increasingly challenge the clear distinction
between natural and AI-generated content, necessitating robust watermarking
techniques. Watermarks are typically expected to preserve the integrity of the
target image, withstand removal attempts, and prevent unauthorized replication
onto unrelated images. To address this need, recent methods embed persistent
watermarks into images produced by diffusion models using the initial noise.
Yet, to do so, they either distort the distribution of generated images or rely
on searching through a long dictionary of used keys for detection.
  In this paper, we propose a novel watermarking method that embeds semantic
information about the generated image directly into the watermark, enabling a
distortion-free watermark that can be verified without requiring a database of
key patterns. Instead, the key pattern can be inferred from the semantic
embedding of the image using locality-sensitive hashing. Furthermore,
conditioning the watermark detection on the original image content improves
robustness against forgery attacks. To demonstrate that, we consider two
largely overlooked attack strategies: (i) an attacker extracting the initial
noise and generating a novel image with the same pattern; (ii) an attacker
inserting an unrelated (potentially harmful) object into a watermarked image,
possibly while preserving the watermark. We empirically validate our method's
increased robustness to these attacks. Taken together, our results suggest that
content-aware watermarks can mitigate risks arising from image-generative
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning in Structured Stackelberg Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.09006v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.09006v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria-Florina Balcan, Kiriaki Fragkia, Keegan Harris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study structured Stackelberg games, in which both players (the leader and
the follower) observe contextual information about the state of the world at
time of play. The leader plays against one of a finite number of followers, but
the follower's type is not known until after the game has ended. Importantly,
we assume a fixed relationship between the contextual information and the
follower's type, thereby allowing the leader to leverage this additional
structure when deciding her strategy. Under this setting, we find that standard
learning theoretic measures of complexity do not characterize the difficulty of
the leader's learning task. Instead, we introduce a new notion of dimension,
the Stackelberg-Littlestone dimension, which we show characterizes the
instance-optimal regret of the leader in the online setting. Based on this, we
also provide a provably optimal learning algorithm. We extend our results to
the distributional setting, where we use two new notions of dimension, the
$\gamma$-Stackelberg-Natarajan dimension and $\gamma$-Stackelberg-Graph
dimension. We prove that these control the sample complexity lower and upper
bounds respectively, and we design a simple, improper algorithm that achieves
the upper bound.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attack Anything: Blind DNNs via Universal Background Adversarial Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00029v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00029v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Lian, Shaohui Mei, Xiaofei Wang, Yi Wang, Lefan Wang, Yingjie Lu, Mingyang Ma, Lap-Pui Chau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been widely substantiated that deep neural networks (DNNs) are
susceptible and vulnerable to adversarial perturbations. Existing studies
mainly focus on performing attacks by corrupting targeted objects (physical
attack) or images (digital attack), which is intuitively acceptable and
understandable in terms of the attack's effectiveness. In contrast, our focus
lies in conducting background adversarial attacks in both digital and physical
domains, without causing any disruptions to the targeted objects themselves.
Specifically, an effective background adversarial attack framework is proposed
to attack anything, by which the attack efficacy generalizes well between
diverse objects, models, and tasks. Technically, we approach the background
adversarial attack as an iterative optimization problem, analogous to the
process of DNN learning. Besides, we offer a theoretical demonstration of its
convergence under a set of mild but sufficient conditions. To strengthen the
attack efficacy and transferability, we propose a new ensemble strategy
tailored for adversarial perturbations and introduce an improved smooth
constraint for the seamless connection of integrated perturbations. We conduct
comprehensive and rigorous experiments in both digital and physical domains
across various objects, models, and tasks, demonstrating the effectiveness of
attacking anything of the proposed method. The findings of this research
substantiate the significant discrepancy between human and machine vision on
the value of background variations, which play a far more critical role than
previously recognized, necessitating a reevaluation of the robustness and
reliability of DNNs. The code will be publicly available at
https://github.com/JiaweiLian/Attack_Anything
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing OOD Detection Using Latent Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16525v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16525v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Gao, Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is crucial for the reliable deployment of
machine learning models in real-world scenarios, enabling the identification of
unknown samples or objects. A prominent approach to enhance OOD detection
performance involves leveraging auxiliary datasets for training. Recent efforts
have explored using generative models, such as Stable Diffusion (SD), to
synthesize outlier data in the pixel space. However, synthesizing OOD data in
the pixel space can lead to reduced robustness due to over-generation. To
address this challenge, we propose Outlier-Aware Learning (OAL), a novel
framework that generates synthetic OOD training data within the latent space,
taking a further step to study how to utilize Stable Diffusion for developing a
latent-based outlier synthesis approach. This improvement facilitates network
training with fewer outliers and less computational cost. Besides, to
regularize the model's decision boundary, we develop a mutual information-based
contrastive learning module (MICL) that amplifies the distinction between
In-Distribution (ID) and collected OOD data. Moreover, we develop a knowledge
distillation module to prevent the degradation of ID classification accuracy
when training with OOD data. The superior performance of our method on several
benchmark datasets demonstrates its efficiency and effectiveness. Source code
is available in https://github.com/HengGao12/OAL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Review</span> of Diffusion Models in Smart Agriculture:
  Progress, Applications, and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18376v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18376v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Hu, Haodong Chen, Choon Ki Ahn, Danfeng Hong, Qianqian Duan, Huiliang Shang, Guoxiang Li, Linhua Jiang, Dawei Zhang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the global population increasing and arable land resources becoming
increasingly limited, smart and precision agriculture have emerged as essential
directions for sustainable agricultural development. Artificial intelligence
(AI), particularly deep learning models, has been widely adopted in
applications such as crop monitoring, pest detection, and yield prediction.
Among recent generative models, diffusion models have demonstrated considerable
potential in agricultural image processing, data augmentation, and remote
sensing analysis. Compared to traditional generative adversarial networks
(GANs), diffusion models exhibit greater training stability and superior image
generation quality, effectively addressing challenges such as limited annotated
datasets and imbalanced sample distributions in agricultural scenarios. This
paper reviews recent advancements in the application of diffusion models within
agriculture, focusing on their roles in crop disease and pest detection, remote
sensing image enhancement, crop growth prediction, and agricultural resource
management. Empirical studies show that diffusion models significantly enhance
the performance of downstream models by improving accuracy, robustness, and
generalization in tasks involving image synthesis, augmentation, and denoising
under complex environmental conditions. Despite ongoing challenges in
computational efficiency and domain generalization, diffusion models are
expected to play an increasingly important role in the future of intelligent
agriculture. As the technology continues to evolve, it holds substantial
promise for addressing pressing global issues in food security and
environmental sustainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Friend or Foe? Harnessing Controllable Overfitting for Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Qian, Bingke Zhu, Yingying Chen, Ming Tang, Jinqiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Overfitting has traditionally been viewed as detrimental to anomaly
detection, where excessive generalization often limits models' sensitivity to
subtle anomalies. Our work challenges this conventional view by introducing
Controllable Overfitting-based Anomaly Detection (COAD), a novel framework that
strategically leverages overfitting to enhance anomaly discrimination
capabilities. We propose the Aberrance Retention Quotient (ARQ), a novel metric
that systematically quantifies the extent of overfitting, enabling the
identification of an optimal golden overfitting interval wherein model
sensitivity to anomalies is maximized without sacrificing generalization. To
comprehensively capture how overfitting affects detection performance, we
further propose the Relative Anomaly Distribution Index (RADI), a metric
superior to traditional AUROC by explicitly modeling the separation between
normal and anomalous score distributions. Theoretically, RADI leverages ARQ to
track and evaluate how overfitting impacts anomaly detection, offering an
integrated approach to understanding the relationship between overfitting
dynamics and model efficacy. We also rigorously validate the statistical
efficacy of Gaussian noise as pseudo-anomaly generators, reinforcing the
method's broad applicability. Empirical evaluations demonstrate that our
controllable overfitting method achieves State-Of-The-Art(SOTA) performance in
both one-class and multi-class anomaly detection tasks, thus redefining
overfitting as a powerful strategy rather than a limitation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the evaluators: Towards human-aligned metrics for missing
  markers reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14334v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14334v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taras Kucherenko, Derek Peristy, Judith Bütepage
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animation data is often obtained through optical motion capture systems,
which utilize a multitude of cameras to establish the position of optical
markers. However, system errors or occlusions can result in missing markers,
the manual cleaning of which can be time-consuming. This has sparked interest
in machine learning-based solutions for missing marker reconstruction in the
academic community. Most academic papers utilize a simplistic mean square error
as the main metric. In this paper, we show that this metric does not correlate
with subjective perception of the fill quality. Additionally, we introduce and
evaluate a set of better-correlated metrics that can drive progress in the
field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the ACM International Conference on Multimedia 2025 (ACM
  MM'25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic brain tumor segmentation in 2D intra-operative ultrasound
  images using magnetic resonance imaging tumor annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14017v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14017v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathilde Faanes, Ragnhild Holden Helland, Ole Solheim, Sébastien Muller, Ingerid Reinertsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic segmentation of brain tumors in intra-operative ultrasound (iUS)
images could facilitate localization of tumor tissue during resection surgery.
The lack of large annotated datasets limits the current models performances. In
this paper, we investigated the use of tumor annotations in magnetic resonance
imaging (MRI) scans, which are more accessible than annotations in iUS images,
for training of deep learning models for iUS brain tumor segmentation. We used
180 annotated MRI scans with corresponding unannotated iUS images, and 29
annotated iUS images. Image registration was performed to transfer the MRI
annotations to the corresponding iUS images before training the nnU-Net model
with different configurations of the data and label origins. The results showed
no significant difference in Dice score for a model trained with only MRI
annotated tumors compared to models trained with only iUS annotations and both,
and to expert annotations, indicating that MRI tumor annotations can be used as
a substitute for iUS tumor annotations to train a deep learning model for
automatic brain tumor segmentation in iUS images. The best model obtained an
average Dice score of $0.62\pm0.31$, compared to $0.67\pm0.25$ for an expert
neurosurgeon, where the performance on larger tumors were similar, but lower
for the models on smaller tumors. In addition, the results showed that removing
smaller tumors from the training sets improved the results. The main models are
available here:
https://github.com/mathildefaanes/us_brain_tumor_segmentation/tree/main
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14, 5figures. This work has been submitted to the IEEE for possible
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy Amplification by Structured Subsampling for Deep Differentially
  Private Time Series Forecasting <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02410v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02410v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Schuchardt, Mina Dalirrooyfard, Jed Guzelkabaagac, Anderson Schneider, Yuriy Nevmyvaka, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many forms of sensitive data, such as web traffic, mobility data, or hospital
occupancy, are inherently sequential. The standard method for training machine
learning models while ensuring privacy for units of sensitive information, such
as individual hospital visits, is differentially private stochastic gradient
descent (DP-SGD). However, we observe in this work that the formal guarantees
of DP-SGD are incompatible with time series specific tasks like forecasting,
since they rely on the privacy amplification attained by training on small,
unstructured batches sampled from an unstructured dataset. In contrast, batches
for forecasting are generated by (1) sampling sequentially structured time
series from a dataset, (2) sampling contiguous subsequences from these series,
and (3) partitioning them into context and ground-truth forecast windows. We
theoretically analyze the privacy amplification attained by this structured
subsampling to enable the training of forecasting models with sound and tight
event- and user-level privacy guarantees. Towards more private models, we
additionally prove how data augmentation amplifies privacy in self-supervised
training of sequence models. Our empirical evaluation demonstrates that
amplification by structured subsampling enables the training of forecasting
models with strong formal privacy guarantees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as ICML 2025 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ medDreamer: Model-Based Reinforcement Learning with Latent Imagination
  on Complex EHRs for Clinical Decision Support 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianyi Xu, Gousia Habib, Dilruk Perera, Mengling Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Timely and personalized treatment decisions are essential across a wide range
of healthcare settings where patient responses can vary significantly and
evolve over time. Clinical data used to support these treatment decisions are
often irregularly sampled, where missing data frequencies may implicitly convey
information about the patient's condition. Existing Reinforcement Learning (RL)
based clinical decision support systems often ignore the missing patterns and
distort them with coarse discretization and simple imputation. They are also
predominantly model-free and largely depend on retrospective data, which could
lead to insufficient exploration and bias by historical behaviors. To address
these limitations, we propose medDreamer, a novel model-based reinforcement
learning framework for personalized treatment recommendation. medDreamer
contains a world model with an Adaptive Feature Integration module that
simulates latent patient states from irregular data and a two-phase policy
trained on a hybrid of real and imagined trajectories. This enables learning
optimal policies that go beyond the sub-optimality of historical clinical
decisions, while remaining close to real clinical data. We evaluate medDreamer
on both sepsis and mechanical ventilation treatment tasks using two large-scale
Electronic Health Records (EHRs) datasets. Comprehensive evaluations show that
medDreamer significantly outperforms model-free and model-based baselines in
both clinical outcomes and off-policy metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05108v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05108v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anja Surina, Amin Mansouri, Lars Quaedvlieg, Amal Seddas, Maryna Viazovska, Emmanuel Abbe, Caglar Gulcehre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering efficient algorithms for solving complex problems has been an
outstanding challenge in mathematics and computer science, requiring
substantial human expertise over the years. Recent advancements in evolutionary
search with large language models (LLMs) have shown promise in accelerating the
discovery of algorithms across various domains, particularly in mathematics and
optimization. However, existing approaches treat the LLM as a static generator,
missing the opportunity to update the model with the signal obtained from
evolutionary exploration. In this work, we propose to augment LLM-based
evolutionary search by continuously refining the search operator - the LLM -
through reinforcement learning (RL) fine-tuning. Our method leverages
evolutionary search as an exploration strategy to discover improved algorithms,
while RL optimizes the LLM policy based on these discoveries. Our experiments
on combinatorial optimization tasks demonstrate that integrating RL with
evolutionary search accelerates the discovery of superior algorithms,
showcasing the potential of RL-enhanced evolutionary strategies for algorithm
design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do MLLMs Capture How Interfaces Guide User Behavior? A Benchmark for
  Multimodal UI/UX Design Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05026v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05026v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehyun Jeon, Min Soo Kim, Jang Han Yoon, Sumin Shim, Yejin Choi, Hanbin Kim, Youngjae Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User interface (UI) design goes beyond visuals, guiding user behavior and
overall user experience (UX). Strategically crafted interfaces, for example,
can boost sign-ups and drive business sales, underscoring the shift toward
UI/UX as a unified design concept. While recent studies have explored UI
quality evaluation using Multimodal Large Language Models (MLLMs), they largely
focus on surface-level features, overlooking behavior-oriented aspects. To fill
this gap, we introduce WiserUI-Bench, a novel benchmark for assessing models'
multimodal understanding of UI/UX design. It includes 300 diverse real-world UI
image pairs, each consisting of two design variants A/B-tested at scale by
actual companies, where one was empirically validated to steer more user
actions than the other. Each pair is accompanied one or more of 684
expert-curated rationales that capture key factors behind each winning design's
effectiveness, spanning diverse cognitive dimensions of UX. Our benchmark
supports two core tasks: (1) selecting the more effective UI/UX design by
predicting the A/B test verified winner and (2) assessing how well a model,
given the winner, can explain its effectiveness in alignment with expert
reasoning. Experiments across several MLLMs show that current models exhibit
limited nuanced reasoning about UI/UX design and its behavioral impact. We
believe our work will foster research in UI/UX understanding and enable broader
applications such as behavior-aware interface optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 25 figures, Our code and dataset:
  https://github.com/jeochris/wiserui-bench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustly Learning Monotone Generalized Linear Models via Data
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikos Zarifis, Puqian Wang, Ilias Diakonikolas, Jelena Diakonikolas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the task of learning Generalized Linear models (GLMs) in the
agnostic model under the Gaussian distribution. We give the first
polynomial-time algorithm that achieves a constant-factor approximation for
\textit{any} monotone Lipschitz activation. Prior constant-factor GLM learners
succeed for a substantially smaller class of activations. Our work resolves a
well-known open problem, by developing a robust counterpart to the classical
GLMtron algorithm (Kakade et al., 2011). Our robust learner applies more
generally, encompassing all monotone activations with bounded
$(2+\zeta)$-moments, for any fixed $\zeta>0$ -- a condition that is essentially
necessary. To obtain our results, we leverage a novel data augmentation
technique with decreasing Gaussian noise injection and prove a number of
structural results that may be useful in other settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Slicing the Gaussian Mixture Wasserstein Distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Piening, Robert Beinert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian mixture models (GMMs) are widely used in machine learning for tasks
such as clustering, classification, image reconstruction, and generative
modeling. A key challenge in working with GMMs is defining a computationally
efficient and geometrically meaningful metric. The mixture Wasserstein (MW)
distance adapts the Wasserstein metric to GMMs and has been applied in various
domains, including domain adaptation, dataset comparison, and reinforcement
learning. However, its high computational cost -- arising from repeated
Wasserstein distance computations involving matrix square root estimations and
an expensive linear program -- limits its scalability to high-dimensional and
large-scale problems. To address this, we propose multiple novel slicing-based
approximations to the MW distance that significantly reduce computational
complexity while preserving key optimal transport properties. From a
theoretical viewpoint, we establish several weak and strong equivalences
between the introduced metrics, and show the relations to the original MW
distance and the well-established sliced Wasserstein distance. Furthermore, we
validate the effectiveness of our approach through numerical experiments,
demonstrating computational efficiency and applications in clustering,
perceptual image comparison, and GMM minimization
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Deepfake Detectors in the Wild <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viacheslav Pirogov, Maksim Artemev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deepfakes powered by advanced machine learning models present a significant
and evolving threat to identity verification and the authenticity of digital
media. Although numerous detectors have been developed to address this problem,
their effectiveness has yet to be tested when applied to real-world data. In
this work we evaluate modern deepfake detectors, introducing a novel testing
procedure designed to mimic real-world scenarios for deepfake detection. Using
state-of-the-art deepfake generation methods, we create a comprehensive dataset
containing more than 500,000 high-quality deepfake images. Our analysis shows
that detecting deepfakes still remains a challenging task. The evaluation shows
that in fewer than half of the deepfake detectors tested achieved an AUC score
greater than 60%, with the lowest being 50%. We demonstrate that basic image
manipulations, such as JPEG compression or image enhancement, can significantly
reduce model performance. All code and data are publicly available at
https://github.com/SumSubstance/Deepfake-Detectors-in-the-Wild.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the ICML 2025 Workshop 'DataWorld: Unifying Data Curation
  Frameworks Across Domains'</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparison of Affine and Rational Quadratic Spline Coupling and
  Autoregressive Flows through Robust Statistical Tests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12024v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12024v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Coccaro, Marco Letizia, Humberto Reyes-Gonzalez, Riccardo Torre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Normalizing flows have emerged as a powerful brand of generative models, as
they not only allow for efficient sampling of complicated target distributions
but also deliver density estimation by construction. We propose here an
in-depth comparison of coupling and autoregressive flows, both based on
symmetric (affine) and non-symmetric (rational quadratic spline) bijectors,
considering four different architectures: real-valued non-Volume preserving
(RealNVP), masked autoregressive flow (MAF), coupling rational quadratic spline
(C-RQS), and autoregressive rational quadratic spline (A-RQS). We focus on a
set of multimodal target distributions of increasing dimensionality ranging
from 4 to 400. The performances were compared by means of different test
statistics for two-sample tests, built from known distance measures: the sliced
Wasserstein distance, the dimension-averaged one-dimensional
Kolmogorov--Smirnov test, and the Frobenius norm of the difference between
correlation matrices. Furthermore, we included estimations of the variance of
both the metrics and the trained models. Our results indicate that the A-RQS
algorithm stands out both in terms of accuracy and training speed. Nonetheless,
all the algorithms are generally able, without too much fine-tuning, to learn
complicated distributions with limited training data and in a reasonable time
of the order of hours on a Tesla A40 GPU. The only exception is the C-RQS,
which takes significantly longer to train, does not always provide good
accuracy, and becomes unstable for large dimensionalities. All algorithms were
implemented using \textsc{TensorFlow2} and \textsc{TensorFlow Probability} and
have been made available on
\href{https://github.com/NF4HEP/NormalizingFlowsHD}{GitHub}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3: published version; 25 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Transformer</span> Meets Twicing: Harnessing Unattended Residual Information <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00687v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00687v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laziz Abdullaev, Tan M. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based deep learning models have achieved state-of-the-art
performance across numerous language and vision tasks. While the self-attention
mechanism, a core component of transformers, has proven capable of handling
complex data patterns, it has been observed that the representational capacity
of the attention matrix degrades significantly across transformer layers,
thereby hurting its overall performance. In this work, we leverage the
connection between self-attention computations and low-pass non-local means
(NLM) smoothing filters and propose the Twicing Attention, a novel attention
mechanism that uses kernel twicing procedure in nonparametric regression to
alleviate the low-pass behavior of associated NLM smoothing with compelling
theoretical guarantees and enhanced adversarial robustness. This approach
enables the extraction and reuse of meaningful information retained in the
residuals following the imperfect smoothing operation at each layer. Our
proposed method offers two key advantages over standard self-attention: 1) a
provably slower decay of representational capacity and 2) improved robustness
and accuracy across various data modalities and tasks. We empirically
demonstrate the performance gains of our model over baseline transformers on
multiple tasks and benchmarks, including image classification and language
modeling, on both clean and corrupted data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages in the main text. Published at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion
  Model <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15322v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15322v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoye Chai, Shiyuan Zhang, Xiaoqian Qi, Baohua Qiu, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile traffic forecasting allows operators to anticipate network dynamics
and performance in advance, offering substantial potential for enhancing
service quality and improving user experience. However, existing models are
often task-oriented and are trained with tailored data, which limits their
effectiveness in diverse mobile network tasks of Base Station (BS) deployment,
resource allocation, energy optimization, etc. and hinders generalization
across different urban environments. Foundation models have made remarkable
strides across various domains of NLP and CV due to their multi-tasking
adaption and zero/few-shot learning capabilities. In this paper, we propose an
innovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to
handle diverse forecasting tasks of short/long-term predictions and
distribution generation across multiple cities to support network planning and
optimization. FoMo combines diffusion models and transformers, where various
spatio-temporal masks are proposed to enable FoMo to learn intrinsic features
of different tasks, and a contrastive learning strategy is developed to capture
the correlations between mobile traffic and urban contexts, thereby improving
its transfer learning capability. Extensive experiments on 9 real-world
datasets demonstrate that FoMo outperforms current models concerning diverse
forecasting tasks and zero/few-shot learning, showcasing a strong universality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 ACM SIGKDD International Conference on Knowledge Discovery and
  Data Mining, KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Out-of-Distribution Detection: A Task-Oriented <span class="highlight-title">Survey</span> of Recent Advances 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11884v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11884v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Lu, Yingsheng Wang, Lijun Sheng, Lingxiao He, Aihua Zheng, Jian Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection aims to detect test samples outside the
training category space, which is an essential component in building reliable
machine learning systems. Existing reviews on OOD detection primarily focus on
method taxonomy, surveying the field by categorizing various approaches.
However, many recent works concentrate on non-traditional OOD detection
scenarios, such as test-time adaptation, multi-modal data sources and other
novel contexts. In this survey, we uniquely review recent advances in OOD
detection from the task-oriented perspective for the first time. According to
the user's access to the model, that is, whether the OOD detection method is
allowed to modify or retrain the model, we classify the methods as
training-driven or training-agnostic. Besides, considering the rapid
development of pre-trained models, large pre-trained model-based OOD detection
is also regarded as an important category and discussed separately.
Furthermore, we provide a discussion of the evaluation scenarios, a variety of
applications, and several future research directions. We believe this survey
with new taxonomy will benefit the proposal of new methods and the expansion of
more practical scenarios. A curated list of related papers is provided in the
Github repository:
https://github.com/shuolucs/Awesome-Out-Of-Distribution-Detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Computing Surveys (CSUR) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-Independent Machine Learning Approach for Nanometric Axial
  Localization and Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14754v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14754v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Alexandrov, Giovanni Acampora, Giovanni De Lellis, Antonia Di Crescenzo, Chiara Errico, Daria Morozova, Valeri Tioukov, Autilia Vittiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately tracking particles and determining their coordinate along the
optical axis is a major challenge in optical microscopy, especially when
extremely high precision is needed. In this study, we introduce a deep learning
approach using convolutional neural networks (CNNs) that can determine axial
coordinates from dual-focal-plane images without relying on predefined models.
Our method achieves an axial localization precision of 40 nanometers-six times
better than traditional single-focal-plane techniques. The model's simple
design and strong performance make it suitable for a wide range of uses,
including dark matter detection, proton therapy for cancer, and radiation
protection in space. It also shows promise in fields like biological imaging,
materials science, and environmental monitoring. This work highlights how
machine learning can turn complex image data into reliable, precise
information, offering a flexible and powerful tool for many scientific
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Multi-Label Contrastive Learning for Protein-Protein
  Interaction Prediction Across Organisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02724v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02724v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyi Liu, Buwen Liang, Yuetong Fang, Zixuan Jiang, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in AI for science have highlighted the power of contrastive
learning in bridging heterogeneous biological data modalities. Building on this
paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction
across Organisms), a hierarchical contrastive framework for protein-protein
interaction(PPI) prediction, where protein sequences and their hierarchical
attributes are aligned through multi-tiered biological representation matching.
The proposed approach incorporates hierarchical contrastive loss functions that
emulate the structured relationship among functional classes of proteins. The
framework adaptively incorporates domain and family knowledge through a
data-driven penalty mechanism, enforcing consistency between the learned
embedding space and the intrinsic hierarchy of protein functions. Experiments
on benchmark datasets demonstrate that HIPPO achieves state-of-the-art
performance, outperforming existing methods and showing robustness in low-data
regimes. Notably, the model demonstrates strong zero-shot transferability to
other species without retraining, enabling reliable PPI prediction and
functional inference even in less characterized or rare organisms where
experimental data are limited. Further analysis reveals that hierarchical
feature fusion is critical for capturing conserved interaction determinants,
such as binding motifs and functional annotations. This work advances
cross-species PPI prediction and provides a unified framework for interaction
prediction in scenarios with sparse or imbalanced multi-species data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and
  Post-LN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengxiang Li, Lu Yin, Shiwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved remarkable success, yet recent
findings reveal that their deeper layers often contribute minimally and can be
pruned without affecting overall performance. While some view this as an
opportunity for model compression, we identify it as a training shortfall
rooted in the widespread use of Pre-Layer Normalization (Pre-LN). We
demonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads
to diminished gradient norms in its deeper layers, reducing their
effectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger
gradient norms in deeper layers but suffers from vanishing gradients in earlier
layers. To address this, we introduce Mix-LN, a novel normalization technique
that combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN
applies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring
more uniform gradients across layers. This allows all parts of the
network--both shallow and deep layers--to contribute effectively to training.
Extensive experiments with various model sizes from 70M to 7B demonstrate that
Mix-LN consistently outperforms both Pre-LN and Post-LN, promoting more
balanced, healthier gradient norms throughout the network, and enhancing the
overall quality of LLM pre-training. Furthermore, we demonstrate that models
pre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN
during supervised fine-tuning (SFT) and reinforcement learning from human
feedback (RLHF), highlighting the critical importance of high-quality deep
layers. By effectively addressing the inefficiencies of deep layers in current
LLMs, Mix-LN unlocks their potential, enhancing model capacity without
increasing model size. Our code is available at
https://github.com/pixeli99/MixLN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EQ-VAE: Equivariance Regularized Latent Space for Improved Generative
  Image Modeling <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09509v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09509v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, Nikos Komodakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent generative models have emerged as a leading approach for high-quality
image synthesis. These models rely on an autoencoder to compress images into a
latent space, followed by a generative model to learn the latent distribution.
We identify that existing autoencoders lack equivariance to semantic-preserving
transformations like scaling and rotation, resulting in complex latent spaces
that hinder generative performance. To address this, we propose EQ-VAE, a
simple regularization approach that enforces equivariance in the latent space,
reducing its complexity without degrading reconstruction quality. By finetuning
pre-trained autoencoders with EQ-VAE, we enhance the performance of several
state-of-the-art generative models, including DiT, SiT, REPA and MaskGIT,
achieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning.
EQ-VAE is compatible with both continuous and discrete autoencoders, thus
offering a versatile enhancement for a wide range of latent generative models.
Project page and code: https://eq-vae.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clustered Federated Learning for Generalizable FDIA Detection in Smart
  Grids with Heterogeneous Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14999v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14999v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfeng Li, Junhong Liu, Zhaohui Yang, Guofu Liao, Chuyun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  False Data Injection Attacks (FDIAs) pose severe security risks to smart
grids by manipulating measurement data collected from spatially distributed
devices such as SCADA systems and PMUs. These measurements typically exhibit
Non-Independent and Identically Distributed (Non-IID) characteristics across
different regions, which significantly challenges the generalization ability of
detection models. Traditional centralized training approaches not only face
privacy risks and data sharing constraints but also incur high transmission
costs, limiting their scalability and deployment feasibility. To address these
issues, this paper proposes a privacy-preserving federated learning framework,
termed Federated Cluster Average (FedClusAvg), designed to improve FDIA
detection in Non-IID and resource-constrained environments. FedClusAvg
incorporates cluster-based stratified sampling and hierarchical communication
(client-subserver-server) to enhance model generalization and reduce
communication overhead. By enabling localized training and weighted parameter
aggregation, the algorithm achieves accurate model convergence without
centralizing sensitive data. Experimental results on benchmark smart grid
datasets demonstrate that FedClusAvg not only improves detection accuracy under
heterogeneous data distributions but also significantly reduces communication
rounds and bandwidth consumption. This work provides an effective solution for
secure and efficient FDIA detection in large-scale distributed power systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Electrocardiogram Foundation Model Built on over 10 Million
  Recordings with External Evaluation across Multiple Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04133v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04133v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Li, Aaron Aguirre, Junior Moura, Che Liu, Lanhai Zhong, Chenxi Sun, Gari Clifford, Brandon Westover, Shenda Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) has demonstrated significant potential in ECG
analysis and cardiovascular disease assessment. Recently, foundation models
have played a remarkable role in advancing medical AI. The development of an
ECG foundation model holds the promise of elevating AI-ECG research to new
heights. However, building such a model faces several challenges, including
insufficient database sample sizes and inadequate generalization across
multiple domains. Additionally, there is a notable performance gap between
single-lead and multi-lead ECG analyses. We introduced an ECG Foundation Model
(ECGFounder), a general-purpose model that leverages real-world ECG annotations
from cardiology experts to broaden the diagnostic capabilities of ECG analysis.
ECGFounder was trained on over 10 million ECGs with 150 label categories from
the Harvard-Emory ECG Database, enabling comprehensive cardiovascular disease
diagnosis through ECG analysis. The model is designed to be both an effective
out-of-the-box solution, and a to be fine-tunable for downstream tasks,
maximizing usability. Importantly, we extended its application to lower rank
ECGs, and arbitrary single-lead ECGs in particular. ECGFounder is applicable to
supporting various downstream tasks in mobile monitoring scenarios.
Experimental results demonstrate that ECGFounder achieves expert-level
performance on internal validation sets, with AUROC exceeding 0.95 for eighty
diagnoses. It also shows strong classification performance and generalization
across various diagnoses on external validation sets. When fine-tuned,
ECGFounder outperforms baseline models in demographic analysis, clinical event
detection, and cross-modality cardiac rhythm diagnosis. The trained model and
data will be publicly released upon publication through the bdsp.io. Our code
is available at https://github.com/PKUDigitalHealth/ECGFounder
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/PKUDigitalHealth/ECGFounder</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Avoiding Leakage Poisoning: Concept Interventions Under Distribution
  Shifts <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17921v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17921v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateo Espinosa Zarlenga, Gabriele Dominici, Pietro Barbiero, Zohreh Shams, Mateja Jamnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate how concept-based models (CMs) respond to
out-of-distribution (OOD) inputs. CMs are interpretable neural architectures
that first predict a set of high-level concepts (e.g., stripes, black) and then
predict a task label from those concepts. In particular, we study the impact of
concept interventions (i.e., operations where a human expert corrects a CM's
mispredicted concepts at test time) on CMs' task predictions when inputs are
OOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we
term leakage poisoning, that prevents them from properly improving their
accuracy when intervened on for OOD inputs. To address this, we introduce
MixCEM, a new CM that learns to dynamically exploit leaked information missing
from its concepts only when this information is in-distribution. Our results
across tasks with and without complete sets of concept annotations demonstrate
that MixCEMs outperform strong baselines by significantly improving their
accuracy for both in-distribution and OOD samples in the presence and absence
of concept interventions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the Forty-Second International Conference on Machine
  Learning (ICML 2025). Post-conference manuscript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rec-AD: An Efficient Computation Framework for FDIA Detection Based on
  Tensor Train Decomposition and Deep Learning Recommendation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14668v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14668v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfeng Li, Junhong Liu, Zhaohui Yang, Guofu Liao, Chuyun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models have been widely adopted for False Data Injection Attack
(FDIA) detection in smart grids due to their ability to capture unstructured
and sparse features. However, the increasing system scale and data
dimensionality introduce significant computational and memory burdens,
particularly in large-scale industrial datasets, limiting detection efficiency.
To address these issues, this paper proposes Rec-AD, a computationally
efficient framework that integrates Tensor Train decomposition with the Deep
Learning Recommendation Model (DLRM). Rec-AD enhances training and inference
efficiency through embedding compression, optimized data access via index
reordering, and a pipeline training mechanism that reduces memory communication
overhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing
FDIA detection systems without code modifications. Experimental results show
that Rec-AD significantly improves computational throughput and real-time
detection performance, narrowing the attack window and increasing attacker
cost. These advancements strengthen edge computing capabilities and
scalability, providing robust technical support for smart grid security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collision-based Dynamics for Multi-Marginal Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsen Sadr, Hossein Gorji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the Boltzmann kinetics, we propose a collision-based dynamics
with a Monte Carlo solution algorithm that approximates the solution of the
multi-marginal optimal transport problem via randomized pairwise swapping of
sample indices. The computational complexity and memory usage of the proposed
method scale linearly with the number of samples, making it highly attractive
for high-dimensional settings. In several examples, we demonstrate the
efficiency of the proposed method compared to the state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pimba: A Processing-in-Memory Acceleration for Post-<span class="highlight-title">Transformer</span> Large
  Language Model Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.10178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.10178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonung Kim, Yubin Lee, Yoonsung Kim, Jinwoo Hwang, Seongryong Oh, Jiyong Jung, Aziz Huseynov, Woong Gyu Park, Chang Hyun Park, Divya Mahajan, Jongse Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers are the driving force behind today's Large Language Models
(LLMs), serving as the foundation for their performance and versatility. Yet,
their compute and memory costs grow with sequence length, posing scalability
challenges for long-context inferencing. In response, the algorithm community
is exploring alternative architectures, such as state space models (SSMs),
linear attention, and recurrent neural networks (RNNs), which we refer to as
post-transformers. This shift presents a key challenge: building a serving
system that efficiently supports both transformer and post-transformer LLMs
within a unified framework. To address this challenge, we analyze the
performance characteristics of transformer and post-transformer LLMs. Despite
their algorithmic differences, both are fundamentally limited by memory
bandwidth under batched inference due to attention in transformers and state
updates in post-transformers. Further analyses suggest two additional insights:
(1) state update operations, unlike attention, incur high hardware cost, making
per-bank PIM acceleration inefficient, and (2) different low-precision
arithmetic methods offer varying accuracy-area tradeoffs, while we identify
Microsoft's MX as the Pareto-optimal choice. Building on these insights, we
design Pimba as an array of State-update Processing Units (SPUs), each shared
between two banks to enable interleaved access to PIM. Each SPU includes a
State-update Processing Engine (SPE) that comprises element-wise multipliers
and adders using MX-based quantized arithmetic, enabling efficient execution of
state update and attention operations. Our evaluation shows that, compared to
LLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 4.1x and 2.1x
higher token generation throughput, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Survivability of Backdoor Attacks on Unconstrained Face Recognition
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01607v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01607v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Le Roux, Yannick Teglia, Teddy Furon, Philippe Loubet-Moundi, Eric Bourbao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread use of deep learning face recognition raises several security
concerns. Although prior works point at existing vulnerabilities, DNN backdoor
attacks against real-life, unconstrained systems dealing with images captured
in the wild remain a blind spot of the literature. This paper conducts the
first system-level study of backdoors in deep learning-based face recognition
systems. This paper yields four contributions by exploring the feasibility of
DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the
first time two backdoor attacks on the face detection task: face generation and
face landmark shift attacks. We then show that face feature extractors trained
with large margin losses also fall victim to backdoor attacks. Combining our
models, we then show using 20 possible pipeline configurations and 15 attack
cases that a single backdoor enables an attacker to bypass the entire function
of a system. Finally, we provide stakeholders with several best practices and
countermeasures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StackLiverNet: A Novel Stacked Ensemble Model for Accurate and
  Interpretable Liver Disease Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Ehsanul Haque, S. M. Jahidul Islam, Shakil Mia, Rumana Sharmin,  Ashikuzzaman, Md Samir Morshed, Md. Tahmidul Huque
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Liver diseases are a serious health concern in the world, which requires
precise and timely diagnosis to enhance the survival chances of patients. The
current literature implemented numerous machine learning and deep learning
models to classify liver diseases, but most of them had some issues like high
misclassification error, poor interpretability, prohibitive computational
expense, and lack of good preprocessing strategies. In order to address these
drawbacks, we introduced StackLiverNet in this study; an interpretable stacked
ensemble model tailored to the liver disease detection task. The framework uses
advanced data preprocessing and feature selection technique to increase model
robustness and predictive ability. Random undersampling is performed to deal
with class imbalance and make the training balanced. StackLiverNet is an
ensemble of several hyperparameter-optimized base classifiers, whose
complementary advantages are used through a LightGBM meta-model. The provided
model demonstrates excellent performance, with the testing accuracy of 99.89%,
Cohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and
efficient training and inference speeds that are amenable to clinical practice
(training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local
Interpretable Model-Agnostic Explanations (LIME) are applied to generate
transparent explanations of individual predictions, revealing high
concentrations of Alkaline Phosphatase and moderate SGOT as important
observations of liver disease. Also, SHAP was used to rank features by their
global contribution to predictions, while the Morris method confirmed the most
influential features through sensitivity analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and presented paper of THE 16th INTERNATIONAL IEEE
  CONFERENCE ON COMPUTING, COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT)
  INDIA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Manifold-regularised Large-Margin $\ell_p$-SVDD for Multidimensional
  Time Series Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shervin Rahimzadeh Arashloo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We generalise the recently introduced large-margin $\ell_p$-SVDD approach to
exploit the geometry of data distribution via manifold regularising for time
series anomaly detection. Specifically, we formulate a manifold-regularised
variant of the $\ell_p$-SVDD method to encourage label smoothness on the
underlying manifold to capture structural information for improved detection
performance. Drawing on an existing Representer theorem, we then provide an
effective optimisation technique for the proposed method.
  We theoretically study the proposed approach using Rademacher complexities to
analyse its generalisation performance and also provide an experimental
assessment of the proposed method across various data sets to compare its
performance against other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdapFair: Ensuring Adaptive Fairness for Machine Learning Operations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15088v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15088v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghui Huang, Zihao Tang, Xiangyu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The biases and discrimination of machine learning algorithms have attracted
significant attention, leading to the development of various algorithms
tailored to specific contexts. However, these solutions often fall short of
addressing fairness issues inherent in machine learning operations. In this
paper, we present an adaptive debiasing framework designed to find an optimal
fair transformation of input data that maximally preserves data predictability
under dynamic conditions. A distinctive feature of our approach is its
flexibility and efficiency. It can be integrated with pretrained black-box
classifiers, providing fairness guarantees with minimal retraining efforts,
even in the face of frequent data drifts, evolving fairness requirements, and
batches of similar tasks. To achieve this, we leverage the normalizing flows to
enable efficient, information-preserving data transformation, ensuring that no
critical information is lost during the debiasing process. Additionally, we
incorporate the Wasserstein distance as the fairness measure to guide the
optimization of data transformations. Finally, we introduce an efficient
optimization algorithm with closed-formed gradient computations, making our
framework scalable and suitable for dynamic, real-world environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages,15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Reasoning Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21734v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21734v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guan Wang, Jin Li, Yuhao Sun, Xing Chen, Changling Liu, Yue Wu, Meng Lu, Sen Song, Yasin Abbasi Yadkori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning, the process of devising and executing complex goal-oriented action
sequences, remains a critical challenge in AI. Current large language models
(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from
brittle task decomposition, extensive data requirements, and high latency.
Inspired by the hierarchical and multi-timescale processing in the human brain,
we propose the Hierarchical Reasoning Model (HRM), a novel recurrent
architecture that attains significant computational depth while maintaining
both training stability and efficiency. HRM executes sequential reasoning tasks
in a single forward pass without explicit supervision of the intermediate
process, through two interdependent recurrent modules: a high-level module
responsible for slow, abstract planning, and a low-level module handling rapid,
detailed computations. With only 27 million parameters, HRM achieves
exceptional performance on complex reasoning tasks using only 1000 training
samples. The model operates without pre-training or CoT data, yet achieves
nearly perfect performance on challenging tasks including complex Sudoku
puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms
much larger models with significantly longer context windows on the Abstraction
and Reasoning Corpus (ARC), a key benchmark for measuring artificial general
intelligence capabilities. These results underscore HRM's potential as a
transformative advancement toward universal computation and general-purpose
reasoning systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedGNN: Capturing the Links Between Urban Characteristics and Medical
  Prescriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minwei Zhao, Sanja Scepanovic, Stephen Law, Ivica Obadic, Cai Wu, Daniele Quercia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how urban socio-demographic and environmental factors relate
with health is essential for public health and urban planning. However,
traditional statistical methods struggle with nonlinear effects, while machine
learning models often fail to capture geographical (nearby areas being more
similar) and topological (unequal connectivity between places) effects in an
interpretable way. To address this, we propose MedGNN, a spatio-topologically
explicit framework that constructs a 2-hop spatial graph, integrating
positional and locational node embeddings with urban characteristics in a graph
neural network. Applied to MEDSAT, a comprehensive dataset covering over 150
environmental and socio-demographic factors and six prescription outcomes
(depression, anxiety, diabetes, hypertension, asthma, and opioids) across 4,835
Greater London neighborhoods, MedGNN improved predictions by over 25% on
average compared to baseline methods. Using depression prescriptions as a case
study, we analyzed graph embeddings via geographical principal component
analysis, identifying findings that: align with prior research (e.g., higher
antidepressant prescriptions among older and White populations), contribute to
ongoing debates (e.g., greenery linked to higher and NO2 to lower
prescriptions), and warrant further study (e.g., canopy evaporation correlated
with fewer prescriptions). These results demonstrate MedGNN's potential, and
more broadly, of carefully applied machine learning, to advance
transdisciplinary public health research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Hyper-Graph Convolution Network for Skeleton-based Human Action
  Recognition with Virtual Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14796v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14796v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youwei Zhou, Tianyang Xu, Cong Wu, Xiaojun Wu, Josef Kittler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The shared topology of human skeletons motivated the recent investigation of
graph convolutional network (GCN) solutions for action recognition. However,
most of the existing GCNs rely on the binary connection of two neighboring
vertices (joints) formed by an edge (bone), overlooking the potential of
constructing multi-vertex convolution structures. Although some studies have
attempted to utilize hyper-graphs to represent the topology, they rely on a
fixed construction strategy, which limits their adaptivity in uncovering the
intricate latent relationships within the action. In this paper, we address
this oversight and explore the merits of an adaptive hyper-graph convolutional
network (Hyper-GCN) to achieve the aggregation of rich semantic information
conveyed by skeleton vertices. In particular, our Hyper-GCN adaptively
optimises the hyper-graphs during training, revealing the action-driven
multi-vertex relations. Besides, virtual connections are often designed to
support efficient feature aggregation, implicitly extending the spectrum of
dependencies within the skeleton. By injecting virtual connections into
hyper-graphs, the semantic clues of diverse action categories can be
highlighted. The results of experiments conducted on the NTU-60, NTU-120, and
NW-UCLA datasets demonstrate the merits of our Hyper-GCN, compared to the
state-of-the-art methods. The code is available at
https://github.com/6UOOON9/Hyper-GCN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CodeIF: Benchmarking the Instruction-Following Capabilities of Large
  Language Models for Code Generation <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19166v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19166v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiwen Yan, Hongcheng Guo, Xuanqing Shi, Shaosheng Cao, Donglin Di, Zhoujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of Large Language Models (LLMs), the demand for
robust instruction-following capabilities in code generation tasks has grown
significantly. Code generation not only facilitates faster prototyping and
automated testing, but also augments developer efficiency through improved
maintainability and reusability of code. In this paper, we introduce CodeIF,
the first benchmark specifically designed to assess the abilities of LLMs to
adhere to task-oriented instructions within diverse code generation scenarios.
CodeIF encompasses a broad range of tasks, including function synthesis, error
debugging, algorithmic refactoring, and code explanation, thereby providing a
comprehensive suite to evaluate model performance across varying complexity
levels and programming domains. We conduct extensive experiments with LLMs,
analyzing their strengths and limitations in meeting the demands of these
tasks. The experimental results offer valuable insights into how well current
models align with human instructions, as well as the extent to which they can
generate consistent, maintainable, and contextually relevant code. Our findings
not only underscore the critical role that instruction-following LLMs can play
in modern software development, but also illuminate pathways for future
research aimed at enhancing their adaptability, reliability, and overall
effectiveness in automated code generation. CodeIF data and code are publicly
available: https://github.com/lin-rany/codeIF
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as an ACL 2025 Industry Track paper (15 pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shaping Sparse Rewards in Reinforcement Learning: A Semi-supervised
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyun Li, Wenjie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world scenarios, reward signal for agents are exceedingly
sparse, making it challenging to learn an effective reward function for reward
shaping. To address this issue, the proposed approach in this paper performs
reward shaping not only by utilizing non-zero-reward transitions but also by
employing the \emph{Semi-Supervised Learning} (SSL) technique combined with a
novel data augmentation to learn trajectory space representations from the
majority of transitions, {i.e}., zero-reward transitions, thereby improving the
efficacy of reward shaping. Experimental results in Atari and robotic
manipulation demonstrate that our method outperforms supervised-based
approaches in reward inference, leading to higher agent scores. Notably, in
more sparse-reward environments, our method achieves up to twice the peak
scores compared to supervised baselines. The proposed double entropy data
augmentation enhances performance, showcasing a 15.8\% increase in best score
over other augmentation methods
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Angle and Amplitude Encoding Strategies for Variational
  Quantum Machine Learning: their impact on model's accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Tudisco, Andrea Marchesin, Maurizio Zamboni, Mariagrazia Graziano, Giovanna Turvani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Quantum Computing and Machine Learning have increased
attention to Quantum Machine Learning (QML), which aims to develop machine
learning models by exploiting the quantum computing paradigm. One of the widely
used models in this area is the Variational Quantum Circuit (VQC), a hybrid
model where the quantum circuit handles data inference while classical
optimization adjusts the parameters of the circuit. The quantum circuit
consists of an encoding layer, which loads data into the circuit, and a
template circuit, known as the ansatz, responsible for processing the data.
This work involves performing an analysis by considering both Amplitude- and
Angle-encoding models, and examining how the type of rotational gate applied
affects the classification performance of the model. This comparison is carried
out by training the different models on two datasets, Wine and Diabetes, and
evaluating their performance. The study demonstrates that, under identical
model topologies, the difference in accuracy between the best and worst models
ranges from 10% to 30%, with differences reaching up to 41%. Moreover, the
results highlight how the choice of rotational gates used in encoding can
significantly impact the model's classification performance. The findings
confirm that the embedding represents a hyperparameter for VQC models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Step-wise Adaptive Integration of Supervised Fine-tuning and
  Reinforcement Learning for Task-Specific LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13026v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13026v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Chen, Fazhong Liu, Naruto Liu, Yuhan Luo, Erqu Qin, Harry Zheng, Tian Dong, Haojin Zhu, Yan Meng, Xiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel at mathematical reasoning and logical
problem-solving. The current popular training paradigms primarily use
supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the
models' reasoning abilities. However, when using SFT or RL alone, there are
respective challenges: SFT may suffer from overfitting, while RL is prone to
mode collapse. The state-of-the-art methods have proposed hybrid training
schemes. However, static switching faces challenges such as poor generalization
across different tasks and high dependence on data quality. In response to
these challenges, inspired by the curriculum learning-quiz mechanism in human
reasoning cultivation, We propose SASR, a step-wise adaptive hybrid training
framework that theoretically unifies SFT and RL and dynamically balances the
two throughout optimization. SASR uses SFT for initial warm-up to establish
basic reasoning skills, and then uses an adaptive dynamic adjustment algorithm
based on gradient norm and divergence relative to the original distribution to
seamlessly integrate SFT with the online RL method GRPO. By monitoring the
training status of LLMs and adjusting the training process in sequence, SASR
ensures a smooth transition between training schemes, maintaining core
reasoning abilities while exploring different paths. Experimental results
demonstrate that SASR outperforms SFT, RL, and static hybrid training methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WhoFi: Deep Person Re-Identification via Wi-Fi Channel Signal Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.12869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.12869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danilo Avola, Emad Emam, Dario Montagnini, Daniele Pannone, Amedeo Ranaldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person Re-Identification is a key and challenging task in video surveillance.
While traditional methods rely on visual data, issues like poor lighting,
occlusion, and suboptimal angles often hinder performance. To address these
challenges, we introduce WhoFi, a novel pipeline that utilizes Wi-Fi signals
for person re-identification. Biometric features are extracted from Channel
State Information (CSI) and processed through a modular Deep Neural Network
(DNN) featuring a Transformer-based encoder. The network is trained using an
in-batch negative loss function to learn robust and generalizable biometric
signatures. Experiments on the NTU-Fi dataset show that our approach achieves
competitive results compared to state-of-the-art methods, confirming its
effectiveness in identifying individuals via Wi-Fi signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-VQC: A Novel QML Approach for Enhancing Healthcare Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Tudisco, Deborah Volpe, Giovanna Turvani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and reliable diagnosis of diseases is crucial in enabling timely
medical treatment and enhancing patient survival rates. In recent years,
Machine Learning has revolutionized diagnostic practices by creating
classification models capable of identifying diseases. However, these
classification problems often suffer from significant class imbalances, which
can inhibit the effectiveness of traditional models. Therefore, the interest in
Quantum models has arisen, driven by the captivating promise of overcoming the
limitations of the classical counterpart thanks to their ability to express
complex patterns by mapping data in a higher-dimensional computational space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Network-Based Predictor for Optimal Quantum Hardware
  Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.19093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.19093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Tudisco, Deborah Volpe, Giacomo Orlandi, Giovanna Turvani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing variety of quantum hardware technologies, each with unique
peculiarities such as connectivity and native gate sets, creates challenges
when selecting the best platform for executing a specific quantum circuit. This
selection process usually involves a brute-force approach: compiling the
circuit on various devices and evaluating performance based on factors such as
circuit depth and gate fidelity. However, this method is computationally
expensive and does not scale well as the number of available quantum processors
increases. In this work, we propose a Graph Neural Network (GNN)-based
predictor that automates hardware selection by analyzing the Directed Acyclic
Graph (DAG) representation of a quantum circuit. Our study evaluates 498
quantum circuits (up to 27 qubits) from the MQT Bench dataset, compiled using
Qiskit on four devices: three superconducting quantum processors (IBM-Kyiv,
IBM-Brisbane, IBM-Sherbrooke) and one trapped-ion processor (IONQ-Forte).
Performance is estimated using a metric that integrates circuit depth and gate
fidelity, resulting in a dataset where 93 circuits are optimally compiled on
the trapped-ion device, while the remaining circuits prefer superconducting
platforms. By exploiting graph-based machine learning, our approach avoids
extracting the circuit features for the model evaluation but directly embeds it
as a graph, significantly accelerating the optimal target decision-making
process and maintaining all the information. Experimental results prove 94.4%
accuracy and an 85.5% F1 score for the minority class, effectively predicting
the best compilation target. The developed code is publicly available on GitHub
(https://github.com/antotu/GNN-Model-Quantum-Predictor).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ME-IGM: Individual-Global-Max in Maximum Entropy Multi-Agent
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13930v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13930v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Tse Chen, Yuxuan Li, Shiyu Huang, Jiayu Chen, Jeff Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent credit assignment is a fundamental challenge for cooperative
multi-agent reinforcement learning (MARL), where a team of agents learn from
shared reward signals. The Individual-Global-Max (IGM) condition is a widely
used principle for multi-agent credit assignment, requiring that the joint
action determined by individual Q-functions maximizes the global Q-value.
Meanwhile, the principle of maximum entropy has been leveraged to enhance
exploration in MARL. However, we identify a critical limitation in existing
maximum entropy MARL methods: a misalignment arises between local policies and
the joint policy that maximizes the global Q-value, leading to violations of
the IGM condition. To address this misalignment, we propose an order-preserving
transformation. Building on it, we introduce ME-IGM, a novel maximum entropy
MARL algorithm compatible with any credit assignment mechanism that satisfies
the IGM condition while enjoying the benefits of maximum entropy exploration.
We empirically evaluate two variants of ME-IGM: ME-QMIX and ME-QPLEX, in
non-monotonic matrix games, and demonstrate their state-of-the-art performance
across 17 scenarios in SMAC-v2 and Overcooked.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ParetoHqD: Fast Offline Multiobjective Alignment of Large Language
  Models using Pareto High-quality Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.16628v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.16628v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Gu, Handing Wang, Yi Mei, Mengjie Zhang, Yaochu Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models with multiple human expectations and values is
crucial for ensuring that they adequately serve a variety of user needs. To
this end, offline multiobjective alignment algorithms such as the
Rewards-in-Context algorithm have shown strong performance and efficiency.
However, inappropriate preference representations and training with imbalanced
reward scores limit the performance of such algorithms. In this work, we
introduce ParetoHqD that addresses the above issues by representing human
preferences as preference directions in the objective space and regarding data
near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD
follows a two-stage supervised fine-tuning process, where each stage uses an
individual Pareto high-quality training set that best matches its preference
direction. The experimental results have demonstrated the superiority of
ParetoHqD over five baselines on two multiobjective alignment tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ADformer: A Multi-Granularity Spatial-Temporal <span class="highlight-title">Transformer</span> for EEG-Based
  Alzheimer Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00032v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00032v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihe Wang, Nadia Mammone, Darina Petrovsky, Alexandros T. Tzallas, Francesco C. Morabito, Xiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalography (EEG) has emerged as a cost-effective and efficient
tool to support neurologists in the detection of Alzheimer's Disease (AD).
However, most existing approaches rely heavily on manual feature engineering or
data transformation. While such techniques may provide benefits when working
with small-scale datasets, they often lead to information loss and distortion
when applied to large-scale data, ultimately limiting model performance.
Moreover, the limited subject scale and demographic diversity of datasets used
in prior studies hinder comprehensive evaluation of model robustness and
generalizability, thus restricting their applicability in real-world clinical
settings. To address these challenges, we propose ADformer, a novel
multi-granularity spatial-temporal transformer designed to capture both
temporal and spatial features from raw EEG signals, enabling effective
end-to-end representation learning. Our model introduces multi-granularity
embedding strategies across both spatial and temporal dimensions, leveraging a
two-stage intra-inter granularity self-attention mechanism to learn both local
patterns within each granularity and global dependencies across granularities.
We evaluate ADformer on 4 large-scale datasets comprising a total of 1,713
subjects, representing one of the largest corpora for EEG-based AD detection to
date, under a cross-validated, subject-independent setting. Experimental
results demonstrate that ADformer consistently outperforms existing methods,
achieving subject-level F1 scores of 92.82%, 89.83%, 67.99%, and 83.98% on the
4 datasets, respectively, in distinguishing AD from healthy control (HC)
subjects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work will be submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChemMLLM: Chemical Multimodal Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Tan, Dongzhan Zhou, Peng Xia, Wanhao Liu, Wanli Ouyang, Lei Bai, Yuqiang Li, Tianfan Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have made impressive progress in
many applications in recent years. However, chemical MLLMs that can handle
cross-modal understanding and generation remain underexplored. To fill this
gap, we propose ChemMLLM, a unified chemical multimodal large language model
for molecule understanding and generation. Also, we design five multimodal
tasks across text, molecular SMILES strings, and image, and curate the
datasets. We benchmark ChemMLLM against a range of general leading MLLMs and
Chemical LLMs on these tasks. Experimental results show that ChemMLLM achieves
superior performance across all evaluated tasks. For example, in molecule image
optimization task, ChemMLLM outperforms the best baseline (GPT-4o) by 116.75\%
(4.27 vs 1.97 property improvement). The code is publicly available at
https://github.com/bbsbz/ChemMLLM.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14037v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14037v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Franceschelli, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their growing capabilities, language models still frequently
reproduce content from their training data, generate repetitive text, and favor
common grammatical patterns and vocabulary. A possible cause is the decoding
strategy: the most common strategies either consider only the most probable
tokens, which reduces output diversity, or increase the likelihood of unlikely
tokens, compromising output accuracy and correctness. In this paper, we propose
DiffSampling, a new decoding method that leverages a mathematical analysis of
the token probability distribution to ensure the generation of contextually
appropriate text. In particular, the difference between consecutive, sorted
probabilities can be used to truncate incorrect tokens. In addition, we also
propose two variations of the proposed method that aim to correct the subtle
inconsistencies of common sampling strategies. Experiments involving four
different text-generation tasks demonstrate that our approach consistently
performs at least on par with the existing methods it builds upon in terms of
quality, while potentially improving output diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring utilization of generative AI for research and education in
  data-driven materials science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takahiro Misawa, Ai Koizumi, Ryo Tamura, Kazuyoshi Yoshimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI has recently had a profound impact on various fields, including
daily life, research, and education. To explore its efficient utilization in
data-driven materials science, we organized a hackathon -- AIMHack2024 -- in
July 2024. In this hackathon, researchers from fields such as materials
science, information science, bioinformatics, and condensed matter physics
worked together to explore how generative AI can facilitate research and
education. Based on the results of the hackathon, this paper presents topics
related to (1) conducting AI-assisted software trials, (2) building AI tutors
for software, and (3) developing GUI applications for software. While
generative AI continues to evolve rapidly, this paper provides an early record
of its application in data-driven materials science and highlights strategies
for integrating AI into research and education.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Hu, Zijie Xin, Xirong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ad-hoc Video Search (AVS) involves using a textual query to search for
multiple relevant videos in a large collection of unlabeled short videos. The
main challenge of AVS is the visual diversity of relevant videos. A simple
query such as "Find shots of a man and a woman dancing together indoors" can
span a multitude of environments, from brightly lit halls and shadowy bars to
dance scenes in black-and-white animations. It is therefore essential to
retrieve relevant videos as comprehensively as possible. Current solutions for
the AVS task primarily fuse multiple features into one or more common spaces,
yet overlook the need for diverse spaces. To fully exploit the expressive
capability of individual features, we propose LPD, short for Learning Partially
Decorrelated common spaces. LPD incorporates two key innovations:
feature-specific common space construction and the de-correlation loss.
Specifically, LPD learns a separate common space for each video and text
feature, and employs de-correlation loss to diversify the ordering of negative
samples across different spaces. To enhance the consistency of multi-space
convergence, we designed an entropy-based fair multi-space triplet ranking
loss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify
the effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces
highlight its ability to enhance result diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACMMM2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GaussianCross: Cross-modal <span class="highlight-title">Self-supervised</span> 3D Representation Learning
  via Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yao, Yi Wang, Yi Zhang, Moyun Liu, Lap-Pui Chau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The significance of informative and robust point representations has been
widely acknowledged for 3D scene understanding. Despite existing
self-supervised pre-training counterparts demonstrating promising performance,
the model collapse and structural information deficiency remain prevalent due
to insufficient point discrimination difficulty, yielding unreliable
expressions and suboptimal performance. In this paper, we present
GaussianCross, a novel cross-modal self-supervised 3D representation learning
architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques
to address current challenges. GaussianCross seamlessly converts
scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian
representation without missing details, enabling stable and generalizable
pre-training. Subsequently, a tri-attribute adaptive distillation splatting
module is incorporated to construct a 3D feature field, facilitating synergetic
feature capturing of appearance, geometry, and semantic cues to maintain
cross-modal consistency. To validate GaussianCross, we perform extensive
evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In
particular, GaussianCross shows a prominent parameter and data efficiency,
achieving superior performance through linear probing (<0.1% parameters) and
limited data training (1% of scenes) compared to state-of-the-art methods.
Furthermore, GaussianCross demonstrates strong generalization capabilities,
improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on
ScanNet200 semantic and instance segmentation tasks, respectively, supporting
the effectiveness of our approach. The code, weights, and visualizations are
publicly available at
\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, accepted by MM'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On-the-Fly Object-aware Representative Point Selection in Point Cloud 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Zhang, Ziwei Wang, Hai Dong, Zhifeng Bao, Jiajun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point clouds are essential for object modeling and play a critical role in
assisting driving tasks for autonomous vehicles (AVs). However, the significant
volume of data generated by AVs creates challenges for storage, bandwidth, and
processing cost. To tackle these challenges, we propose a representative point
selection framework for point cloud downsampling, which preserves critical
object-related information while effectively filtering out irrelevant
background points. Our method involves two steps: (1) Object Presence
Detection, where we introduce an unsupervised density peak-based classifier and
a supervised Na\"ive Bayes classifier to handle diverse scenarios, and (2)
Sampling Budget Allocation, where we propose a strategy that selects
object-relevant points while maintaining a high retention rate of object
information. Extensive experiments on the KITTI and nuScenes datasets
demonstrate that our method consistently outperforms state-of-the-art baselines
in both efficiency and effectiveness across varying sampling rates. As a
model-agnostic solution, our approach integrates seamlessly with diverse
downstream models, making it a valuable and scalable addition to the 3D point
cloud downsampling toolkit for AV applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ActAlign: Zero-Shot Fine-Grained Video Classification via
  Language-Guided Sequence Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.22967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.22967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Aghdam, Vincent Tao Hu, Björn Ommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the task of zero-shot video classification for extremely
fine-grained actions (e.g., Windmill Dunk in basketball), where no video
examples or temporal annotations are available for unseen classes. While
image-language models (e.g., CLIP, SigLIP) show strong open-set recognition,
they lack temporal modeling needed for video understanding. We propose
ActAlign, a truly zero-shot, training-free method that formulates video
classification as a sequence alignment problem, preserving the generalization
strength of pretrained image-language models. For each class, a large language
model (LLM) generates an ordered sequence of sub-actions, which we align with
video frames using Dynamic Time Warping (DTW) in a shared embedding space.
Without any video-text supervision or fine-tuning, ActAlign achieves 30.5%
accuracy on ActionAtlas--the most diverse benchmark of fine-grained actions
across multiple sports--where human performance is only 61.6%. ActAlign
outperforms billion-parameter video-language models while using 8x fewer
parameters. Our approach is model-agnostic and domain-general, demonstrating
that structured language priors combined with classical alignment methods can
unlock the open-set recognition potential of image-language models for
fine-grained video understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint manuscript - Project page:
  https://amir-aghdam.github.io/act-align/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is It Really You? Exploring Biometric Verification Scenarios in
  Photorealistic Talking-Head Avatar Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Pedrouzo-Rodriguez, Pedro Delgado-DeRobles, Luis F. Gomez, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photorealistic talking-head avatars are becoming increasingly common in
virtual meetings, gaming, and social platforms. These avatars allow for more
immersive communication, but they also introduce serious security risks. One
emerging threat is impersonation: an attacker can steal a user's avatar,
preserving his appearance and voice, making it nearly impossible to detect its
fraudulent usage by sight or sound alone. In this paper, we explore the
challenge of biometric verification in such avatar-mediated scenarios. Our main
question is whether an individual's facial motion patterns can serve as
reliable behavioral biometrics to verify their identity when the avatar's
visual appearance is a facsimile of its owner. To answer this question, we
introduce a new dataset of realistic avatar videos created using a
state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and
impostor avatar videos. We also propose a lightweight, explainable
spatio-temporal Graph Convolutional Network architecture with temporal
attention pooling, that uses only facial landmarks to model dynamic facial
gestures. Experimental results demonstrate that facial motion cues enable
meaningful identity verification with AUC values approaching 80%. The proposed
benchmark and biometric system are available for the research community in
order to bring attention to the urgent need for more advanced behavioral
biometric defenses in avatar-based communication systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the IEEE International Joint Conference on Biometrics
  (IJCB 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Listening to the Unspoken: Exploring "365" Aspects of Multimodal
  Interview Performance Assessment <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22676v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22676v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Li, Yang Wang, Wenhao Qian, Zhenzhen Hu, Richang Hong, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interview performance assessment is essential for determining candidates'
suitability for professional positions. To ensure holistic and fair
evaluations, we propose a novel and comprehensive framework that explores
``365'' aspects of interview performance by integrating \textit{three}
modalities (video, audio, and text), \textit{six} responses per candidate, and
\textit{five} key evaluation dimensions. The framework employs
modality-specific feature extractors to encode heterogeneous data streams and
subsequently fused via a Shared Compression Multilayer Perceptron. This module
compresses multimodal embeddings into a unified latent space, facilitating
efficient feature interaction. To enhance prediction robustness, we incorporate
a two-level ensemble learning strategy: (1) independent regression heads
predict scores for each response, and (2) predictions are aggregated across
responses using a mean-pooling mechanism to produce final scores for the five
target dimensions. By listening to the unspoken, our approach captures both
explicit and implicit cues from multimodal data, enabling comprehensive and
unbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our
framework secured first place in the AVI Challenge 2025, demonstrating its
effectiveness and robustness in advancing automated and multimodal interview
performance assessment. The full implementation is available at
https://github.com/MSA-LMC/365Aspects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, ACM MM 2025.
  github:https://github.com/MSA-LMC/365Aspects</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Sub-Genre Classification For Mainstage Dance Music <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06690v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06690v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhi Shu, Xinglin Li, Hongyu Jiang, Minghao Fu, Xinyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music classification, a cornerstone of music information retrieval, supports
a wide array of applications. To address the lack of comprehensive datasets and
effective methods for sub-genre classification in mainstage dance music, we
introduce a novel benchmark featuring a new dataset and baseline. Our dataset
expands the scope of sub-genres to reflect the diversity of recent mainstage
live sets performed by leading DJs at global music festivals, capturing the
vibrant and rapidly evolving electronic dance music (EDM) scene that engages
millions of fans worldwide. We employ a continuous soft labeling approach to
accommodate tracks blending multiple sub-genres, preserving their inherent
complexity. Experiments demonstrate that even state-of-the-art multimodal large
language models (MLLMs) struggle with this task, while our specialized baseline
models achieve high accuracy. This benchmark supports applications such as
music recommendation, DJ set curation, and interactive multimedia systems, with
video demos provided. Our code and data are all open-sourced at
https://github.com/Gariscat/housex-v2.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WASPAA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D-Judge: How Far Are We? Evaluating the Discrepancies Between
  AI-synthesized Images and Natural Images through Multimodal Guidance <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17632v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17632v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renyang Liu, Ziyu Lyu, Wei Zhou, See-Kiong Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving field of Artificial Intelligence Generated Content
(AIGC), a central challenge is distinguishing AI-synthesized images from
natural images. Despite the impressive capabilities of advanced AI generative
models in producing visually compelling content, significant discrepancies
remain when compared to natural images. To systematically investigate and
quantify these differences, we construct a large-scale multimodal dataset named
DANI, comprising 5,000 natural images and over 440,000 AI-generated image
(AIGI) samples produced by nine representative models using both unimodal and
multimodal prompts, including Text-to-Image (T2I), Image-to-Image (I2I), and
Text and Image-to-Image (TI2I). We then introduce D-Judge, a benchmark designed
to answer the critical question: how far are AI-generated images from truly
realistic images? Our fine-grained evaluation framework assesses DANI across
five key dimensions: naive visual quality, semantic alignment, aesthetic
appeal, downstream task applicability, and coordinated human validation.
Extensive experiments reveal substantial discrepancies across these dimensions,
highlighting the importance of aligning quantitative metrics with human
judgment to achieve a comprehensive understanding of AI-generated image
quality. The code and dataset are publicly available at:
https://github.com/ryliu68/DJudge and
https://huggingface.co/datasets/Renyang/DANI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiashuo Yu, Yue Wu, Meng Chu, Zhifei Ren, Zizheng Huang, Pei Chu, Ruijie Zhang, Yinan He, Qirui Li, Songze Li, Zhenxiang Li, Zhongying Tu, Conghui He, Yu Qiao, Yali Wang, Yi Wang, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present VRBench, the first long narrative video benchmark crafted for
evaluating large models' multi-step reasoning capabilities, addressing
limitations in existing evaluations that overlook temporal reasoning and
procedural validity. It comprises 960 long videos (with an average duration of
1.6 hours), along with 8,243 human-labeled multi-step question-answering pairs
and 25,106 reasoning steps with timestamps. These videos are curated via a
multi-stage filtering process including expert inter-rater reviewing to
prioritize plot coherence. We develop a human-AI collaborative framework that
generates coherent reasoning chains, each requiring multiple temporally
grounded steps, spanning seven types (e.g., event attribution, implicit
inference). VRBench designs a multi-phase evaluation pipeline that assesses
models at both the outcome and process levels. Apart from the MCQs for the
final results, we propose a progress-level LLM-guided scoring metric to
evaluate the quality of the reasoning chain from multiple dimensions
comprehensively. Through extensive evaluations of 12 LLMs and 19 VLMs on
VRBench, we undertake a thorough analysis and provide valuable insights that
advance the field of multi-step reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AudioGen-Omni: A Unified Multimodal Diffusion <span class="highlight-title">Transformer</span> for
  Video-Synchronized Audio, Speech, and Song Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00733v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00733v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Wang, Jun Wang, Feng Deng, Chen Zhang, Di Zhang, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present AudioGen-Omni - a unified approach based on multimodal diffusion
transformers (MMDit), capable of generating high-fidelity audio, speech, and
songs coherently synchronized with the input video. AudioGen-Omni introduces a
novel joint training paradigm that seamlessly integrates large-scale
video-text-audio corpora, enabling a model capable of generating semantically
rich, acoustically diverse audio conditioned on multimodal inputs and adaptable
to a wide range of audio generation tasks. AudioGen-Omni employs a unified
lyrics-transcription encoder that encodes graphemes and phonemes from both sung
and spoken inputs into dense frame-level representations. Dense frame-level
representations are fused using an AdaLN-based joint attention mechanism
enhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein
RoPE is selectively applied to temporally structured modalities to ensure
precise and robust cross-modal alignment. By unfreezing all modalities and
masking missing inputs, AudioGen-Omni mitigates the semantic constraints of
text-frozen paradigms, enabling effective cross-modal conditioning. This joint
training approach enhances audio quality, semantic alignment, and lip-sync
accuracy, while also achieving state-of-the-art results on
Text-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8
seconds of audio, it offers substantial improvements in both efficiency and
generality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-08-03T00:00:00Z">2025-08-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">35</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic
  Association and Long Story Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Wu, Jiangnan Li, Yuqing Li, Lemao Liu, Liyan Xu, Jiwei Li, Dit-Yan Yeung, Jie Zhou, Mo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) over long documents typically involves
splitting the text into smaller chunks, which serve as the basic units for
retrieval. However, due to dependencies across the original document,
contextual information is often essential for accurately interpreting each
chunk. To address this, prior work has explored encoding longer context windows
to produce embeddings for longer chunks. Despite these efforts, gains in
retrieval and downstream tasks remain limited. This is because (1) longer
chunks strain the capacity of embedding models due to the increased amount of
information they must encode, and (2) many real-world applications still
require returning localized evidence due to constraints on model or human
bandwidth.
  We propose an alternative approach to this challenge by representing short
chunks in a way that is conditioned on a broader context window to enhance
retrieval performance -- i.e., situating a chunk's meaning within its context.
We further show that existing embedding models are not well-equipped to encode
such situated context effectively, and thus introduce a new training paradigm
and develop the situated embedding models (SitEmb). To evaluate our method, we
curate a book-plot retrieval dataset specifically designed to assess situated
retrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3
substantially outperforms state-of-the-art embedding models, including several
with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model
further improves performance by over 10% and shows strong results across
different languages and several downstream applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our trained models can be downloaded from:
  https://huggingface.co/SituatedEmbedding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROVER: Recursive Reasoning Over Videos with Vision-Language Models for
  Embodied Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Schroeder, Ondrej Biza, Thomas Weng, Hongyin Luo, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have exhibited impressive capabilities across
diverse image understanding tasks, but still struggle in settings that require
reasoning over extended sequences of camera frames from a video. This limits
their utility in embodied settings, which require reasoning over long frame
sequences from a continuous stream of visual input at each moment of a task
attempt. To address this limitation, we propose ROVER (Reasoning Over VidEo
Recursively), a framework that enables the model to recursively decompose
long-horizon video trajectories into segments corresponding to shorter subtasks
within the trajectory. In doing so, ROVER facilitates more focused and accurate
reasoning over temporally localized frame sequences without losing global
context. We evaluate ROVER, implemented using an in-context learning approach,
on diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa
that consists of 543 videos showing both expert and perturbed non-expert
trajectories across 27 robotic manipulation tasks. ROVER outperforms strong
baselines across three video reasoning tasks: task progress estimation,
frame-level natural language reasoning, and video question answering. We
observe that, by reducing the number of frames the model reasons over at each
timestep, ROVER mitigates hallucinations, especially during unexpected or
non-optimal moments of a trajectory. In addition, by enabling the
implementation of a subtask-specific sliding context window, ROVER's time
complexity scales linearly with video length, an asymptotic improvement over
baselines. Demos, code, and data available at: https://rover-vlm.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Word Overuse and Alignment in Large Language Models: The Influence of
  Learning from Human Feedback <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom S. Juzek, Zina B. Ward
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are known to overuse certain terms like "delve"
and "intricate." The exact reasons for these lexical choices, however, have
been unclear. Using Meta's Llama model, this study investigates the
contribution of Learning from Human Feedback (LHF), under which we subsume
Reinforcement Learning from Human Feedback and Direct Preference Optimization.
We present a straightforward procedure for detecting the lexical preferences of
LLMs that are potentially LHF-induced. Next, we more conclusively link LHF to
lexical overuse by experimentally emulating the LHF procedure and demonstrating
that participants systematically prefer text variants that include certain
words. This lexical overuse can be seen as a sort of misalignment, though our
study highlights the potential divergence between the lexical expectations of
different populations -- namely LHF workers versus LLM users. Our work
contributes to the growing body of research on explainable artificial
intelligence and emphasizes the importance of both data and procedural
transparency in alignment research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the Proceedings of the 5th Workshop on
  Bias and Fairness in AI (BIAS 2025) at ECML PKDD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum-RAG and Pun<span class="highlight-title">GPT</span>2: Advancing Low-Resource Language Generation and
  Retrieval for the Punjabi Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaskaranjeet Singh, Rakesh Thakur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rapid advancement of large language models (LLMs), low-resource
languages remain largely excluded from the NLP landscape. We present PunGPT2,
the first fully open-source suite of Punjabi large language models, trained
from scratch on a 35GB domain-diverse corpus encompassing literature, religious
texts, news, and social discourse. Unlike prior multilingual approaches,
PunGPT2 captures rich syntactic and morphological features unique to Punjabi
through a tokenizer optimised with byte pair encoding and linguistically
aligned pretraining objectives. To improve factual grounding and domain recall,
we introduce Pun-RAG, a retrieval-augmented generation framework combining
PunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We
further develop Pun-Instruct, a parameter-efficient, instruction-tuned variant
using QLoRA, enabling robust zero-shot and instruction-following performance
with significantly reduced compute needs.
  As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system
that fuses sparse (BM25) and dense methods with quantum-inspired semantic
matching. By encoding queries using amplitude-based embeddings and retrieving
via quantum kernel similarity, Quantum-RAG achieves improved contextual
relevance with minimal memory overhead marking the first practical integration
of quantum representations in low-resource language generation. Our models
significantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in
perplexity, factuality, and fluency. This work provides a scalable,
reproducible blueprint for extending LLM capabilities to underrepresented
languages and pioneers quantum-aware retrieval in low-resource NLP
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decomposing Representation Space into Interpretable Subspaces with
  Unsupervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinting Huang, Michael Hahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding internal representations of neural models is a core interest of
mechanistic interpretability. Due to its large dimensionality, the
representation space can encode various aspects about inputs. To what extent
are different aspects organized and encoded in separate subspaces? Is it
possible to find these ``natural'' subspaces in a purely unsupervised way?
Somewhat surprisingly, we can indeed achieve this and find interpretable
subspaces by a seemingly unrelated training objective. Our method, neighbor
distance minimization (NDM), learns non-basis-aligned subspaces in an
unsupervised manner. Qualitative analysis shows subspaces are interpretable in
many cases, and encoded information in obtained subspaces tends to share the
same abstract concept across different inputs, making such subspaces similar to
``variables'' used by the model. We also conduct quantitative experiments using
known circuits in GPT-2; results show a strong connection between subspaces and
circuit variables. We also provide evidence showing scalability to 2B models by
finding separate subspaces mediating context and parametric knowledge routing.
Viewed more broadly, our findings offer a new perspective on understanding
model internals and building circuits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Decentralized Framework for Ethical Authorship Validation in Academic
  Publishing: Leveraging Self-Sovereign Identity and Blockchain Technology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamal Al-Sabahi, Yousuf Khamis Al Mabsali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Academic publishing, integral to knowledge dissemination and scientific
advancement, increasingly faces threats from unethical practices such as
unconsented authorship, gift authorship, author ambiguity, and undisclosed
conflicts of interest. While existing infrastructures like ORCID effectively
disambiguate researcher identities, they fall short in enforcing explicit
authorship consent, accurately verifying contributor roles, and robustly
detecting conflicts of interest during peer review. To address these
shortcomings, this paper introduces a decentralized framework leveraging
Self-Sovereign Identity (SSI) and blockchain technology. The proposed model
uses Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) to
securely verify author identities and contributions, reducing ambiguity and
ensuring accurate attribution. A blockchain-based trust registry records
authorship consent and peer-review activity immutably. Privacy-preserving
cryptographic techniques, especially Zero-Knowledge Proofs (ZKPs), support
conflict-of-interest detection without revealing sensitive data. Verified
authorship metadata and consent records are embedded in publications,
increasing transparency. A stakeholder survey of researchers, editors, and
reviewers suggests the framework improves ethical compliance and confidence in
scholarly communication. This work represents a step toward a more transparent,
accountable, and trustworthy academic publishing ecosystem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Replay and Gradient Alignment for Continual <span class="highlight-title">Pre-Train</span>ing of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Istabrak Abbes, Gopeshh Subbaraj, Matthew Riemer, Nizar Islah, Benjamin Therien, Tsuguchika Tabaru, Hiroaki Kingetsu, Sarath Chandar, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training large language models (LLMs) typically involves pre-training on
massive corpora, only to restart the process entirely when new data becomes
available. A more efficient and resource-conserving approach would be continual
pre-training, where models are updated with new data rather than retraining
from scratch. However, the introduction of new data often causes distribution
shifts, leading to performance degradation on previously learned tasks. In this
paper, we take a deeper look at two popular proposals for addressing this
distribution shift within the continual learning literature: experience replay
and gradient alignment. We consider continual pre-training of models within the
Llama family of architectures at a large scale across languages with 100
billion tokens of training data in each language, finding that both replay and
gradient alignment lead to more stable learning without forgetting. This
conclusion holds both as we vary the model scale and as we vary the number and
diversity of tasks. Moreover, we are the first to demonstrate the effectiveness
of gradient alignment techniques in the context of LLM pre-training and propose
an efficient implementation of meta-experience replay (MER) that imbues
experience replay with the benefits of gradient alignment despite negligible
compute and memory overhead. Our scaling analysis across model sizes and replay
rates indicates that small rates of replaying old examples are definitely a
more valuable use of compute than investing in model size, but that it is more
compute efficient to scale the size of the model than invest in high rates of
replaying old examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aldan Creo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-generated text detectors have become essential tools for maintaining
content authenticity, yet their robustness against evasion attacks remains
questionable. We present PDFuzz, a novel attack that exploits the discrepancy
between visual text layout and extraction order in PDF documents. Our method
preserves exact textual content while manipulating character positioning to
scramble extraction sequences. We evaluate this approach against the ArguGPT
detector using a dataset of human and AI-generated text. Our results
demonstrate complete evasion: detector performance drops from (93.6 $\pm$ 1.4)
% accuracy and 0.938 $\pm$ 0.014 F1 score to random-level performance ((50.4
$\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity.
Our work reveals a vulnerability in current detection systems that is inherent
to PDF document structures and underscores the need for implementing sturdy
safeguards against such attacks. We make our code publicly available at
https://github.com/ACMCMC/PDFuzz.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/ACMCMC/PDFuzz</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Probing for Hallucination Detection and Mitigation in
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijun Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have demonstrated remarkable capabilities across
diverse tasks, yet they frequently generate hallucinations outputs that are
fluent but factually incorrect or unsupported. We propose Counterfactual
Probing, a novel approach for detecting and mitigating hallucinations in LLM
outputs. Our method dynamically generates counterfactual statements that appear
plausible but contain subtle factual errors, then evaluates the model's
sensitivity to these perturbations. We hypothesize that genuine knowledge
exhibits robustness to counterfactual variations, while hallucinated content
shows inconsistent confidence patterns when confronted with plausible
alternatives. Our comprehensive evaluation on TruthfulQA, factual statement
datasets, and curated hallucination examples demonstrates that counterfactual
probing achieves superior detection performance compared to baseline methods,
while our adaptive mitigation strategies reduce hallucination scores by an
average of 24.5%. The approach requires no model retraining and can be
integrated into existing LLM pipelines as a realtime verification mechanism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Guo, Cong Guo, Aiwen Sun, Hongliang He, Xinyu Yang, Yue Lu, Yingji Zhang, Xuntao Guo, Dong Zhang, Jianzhuang Liu, Jiang Duan, Yijia Xiao, Liangjian Wen, Hai-Ming Xu, Yong Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large-scale models have significantly advanced the development of
web agents, enabling perception and interaction with digital environments akin
to human cognition. In this paper, we argue that web agents must first acquire
sufficient knowledge to effectively engage in cognitive reasoning. Therefore,
we decompose a web agent's capabilities into two essential stages: knowledge
content learning and cognitive processes. To formalize this, we propose
Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and
Procedural. In this framework, knowledge content learning corresponds to the
agent's processes of Memorizing and Understanding, which rely on the first two
knowledge types, representing the "what" of learning. Conversely, cognitive
processes correspond to Exploring, grounded in Procedural knowledge, defining
the "how" of reasoning and action. To facilitate knowledge acquisition, we
construct the Web-CogDataset, a structured resource curated from 14 real-world
websites, designed to systematically instill core knowledge necessary for web
agent. This dataset serves as the agent's conceptual grounding-the "nouns" upon
which comprehension is built-as well as the basis for learning how to reason
and act. Building on this foundation, we operationalize these processes through
a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing
and training our proposed agent, the Web-CogReasoner. Extensive experimentation
reveals its significant superiority over existing models, especially in
generalizing to unseen tasks where structured knowledge is decisive. To enable
rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation
suite designed to assess and compare agent performance across the delineated
knowledge domains and cognitive capabilities. Our code and data is open sourced
at https://github.com/Gnonymous/Web-CogReasoner
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code and data is open sourced at
  https://github.com/Gnonymous/Web-CogReasoner</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLP Memory: Language Modeling with Retriever-<span class="highlight-title">pretrain</span>ed External Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rubin Wei, Jiaqi Cao, Jiarui Wang, Jushi Kai, Qipeng Guo, Bowen Zhou, Zhouhan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While modern decoder-only LLMs achieve superior performance across various
domains, hallucinations have risen to be a common problem in their generated
text, hindering their application in knowledge-intensive tasks.
Retriever-augmented generation (RAG) offers a solution, but the non-parametric
nature of the retriever hinders its deep interaction with LLM. In this work, we
propose to decouple memorization from the LLM decoder using a pretrained,
differentiable external memory. The external memory is an MLP pretrained by
imitating the behavior of a retriever on the entire pretraining dataset. Our
resulting architecture, which comprises a transformer decoder and an external
MLP memory pretrained on language modeling and retriever imitation
respectively, demonstrates strong perplexity and performance on downstream
tasks. Experiments show our architecture exhibits steeper power-law scaling
with model size, achieving 17.5% and 24.1% improvement on WikiText-103 and Web
datasets compared to decoder-only models while benefiting from added training
without overfitting. We demonstrate superior performance on three hallucination
benchmarks and nine memory-intensive tasks. Additionally, our approach delivers
$80\times$ speedup over $k$NN-LM (500M tokens) and $1.3\times$ faster inference
than decoder-only models. Unlike $k$NN-LM, which impairs reasoning, our MLP
memory improves StrategyQA performance. We will open-source our code and models
in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning
  over Heterogeneous Knowledge Graphs for the Circular Economy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhao, Chengxiao Dai, Wei Zhuo, Tan Chuan Fu, Yue Xiu, Dusit Niyato, Jonathan Z. Low, Eugene Ho Hong Zhuang, Daren Zong Loong Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering over heterogeneous knowledge graphs (KGQA) involves
reasoning across diverse schemas, incomplete alignments, and distributed data
sources. Existing text-to-SPARQL approaches rely on large-scale domain-specific
fine-tuning or operate within single-graph settings, limiting their
generalizability in low-resource domains and their ability to handle queries
spanning multiple graphs. These challenges are particularly relevant in domains
such as the circular economy, where information about classifications,
processes, and emissions is distributed across independently curated knowledge
graphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes
KGQA into subtasks managed by specialized agents responsible for retrieval,
query generation, and verification. A scheduler assigns subgoals to different
graphs using weak-to-strong alignment strategies. A two-stage verifier detects
structurally invalid and semantically underspecified queries through symbolic
validation and counterfactual consistency checks. Experiments on real-world
circular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy
by 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing
the average prompt length by 46.4%. These results demonstrate the benefits of
agent-based schema-aware reasoning for scalable KGQA and support
decision-making in sustainability domains through robust cross-graph reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir DN Cohen, Hilla Merhav, Yoav Goldberg, Reut Tsarfaty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly
on morpho-syntactic tasks, neglecting the semantic dimension of language
understanding. To bridge this gap, we set out to deliver a Hebrew Machine
Reading Comprehension (MRC) dataset, where MRC is to be realized as extractive
Question Answering. The morphologically rich nature of Hebrew poses a challenge
to this endeavor: the indeterminacy and non-transparency of span boundaries in
morphologically complex forms lead to annotation inconsistencies,
disagreements, and flaws in standard evaluation metrics.
  To remedy this, we devise a novel set of guidelines, a controlled
crowdsourcing protocol, and revised evaluation metrics that are suitable for
the morphologically rich nature of the language. Our resulting benchmark, HeQ
(Hebrew QA), features 30,147 diverse question-answer pairs derived from both
Hebrew Wikipedia articles and Israeli tech news. Our empirical investigation
reveals that standard evaluation metrics such as F1 scores and Exact Match (EM)
are not appropriate for Hebrew (and other MRLs), and we propose a relevant
enhancement.
  In addition, our experiments show low correlation between models' performance
on morpho-syntactic tasks and on MRC, which suggests that models designed for
the former might underperform on semantics-heavy tasks. The development and
exploration of HeQ illustrate some of the challenges MRLs pose in natural
language understanding (NLU), fostering progression towards more and better NLU
models for Hebrew and other MRLs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic
  Sign Language Recognition on the Isharah Datase 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatimah Mohamed Emad Elden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of Continuous Sign Language Recognition (CSLR) poses substantial
technical challenges, including fluid inter-sign transitions, the absence of
temporal boundaries, and co-articulation effects. This paper, developed for the
MSLR 2025 Workshop Challenge at ICCV 2025, addresses the critical challenge of
signer-independent recognition to advance the generalization capabilities of
CSLR systems across diverse signers. A data-centric methodology is proposed,
centered on systematic feature engineering, a robust preprocessing pipeline,
and an optimized model architecture. Key contributions include a principled
feature selection process guided by Exploratory Data Analysis (EDA) to isolate
communicative keypoints, a rigorous preprocessing pipeline incorporating
DBSCAN-based outlier filtering and spatial normalization, and the novel
CSLRConformer architecture. This architecture adapts the hybrid CNN-Transformer
design of the Conformer model, leveraging its capacity to model local temporal
dependencies and global sequence context; a characteristic uniquely suited for
the spatio-temporal dynamics of sign language. The proposed methodology
achieved a competitive performance, with a Word Error Rate (WER) of 5.60% on
the development set and 12.01% on the test set, a result that secured a 3rd
place ranking on the official competition platform. This research validates the
efficacy of cross-domain architectural adaptation, demonstrating that the
Conformer model, originally conceived for speech recognition, can be
successfully repurposed to establish a new state-of-the-art performance in
keypoint-based CSLR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comprehensive taxonomy of hallucinations in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Cossio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized natural language processing,
yet their propensity for hallucination, generating plausible but factually
incorrect or fabricated content, remains a critical challenge. This report
provides a comprehensive taxonomy of LLM hallucinations, beginning with a
formal definition and a theoretical framework that posits its inherent
inevitability in computable LLMs, irrespective of architecture or training. It
explores core distinctions, differentiating between intrinsic (contradicting
input context) and extrinsic (inconsistent with training data or reality), as
well as factuality (absolute correctness) and faithfulness (adherence to
input). The report then details specific manifestations, including factual
errors, contextual and logical inconsistencies, temporal disorientation,
ethical violations, and task-specific hallucinations across domains like code
generation and multimodal applications. It analyzes the underlying causes,
categorizing them into data-related issues, model-related factors, and
prompt-related influences. Furthermore, the report examines cognitive and human
factors influencing hallucination perception, surveys evaluation benchmarks and
metrics for detection, and outlines architectural and systemic mitigation
strategies. Finally, it introduces web-based resources for monitoring LLM
releases and performance. This report underscores the complex, multifaceted
nature of LLM hallucinations and emphasizes that, given their theoretical
inevitability, future efforts must focus on robust detection, mitigation, and
continuous human oversight for responsible and reliable deployment in critical
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 16 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guozhao Mo, Wenliang Zhong, Jiawei Chen, Xuanang Chen, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of Model Context Protocol (MCP), the number of MCP
servers has surpassed 10,000. However, existing MCP benchmarks are limited to
single-server settings with only a few tools, hindering effective evaluation of
agent capabilities in large-scale, real-world scenarios. To address this
limitation, we present LiveMCPBench, the first comprehensive benchmark
comprising 95 real-world tasks grounded in the MCP ecosystem, designed to
evaluate LLM agents at scale across diverse servers. To support a scalable and
reproducible evaluation pipeline in large-scale MCP environments, we curate
LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and
527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework
that enables automated and adaptive evaluation in dynamic, time-varying task
environments, achieving 81% agreement with human reviewers. Finally, we propose
the MCP Copilot Agent, a multi-step agent that routes tools for dynamic
planning and executes tools for API interaction across the entire LiveMCPTool
suite. Our evaluation covers 10 leading models, with the best-performing model
(Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large
performance variance across models, and several widely-used models perform
poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench
offers the first unified framework for benchmarking LLM agents in realistic,
tool-rich, and dynamic MCP environments, laying a solid foundation for scalable
and reproducible research on agent capabilities. Our code and data will be
publicly available at https://icip-cas.github.io/LiveMCPBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code and data will be publicly available at
  https://icip-cas.github.io/LiveMCPBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Based Methods for Automated Process Reward Data Construction
  and Output Aggregation in Mathematical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuzhou Han, Wray Buntine, Ehsan Shareghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated remarkable capabilities in complex
mathematical reasoning tasks, but they inevitably generate errors throughout
multi-step solutions. Process-level Reward Models (PRMs) have shown great
promise by providing supervision and evaluation at each intermediate step,
thereby effectively improving the models' reasoning abilities. However,
training effective PRMs requires high-quality process reward data, yet existing
methods for constructing such data are often labour-intensive or inefficient.
In this paper, we propose an uncertainty-driven framework for automated process
reward data construction, encompassing both data generation and annotation
processes for PRMs. Additionally, we identify the limitations of both majority
vote and PRMs, and introduce two generic uncertainty-aware output aggregation
methods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which
combine the strengths of majority vote with PRMs. Extensive experiments on
ProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the
proposed PRM data construction framework, and demonstrate that the two output
aggregation methods further improve the mathematical reasoning abilities across
diverse PRMs. The code and data will be publicly available at
https://github.com/Jiuzhouh/UnPRM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Generated Text is Non-Stationary: Detection via Temporal Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alva West, Yixuan Weng, Minjun Zhu, Luodan Zhang, Zhen Lin, Guangsheng Bao, Yue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of AI-generated text detection has evolved from supervised
classification to zero-shot statistical analysis. However, current approaches
share a fundamental limitation: they aggregate token-level measurements into
scalar scores, discarding positional information about where anomalies occur.
Our empirical analysis reveals that AI-generated text exhibits significant
non-stationarity, statistical properties vary by 73.8\% more between text
segments compared to human writing. This discovery explains why existing
detectors fail against localized adversarial perturbations that exploit this
overlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT),
a novel detection paradigm that preserves positional information by
reformulating detection as a signal processing task. TDT treats token-level
discrepancies as a time-series signal and applies Continuous Wavelet Transform
to generate a two-dimensional time-scale representation, capturing both the
location and linguistic scale of statistical anomalies. On the RAID benchmark,
TDT achieves 0.855 AUROC (7.1\% improvement over the best baseline). More
importantly, TDT demonstrates robust performance on adversarial tasks, with
14.1\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its
sophisticated analysis, TDT maintains practical efficiency with only 13\%
computational overhead. Our work establishes non-stationarity as a fundamental
characteristic of AI-generated text and demonstrates that preserving temporal
dynamics is essential for robust detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing the Preference Extractor in Multi-turn Dialogues: From
  Annotating Disasters to Accurate Preference Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Wang, ziru Liu, Pengcheng Tang, Mingyu Zhang, Quanyu Dai, Yue Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying user preferences in dialogue systems is a pivotal aspect of
providing satisfying services. Current research shows that using large language
models (LLMs) to fine-tune a task-specific preference extractor yields
excellent results in terms of accuracy and generalization. However, the primary
challenge stems from the inherent difficulty in obtaining high-quality labeled
multi-turn dialogue data. Accurately tracking user preference transitions
across turns not only demands intensive domain expertise and contextual
consistency maintenance for annotators (termed \textbf{``Annotating
Disaster''}) but also complicates model training due to error propagation in
sequential dependency learning. Inspired by the observation that multi-turn
preference extraction can be decomposed into iterative executions of one-turn
extraction processes. We propose a novel dialogue data generation framework
named \textbf{IterChat}. First, we construct a new data format that categorizes
the dialogue data into attributed historical preferences and one-turn
dialogues. This reduces the probability of annotation errors and improves
annotation efficiency. Then, to generate a high-quality and diverse dialogue
dataset, we adopt GPT4 to pre-define the preference slots in the target
preference extractor task and then randomly sample the subset of the slots and
their corresponding schema values to create the dialogue datasets. Experimental
results indicate that fine-tuning or only few-shot prompting with the new
dialogue format yields superior performance compared to the original multi-turn
dialogues. Additionally, the new data format improves annotator efficiency with
a win rate of 28.4\% higher than the original multi-turn dialogues.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CultureGuard: Towards Culturally-Aware <span class="highlight-title">Dataset</span> and Guard Model for
  Multilingual Safety Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raviraj Joshi, Rakesh Paul, Kanishk Singla, Anusha Kamath, Michael Evans, Katherine Luna, Shaona Ghosh, Utkarsh Vaidya, Eileen Long, Sanjay Singh Chauhan, Niranjan Wartikar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing use of Large Language Models (LLMs) in agentic applications
highlights the need for robust safety guard models. While content safety in
English is well-studied, non-English languages lack similar advancements due to
the high cost of collecting culturally aligned labeled datasets. We present
CultureGuard, a novel solution for curating culturally aligned, high-quality
safety datasets across multiple languages. Our approach introduces a four-stage
synthetic data generation and filtering pipeline: cultural data segregation,
cultural data adaptation, machine translation, and quality filtering. This
pipeline enables the conversion and expansion of the
Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct
languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.
The resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,
comprises 386,661 samples in 9 languages and facilitates the training of
Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.
The final model achieves state-of-the-art performance on several multilingual
content safety benchmarks. We also benchmark the latest open LLMs on
multilingual safety and observe that these LLMs are more prone to give unsafe
responses when prompted in non-English languages. This work represents a
significant step toward closing the safety gap in multilingual LLMs by enabling
the development of culturally aware safety guard models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large
  Language Models as a Symptom of Irrelevancy Disruption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berkay Köprü, Mehrzad Mashal, Yigit Gurses, Akos Kadar, Maximilian Schmitt, Ditty Mathew, Felix Burkhardt, Florian Eyben, Björn W. Schuller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have advanced natural language processing (NLP)
skills such as through next-token prediction and self-attention, but their
ability to integrate broad context also makes them prone to incorporating
irrelevant information. Prior work has focused on semantic leakage, bias
introduced by semantically irrelevant context. In this paper, we introduce
expression leakage, a novel phenomenon where LLMs systematically generate
sentimentally charged expressions that are semantically unrelated to the input
context. To analyse the expression leakage, we collect a benchmark dataset
along with a scheme to automatically generate a dataset from free-form text
from common-crawl. In addition, we propose an automatic evaluation pipeline
that correlates well with human judgment, which accelerates the benchmarking by
decoupling from the need of annotation for each analysed model. Our experiments
show that, as the model scales in the parameter space, the expression leakage
reduces within the same LLM family. On the other hand, we demonstrate that
expression leakage mitigation requires specific care during the model building
process, and cannot be mitigated by prompting. In addition, our experiments
indicate that, when negative sentiment is injected in the prompt, it disrupts
the generation process more than the positive sentiment, causing a higher
expression leakage rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ THREAD: Thinking Deeper with Recursive Spawning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17402v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17402v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Schroeder, Nathaniel Morgan, Hongyin Luo, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown impressive capabilities across
diverse settings, but still struggle as the length and complexity of the
context increases. To address this challenge, we propose Thinking Recursively
and Dynamically (ThReaD). THREAD frames model generation as a thread of
execution that, based on the context, can run to completion or dynamically
spawn new threads. By spawning, threads can offload work (e.g., thinking,
retrieving information) to child threads, which only return tokens needed for
the parent thread to do its work. In effect, this enables the model to adapt,
as needed, the amount of intermediate work used to produce tokens. We apply
THREAD in the settings of LLM task solving and question answering, where the
dynamic threading allows the model to recursively decompose the given task or
question into progressively simpler sub-problems that can be solved by separate
child threads. We test THREAD, implemented using a few-shot learning approach,
on diverse benchmarks for agent tasks and data-grounded question answering.
THREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these
benchmarks, including ALFWorld, TextCraft, and WebShop, along with two new
benchmarks, DataCommons QA and MIMIC-III ICU QA. In addition, THREAD
outperforms existing frameworks by 10% to 50% absolute points with smaller
models, including Llama-3-8b and CodeLlama-7b.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cascade Reward Sampling for Efficient Decoding-Time Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16306v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16306v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolian Li, Yifan Wang, Anamika Lochab, Ananth Grama, Ruqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLMs) with human preferences is essential for
their applications. Recently, decoding-time alignment has emerged as an
effective plug-and-play technique that avoids fine-tuning model parameters.
This approach retains the general utility of pretrained LLMs but often suffers
from significant inefficiencies during decoding, primarily due to wasted token
generation and excessive reward evaluations. To address these challenges, we
introduce Cascade Reward Sampling (CARDS) to resolve both efficiency
bottlenecks in decoding-time alignment. Specifically, we develop a
segment-level rejection sampling algorithm that minimizes redundant
computations of both LLMs and reward models (RMs). Central to CARDS is an
uncertainty-based segmentation mechanism, which ensures the accuracy of RMs
evaluations on incomplete segments. Furthermore, we provide a detailed analysis
of reward scores on segments to elucidate the improved alignment performance.
Experimental results demonstrate that CARDS significantly improves decoding
efficiency, alignment quality, and general utility compared to existing
decoding-time alignment methods, achieving approximately a 70% reduction in
decoding time and over 90% win-ties in utility and safety benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More than Memes: A Multimodal Topic Modeling Approach to Conspiracy
  Theories on Telegram 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08642v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08642v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elisabeth Steffen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the increasing prevalence of (audio-)visual data on social media,
and to capture the evolving and dynamic nature of this communication,
researchers have begun to explore the potential of unsupervised approaches for
analyzing multimodal online content. However, existing research often neglects
visual content beyond memes, and in addition lacks methods to compare topic
models across modalities. Our study addresses these gaps by applying multimodal
topic modeling for analyzing conspiracy theories in German-language Telegram
channels. We use BERTopic with CLIP for the analysis of textual and visual data
in a corpus of ~40, 000 Telegram messages posted in October 2023 in 571
German-language Telegram channels known for disseminating conspiracy theories.
Through this dataset, we provide insights into unimodal and multimodal topic
models by analyzing symmetry and intersections of topics across modalities. We
demonstrate the variety of textual and visual content shared in the channels
discovered through the topic modeling, and propose a conceptual framework for
the analysis of textual and visual discursive strategies in the communication
of conspiracy theories. We apply the framework in a case study of the topic
group Israel Gaza.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ You Can Generate It Again: Data-to-Text Generation with Verification and
  Correction <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Ren, Zeyu Zhang, Lingqiao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small language models like T5 excel in generating high-quality text for
data-to-text tasks, offering adaptability and cost-efficiency compared to Large
Language Models (LLMs). However, they frequently miss keywords, which is
considered one of the most severe and common errors in this task. In this work,
we explore the potential of using feedback systems to enhance semantic fidelity
in smaller language models for data-to-text generation tasks, through our
Verification and Correction Prompting (VCP) approach. In the inference stage,
our approach involves a multi-step process, including generation, verification,
and regeneration stages. During the verification stage, we implement a simple
rule to check for the presence of every keyword in the prediction. Recognizing
that this rule can be inaccurate, we have developed a carefully designed
training procedure, which enabling the model to incorporate feedback from the
error-correcting prompt effectively, despite its potential inaccuracies. The
VCP approach effectively reduces the Semantic Error Rate (SER) while
maintaining the text's quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Modal Semantic Parsing for the Interpretation of Tombstone
  Inscriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.04377v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.04377v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Zhang, Johan Bos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tombstones are historically and culturally rich artifacts, encapsulating
individual lives, community memory, historical narratives and artistic
expression. Yet, many tombstones today face significant preservation
challenges, including physical erosion, vandalism, environmental degradation,
and political shifts. In this paper, we introduce a novel multi-modal framework
for tombstones digitization, aiming to improve the interpretation, organization
and retrieval of tombstone content. Our approach leverages vision-language
models (VLMs) to translate tombstone images into structured Tombstone Meaning
Representations (TMRs), capturing both image and text information. To further
enrich semantic parsing, we incorporate retrieval-augmented generation (RAG)
for integrate externally dependent elements such as toponyms, occupation codes,
and ontological concepts. Compared to traditional OCR-based pipelines, our
method improves parsing accuracy from an F1 score of 36.1 to 89.5. We
additionally evaluate the model's robustness across diverse linguistic and
cultural inscriptions, and simulate physical degradation through image fusion
to assess performance under noisy or damaged conditions. Our work represents
the first attempt to formalize tombstone understanding using large
vision-language models, presenting implications for heritage preservation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACMMM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CheXalign: Preference fine-tuning in chest X-ray interpretation models
  without human feedback <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07025v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07025v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Hein, Zhihong Chen, Sophie Ostmeier, Justin Xu, Maya Varma, Eduardo Pontes Reis, Arne Edward Michalson, Christian Bluethgen, Hyun Joo Shin, Curtis Langlotz, Akshay S Chaudhari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiologists play a crucial role in translating medical images into
actionable reports. However, the field faces staffing shortages and increasing
workloads. While automated approaches using vision-language models (VLMs) show
promise as assistants, they require exceptionally high accuracy. Most current
VLMs in radiology rely solely on supervised fine-tuning. Meanwhile, additional
preference fine-tuning in the post-training pipeline has become standard
practice in the general domain. The challenge in radiology lies in the
prohibitive cost of obtaining radiologist feedback at scale. To address this
challenge, we propose an automated pipeline for preference feedback, focusing
on chest X-ray radiology report generation (RRG). Specifically, our method
leverages publicly available datasets containing pairs of images and
radiologist-written reference reports with reference-based metrics, or Judges,
eliminating the need for additional radiologist feedback. We investigate reward
overoptimization via length exploitation in this setting and introduce a
length-controlled version of the GREEN score. Our best-performing setup
achieves state-of-the-art CheXbert scores on the MIMIC-CXR dataset for the RRG
task while on average maintaining robust performance across six additional
image perception and reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LinguaSynth: Heterogeneous Linguistic Signals for News Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21848v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21848v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duo Zhang, Junyi Mo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has significantly advanced NLP, but its reliance on large
black-box models introduces critical interpretability and computational
efficiency concerns. This paper proposes LinguaSynth, a novel text
classification framework that strategically integrates five complementary
linguistic feature types: lexical, syntactic, entity-level, word-level
semantics, and document-level semantics within a transparent logistic
regression model. Unlike transformer-based architectures, LinguaSynth maintains
interpretability and computational efficiency, achieving an accuracy of 84.89
percent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by
3.32 percent. Through rigorous feature interaction analysis, we show that
syntactic and entity-level signals provide essential disambiguation and
effectively complement distributional semantics. LinguaSynth sets a new
benchmark for interpretable, resource-efficient NLP models and challenges the
prevailing assumption that deep neural networks are necessary for
high-performing text classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both <span class="highlight-title">Prompt</span>
  and Reward Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03262v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03262v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Hu, Jason Klein Liu, Haotian Xu, Wei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in
aligning large language models (LLMs) with human values and preferences. While
state-of-the-art applications like ChatGPT or GPT-4 commonly employ Proximal
Policy Optimization (PPO), the inclusion of a critic network introduces
significant computational overhead. REINFORCE-based methods, such as REINFORCE
Leave One-Out (RLOO), ReMax, and Group Relative Policy Optimization (GRPO),
address this limitation by eliminating the critic network. However, these
approaches face challenges in accurate advantage estimation. Specifically, they
estimate advantages independently for responses to each prompt, which can lead
to overfitting on simpler prompts and vulnerability to reward hacking and may
be biased. To address these challenges, we introduce REINFORCE++, a novel
approach that removes the critic model while using the global advantage
normalization which is unbiased to improve the training stability. Our
empirical evaluation demonstrates that REINFORCE++ exhibits robust performance
across various reward models without requiring prompt set truncation.
Furthermore, it achieves superior generalization in both RLHF and long
chain-of-thought (CoT) settings compared to existing REINFORCE-based methods.
The implementation is available at https://github.com/OpenRLHF/OpenRLHF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>add experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agree to Disagree? A Meta-Evaluation of LLM Misgendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Subramonian, Vagrant Gautam, Preethi Seshadri, Dietrich Klakow, Kai-Wei Chang, Yizhou Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous methods have been proposed to measure LLM misgendering, including
probability-based evaluations (e.g., automatically with templatic sentences)
and generation-based evaluations (e.g., with automatic heuristics or human
validation). However, it has gone unexamined whether these evaluation methods
have convergent validity, that is, whether their results align. Therefore, we
conduct a systematic meta-evaluation of these methods across three existing
datasets for LLM misgendering. We propose a method to transform each dataset to
enable parallel probability- and generation-based evaluation. Then, by
automatically evaluating a suite of 6 models from 3 families, we find that
these methods can disagree with each other at the instance, dataset, and model
levels, conflicting on 20.2% of evaluation instances. Finally, with a human
evaluation of 2400 LLM generations, we show that misgendering behaviour is
complex and goes far beyond pronouns, which automatic evaluations are not
currently designed to capture, suggesting essential disagreement with human
evaluations. Based on our findings, we provide recommendations for future
evaluations of LLM misgendering. Our results are also more widely relevant, as
they call into question broader methodological conventions in LLM evaluation,
which often assume that different evaluation methods agree.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiDoRA: Bi-level Optimization-Based Weight-Decomposed Low-Rank
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peijia Qin, Ruiyi Zhang, Pengtao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) is a flexible and efficient method for
adapting large language models (LLMs) to downstream tasks. Among these methods,
weight-decomposed low-rank adaptation (DoRA) is a promising approach that
decomposes weight matrices into magnitude and direction components to mimic
full fine-tuning (FT) better. However, DoRA's simultaneous optimization of
these components makes it over-expressive, increases the risk of overfitting,
and creates a coupled updating pattern that limits its learning capacity. To
address these issues, we propose Bi-level Optimization-Based Weight-Decomposed
Low-Rank Adaptation (BiDoRA), a novel PEFT method based on a bi-level
optimization framework. BiDoRA fundamentally differs from DoRA by optimizing
the magnitude and direction in two separate, asynchronous loops using distinct
training and validation data splits. This decoupled optimization process
effectively mitigates overfitting and allows for more flexible updates that
align even more closely with FT. For instance, weight decomposition analysis
shows BiDoRA achieves a magnitude-direction update correlation of $-8.042$,
significantly closer to the FT ideal compared to $-1.784$ for DoRA. Evaluation
of BiDoRA on diverse tasks spanning natural language understanding, generation,
token classification, and extremely small biomedical datasets reveals that it
consistently outperforms DoRA and a wide range of leading PEFT methods. This
improvement is statistically significant, as demonstrated on the GLUE benchmark
where BiDoRA surpasses DoRA with a p-value of $2.4\times10^{-4}$ in terms of
the Wilcoxon signed-rank test. The code for BiDoRA is available at
https://github.com/t2ance/BiDoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natapong Nitarach, Warit Sirichotedumrong, Panop Pitchayarthorn, Pittawat Taveekitworachai, Potsawee Manakul, Kunat Pipatanakul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents FinCoT, a structured chain-of-thought (CoT) prompting
framework that embeds domain-specific expert financial reasoning blueprints to
guide large language models' behaviors. We identify three main prompting styles
in financial NLP (FinNLP): (1) standard prompting (zero-shot), (2) unstructured
CoT (free-form reasoning), and (3) structured CoT (with explicitly structured
reasoning steps). Prior work has mainly focused on the first two, while
structured CoT remains underexplored and lacks domain expertise incorporation.
Therefore, we evaluate all three prompting approaches across ten CFA-style
financial domains and introduce FinCoT as the first structured finance-specific
prompting approach incorporating blueprints from domain experts. FinCoT
improves the accuracy of a general-purpose model, Qwen3-8B-Base, from 63.2% to
80.5%, and boosts Fin-R1 (7B), a finance-specific model, from 65.7% to 75.7%,
while reducing output length by up to 8.9x and 1.16x compared to structured CoT
methods, respectively. We find that FinCoT proves most effective for models
lacking financial post-training. Our findings show that FinCoT does not only
improve performance and reduce inference costs but also yields more
interpretable and expert-aligned reasoning traces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reasoning with Exploration: An Entropy Perspective on Reinforcement
  Learning for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Balancing exploration and exploitation is a central goal in reinforcement
learning (RL). Despite recent advances in enhancing large language model (LLM)
reasoning, most methods lean toward exploitation, and increasingly encounter
performance plateaus. In this work, we revisit entropy -- a signal of
exploration in RL -- and examine its relationship to exploratory reasoning in
LLMs. Through empirical analysis, we uncover positive correlations between
high-entropy regions and three types of exploratory reasoning actions: (1)
pivotal tokens that determine or connect logical steps, (2) reflective actions
such as self-verification and correction, and (3) rare behaviors under-explored
by the base LLMs. Motivated by this, we introduce a minimal modification to
standard RL with only one line of code: augmenting the advantage function with
an entropy-based term. Unlike traditional maximum-entropy methods which
encourage exploration by promoting uncertainty, we encourage exploration by
promoting longer and deeper reasoning chains. Notably, our method achieves
significant gains on the Pass@K metric -- an upper-bound estimator of LLM
reasoning capabilities -- even when evaluated with extremely large K values,
pushing the boundaries of LLM reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte
  Carlo Tree Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16607v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16607v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuozhi Yuan, Limin Chen, Miaomiao Yuan, Jin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at
translating natural language questions into SQL queries. While recent advances
in large language models have greatly improved performance, most existing
approaches depend on models with tens of billions of parameters or costly APIs,
limiting their applicability in resource-constrained environments. For real
world, especially on edge devices, it is crucial for Text-to-SQL to ensure
cost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL
is of great practical significance. However, smaller LLMs often struggle with
complicated user instruction, redundant schema linking or syntax correctness.
To address these challenges, we propose MCTS-SQL, a novel framework that uses
Monte Carlo Tree Search to guide SQL generation through multi-step refinement.
Since the light-weight models' weak performance of single-shot prediction, we
generate better results through several trials with feedback. However, directly
applying MCTS-based methods inevitably leads to significant time and
computational overhead. Driven by this issue, we propose a token-level
prefix-cache mechanism that stores prior information during iterations,
effectively improved the execution speed. Experiments results on the SPIDER and
BIRD benchmarks demonstrate the effectiveness of our approach. Using a small
open-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When
leveraging a more powerful model Gemini 2.5 to explore the performance upper
bound, we achieved results competitive with the SOTA. Our findings demonstrate
that even small models can be effectively deployed in practical Text-to-SQL
systems with the right strategy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shifting AI Efficiency From Model-Centric to Data-Centric Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19147v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19147v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuyang Liu, Zichen Wen, Shaobo Wang, Junjie Chen, Zhishan Tao, Yubo Wang, Xiangqi Jin, Chang Zou, Yiyu Wang, Chenfei Liao, Xu Zheng, Honggang Chen, Weijia Li, Xuming Hu, Conghui He, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models (LLMs) and multi-modal LLMs
(MLLMs) has historically relied on model-centric scaling through increasing
parameter counts from millions to hundreds of billions to drive performance
gains. However, as we approach hardware limits on model size, the dominant
computational bottleneck has fundamentally shifted to the quadratic cost of
self-attention over long token sequences, now driven by ultra-long text
contexts, high-resolution images, and extended videos. In this position paper,
\textbf{we argue that the focus of research for efficient AI is shifting from
model-centric compression to data-centric compression}. We position token
compression as the new frontier, which improves AI efficiency via reducing the
number of tokens during model training or inference. Through comprehensive
analysis, we first examine recent developments in long-context AI across
various domains and establish a unified mathematical framework for existing
model efficiency strategies, demonstrating why token compression represents a
crucial paradigm shift in addressing long-context overhead. Subsequently, we
systematically review the research landscape of token compression, analyzing
its fundamental benefits and identifying its compelling advantages across
diverse scenarios. Furthermore, we provide an in-depth analysis of current
challenges in token compression research and outline promising future
directions. Ultimately, our work aims to offer a fresh perspective on AI
efficiency, synthesize existing research, and catalyze innovative developments
to address the challenges that increasing context lengths pose to the AI
community's advancement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project:
  \url{https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression}</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Reciprocal Recommender Systems for User-to-User Matching <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuki Kawamura, Takuma Udagawa, Kei Tateno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reciprocal recommender systems (RRS) in dating, gaming, and talent platforms
require mutual acceptance for a match. Logged data, however, over-represents
popular profiles due to past exposure policies, creating feedback loops that
skew learning and fairness. We introduce Counterfactual Reciprocal Recommender
Systems (CFRR), a causal framework to mitigate this bias. CFRR uses inverse
propensity scored, self-normalized objectives. Experiments show CFRR improves
NDCG@10 by up to 3.5% (e.g., from 0.459 to 0.475 on DBLP, from 0.299 to 0.307
on Synthetic), increases long-tail user coverage by up to 51% (from 0.504 to
0.763 on Synthetic), and reduces Gini exposure inequality by up to 24% (from
0.708 to 0.535 on Synthetic). CFRR offers a promising approach for more
accurate and fair user-to-user matching.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures. Accepted for publication at the Workshop on
  Two-sided Marketplace Optimization (TSMO '25), held in conjunction with the
  31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2025),
  Toronto, Canada</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChEmbed: Enhancing Chemical Literature Search Through Domain-Specific
  Text Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Shiraee Kasmaee, Mohammad Khodadad, Mehdi Astaraki, Mohammad Arshi Saloot, Nicholas Sherck, Hamidreza Mahyar, Soheila Samiee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems in chemistry heavily depend on
accurate and relevant retrieval of chemical literature. However,
general-purpose text embedding models frequently fail to adequately represent
complex chemical terminologies, resulting in suboptimal retrieval quality.
Specialized embedding models tailored to chemical literature retrieval have not
yet been developed, leaving a substantial performance gap. To address this
challenge, we introduce ChEmbed, a domain-adapted family of text embedding
models fine-tuned on a dataset comprising chemistry-specific text from the
PubChem, Semantic Scholar, and ChemRxiv corpora. To create effective training
data, we employ large language models to synthetically generate queries,
resulting in approximately 1.7 million high-quality query-passage pairs.
Additionally, we augment the tokenizer by adding 900 chemically specialized
tokens to previously unused slots, which significantly reduces the
fragmentation of chemical entities, such as IUPAC names. ChEmbed also maintains
a 8192-token context length, enabling the efficient retrieval of longer
passages compared to many other open-source embedding models, which typically
have a context length of 512 or 2048 tokens. Evaluated on our newly introduced
ChemRxiv Retrieval benchmark, ChEmbed outperforms state-of-the-art general
embedding models, raising nDCG@10 from 0.82 to 0.91 (+9 pp). ChEmbed represents
a practical, lightweight, and reproducible embedding solution that effectively
improves retrieval for chemical literature search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-grained Alignment of Large Language Models for General Medication
  Recommendation without Overprescription 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03687v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03687v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Zhao, Chenxiao Fan, Junlong Liu, Zheng Wang, Xiangnan He, Chongming Gao, Juan Li, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) holds significant promise in achieving general
medication recommendation systems owing to their comprehensive interpretation
of clinical notes and flexibility to medication encoding. We evaluated both
general-purpose and medical-specific LLMs for medication recommendations,
showing their unsatisfactory precision and severe overprescription. To address
this, we introduce Language-Assisted Medication Recommendation, which tailors
LLMs for medication recommendation in a medication-aware manner, improving the
usage of clinical notes. Fine-tuning LLMs with this framework can outperform
existing methods by more than 10% in internal validation and generalize across
temporal and external validations. Furthermore, the model maintains high
accuracy when encountering out-of-distribution medication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clue-RAG: Towards Accurate and Cost-Efficient Graph-based RAG via
  Multi-Partite Graph and Query-Driven Iterative Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.08445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.08445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaodong Su, Yixiang Fang, Yingli Zhou, Quanqing Xu, Chuanhui Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable progress of Large Language Models (LLMs), their
performance in question answering (QA) remains limited by the lack of
domain-specific and up-to-date knowledge. Retrieval-Augmented Generation (RAG)
addresses this limitation by incorporating external information, often from
graph-structured data. However, existing graph-based RAG methods suffer from
poor graph quality due to incomplete extraction and insufficient utilization of
query information during retrieval. To overcome these limitations, we propose
Clue-RAG, a novel approach that introduces (1) a multi-partite graph index
incorporates Chunk, knowledge unit, and entity to capture semantic content at
multiple levels of granularity, coupled with a hybrid extraction strategy that
reduces LLM token usage while still producing accurate and disambiguated
knowledge units, and (2) Q-Iter, a query-driven iterative retrieval strategy
that enhances relevance through semantic search and constrained graph
traversal. Experiments on three QA benchmarks show that Clue-RAG significantly
outperforms state-of-the-art baselines, achieving up to 99.33% higher Accuracy
and 113.51% higher F1 score while reducing indexing costs by 72.58%.
Remarkably, Clue-RAG matches or outperforms baselines even without using an LLM
for indexing. These results demonstrate the effectiveness and cost-efficiency
of Clue-RAG in advancing graph-based RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context Guided <span class="highlight-title">Transformer</span> Entropy Modeling for Video Compression <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlong Tong, Wei Zhang, Yaohui Jin, Xiaoyu Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional entropy models effectively leverage spatio-temporal contexts to
reduce video redundancy. However, incorporating temporal context often
introduces additional model complexity and increases computational cost. In
parallel, many existing spatial context models lack explicit modeling the
ordering of spatial dependencies, which may limit the availability of relevant
context during decoding. To address these issues, we propose the Context Guided
Transformer (CGT) entropy model, which estimates probability mass functions of
the current frame conditioned on resampled temporal context and
dependency-weighted spatial context. A temporal context resampler learns
predefined latent queries to extract critical temporal information using
transformer encoders, reducing downstream computational overhead. Meanwhile, a
teacher-student network is designed as dependency-weighted spatial context
assigner to explicitly model the dependency of spatial context order. The
teacher generates an attention map to represent token importance and an entropy
map to reflect prediction certainty from randomly masked inputs, guiding the
student to select the weighted top-k tokens with the highest spatial
dependency. During inference, only the student is used to predict undecoded
tokens based on high-dependency context. Experimental results demonstrate that
our CGT model reduces entropy modeling time by approximately 65% and achieves
an 11% BD-Rate reduction compared to the previous state-of-the-art conditional
entropy model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRKF: Decoupled Representations with Knowledge Fusion for Multimodal
  Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, Daibing Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal emotion recognition (MER) aims to identify emotional states by
integrating and analyzing information from multiple modalities. However,
inherent modality heterogeneity and inconsistencies in emotional cues remain
key challenges that hinder performance. To address these issues, we propose a
Decoupled Representations with Knowledge Fusion (DRKF) method for MER. DRKF
consists of two main modules: an Optimized Representation Learning (ORL) Module
and a Knowledge Fusion (KF) Module. ORL employs a contrastive mutual
information estimation method with progressive modality augmentation to
decouple task-relevant shared representations and modality-specific features
while mitigating modality heterogeneity. KF includes a lightweight
self-attention-based Fusion Encoder (FE) that identifies the dominant modality
and integrates emotional information from other modalities to enhance the fused
representation. To handle potential errors from incorrect dominant modality
selection under emotionally inconsistent conditions, we introduce an Emotion
Discrimination Submodule (ED), which enforces the fused representation to
retain discriminative cues of emotional inconsistency. This ensures that even
if the FE selects an inappropriate dominant modality, the Emotion
Classification Submodule (EC) can still make accurate predictions by leveraging
preserved inconsistency information. Experiments show that DRKF achieves
state-of-the-art (SOTA) performance on IEMOCAP, MELD, and M3ED. The source code
is publicly available at https://github.com/PANPANKK/DRKF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ACM Multimedia 2025. 10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More than Memes: A Multimodal Topic Modeling Approach to Conspiracy
  Theories on Telegram 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08642v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08642v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elisabeth Steffen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the increasing prevalence of (audio-)visual data on social media,
and to capture the evolving and dynamic nature of this communication,
researchers have begun to explore the potential of unsupervised approaches for
analyzing multimodal online content. However, existing research often neglects
visual content beyond memes, and in addition lacks methods to compare topic
models across modalities. Our study addresses these gaps by applying multimodal
topic modeling for analyzing conspiracy theories in German-language Telegram
channels. We use BERTopic with CLIP for the analysis of textual and visual data
in a corpus of ~40, 000 Telegram messages posted in October 2023 in 571
German-language Telegram channels known for disseminating conspiracy theories.
Through this dataset, we provide insights into unimodal and multimodal topic
models by analyzing symmetry and intersections of topics across modalities. We
demonstrate the variety of textual and visual content shared in the channels
discovered through the topic modeling, and propose a conceptual framework for
the analysis of textual and visual discursive strategies in the communication
of conspiracy theories. We apply the framework in a case study of the topic
group Israel Gaza.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Modal Semantic Parsing for the Interpretation of Tombstone
  Inscriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.04377v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.04377v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Zhang, Johan Bos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tombstones are historically and culturally rich artifacts, encapsulating
individual lives, community memory, historical narratives and artistic
expression. Yet, many tombstones today face significant preservation
challenges, including physical erosion, vandalism, environmental degradation,
and political shifts. In this paper, we introduce a novel multi-modal framework
for tombstones digitization, aiming to improve the interpretation, organization
and retrieval of tombstone content. Our approach leverages vision-language
models (VLMs) to translate tombstone images into structured Tombstone Meaning
Representations (TMRs), capturing both image and text information. To further
enrich semantic parsing, we incorporate retrieval-augmented generation (RAG)
for integrate externally dependent elements such as toponyms, occupation codes,
and ontological concepts. Compared to traditional OCR-based pipelines, our
method improves parsing accuracy from an F1 score of 36.1 to 89.5. We
additionally evaluate the model's robustness across diverse linguistic and
cultural inscriptions, and simulate physical degradation through image fusion
to assess performance under noisy or damaged conditions. Our work represents
the first attempt to formalize tombstone understanding using large
vision-language models, presenting implications for heritage preservation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACMMM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAISE: Realness Assessment for Image Synthesis and Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniruddha Mukherjee, Spriha Dubey, Somdyuti Paul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of generative AI has enabled the creation of highly
photorealistic visual content, offering practical substitutes for real images
and videos in scenarios where acquiring real data is difficult or expensive.
However, reliably substituting real visual content with AI-generated
counterparts requires robust assessment of the perceived realness of
AI-generated visual content, a challenging task due to its inherent subjective
nature. To address this, we conducted a comprehensive human study evaluating
the perceptual realness of both real and AI-generated images, resulting in a
new dataset, containing images paired with subjective realness scores,
introduced as RAISE in this paper. Further, we develop and train multiple
models on RAISE to establish baselines for realness prediction. Our
experimental results demonstrate that features derived from deep foundation
vision models can effectively capture the subjective realness. RAISE thus
provides a valuable resource for developing robust, objective models of
perceptual realness assessment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuteSwap: Visual-informed Silent Video Identity Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.00498v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.00498v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Liu, Yu Fang, Zhouhan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional voice conversion modifies voice characteristics from a source
speaker to a target speaker, relying on audio input from both sides. However,
this process becomes infeasible when clean audio is unavailable, such as in
silent videos or noisy environments. In this work, we focus on the task of
Silent Face-based Voice Conversion (SFVC), which does voice conversion entirely
from visual inputs. i.e., given images of a target speaker and a silent video
of a source speaker containing lip motion, SFVC generates speech aligning the
identity of the target speaker while preserving the speech content in the
source silent video. As this task requires generating intelligible speech and
converting identity using only visual cues, it is particularly challenging. To
address this, we introduce MuteSwap, a novel framework that employs contrastive
learning to align cross-modality identities and minimize mutual information to
separate shared visual features. Experimental results show that MuteSwap
achieves impressive performance in both speech synthesis and identity
conversion, especially under noisy conditions where methods dependent on audio
input fail to produce intelligible results, demonstrating both the
effectiveness of our training approach and the feasibility of SFVC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Temporal-Aware Multi-Modal Retrieval Augmented Generation in
  Finance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.05185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.05185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengbin Zhu, Junfeng Li, Liangming Pan, Wenjie Wang, Fuli Feng, Chao Wang, Huanbo Luan, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finance decision-making often relies on in-depth data analysis across various
data sources, including financial tables, news articles, stock prices, etc. In
this work, we introduce FinTMMBench, the first comprehensive benchmark for
evaluating temporal-aware multi-modal Retrieval-Augmented Generation (RAG)
systems in finance. Built from heterologous data of NASDAQ 100 companies,
FinTMMBench offers three significant advantages. 1) Multi-modal Corpus: It
encompasses a hybrid of financial tables, news articles, daily stock prices,
and visual technical charts as the corpus. 2) Temporal-aware Questions: Each
question requires the retrieval and interpretation of its relevant data over a
specific time period, including daily, weekly, monthly, quarterly, and annual
periods. 3) Diverse Financial Analysis Tasks: The questions involve 10
different financial analysis tasks designed by domain experts, including
information extraction, trend analysis, sentiment analysis and event detection,
etc. We further propose a novel TMMHybridRAG method, which first leverages LLMs
to convert data from other modalities (e.g., tabular, visual and time-series
data) into textual format and then incorporates temporal information in each
node when constructing graphs and dense indexes. Its effectiveness has been
validated in extensive experiments, but notable gaps remain, highlighting the
challenges presented by our FinTMMBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Prototype Knowledge Transfer for Federated Learning with Mixed
  Modalities and Heterogeneous Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04400v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04400v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keke Gai, Mohan Wang, Jing Yu, Dongjue Wang, Qi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Federated Learning (MFL) with mixed modalities enables unimodal
and multimodal clients to collaboratively train models while ensuring clients'
privacy. As a representative sample of local data, prototypes offer an approach
with low resource consumption and no reliance on prior knowledge for MFL with
mixed modalities. However, existing prototype-based MFL methods assume unified
labels across clients and identical tasks per client, which is impractical in
MFL with mixed modalities. In this work, we propose an Adaptive prototype-based
Multimodal Federated Learning (AproMFL) framework for mixed modalities to
address the aforementioned issues. Our AproMFL transfers knowledge through
adaptively-constructed prototypes without unified labels. Clients adaptively
select prototype construction methods in line with labels; server converts
client prototypes into unified multimodal prototypes and cluster them to form
global prototypes. To address model aggregation issues in task heterogeneity,
we develop a client relationship graph-based scheme to dynamically adjust
aggregation weights. Furthermore, we propose a global prototype knowledge
transfer loss and a global model knowledge transfer loss to enable the transfer
of global knowledge to local knowledge. Experimental results show that AproMFL
outperforms four baselines on three highly heterogeneous datasets
($\alpha=0.1$) and two heterogeneous tasks, with the optimal results in
accuracy and recall being 0.42%~6.09% and 1.6%~3.89% higher than those of
FedIoT (FedAvg-based MFL), respectively.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-08-02T00:00:00Z">2025-08-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Personalization: Unifying Recommender Systems with Large
  Language Models <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danial Ebrat, Tina Aminian, Sepideh Ahmadian, Luis Rueda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are essential for guiding users through the vast and
diverse landscape of digital content by delivering personalized and relevant
suggestions. However, improving both personalization and interpretability
remains a challenge, particularly in scenarios involving limited user feedback
or heterogeneous item attributes. In this article, we propose a novel hybrid
recommendation framework that combines Graph Attention Networks (GATs) with
Large Language Models (LLMs) to address these limitations. LLMs are first used
to enrich user and item representations by generating semantically meaningful
profiles based on metadata such as titles, genres, and overviews. These
enriched embeddings serve as initial node features in a user and movie
bipartite graph, which is processed using a GAT based collaborative filtering
model. To enhance ranking accuracy, we introduce a hybrid loss function that
combines Bayesian Personalized Ranking (BPR), cosine similarity, and robust
negative sampling. Post-processing involves reranking the GAT-generated
recommendations using the LLM, which also generates natural-language
justifications to improve transparency. We evaluated our model on benchmark
datasets, including MovieLens 100k and 1M, where it consistently outperforms
strong baselines. Ablation studies confirm that LLM-based embeddings and the
cosine similarity term significantly contribute to performance gains. This work
demonstrates the potential of integrating LLMs to improve both the accuracy and
interpretability of recommender systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Second Workshop on Generative AI for Recommender Systems and
  Personalization at the ACM Conference on Knowledge Discovery and Data Mining
  (GenAIRecP@KDD 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Req-Rec: Enhancing Requirements Elicitation for Increasing Stakeholder's
  Satisfaction Using a Collaborative Filtering Based Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Fallahi, Amineh Amini, Azam Bastanfard, Hadi Saboohi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success or failure of a project is highly related to recognizing the
right stakeholders and accurately finding and discovering their requirements.
However, choosing the proper elicitation technique was always a considerable
challenge for efficient requirement engineering. As a consequence of the swift
improvement of digital technologies since the past decade, recommender systems
have become an efficient channel for making a deeply personalized interactive
communication with stakeholders. In this research, a new method, called the
Req-Rec (Requirements Recommender), is proposed. It is a hybrid recommender
system based on the collaborative filtering approach and the repertory grid
technique as the core component. The primary goal of Req-Rec is to increase
stakeholder satisfaction by assisting them in the requirement elicitation
phase. Based on the results, the method efficiently could overcome weaknesses
of common requirement elicitation techniques, such as time limitation,
location-based restrictions, and bias in requirements' elicitation process.
Therefore, recommending related requirements assists stakeholders in becoming
more aware of different aspects of the project.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>March 2023, 28 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SaviorRec: Semantic-Behavior Alignment for Cold-Start Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yining Yao, Ziwei Li, Shuwen Xiao, Boya Du, Jialin Zhu, Junjun Zheng, Xiangheng Kong, Yuning Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recommendation systems, predicting Click-Through Rate (CTR) is crucial for
accurately matching users with items. To improve recommendation performance for
cold-start and long-tail items, recent studies focus on leveraging item
multimodal features to model users' interests. However, obtaining multimodal
representations for items relies on complex pre-trained encoders, which incurs
unacceptable computation cost to train jointly with downstream ranking models.
Therefore, it is important to maintain alignment between semantic and behavior
space in a lightweight way.
  To address these challenges, we propose a Semantic-Behavior Alignment for
Cold-start Recommendation framework, which mainly focuses on utilizing
multimodal representations that align with the user behavior space to predict
CTR. First, we leverage domain-specific knowledge to train a multimodal encoder
to generate behavior-aware semantic representations. Second, we use residual
quantized semantic ID to dynamically bridge the gap between multimodal
representations and the ranking model, facilitating the continuous
semantic-behavior alignment. We conduct our offline and online experiments on
the Taobao, one of the world's largest e-commerce platforms, and have achieved
an increase of 0.83% in offline AUC, 13.21% clicks increase and 13.44% orders
increase in the online A/B test, emphasizing the efficacy of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Koshkin, Pengyu Dai, Nozomi Fujikawa, Masahito Togami, Marco Visentini-Scarzanella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an autonomous framework that leverages Large Language Models
(LLMs) to automate end-to-end business analysis and market report generation.
At its core, the system employs specialized agents - Researcher, Reviewer,
Writer, and Retriever - that collaborate to analyze data and produce
comprehensive reports. These agents learn from real professional consultants'
presentation materials at Amazon through in-context learning to replicate
professional analytical methodologies. The framework executes a multi-step
process: querying databases, analyzing data, generating insights, creating
visualizations, and composing market reports. We also introduce a novel
LLM-based evaluation system for assessing report quality, which shows alignment
with expert human evaluations. Building on these evaluations, we implement an
iterative improvement mechanism that optimizes report quality through automated
review cycles. Experimental results show that report quality can be improved by
both automated review cycles and consultants' unstructured knowledge. In
experimental validation, our framework generates detailed 6-page reports in 7
minutes at a cost of approximately \$1. Our work could be an important step to
automatically create affordable market insights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BioDisco: Multi-agent hypothesis generation with dual-mode evidence,
  iterative feedback and temporal evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujing Ke, Kevin George, Kathan Pandya, David Blumenthal, Maximilian Sprang, Gerrit Großmann, Sebastian Vollmer, David Antony Selby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying novel hypotheses is essential to scientific research, yet this
process risks being overwhelmed by the sheer volume and complexity of available
information. Existing automated methods often struggle to generate novel and
evidence-grounded hypotheses, lack robust iterative refinement and rarely
undergo rigorous temporal evaluation for future discovery potential. To address
this, we propose BioDisco, a multi-agent framework that draws upon language
model-based reasoning and a dual-mode evidence system (biomedical knowledge
graphs and automated literature retrieval) for grounded novelty, integrates an
internal scoring and feedback loop for iterative refinement, and validates
performance through pioneering temporal and human evaluations and a
Bradley-Terry paired comparison model to provide statistically-grounded
assessment. Our evaluations demonstrate superior novelty and significance over
ablated configurations representative of existing agentic architectures.
Designed for flexibility and modularity, BioDisco allows seamless integration
of custom language models or knowledge graphs, and can be run with just a few
lines of code. We anticipate researchers using this practical tool as a
catalyst for the discovery of new hypotheses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages main content + 11 pages appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study on Enhancing User Engagement by Employing Gamified Recommender
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Fallahi, Azam Bastanfard, Amineh Amini, Hadi Saboohi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Providing customized products and services in the modern business world is
one of the most efficient solutions to improve users' experience and their
engagements with the industries. To aim, recommender systems, by producing
personalized recommendations, have a crucial role in the digital age. As a
consequence of modern improvements in the internet and online-based
technologies, using gamification rules also increased in various fields. Recent
studies showed that considering gamification concepts in implementing
recommendation systems not only can become helpful to overcome the cold start
and lack of sufficient data, moreover, can effectively improve user engagement.
Gamification can motivate individuals to have more activities on the system;
these interactions are valuable resources of data for recommender engines.
Unlike the past related works about using gamified recommendation systems in
different environments or studies that particularly surveyed gamification
strategies or recommenders separately, this work provides a comprehensive
review of how gamified recommender systems can enhance user engagement in
various domain applications. Furthermore, comparing different approaches for
building recommender systems is followed by in-depth surveying about
investigating the gamified recommender systems, including their approaches,
limitations, evaluation metrics, proposed achievements, datasets, domain areas,
and their recommendation techniques. This exhaustive analysis provides a
detailed picture of the topic's popularity, gaps, and unexplored regions. It is
envisaged that the proposed research and introduced possible future directions
would serve as a stepping stone for researchers interested in using gamified
recommender systems for user satisfaction and engagement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>June 2023, 21 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CM$^3$: Calibrating Multimodal Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhou, Yongjie Wang, Zhiqi Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alignment and uniformity are fundamental principles within the domain of
contrastive learning. In recommender systems, prior work has established that
optimizing the Bayesian Personalized Ranking (BPR) loss contributes to the
objectives of alignment and uniformity. Specifically, alignment aims to draw
together the representations of interacting users and items, while uniformity
mandates a uniform distribution of user and item embeddings across a unit
hypersphere. This study revisits the alignment and uniformity properties within
the context of multimodal recommender systems, revealing a proclivity among
extant models to prioritize uniformity to the detriment of alignment. Our
hypothesis challenges the conventional assumption of equitable item treatment
through a uniformity loss, proposing a more nuanced approach wherein items with
similar multimodal attributes converge toward proximal representations within
the hyperspheric manifold. Specifically, we leverage the inherent similarity
between items' multimodal data to calibrate their uniformity distribution,
thereby inducing a more pronounced repulsive force between dissimilar entities
within the embedding space. A theoretical analysis elucidates the relationship
between this calibrated uniformity loss and the conventional uniformity
function. Moreover, to enhance the fusion of multimodal features, we introduce
a Spherical B\'ezier method designed to integrate an arbitrary number of
modalities while ensuring that the resulting fused features are constrained to
the same hyperspherical manifold. Empirical evaluations conducted on five
real-world datasets substantiate the superiority of our approach over competing
baselines. We also shown that the proposed methods can achieve up to a 5.4%
increase in NDCG@20 performance via the integration of MLLM-extracted features.
Source code is available at: https://github.com/enoche/CM3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working Paper: https://github.com/enoche/CM3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance
  System using Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhou, Peng Sun, Xuanhe Zhou, Qianglei Zang, Ji Xu, Tieying Zhang, Guoliang Li, Fan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The operation and maintenance (O&M) of database systems is critical to
ensuring system availability and performance, typically requiring expert
experience (e.g., identifying metric-to-anomaly relations) for effective
diagnosis and recovery. However, existing automatic database O&M methods,
including commercial products, cannot effectively utilize expert experience. On
the one hand, rule-based methods only support basic O&M tasks (e.g.,
metric-based anomaly detection), which are mostly numerical equations and
cannot effectively incorporate literal O&M experience (e.g., troubleshooting
guidance in manuals). On the other hand, LLM-based methods, which retrieve
fragmented information (e.g., standard documents + RAG), often generate
inaccurate or generic results. To address these limitations, we present
DBAIOps, a novel hybrid database O&M system that combines reasoning LLMs with
knowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a
heterogeneous graph model for representing the diagnosis experience, and
proposes a semi-automatic graph construction algorithm to build that graph from
thousands of documents. Second, DBAIOps develops a collection of (800+)
reusable anomaly models that identify both directly alerted metrics and
implicitly correlated experience and metrics. Third, for each anomaly, DBAIOps
proposes a two-stage graph evolution mechanism to explore relevant diagnosis
paths and identify missing relations automatically. It then leverages a
reasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear
diagnosis reports for both DBAs and common users. Our evaluation over four
mainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates
that DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher
in root cause and human evaluation accuracy, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>DBAIOps supports 25 database systems and has been deployed in 20
  real-world scenarios, covering domains like finance, energy, and healthcare.
  See website at: https://www.dbaiops.com; See code at:
  https://github.com/weAIDB/DBAIOps/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Bridging <span class="highlight-title">Review</span> Sparsity in Recommendation with Textual Edge
  Graph Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leyao Wang, Xutao Mao, Xuhui Zhan, Yuying Zhao, Bo Ni, Ryan A. Rossi, Nesreen K. Ahmed, Tyler Derr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Textual reviews enrich recommender systems with fine-grained preference
signals and enhanced explainability. However, in real-world scenarios, users
rarely leave reviews, resulting in severe sparsity that undermines the
effectiveness of existing models. A natural solution is to impute or generate
missing reviews to enrich the data. However, conventional imputation techniques
-- such as matrix completion and LLM-based augmentation -- either lose
contextualized semantics by embedding texts into vectors, or overlook
structural dependencies among user-item interactions. To address these
shortcomings, we propose TWISTER (ToWards Imputation on Sparsity with Textual
Edge Graph Representation), a unified framework that imputes missing reviews by
jointly modeling semantic and structural signals. Specifically, we represent
user-item interactions as a Textual-Edge Graph (TEG), treating reviews as edge
attributes. To capture relational context, we construct line-graph views and
employ a large language model as a graph-aware aggregator. For each interaction
lacking a textual review, our model aggregates the neighborhood's
natural-language representations to generate a coherent and personalized
review. Experiments on the Amazon and Goodreads datasets show that TWISTER
consistently outperforms traditional numeric, graph-based, and LLM baselines,
delivering higher-quality imputed reviews and, more importantly, enhanced
recommendation performance. In summary, TWISTER generates reviews that are more
helpful, authentic, and specific, while smoothing structural signals for
improved recommendations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CTR-Driven Ad Text Generation via Online Feedback Preference
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20227v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20227v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanda Chen, Zihui Ren, Qixiang Gao, Jiale Chen, Si Chen, Xubin Li, Tiezheng Ge, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advertising text plays a critical role in determining click-through rates
(CTR) in online advertising. Large Language Models (LLMs) offer significant
efficiency advantages over manual ad text creation. However, LLM-generated ad
texts do not guarantee higher CTR performance compared to human-crafted texts,
revealing a gap between generation quality and online performance of ad texts.
In this work, we propose a novel ad text generation method which optimizes for
CTR through preference optimization from online feedback. Our approach adopts
an innovative two-stage framework: (1) diverse ad text sampling via one-shot
in-context learning, using retrieval-augmented generation (RAG) to provide
exemplars with chain-of-thought (CoT) reasoning; (2) CTR-driven preference
optimization from online feedback, which weighs preference pairs according to
their CTR gains and confidence levels. Through our method, the resulting model
enables end-to-end generation of high-CTR ad texts. Extensive experiments have
demonstrated the effectiveness of our method in both offline and online
metrics. Notably, we have applied our method on a large-scale online shopping
platform and achieved significant CTR improvements, showcasing its strong
applicability and effectiveness in advertising systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for
  Pre-Ranking Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19849v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19849v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoqiang Yang, Congde Yuan, Kun Bai, Mengzhuo Guo, Wei Yang, Chao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online display advertising platforms rely on pre-ranking systems to
efficiently filter and prioritize candidate ads from large corpora, balancing
relevance to users with strict computational constraints. The prevailing
two-tower architecture, though highly efficient due to its decoupled design and
pre-caching, suffers from cross-domain interaction and coarse similarity
metrics, undermining its capacity to model complex user-ad relationships. In
this study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)
model, a new architecture that augments the two-tower paradigm with two key
components: $\textit{generators}$ that pre-generate holistic vectors
incorporating coarse-grained user-ad interactions through a dual-generator
framework with a cosine-similarity-based generation loss as the training
objective, and $\textit{multi-head representers}$ that project embeddings into
multiple latent subspaces to capture fine-grained, multi-faceted user interests
and multi-dimensional ad attributes. This design enhances modeling
effectiveness without compromising inference efficiency. Extensive experiments
on public datasets and large-scale online A/B testing on Tencent's advertising
platform demonstrate that HIT significantly outperforms several baselines in
relevance metrics, yielding a $1.66\%$ increase in Gross Merchandise Volume and
a $1.55\%$ improvement in Return on Investment, alongside similar serving
latency to the vanilla two-tower models. The HIT model has been successfully
deployed in Tencent's online display advertising system, serving billions of
impressions daily. The code is available at
https://github.com/HarveyYang123/HIT_model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CM$^3$: Calibrating Multimodal Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhou, Yongjie Wang, Zhiqi Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alignment and uniformity are fundamental principles within the domain of
contrastive learning. In recommender systems, prior work has established that
optimizing the Bayesian Personalized Ranking (BPR) loss contributes to the
objectives of alignment and uniformity. Specifically, alignment aims to draw
together the representations of interacting users and items, while uniformity
mandates a uniform distribution of user and item embeddings across a unit
hypersphere. This study revisits the alignment and uniformity properties within
the context of multimodal recommender systems, revealing a proclivity among
extant models to prioritize uniformity to the detriment of alignment. Our
hypothesis challenges the conventional assumption of equitable item treatment
through a uniformity loss, proposing a more nuanced approach wherein items with
similar multimodal attributes converge toward proximal representations within
the hyperspheric manifold. Specifically, we leverage the inherent similarity
between items' multimodal data to calibrate their uniformity distribution,
thereby inducing a more pronounced repulsive force between dissimilar entities
within the embedding space. A theoretical analysis elucidates the relationship
between this calibrated uniformity loss and the conventional uniformity
function. Moreover, to enhance the fusion of multimodal features, we introduce
a Spherical B\'ezier method designed to integrate an arbitrary number of
modalities while ensuring that the resulting fused features are constrained to
the same hyperspherical manifold. Empirical evaluations conducted on five
real-world datasets substantiate the superiority of our approach over competing
baselines. We also shown that the proposed methods can achieve up to a 5.4%
increase in NDCG@20 performance via the integration of MLLM-extracted features.
Source code is available at: https://github.com/enoche/CM3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working Paper: https://github.com/enoche/CM3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conquering High Packet-Loss Erasure: MoE Swin <span class="highlight-title">Transformer</span>-Based Video
  Semantic Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Teng, Senran Fan, Chen Dong, Haotai Liang, Zhicheng Bao, Xiaodong Xu, Rui Meng, Ping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic communication with joint semantic-channel coding robustly transmits
diverse data modalities but faces challenges in mitigating semantic information
loss due to packet drops in packet-based systems. Under current protocols,
packets with errors are discarded, preventing the receiver from utilizing
erroneous semantic data for robust decoding. To address this issue, a
packet-loss-resistant MoE Swin Transformer-based Video Semantic Communication
(MSTVSC) system is proposed in this paper. Semantic vectors are encoded by
MSTVSC and transmitted through upper-layer protocol packetization. To
investigate the impact of the packetization, a theoretical analysis of the
packetization strategy is provided. To mitigate the semantic loss caused by
packet loss, a 3D CNN at the receiver recovers missing information using
un-lost semantic data and an packet-loss mask matrix. Semantic-level
interleaving is employed to reduce concentrated semantic loss from packet
drops. To improve compression, a common-individual decomposition approach is
adopted, with downsampling applied to individual information to minimize
redundancy. The model is lightweighted for practical deployment. Extensive
simulations and comparisons demonstrate strong performance, achieving an
MS-SSIM greater than 0.6 and a PSNR exceeding 20 dB at a 90% packet loss rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Han, Beier Zhu, Yanlong Xu, Peipei Song, Xun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their strong performance in multimodal emotion reasoning, existing
Multimodal Large Language Models (MLLMs) often overlook the scenarios involving
emotion conflicts, where emotional cues from different modalities are
inconsistent. To fill this gap, we first introduce CA-MER, a new benchmark
designed to examine MLLMs under realistic emotion conflicts. It consists of
three subsets: video-aligned, audio-aligned, and consistent, where only one or
all modalities reflect the true emotion. However, evaluations on our CA-MER
reveal that current state-of-the-art emotion MLLMs systematically over-rely on
audio signal during emotion conflicts, neglecting critical cues from visual
modality. To mitigate this bias, we propose MoSEAR, a parameter-efficient
framework that promotes balanced modality integration. MoSEAR consists of two
modules: (1)MoSE, modality-specific experts with a regularized gating mechanism
that reduces modality bias in the fine-tuning heads; and (2)AR, an attention
reallocation mechanism that rebalances modality contributions in frozen
backbones during inference. Our framework offers two key advantages: it
mitigates emotion conflicts and improves performance on consistent
samples-without incurring a trade-off between audio and visual modalities.
Experiments on multiple benchmarks-including MER2023, EMER, DFEW, and our
CA-MER-demonstrate that MoSEAR achieves state-of-the-art performance,
particularly under modality conflict conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Multimedia 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-based Interaction Augmentation Network for Robust Multimodal
  Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hu Zhangfeng, Shi mengxin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inevitable modality imperfection in real-world scenarios poses
significant challenges for Multimodal Sentiment Analysis (MSA). While existing
methods tailor reconstruction or joint representation learning strategies to
restore missing semantics, they often overlook complex dependencies within and
across modalities. Consequently, they fail to fully leverage available
modalities to capture complementary semantics. To this end, this paper proposes
a novel graph-based framework to exploit both intra- and inter-modality
interactions, enabling imperfect samples to derive missing semantics from
complementary parts for robust MSA. Specifically, we first devise a learnable
hypergraph to model intra-modality temporal dependencies to exploit contextual
information within each modality. Then, a directed graph is employed to explore
inter-modality correlations based on attention mechanism, capturing
complementary information across different modalities. Finally, the knowledge
from perfect samples is integrated to supervise our interaction processes,
guiding the model toward learning reliable and robust joint representations.
Extensive experiments on MOSI and MOSEI datasets demonstrate the effectiveness
of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized
  Diffusion-based Voice Cloning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12332v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12332v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianyue Hu, Junyan Wu, Wei Lu, Xiangyang Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Models (DMs) have achieved remarkable success in realistic voice
cloning (VC), while they also increase the risk of malicious misuse. Existing
proactive defenses designed for traditional VC models aim to disrupt the
forgery process, but they have been proven incompatible with DMs due to the
intricate generative mechanisms of diffusion. To bridge this gap, we introduce
VoiceCloak, a multi-dimensional proactive defense framework with the goal of
obfuscating speaker identity and degrading perceptual quality in potential
unauthorized VC. To achieve these goals, we conduct a focused analysis to
identify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt
the cloning process by introducing adversarial perturbations into the reference
audio. Specifically, to obfuscate speaker identity, VoiceCloak first targets
speaker identity by distorting representation learning embeddings to maximize
identity variation, which is guided by auditory perception principles.
Additionally, VoiceCloak disrupts crucial conditional guidance processes,
particularly attention context, thereby preventing the alignment of vocal
characteristics that are essential for achieving convincing cloning. Then, to
address the second objective, VoiceCloak introduces score magnitude
amplification to actively steer the reverse trajectory away from the generation
of high-quality speech. Noise-guided semantic corruption is further employed to
disrupt structural speech semantics captured by DMs, degrading output quality.
Extensive experiments highlight VoiceCloak's outstanding defense success rate
against unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak
are available at https://voice-cloak.github.io/VoiceCloak/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIP Brings Better Features to Visual Aesthetics Learners <span class="chip">ICME 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liwu Xu, Jinjin Xu, Yuzhe Yang, Xilu Wang, Yijie Huang, Yaqian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Aesthetics Assessment (IAA) is a challenging task due to its subjective
nature and expensive manual annotations. Recent large-scale vision-language
models, such as Contrastive Language-Image Pre-training (CLIP), have shown
their promising representation capability for various downstream tasks.
However, the application of CLIP to resource-constrained and low-data IAA tasks
remains limited. While few attempts to leverage CLIP in IAA have mainly focused
on carefully designed prompts, we extend beyond this by allowing models from
different domains and with different model sizes to acquire knowledge from
CLIP. To achieve this, we propose a unified and flexible two-phase CLIP-based
Semi-supervised Knowledge Distillation (CSKD) paradigm, aiming to learn a
lightweight IAA model while leveraging CLIP's strong generalization capability.
Specifically, CSKD employs a feature alignment strategy to facilitate the
distillation of heterogeneous CLIP teacher and IAA student models, effectively
transferring valuable features from pre-trained visual representations to two
lightweight IAA models, respectively. To efficiently adapt to downstream IAA
tasks in a low-data regime, the two strong visual aesthetics learners then
conduct distillation with unlabeled examples for refining and transferring the
task-specific knowledge collaboratively. Extensive experiments demonstrate that
the proposed CSKD achieves state-of-the-art performance on multiple widely used
IAA benchmarks. Furthermore, analysis of attention distance and entropy before
and after feature alignment shows the effective transfer of CLIP's feature
representation to IAA models, which not only provides valuable guidance for the
model initialization of IAA but also enhances the aesthetic feature
representation of IAA models. Code will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICME 2025 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LAVA: Language Driven Scalable and Versatile Traffic Video Analytics <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.19821v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.19821v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanrui Yu, Tianfei Zhou, Jiaxin Sun, Lianpeng Qiao, Lizhong Ding, Ye Yuan, Guoren Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern urban environments, camera networks generate massive amounts of
operational footage -- reaching petabytes each day -- making scalable video
analytics essential for efficient processing. Many existing approaches adopt an
SQL-based paradigm for querying such large-scale video databases; however, this
constrains queries to rigid patterns with predefined semantic categories,
significantly limiting analytical flexibility. In this work, we explore a
language-driven video analytics paradigm aimed at enabling flexible and
efficient querying of high-volume video data driven by natural language.
Particularly, we build \textsc{Lava}, a system that accepts natural language
queries and retrieves traffic targets across multiple levels of granularity and
arbitrary categories. \textsc{Lava} comprises three main components: 1) a
multi-armed bandit-based efficient sampling method for video segment-level
localization;
  2) a video-specific open-world detection module for object-level retrieval;
and 3) a long-term object trajectory extraction scheme for temporal object
association, yielding complete trajectories for object-of-interests. To support
comprehensive evaluation, we further develop a novel benchmark by providing
diverse, semantically rich natural language predicates and fine-grained
annotations for multiple videos. Experiments on this benchmark demonstrate that
\textsc{Lava} improves $F_1$-scores for selection queries by $\mathbf{14\%}$,
reduces MPAE for aggregation queries by $\mathbf{0.39}$, and achieves top-$k$
precision of $\mathbf{86\%}$, while processing videos $ \mathbf{9.6\times} $
faster than the most accurate baseline. Our code and dataset are available at
https://github.com/yuyanrui/LAVA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2025, code: https://github.com/yuyanrui/LAVA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anti-Inpainting: A Proactive Defense Approach against Malicious
  Diffusion-based Inpainters under Unknown Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13023v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13023v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimao Guo, Zuomin Qu, Wei Lu, Xiangyang Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing prevalence of diffusion-based malicious image
manipulation, existing proactive defense methods struggle to safeguard images
against tampering under unknown conditions. To address this, we propose
Anti-Inpainting, a proactive defense approach that achieves protection
comprising three novel modules. First, we introduce a multi-level deep feature
extractor to obtain intricate features from the diffusion denoising process,
enhancing protective effectiveness. Second, we design a multi-scale,
semantic-preserving data augmentation technique to enhance the transferability
of adversarial perturbations across unknown conditions. Finally, we propose a
selection-based distribution deviation optimization strategy to bolster
protection against manipulations guided by diverse random seeds. Extensive
experiments on InpaintGuardBench and CelebA-HQ demonstrate that Anti-Inpainting
effectively defends against diffusion-based inpainters under unknown
conditions. Additionally, our approach demonstrates robustness against various
image purification methods and transferability across different diffusion model
versions.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-08-01T00:00:00Z">2025-08-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">21</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Domain Web Information Extraction at Pinterest 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Farag, Patrick Halina, Andrey Zaytsev, Alekhya Munagala, Imtihan Ahmed, Junhao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The internet offers a massive repository of unstructured information, but
it's a significant challenge to convert this into a structured format. At
Pinterest, the ability to accurately extract structured product data from
e-commerce websites is essential to enhance user experiences and improve
content distribution. In this paper, we present Pinterest's system for
attribute extraction, which achieves remarkable accuracy and scalability at a
manageable cost. Our approach leverages a novel webpage representation that
combines structural, visual, and text modalities into a compact form,
optimizing it for small model learning. This representation captures each
visible HTML node with its text, style and layout information. We show how this
allows simple models such as eXtreme Gradient Boosting (XGBoost) to extract
attributes more accurately than much more complex Large Language Models (LLMs)
such as Generative Pre-trained Transformer (GPT). Our results demonstrate a
system that is highly scalable, processing over 1,000 URLs per second, while
being 1000 times more cost-effective than the cheapest GPT alternatives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Cold Start For next-article Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Elgohary, Nathan Jorgenson, Trenton Marple
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This replication study modifies ALMM, the Adaptive Linear Mapping Model
constructed for the next song recommendation, to the news recommendation
problem on the MIND dataset. The original version of ALMM computes latent
representations for users, last-time items, and current items in a tensor
factorization structure and learns a linear mapping from content features to
latent item vectors. Our replication aims to improve recommendation performance
in cold-start scenarios by restructuring this model to sequential news click
behavior, viewing consecutively read articles as (last news, next news) tuples.
Instead of the original audio features, we apply BERT and a TF-IDF (Term
Frequency-Inverse Document Frequency) to news titles and abstracts to extract
token contextualized representations and align them with triplet-based user
reading patterns. We also propose a reproducibly thorough pre-processing
pipeline combining news filtering and feature integrity validation. Our
implementation of ALMM with TF-IDF shows relatively improved recommendation
accuracy and robustness over Forbes and Oord baseline models in the cold-start
scenario. We demonstrate that ALMM in a minimally modified state is not
suitable for next news recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqun Chen, Erhan Zhang, Lingyong Yan, Shuaiqiang Wang, Jizhou Huang, Dawei Yin, Jiaxin Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has
become pivotal in enhancing response accuracy and reducing hallucination
issues. The architecture of RAG systems varies significantly, encompassing
single-round RAG, iterative RAG, and reasoning RAG, each tailored to address
different types of queries. Due to the varying complexity of real-world
queries, a fixed RAG pipeline often struggles to balance performance and cost
efficiency across different queries. To address this challenge, we propose an
adaptive RAG framework called MAO-ARAG, which leverages multi-agent
orchestration. Our adaptive RAG is conceived as a multi-turn framework.
Specifically, we define multiple executor agents, representing typical RAG
modules such as query reformulation agents, document selection agent, and
generation agents. A planner agent intelligently selects and integrates the
appropriate agents from these executors into a suitable workflow tailored for
each query, striving for high-quality answers while maintaining reasonable
costs. During each turn, the planner agent is trained using reinforcement
learning, guided by an outcome-based reward (F1 score) and a cost-based
penalty, continuously improving answer quality while keeping costs within a
reasonable range. Experiments conducted on multiple QA datasets demonstrate
that our approach, which dynamically plans workflows for each query, not only
achieves high answer quality but also maintains both cost and latency within
acceptable limits.The code of MAO-ARAG is on
https://github.com/chenyiqun/Agentic-RAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing the Power of Interleaving and Counterfactual Evaluation for
  Airbnb Search Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Zhang, Alex Deng, Michelle Du, Huiji Gao, Liwei He, Sanjeev Katariya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluation plays a crucial role in the development of ranking algorithms on
search and recommender systems. It enables online platforms to create
user-friendly features that drive commercial success in a steady and effective
manner. The online environment is particularly conducive to applying causal
inference techniques, such as randomized controlled experiments (known as A/B
test), which are often more challenging to implement in fields like medicine
and public policy. However, businesses face unique challenges when it comes to
effective A/B test. Specifically, achieving sufficient statistical power for
conversion-based metrics can be time-consuming, especially for significant
purchases like booking accommodations. While offline evaluations are quicker
and more cost-effective, they often lack accuracy and are inadequate for
selecting candidates for A/B test. To address these challenges, we developed
interleaving and counterfactual evaluation methods to facilitate rapid online
assessments for identifying the most promising candidates for A/B tests. Our
approach not only increased the sensitivity of experiments by a factor of up to
100 (depending on the approach and metrics) compared to traditional A/B testing
but also streamlined the experimental process. The practical insights gained
from usage in production can also benefit organizations with similar interests.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Experimental Evaluation of Dynamic Topic Modeling Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ngozichukwuka Onah, Nadine Steinmetz, Hani Al-Sayeh, Kai-Uwe Sattler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The amount of text generated daily on social media is gigantic and analyzing
this text is useful for many purposes. To understand what lies beneath a huge
amount of text, we need dependable and effective computing techniques from
self-powered topic models. Nevertheless, there are currently relatively few
thorough quantitative comparisons between these models. In this study, we
compare these models and propose an assessment metric that documents how the
topics change in time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian
  Common Law System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Kumar Nigam, Balaramamahanthi Deepak Patnaik, Shivam Mishra, Ajay Varghese Thomas, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,
aiming to automate judicial outcome forecasting and enhance interpretability in
legal reasoning. While previous approaches in the Indian context have relied on
internal case content such as facts, issues, and reasoning, they often overlook
a core element of common law systems, which is reliance on statutory provisions
and judicial precedents. In this work, we propose NyayaRAG, a
Retrieval-Augmented Generation (RAG) framework that simulates realistic
courtroom scenarios by providing models with factual case descriptions,
relevant legal statutes, and semantically retrieved prior cases. NyayaRAG
evaluates the effectiveness of these combined inputs in predicting court
decisions and generating legal explanations using a domain-specific pipeline
tailored to the Indian legal system. We assess performance across various input
configurations using both standard lexical and semantic metrics as well as
LLM-based evaluators such as G-Eval. Our results show that augmenting factual
inputs with structured legal knowledge significantly improves both predictive
accuracy and explanation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segment First, Retrieve Better: Realistic Legal Search via Rhetorical
  Role-Based Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Kumar Nigam, Tanmay Dubey, Noel Shallum, Arnab Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal precedent retrieval is a cornerstone of the common law system, governed
by the principle of stare decisis, which demands consistency in judicial
decisions. However, the growing complexity and volume of legal documents
challenge traditional retrieval methods. TraceRetriever mirrors real-world
legal search by operating with limited case information, extracting only
rhetorically significant segments instead of requiring complete documents. Our
pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining
initial results through Reciprocal Rank Fusion before final re-ranking.
Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier
trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,
TraceRetriever addresses growing document volume challenges while aligning with
practical search constraints, reliable and scalable foundation for precedent
retrieval enhancing legal research when only partial case knowledge is
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-based Motion Retrieval using Open Vocabulary Methods for
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Englmeier, Max A. Büttner, Katharina Winter, Fabian B. Flohr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving systems must operate reliably in safety-critical
scenarios, particularly those involving unusual or complex behavior by
Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets
is essential for robust evaluation and generalization, but retrieving such rare
human behavior scenarios within the long tail of large-scale datasets is
challenging. To support targeted evaluation of autonomous driving systems in
diverse, human-centered scenarios, we propose a novel context-aware motion
retrieval framework. Our method combines Skinned Multi-Person Linear
(SMPL)-based motion sequences and corresponding video frames before encoding
them into a shared multimodal embedding space aligned with natural language.
Our approach enables the scalable retrieval of human behavior and their context
through text queries. This work also introduces our dataset WayMoCo, an
extension of the Waymo Open Dataset. It contains automatically labeled motion
and scene context descriptions derived from generated pseudo-ground-truth SMPL
sequences and corresponding image data. Our approach outperforms
state-of-the-art models by up to 27.5% accuracy in motion-context retrieval,
when evaluated on the WayMoCo dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figure, project page
  https://iv.ee.hm.edu/contextmotionclip/, submitted to IEEE Transactions on
  Intelligent Vehicles (T-IV), This work has been submitted to the IEEE for
  possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for
  Document Question-Answering with Hierarchical Index and Multi-Granularity
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Gong, Yihua Huang, Chengcheng Mai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The multi-modal long-context document question-answering task aims to locate
and integrate multi-modal evidences (such as texts, tables, charts, images, and
layouts) distributed across multiple pages, for question understanding and
answer generation. The existing methods can be categorized into Large
Vision-Language Model (LVLM)-based and Retrieval-Augmented Generation
(RAG)-based methods. However, the former were susceptible to hallucinations,
while the latter struggled for inter-modal disconnection and cross-page
fragmentation. To address these challenges, a novel multi-modal RAG model,
named MMRAG-DocQA, was proposed, leveraging both textual and visual information
across long-range pages to facilitate accurate question answering. A
hierarchical indexing method with the integration of flattened in-page chunks
and topological cross-page chunks was designed to jointly establish in-page
multi-modal associations and long-distance cross-page dependencies. By means of
joint similarity evaluation and large language model (LLM)-based re-ranking, a
multi-granularity semantic retrieval method, including the page-level parent
page retrieval and document-level summary retrieval, was proposed to foster
multi-modal evidence connection and long-distance evidence integration and
reasoning. Experimental results performed on public datasets, MMLongBench-Doc
and LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in
understanding and answering modality-rich and multi-page documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Session-Based Recommendation with Validated and Enriched LLM Intents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyuseok Lee, Yaokun Liu, Yifan Liu, Susik Yoon, Dong Wang, SeongKu Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Session-based recommendation (SBR) aims to predict the next item for an
anonymous user in a timely manner. However, SBR suffers from data sparsity due
to the short and anonymous nature of sessions. Recently, an emerging line of
work has explored inferring the underlying user intents of a session using
large language models (LLMs), with the generated intents serving as auxiliary
training signals to enhance SBR models. Despite its promise, this approach
faces three key challenges: validating intent quality, incorporating
session-level multi-intents, and complementing inevitable LLM failure cases. In
this paper, we propose VELI4SBR, a two-stage framework that leverages Validated
and Enriched LLM-generated Intents for SBR. In the first stage, we generate
high-quality intents using a predict-and-correct loop that validates the
informativeness of LLM-generated intents with a global intent pool to constrain
the LLM's output space and reduce hallucination. In the second stage, we
enhance the SBR model using the generated intents through a lightweight
multi-intent prediction and fusion mechanism. Furthermore, we introduce a
training strategy that compensates for LLM failures by inferring intents from
inter-session behavioral similarities. Extensive experiments show that VELI4SBR
outperforms state-of-the-art baselines while improving explainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M^2VAE: Multi-Modal Multi-View Variational Autoencoder for Cold-start
  Item Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuan He, Yongchao Liu, Qiang Li, Wenliang Zhong, Chuntao Hong, Xinwei Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cold-start item recommendation is a significant challenge in recommendation
systems, particularly when new items are introduced without any historical
interaction data. While existing methods leverage multi-modal content to
alleviate the cold-start issue, they often neglect the inherent multi-view
structure of modalities, the distinction between shared and modality-specific
features. In this paper, we propose Multi-Modal Multi-View Variational
AutoEncoder (M^2VAE), a generative model that addresses the challenges of
modeling common and unique views in attribute and multi-modal features, as well
as user preferences over single-typed item features. Specifically, we generate
type-specific latent variables for item IDs, categorical attributes, and image
features, and use Product-of-Experts (PoE) to derive a common representation. A
disentangled contrastive loss decouples the common view from unique views while
preserving feature informativeness. To model user inclinations, we employ a
preference-guided Mixture-of-Experts (MoE) to adaptively fuse representations.
We further incorporate co-occurrence signals via contrastive learning,
eliminating the need for pretraining. Extensive experiments on real-world
datasets validate the effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Relevance Meets Novelty: Dual-Stable Periodic Optimization for
  Exploratory Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongxiang Lin, Hao Guo, Zeshun Li, Erpeng Xue, Yongqian He, Xiangyu Hou, Zhaoyu Hu, Lei Wang, Sheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional recommendation systems tend to trap users in strong feedback
loops by excessively pushing content aligned with their historical preferences,
thereby limiting exploration opportunities and causing content fatigue.
Although large language models (LLMs) demonstrate potential with their diverse
content generation capabilities, existing LLM-enhanced dual-model frameworks
face two major limitations: first, they overlook long-term preferences driven
by group identity, leading to biased interest modeling; second, they suffer
from static optimization flaws, as a one-time alignment process fails to
leverage incremental user data for closed-loop optimization. To address these
challenges, we propose the Co-Evolutionary Alignment (CoEA) method. For
interest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE)
module, jointly modeling long-term group identity and short-term individual
interests through parallel processing of behavioral sequences. For static
optimization limitations, we design a Periodic Collaborative Optimization (PCO)
mechanism. This mechanism regularly conducts preference verification on
incremental data using the Relevance LLM, then guides the Novelty LLM to
perform fine-tuning based on the verification results, and subsequently feeds
back the output of the incrementally fine-tuned Novelty LLM to the Relevance
LLM for re-evaluation, thereby achieving a dynamic closed-loop optimization.
Extensive online and offline experiments verify the effectiveness of the CoEA
model in exploratory recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Unified User Quantized Tokenizers for User Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuan He, Yang Chen, Wuliang Huang, Tianyi Zheng, Jianhu Chen, Bin Dou, Yice Luo, Yun Zhu, Baokun Wang, Yongchao Liu, Xing Fu, Yu Cheng, Chuntao Hong, Weiqiang Wang, Xin-Wei Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-source user representation learning plays a critical role in enabling
personalized services on web platforms (e.g., Alipay). While prior works have
adopted late-fusion strategies to combine heterogeneous data sources, they
suffer from three key limitations: lack of unified representation frameworks,
scalability and storage issues in data compression, and inflexible cross-task
generalization. To address these challenges, we propose U^2QT (Unified User
Quantized Tokenizers), a novel framework that integrates cross-domain knowledge
transfer with early fusion of heterogeneous domains. Our framework employs a
two-stage architecture: first, a causal Q-Former projects domain-specific
features into a shared causal representation space to preserve inter-modality
dependencies; second, a multi-view RQ-VAE discretizes causal embeddings into
compact tokens through shared and source-specific codebooks, enabling efficient
storage while maintaining semantic coherence. Experimental results showcase
U^2QT's advantages across diverse downstream tasks, outperforming task-specific
baselines in future behavior prediction and recommendation tasks while
achieving efficiency gains in storage and computation. The unified tokenization
framework enables seamless integration with language models and supports
industrial-scale applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Generator to Embedder: Harnessing Innate Abilities of Multimodal
  LLMs via Building Zero-Shot Discriminative Embedding Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeong-Joon Ju, Seong-Whan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have emerged as a promising solution
for universal embedding tasks, yet adapting their generative nature for
discriminative representation learning remains a significant challenge. The
dominant paradigm of large-scale contrastive pre-training suffers from critical
inefficiencies, including prohibitive computational costs and a failure to
leverage the intrinsic, instruction-following capabilities of MLLMs. To
overcome these limitations, we propose an efficient framework for universal
multimodal embeddings, which bridges this gap by centering on two synergistic
components. First, our hierarchical embedding prompt template employs a
two-level instruction architecture that forces the model to produce
discriminative representations. Building on this strong foundation, our second
component, self-aware hard negative sampling, redefines the fine-tuning process
by leveraging the model's own understanding to efficiently mine challenging
negatives while actively filtering out potential false negatives. Our
comprehensive experiments show that our hierarchical prompt achieves zero-shot
performance competitive with contrastively trained baselines and enhances the
fine-tuning process by lifting a simple in-batch negative baseline by 4.8
points on the MMEB benchmark. We further boost the performance via our
self-aware hard negative sampling, achieving the state-of-the-art performance
without the contrative pre-training. Our work presents an effective and
efficient pathway to adapt MLLMs for universal embedding tasks, significantly
reducing training time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjin Qian, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose MetaAgent, an agentic paradigm inspired by the
principle of learning-by-doing, where expertise is developed through hands-on
practice and continual self-improvement. MetaAgent starts with a minimal
workflow, equipped only with basic reasoning and adaptive help-seeking
abilities. When a knowledge gap is encountered, MetaAgent generates natural
language help requests, which are routed to the most suitable external tool by
a dedicated tool router. As MetaAgent solves tasks, it continually conducts
self-reflection and answer verification, distilling actionable experience into
concise texts that are dynamically incorporated into future task contexts.
Besides, MetaAgent autonomously builds in-house tools and a persistent
knowledge base by organizing its tool-use history, further enhancing its
ability to retrieve and integrate relevant information We term this continual,
data-driven process as \textit{meta tool learning}, through which MetaAgent
incrementally refines its reasoning and tool-use strategies, without changing
model parameters or requiring further post-training. Evaluated on challenging
knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,
MetaAgent consistently outperforms workflow-based baselines and matches or
exceeds end-to-end trained agents, demonstrating the promise of self-evolving
agentic systems for robust, general-purpose knowledge discovery. We provide our
source codes in https://github.com/qhjqhj00/MetaAgent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report, 14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distillation versus Contrastive Learning: How to Train Your Rerankers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.08336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.08336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Xu, Zhiqi Huang, Shengyao Zhuang, Vivek Srikumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training effective text rerankers is crucial for information retrieval. Two
strategies are widely used: contrastive learning (optimizing directly on
ground-truth labels) and knowledge distillation (transferring knowledge from a
larger reranker). While both have been studied extensively, a clear comparison
of their effectiveness for training cross-encoder rerankers under practical
conditions is needed.
  This paper empirically compares these strategies by training rerankers of
different sizes and architectures using both methods on the same data, with a
strong contrastive learning model acting as the distillation teacher. Our
results show that knowledge distillation generally yields better in-domain and
out-of-domain ranking performance than contrastive learning when distilling
from a larger teacher model. This finding is consistent across student model
sizes and architectures. However, distilling from a teacher of the same
capacity does not provide the same advantage, particularly for out-of-domain
tasks. These findings offer practical guidance for choosing a training strategy
based on available teacher models. We recommend using knowledge distillation to
train smaller rerankers if a larger, more powerful teacher is accessible; in
its absence, contrastive learning remains a robust baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JurisTCU: A Brazilian Portuguese Information Retrieval <span class="highlight-title">Dataset</span> with
  Query Relevance Judgments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leandro Carísio Fernandes, Leandro dos Santos Ribeiro, Marcos Vinícius Borela de Castro, Leonardo Augusto da Silva Pacheco, Edans Flávius de Oliveira Sandes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces JurisTCU, a Brazilian Portuguese dataset for legal
information retrieval (LIR). The dataset is freely available and consists of
16,045 jurisprudential documents from the Brazilian Federal Court of Accounts,
along with 150 queries annotated with relevance judgments. It addresses the
scarcity of Portuguese-language LIR datasets with query relevance annotations.
The queries are organized into three groups: real user keyword-based queries,
synthetic keyword-based queries, and synthetic question-based queries.
Relevance judgments were produced through a hybrid approach combining LLM-based
scoring with expert domain validation. We used JurisTCU in 14 experiments using
lexical search (document expansion methods) and semantic search (BERT-based and
OpenAI embeddings). We show that the document expansion methods significantly
improve the performance of standard BM25 search on this dataset, with
improvements exceeding 45% in P@10, R@10, and nDCG@10 metrics when evaluating
short keyword-based queries. Among the embedding models, the OpenAI models
produced the best results, with improvements of approximately 70% in P@10,
R@10, and nDCG@10 metrics for short keyword-based queries, suggesting that
these dense embeddings capture semantic relationships in this domain,
surpassing the reliance on lexical terms. Besides offering a dataset for the
Portuguese-language IR research community, suitable for evaluating search
systems, the results also contribute to enhancing a search system highly
relevant to Brazilian citizens.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RecPS: Privacy Risk Scoring for Recommender Systems <span class="chip">RecSys 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajie He, Yuechun Gu, Keke Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RecSys) have become an essential component of many web
applications. The core of the system is a recommendation model trained on
highly sensitive user-item interaction data. While privacy-enhancing techniques
are actively studied in the research community, the real-world model
development still depends on minimal privacy protection, e.g., via controlled
access. Users of such systems should have the right to choose \emph{not} to
share highly sensitive interactions. However, there is no method allowing the
user to know which interactions are more sensitive than others. Thus,
quantifying the privacy risk of RecSys training data is a critical step to
enabling privacy-aware RecSys model development and deployment. We propose a
membership-inference attack (MIA)- based privacy scoring method, RecPS, to
measure privacy risks at both the interaction and user levels. The RecPS
interaction-level score definition is motivated and derived from differential
privacy, which is then extended to the user-level scoring method. A critical
component is the interaction-level MIA method RecLiRA, which gives high-quality
membership estimation. We have conducted extensive experiments on well-known
benchmark datasets and RecSys models to show the unique features and benefits
of RecPS scoring in risk assessment and RecSys model unlearning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM RecSys 2025; to appear</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Think Before Recommend: Unleashing the Latent Reasoning Power for
  Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.22675v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.22675v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiakai Tang, Sunhao Dai, Teng Shi, Jun Xu, Xu Chen, Wen Chen, Jian Wu, Yuning Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommendation (SeqRec) aims to predict the next item by capturing
sequential patterns from users' historical interactions, playing a crucial role
in many real-world recommender systems. However, existing approaches
predominantly adopt a direct forward computation paradigm, where the final
hidden state of the sequence encoder serves as the user representation. We
argue that this inference paradigm, due to its limited computational depth,
struggles to model the complex evolving nature of user preferences and lacks a
nuanced understanding of long-tail items, leading to suboptimal performance. To
address this issue, we propose \textbf{ReaRec}, the first inference-time
computing framework for recommender systems, which enhances user
representations through implicit multi-step reasoning. Specifically, ReaRec
autoregressively feeds the sequence's last hidden state into the sequential
recommender while incorporating special reasoning position embeddings to
decouple the original item encoding space from the multi-step reasoning space.
Moreover, we introduce two lightweight reasoning-based learning methods,
Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to
further effectively exploit ReaRec's reasoning potential. Extensive experiments
on five public real-world datasets and different SeqRec architectures
demonstrate the generality and effectiveness of our proposed ReaRec.
Remarkably, post-hoc analyses reveal that ReaRec significantly elevates the
performance ceiling of multiple sequential recommendation backbones by
approximately 30\%-50\%. Thus, we believe this work can open a new and
promising avenue for future research in inference-time computing for sequential
recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe machine learning model release from Trusted Research Environments:
  The SACRO-ML package 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01233v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01233v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jim Smith, Richard J. Preen, Andrew McCarthy, Maha Albashir, Alba Crespi-Boixader, Shahzad Mumtaz, Christian Cole, James Liley, Jost Migenda, Simon Rogers, Yola Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SACRO-ML, an integrated suite of open source Python tools to
facilitate the statistical disclosure control (SDC) of machine learning (ML)
models trained on confidential data prior to public release. SACRO-ML combines
(i) a SafeModel package that extends commonly used ML models to provide
ante-hoc SDC by assessing the vulnerability of disclosure posed by the training
regime; and (ii) an Attacks package that provides post-hoc SDC by rigorously
assessing the empirical disclosure risk of a model through a variety of
simulated attacks after training. The SACRO-ML code and documentation are
available under an MIT license at https://github.com/AI-SDC/SACRO-ML
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs
  through Multi-query Parallelism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02962v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02962v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwen Tan, Jiaming Huang, Qintong Wu, Hongxuan Zhang, Chenyi Zhuang, Jinjie Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks, while LLMs remain prone to generating hallucinated or outdated
responses due to their static internal knowledge. Recent advancements in
Retrieval-Augmented Generation (RAG) methods have aimed to enhance models'
search and reasoning capabilities through reinforcement learning (RL). Although
these methods demonstrate promising results, they face challenges in training
stability and encounter issues such as substantial inference time and
restricted capabilities due to reliance on single-query mode. In this paper, we
propose RAG-R1, a novel training framework designed to enable LLMs to
adaptively leverage internal and external knowledge during the reasoning
process. We further expand the generation and retrieval processes within the
framework from single-query mode to multi-query parallelism, with the aim of
reducing inference time and enhancing the model's capabilities. Extensive
experiments on seven question-answering benchmarks demonstrate that our method
outperforms the strongest baseline by up to 13.2% and decreases inference time
by 11.1%.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware
  Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kien T. Pham, Yingqing He, Yazhou Xing, Qifeng Chen, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven video generation aims to synthesize realistic videos that align
with input audio recordings, akin to the human ability to visualize scenes from
auditory input. However, existing approaches predominantly focus on exploring
semantic information, such as the classes of sounding sources present in the
audio, limiting their ability to generate videos with accurate content and
spatial composition. In contrast, we humans can not only naturally identify the
semantic categories of sounding sources but also determine their deeply encoded
spatial attributes, including locations and movement directions. This useful
information can be elucidated by considering specific spatial indicators
derived from the inherent physical properties of sound, such as loudness or
frequency. As prior methods largely ignore this factor, we present SpA2V, the
first framework explicitly exploits these spatial auditory cues from audios to
generate videos with high semantic and spatial correspondence. SpA2V decomposes
the generation process into two stages: 1) Audio-guided Video Planning: We
meticulously adapt a state-of-the-art MLLM for a novel task of harnessing
spatial and semantic cues from input audio to construct Video Scene Layouts
(VSLs). This serves as an intermediate representation to bridge the gap between
the audio and video modalities. 2) Layout-grounded Video Generation: We develop
an efficient and effective approach to seamlessly integrate VSLs as conditional
guidance into pre-trained diffusion models, enabling VSL-grounded video
generation in a training-free manner. Extensive experiments demonstrate that
SpA2V excels in generating realistic videos with semantic and spatial alignment
to the input audios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 33rd ACM Multimedia Conference (MM '25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexia Jolicoeur-Martineau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While AI excels at generating text, audio, images, and videos, creating
interactive audio-visual content such as video games remains challenging.
Current LLMs can generate JavaScript games and animations, but lack automated
evaluation metrics and struggle with complex content that normally requires
teams of humans working for many months (multi-shot, multi-agents) using assets
made by artists. To tackle these issues, we built a new metric and a
multi-agent system.
  We propose AVR-Eval, a relative metric for multimedia content quality using
Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video,
and audio) compares the AVRs of two contents, with a text model reviewing
evaluations to determine superiority. We show that AVR-Eval properly identifies
good from broken or mismatched content.
  We built AVR-Agent, a multi-agent system generating JavaScript code from a
bank of multimedia assets (audio, images, 3D models). The coding agent selects
relevant assets, generates multiple initial codes, uses AVR-Eval to identify
the best version, and iteratively improves it through omni-modal agent feedback
from the AVR.
  We run experiments on games and animations with AVR-Eval (win rate of content
A against B). We find that content generated by AVR-Agent has a significantly
higher win rate against content made through one-shot generation. However,
models struggle to leverage custom assets and AVR feedback effectively, showing
no higher win rate. This reveals a critical gap: while humans benefit from
high-quality assets and audio-visual feedback, current coding models do not
seem to utilize these resources as effectively, highlighting fundamental
differences between human and machine content creation approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for
  Document Question-Answering with Hierarchical Index and Multi-Granularity
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Gong, Yihua Huang, Chengcheng Mai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The multi-modal long-context document question-answering task aims to locate
and integrate multi-modal evidences (such as texts, tables, charts, images, and
layouts) distributed across multiple pages, for question understanding and
answer generation. The existing methods can be categorized into Large
Vision-Language Model (LVLM)-based and Retrieval-Augmented Generation
(RAG)-based methods. However, the former were susceptible to hallucinations,
while the latter struggled for inter-modal disconnection and cross-page
fragmentation. To address these challenges, a novel multi-modal RAG model,
named MMRAG-DocQA, was proposed, leveraging both textual and visual information
across long-range pages to facilitate accurate question answering. A
hierarchical indexing method with the integration of flattened in-page chunks
and topological cross-page chunks was designed to jointly establish in-page
multi-modal associations and long-distance cross-page dependencies. By means of
joint similarity evaluation and large language model (LLM)-based re-ranking, a
multi-granularity semantic retrieval method, including the page-level parent
page retrieval and document-level summary retrieval, was proposed to foster
multi-modal evidence connection and long-distance evidence integration and
reasoning. Experimental results performed on public datasets, MMLongBench-Doc
and LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in
understanding and answering modality-rich and multi-page documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instruction-Grounded Visual Projectors for Continual Learning of
  Generative Vision-Language Models <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyundong Jin, Hyung Jin Chang, Eunwoo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning enables pre-trained generative vision-language models
(VLMs) to incorporate knowledge from new tasks without retraining data from
previous ones. Recent methods update a visual projector to translate visual
information for new tasks, connecting pre-trained vision encoders with large
language models. However, such adjustments may cause the models to prioritize
visual inputs over language instructions, particularly learning tasks with
repetitive types of textual instructions. To address the neglect of language
instructions, we propose a novel framework that grounds the translation of
visual information on instructions for language models. We introduce a mixture
of visual projectors, each serving as a specialized visual-to-language
translation expert based on the given instruction context to adapt to new
tasks. To avoid using experts for irrelevant instruction contexts, we propose
an expert recommendation strategy that reuses experts for tasks similar to
those previously learned. Additionally, we introduce expert pruning to
alleviate interference from the use of experts that cumulatively activated in
previous tasks. Extensive experiments on diverse vision-language tasks
demonstrate that our method outperforms existing continual learning approaches
by generating instruction-following responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CountingFruit: Language-Guided 3D Fruit Counting with Semantic Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengze Li, Yangle Liu, Jieming Ma, Hai-Ning Liang, Yaochun Shen, Huangxiang Li, Zhijing Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate 3D fruit counting in orchards is challenging due to heavy occlusion,
semantic ambiguity between fruits and surrounding structures, and the high
computational cost of volumetric reconstruction. Existing pipelines often rely
on multi-view 2D segmentation and dense volumetric sampling, which lead to
accumulated fusion errors and slow inference. We introduce FruitLangGS, a
language-guided 3D fruit counting framework that reconstructs orchard-scale
scenes using an adaptive-density Gaussian Splatting pipeline with radius-aware
pruning and tile-based rasterization, enabling scalable 3D representation.
During inference, compressed CLIP-aligned semantic vectors embedded in each
Gaussian are filtered via a dual-threshold cosine similarity mechanism,
retrieving Gaussians relevant to target prompts while suppressing common
distractors (e.g., foliage), without requiring retraining or image-space masks.
The selected Gaussians are then sampled into dense point clouds and clustered
geometrically to estimate fruit instances, remaining robust under severe
occlusion and viewpoint variation. Experiments on nine different orchard-scale
datasets demonstrate that FruitLangGS consistently outperforms existing
pipelines in instance counting recall, avoiding multi-view segmentation fusion
errors and achieving up to 99.2\% recall on Fuji-SfM orchard dataset. Ablation
studies further confirm that language-conditioned semantic embedding and
dual-threshold prompt filtering are essential for suppressing distractors and
improving counting accuracy under heavy occlusion. Beyond fruit counting, the
same framework enables prompt-driven 3D semantic retrieval without retraining,
highlighting the potential of language-guided 3D perception for scalable
agricultural scene understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Catching Dark Signals in Algorithms: Unveiling Audiovisual and Thematic
  Markers of Unsafe Content Recommended for Children and Teenagers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.12571v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.12571v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoning Xue, Brian Nishimine, Martin Hilbert, Drew Cingel, Samantha Vigil, Jane Shawcroft, Arti Thakur, Zubair Shafiq, Jingwen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prevalence of short form video platforms, combined with the
ineffectiveness of age verification mechanisms, raises concerns about the
potential harms facing children and teenagers in an algorithm-moderated online
environment. We conducted multimodal feature analysis and thematic topic
modeling of 4,492 short videos recommended to children and teenagers on
Instagram Reels, TikTok, and YouTube Shorts, collected as a part of an
algorithm auditing experiment. This feature-level and content-level analysis
revealed that unsafe (i.e., problematic, mentally distressing) short videos (a)
possess darker visual features and (b) contain explicitly harmful content and
implicit harm from anxiety-inducing ordinary content. We introduce a useful
framework of online harm (i.e., explicit, implicit, unintended), providing a
unique lens for understanding the dynamic, multifaceted online risks facing
children and teenagers. The findings highlight the importance of protecting
younger audiences in critical developmental stages from both explicit and
implicit risks on social media, calling for nuanced content moderation, age
verification, and platform regulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ User Digital Twin-Driven Video Streaming for Customized Preferences and
  Adaptive Transcoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09766v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09766v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen Jimmy, Kalkidan Berhane, Kevin Muhammad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving field of multimedia services, video streaming has
become increasingly prevalent, demanding innovative solutions to enhance user
experience and system efficiency. This paper introduces a novel approach that
integrates user digital twins-a dynamic digital representation of a user's
preferences and behaviors-with traditional video streaming systems. We explore
the potential of this integration to dynamically adjust video preferences and
optimize transcoding processes according to real-time data. The methodology
leverages advanced machine learning algorithms to continuously update the
user's digital twin, which in turn informs the transcoding service to adapt
video parameters for optimal quality and minimal buffering. Experimental
results show that our approach not only improves the personalization of content
delivery but also significantly enhances the overall efficiency of video
streaming services by reducing bandwidth usage and improving video playback
quality. The implications of such advancements suggest a shift towards more
adaptive, user-centric multimedia services, potentially transforming how video
content is consumed and delivered.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: This paper has been withdrawn by arXiv due to
  disputed and unverifiable authorship and affiliation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models
  for Wireless Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05695v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05695v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijiang Yan, Jianhua Pei, Hongda Wu, Hina Tabassum, Ping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel Semantic Communication (SemCom) framework for
real-time adaptive-bitrate video streaming by integrating Latent Diffusion
Models (LDMs) within the FFmpeg techniques. This solution addresses the
challenges of high bandwidth usage, storage inefficiencies, and quality of
experience (QoE) degradation associated with traditional Constant Bitrate
Streaming (CBS) and Adaptive Bitrate Streaming (ABS). The proposed approach
leverages LDMs to compress I-frames into a latent space, offering significant
storage and semantic transmission savings without sacrificing high visual
quality. While retaining B-frames and P-frames as adjustment metadata to
support efficient refinement of video reconstruction at the user side, the
proposed framework further incorporates state-of-the-art denoising and Video
Frame Interpolation (VFI) techniques. These techniques mitigate semantic
ambiguity and restore temporal coherence between frames, even in noisy wireless
communication environments. Experimental results demonstrate the proposed
method achieves high-quality video streaming with optimized bandwidth usage,
outperforming state-of-the-art solutions in terms of QoE and resource
efficiency. This work opens new possibilities for scalable real-time video
streaming in 5G and future post-5G networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE Wireless Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15066v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15066v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyuan Yang, Zichuan Liu, Lei Song, Kai Ying, Zhiguang Wang, Tom Bamford, Svitlana Vyetrenko, Jiang Bian, Qingsong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series anomaly detection is critical across various domains, yet current
approaches often limit analysis to mere binary anomaly classification without
detailed categorization or further explanatory reasoning. To address these
limitations, we propose a novel task, Time-series Reasoning for Anomaly
(Time-RA) that transforms classical time series anomaly detection from a
discriminative into a generative, reasoning-intensive task leveraging Large
Language Models (LLMs). Also, we introduce the first real-world multimodal
benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,
comprising approximately 40,000 samples across 10 real-world domains. Each
sample includes numeric time series data, contextual text information, and
visual representations, each annotated with fine-grained categories (14 types
for univariate anomalies and 6 for multivariate anomalies) and structured
explanatory reasoning. We develop a sophisticated annotation framework
utilizing ensemble-generated labels refined through GPT-4-driven feedback,
ensuring accuracy and interpretability. Extensive benchmarking of LLMs and
multimodal LLMs demonstrates the capabilities and limitations of current
models, highlighting the critical role of supervised fine-tuning. Our dataset
and task pave the way for significant advancements in interpretable time series
anomaly detection and reasoning. The code
(https://github.com/yyysjz1997/Time-RA) and dataset
(https://huggingface.co/datasets/Time-RA/RATs40K) have been fully open-sourced
to support and accelerate future research in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review. 19 pages, 8 figures, 12 tables. Code and dataset are
  publicly available</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Querying Autonomous Vehicle Point Clouds: Enhanced by 3D Object Counting
  with CounterNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.19209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.19209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Zhang, Zhifeng Bao, Hai Dong, Ziwei Wang, Jiajun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles generate massive volumes of point cloud data, yet only a
subset is relevant for specific tasks such as collision detection, traffic
analysis, or congestion monitoring. Effectively querying this data is essential
to enable targeted analytics. In this work, we formalize point cloud querying
by defining three core query types: RETRIEVAL, COUNT, and AGGREGATION, each
aligned with distinct analytical scenarios. All these queries rely heavily on
accurate object counts to produce meaningful results, making precise object
counting a critical component of query execution. Prior work has focused on
indexing techniques for 2D video data, assuming detection models provide
accurate counting information. However, when applied to 3D point cloud data,
state-of-the-art detection models often fail to generate reliable object
counts, leading to substantial errors in query results. To address this
limitation, we propose CounterNet, a heatmap-based network designed for
accurate object counting in large-scale point cloud data. Rather than focusing
on accurate object localization, CounterNet detects object presence by finding
object centers to improve counting accuracy. We further enhance its performance
with a feature map partitioning strategy using overlapping regions, enabling
better handling of both small and large objects in complex traffic scenes. To
adapt to varying frame characteristics, we introduce a per-frame dynamic model
selection strategy that selects the most effective configuration for each
input. Evaluations on three real-world autonomous vehicle datasets show that
CounterNet improves counting accuracy by 5% to 20% across object categories,
resulting in more reliable query outcomes across all supported query types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking
  Portrait <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01064v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01064v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taekyung Ki, Dongchan Min, Gyeongsu Chae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of diffusion-based generative models, portrait
image animation has achieved remarkable results. However, it still faces
challenges in temporally consistent video generation and fast sampling due to
its iterative sampling nature. This paper presents FLOAT, an audio-driven
talking portrait video generation method based on flow matching generative
model. Instead of a pixel-based latent space, we take advantage of a learned
orthogonal motion latent space, enabling efficient generation and editing of
temporally consistent motion. To achieve this, we introduce a transformer-based
vector field predictor with an effective frame-wise conditioning mechanism.
Additionally, our method supports speech-driven emotion enhancement, enabling a
natural incorporation of expressive motions. Extensive experiments demonstrate
that our method outperforms state-of-the-art audio-driven talking portrait
methods in terms of visual quality, motion fidelity, and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025. Project page:
  https://deepbrainai-research.github.io/float/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-07-31T00:00:00Z">2025-07-31</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">22</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio Prototypical Network For Controllable Music Recommendation <span class="chip">SP2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fırat Öncel, Emiliano Penaloza, Haolun Wu, Shubham Gupta, Mirco Ravanelli, Laurent Charlin, Cem Subakan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional recommendation systems represent user preferences in dense
representations obtained through black-box encoder models. While these models
often provide strong recommendation performance, they lack interpretability for
users, leaving users unable to understand or control the system's modeling of
their preferences. This limitation is especially challenging in music
recommendation, where user preferences are highly personal and often evolve
based on nuanced qualities like mood, genre, tempo, or instrumentation. In this
paper, we propose an audio prototypical network for controllable music
recommendation. This network expresses user preferences in terms of prototypes
representative of semantically meaningful features pertaining to musical
qualities. We show that the model obtains competitive recommendation
performance compared to popular baseline models while also providing
interpretable and controllable user profiles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to MLSP2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Melody-Lyrics Matching with Contrastive Alignment Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changhong Wang, Michel Olvera, Gaël Richard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The connection between music and lyrics is far beyond semantic bonds.
Conceptual pairs in the two modalities such as rhythm and rhyme, note duration
and syllabic stress, and structure correspondence, raise a compelling yet
seldom-explored direction in the field of music information retrieval. In this
paper, we present melody-lyrics matching (MLM), a new task which retrieves
potential lyrics for a given symbolic melody from text sources. Rather than
generating lyrics from scratch, MLM essentially exploits the relationships
between melody and lyrics. We propose a self-supervised representation learning
framework with contrastive alignment loss for melody and lyrics. This has the
potential to leverage the abundance of existing songs with paired melody and
lyrics. No alignment annotations are required. Additionally, we introduce
sylphone, a novel representation for lyrics at syllable-level activated by
phoneme identity and vowel stress. We demonstrate that our method can match
melody with coherent and singable lyrics with empirical results and intuitive
examples. We open source code and provide matching examples on the companion
webpage: https://github.com/changhongw/mlm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, 3 tables. This work has been submitted to the
  IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automating AI Failure Tracking: Semantic Association of Reports in AI
  Incident Database <span class="chip">ECAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Russo, Gian Marco Orlando, Valerio La Gatta, Vincenzo Moscato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) systems are transforming critical sectors such
as healthcare, finance, and transportation, enhancing operational efficiency
and decision-making processes. However, their deployment in high-stakes domains
has exposed vulnerabilities that can result in significant societal harm. To
systematically study and mitigate these risk, initiatives like the AI Incident
Database (AIID) have emerged, cataloging over 3,000 real-world AI failure
reports. Currently, associating a new report with the appropriate AI Incident
relies on manual expert intervention, limiting scalability and delaying the
identification of emerging failure patterns.
  To address this limitation, we propose a retrieval-based framework that
automates the association of new reports with existing AI Incidents through
semantic similarity modeling. We formalize the task as a ranking problem, where
each report-comprising a title and a full textual description-is compared to
previously documented AI Incidents based on embedding cosine similarity.
Benchmarking traditional lexical methods, cross-encoder architectures, and
transformer-based sentence embedding models, we find that the latter
consistently achieve superior performance. Our analysis further shows that
combining titles and descriptions yields substantial improvements in ranking
accuracy compared to using titles alone. Moreover, retrieval performance
remains stable across variations in description length, highlighting the
robustness of the framework. Finally, we find that retrieval performance
consistently improves as the training set expands. Our approach provides a
scalable and efficient solution for supporting the maintenance of the AIID.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 28th European Conference on Artificial Intelligence
  (ECAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Education with Ranking Alignment Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haipeng Liu, Yuxuan Liu, Ting Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized question recommendation aims to guide individual students
through questions to enhance their mastery of learning targets. Most previous
methods model this task as a Markov Decision Process and use reinforcement
learning to solve, but they struggle with efficient exploration, failing to
identify the best questions for each student during training. To address this,
we propose Ranking Alignment Recommendation (RAR), which incorporates
collaborative ideas into the exploration mechanism, enabling more efficient
exploration within limited training episodes. Experiments show that RAR
effectively improves recommendation performance, and our framework can be
applied to any RL-based question recommender. Our code is available in
https://github.com/wuming29/RAR.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KLAN: Kuaishou Landing-page Adaptive Navigator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Li, Chang Meng, Jiaqi Fu, Shuchang Liu, Jiashuo Zhang, Tianke Zhang, Xueliang Wang, Xiaoqiang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern online platforms configure multiple pages to accommodate diverse user
needs. This multi-page architecture inherently establishes a two-stage
interaction paradigm between the user and the platform: (1) Stage I: page
navigation, navigating users to a specific page and (2) Stage II: in-page
interaction, where users engage with customized content within the specific
page. While the majority of research has been focusing on the sequential
recommendation task that improves users' feedback in Stage II, there has been
little investigation on how to achieve better page navigation in Stage I. To
fill this gap, we formally define the task of Personalized Landing Page
Modeling (PLPM) into the field of recommender systems: Given a user upon app
entry, the goal of PLPM is to proactively select the most suitable landing page
from a set of candidates (e.g., functional tabs, content channels, or
aggregation pages) to optimize the short-term PDR metric and the long-term user
engagement and satisfaction metrics, while adhering to industrial constraints.
Additionally, we propose KLAN (Kuaishou Landing-page Adaptive Navigator), a
hierarchical solution framework designed to provide personalized landing pages
under the formulation of PLPM. KLAN comprises three key components: (1)
KLAN-ISP captures inter-day static page preference; (2) KLAN-IIT captures
intra-day dynamic interest transitions and (3) KLAN-AM adaptively integrates
both components for optimal navigation decisions. Extensive online experiments
conducted on the Kuaishou platform demonstrate the effectiveness of KLAN,
obtaining +0.205% and +0.192% improvements on in Daily Active Users (DAU) and
user Lifetime (LT). Our KLAN is ultimately deployed on the online platform at
full traffic, serving hundreds of millions of users. To promote further
research in this important area, we will release our dataset and code upon
paper acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We propose PLPM, a new task for selecting optimal landing pages upon
  user entry. Our solution, KLAN, models static and dynamic user interests and
  is successfully deployed on Kuaishou, improving DAU and user lifetime</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards LLM-Enhanced Product Line Scoping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Felfernig, Damian Garber, Viet-Man Le, Sebastian Lubos, Thi Ngoc Trang Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The idea of product line scoping is to identify the set of features and
configurations that a product line should include, i.e., offer for
configuration purposes. In this context, a major scoping task is to find a
balance between commercial relevance and technical feasibility. Traditional
product line scoping approaches rely on formal feature models and require a
manual analysis which can be quite time-consuming. In this paper, we sketch how
Large Language Models (LLMs) can be applied to support product line scoping
tasks with a natural language interaction based scoping process. Using a
working example from the smarthome domain, we sketch how LLMs can be applied to
evaluate different feature model alternatives. We discuss open research
challenges regarding the integration of LLMs with product line scoping.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based
  on Multi-Relational Graphs and Structural Entropy Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongbing Zhang, Fang Nan, Shengxiang Gao, Yuxin Huang, Kaiwen Tan, Zhengtao Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The core challenge faced by multi-document summarization is the complexity of
relationships among documents and the presence of information redundancy. Graph
clustering is an effective paradigm for addressing this issue, as it models the
complex relationships among documents using graph structures and reduces
information redundancy through clustering, achieving significant research
progress. However, existing methods often only consider single-relational
graphs and require a predefined number of clusters, which hinders their ability
to fully represent rich relational information and adaptively partition
sentence groups to reduce redundancy. To overcome these limitations, we propose
MRGSEM-Sum, an unsupervised multi-document summarization framework based on
multi-relational graphs and structural entropy minimization. Specifically, we
construct a multi-relational graph that integrates semantic and discourse
relations between sentences, comprehensively modeling the intricate and dynamic
connections among sentences across documents. We then apply a two-dimensional
structural entropy minimization algorithm for clustering, automatically
determining the optimal number of clusters and effectively organizing sentences
into coherent groups. Finally, we introduce a position-aware compression
mechanism to distill each cluster, generating concise and informative
summaries. Extensive experiments on four benchmark datasets (Multi-News,
DUC-2004, PubMed, and WikiSum) demonstrate that our approach consistently
outperforms previous unsupervised methods and, in several cases, achieves
performance comparable to supervised models and large language models. Human
evaluation demonstrates that the summaries generated by MRGSEM-Sum exhibit high
consistency and coverage, approaching human-level quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Holistic Evaluations of Topic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Compton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic models are gaining increasing commercial and academic interest for
their ability to summarize large volumes of unstructured text. As unsupervised
machine learning methods, they enable researchers to explore data and help
general users understand key themes in large text collections. However, they
risk becoming a 'black box', where users input data and accept the output as an
accurate summary without scrutiny. This article evaluates topic models from a
database perspective, drawing insights from 1140 BERTopic model runs. The goal
is to identify trade-offs in optimizing model parameters and to reflect on what
these findings mean for the interpretation and responsible use of topic models
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-to-SQL Task-oriented Dialogue Ontology Construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renato Vukovic, Carel van Niekerk, Michael Heck, Benjamin Ruppik, Hsien-Chin Lin, Shutong Feng, Nurul Lubis, Milica Gasic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are widely used as general-purpose knowledge
sources, but they rely on parametric knowledge, limiting explainability and
trustworthiness. In task-oriented dialogue (TOD) systems, this separation is
explicit, using an external database structured by an explicit ontology to
ensure explainability and controllability. However, building such ontologies
requires manual labels or supervised training. We introduce TeQoDO: a
Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM
autonomously builds a TOD ontology from scratch without supervision using its
inherent SQL programming capabilities combined with dialogue theory provided in
the prompt. We show that TeQoDO outperforms transfer learning approaches, and
its constructed ontology is competitive on a downstream dialogue state tracking
task. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also
scales to allow construction of much larger ontologies, which we investigate on
a Wikipedia and ArXiv dataset. We view this as a step towards broader
application of ontologies to increase LLM explainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MUST-RAG: MUSical Text Question Answering with Retrieval Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daeyong Kwon, SeungHeon Doh, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large language models (LLMs) have demonstrated
remarkable capabilities across diverse domains. While they exhibit strong
zero-shot performance on various tasks, LLMs' effectiveness in music-related
applications remains limited due to the relatively small proportion of
music-specific knowledge in their training data. To address this limitation, we
propose MusT-RAG, a comprehensive framework based on Retrieval Augmented
Generation (RAG) to adapt general-purpose LLMs for text-only music question
answering (MQA) tasks. RAG is a technique that provides external knowledge to
LLMs by retrieving relevant context information when generating answers to
questions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a
music-specialized vector database for the retrieval stage, and (2) utilizes
context information during both inference and fine-tuning processes to
effectively transform general-purpose LLMs into music-specific models. Our
experiment demonstrates that MusT-RAG significantly outperforms traditional
fine-tuning approaches in enhancing LLMs' music domain adaptation capabilities,
showing consistent improvements across both in-domain and out-of-domain MQA
benchmarks. Additionally, our MusWikiDB proves substantially more effective
than general Wikipedia corpora, delivering superior performance and
computational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Your Spending Needs Attention: Modeling Financial Habits with
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        D. T. Braithwaite, Misael Cavalcanti, R. Austin McEver, Hiroto Udagawa, Daniel Silva, Rohan Ramanath, Felipe Meneses, Arissa Yoshida, Evan Wingert, Matheus Ramos, Brian Zanfelice, Aman Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predictive models play a crucial role in the financial industry, enabling
risk prediction, fraud detection, and personalized recommendations, where
slight changes in core model performance can result in billions of dollars in
revenue or losses. While financial institutions have access to enormous amounts
of user data (e.g., bank transactions, in-app events, and customer support
logs), leveraging this data effectively remains challenging due to its
complexity and scale. Thus, in many financial institutions, most production
models follow traditional machine learning (ML) approaches by converting
unstructured data into manually engineered tabular features. Conversely, other
domains (e.g., natural language processing) have effectively utilized
self-supervised learning (SSL) to learn rich representations from raw data,
removing the need for manual feature extraction. In this paper, we investigate
using transformer-based representation learning models for transaction data,
hypothesizing that these models, trained on massive data, can provide a novel
and powerful approach to understanding customer behavior. We propose a new
method enabling the use of SSL with transaction data by adapting
transformer-based models to handle both textual and structured attributes. Our
approach, denoted nuFormer, includes an end-to-end fine-tuning method that
integrates user embeddings with existing tabular features. Our experiments
demonstrate improvements for large-scale recommendation problems at Nubank.
Notably, these gains are achieved solely through enhanced representation
learning rather than incorporating new data sources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not Just What, But When: Integrating Irregular Intervals to LLM for
  Sequential Recommendation <span class="chip">RecSys 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Wei Du, Takuma Udagawa, Kei Tateno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time intervals between purchasing items are a crucial factor in sequential
recommendation tasks, whereas existing approaches focus on item sequences and
often overlook by assuming the intervals between items are static. However,
dynamic intervals serve as a dimension that describes user profiling on not
only the history within a user but also different users with the same item
history. In this work, we propose IntervalLLM, a novel framework that
integrates interval information into LLM and incorporates the novel
interval-infused attention to jointly consider information of items and
intervals. Furthermore, unlike prior studies that address the cold-start
scenario only from the perspectives of users and items, we introduce a new
viewpoint: the interval perspective to serve as an additional metric for
evaluating recommendation methods on the warm and cold scenarios. Extensive
experiments on 3 benchmarks with both traditional- and LLM-based baselines
demonstrate that our IntervalLLM achieves not only 4.4% improvements in average
but also the best-performing warm and cold scenarios across all users, items,
and the proposed interval perspectives. In addition, we observe that the cold
scenario from the interval perspective experiences the most significant
performance drop among all recommendation methods. This finding underscores the
necessity of further research on interval-based cold challenges and our
integration of interval information in the realm of sequential recommendation
tasks. Our code is available here:
https://github.com/sony/ds-research-code/tree/master/recsys25-IntervalLLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by RecSys 2025 short paper track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Recommenders Self-Aware? Label-Free Recommendation Performance
  Estimation via Model Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayu Li, Ziyi Ye, Guohao Jian, Zhiqiang Guo, Weizhi Ma, Qingyao Ai, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can a recommendation model be self-aware? This paper investigates the
recommender's self-awareness by quantifying its uncertainty, which provides a
label-free estimation of its performance. Such self-assessment can enable more
informed understanding and decision-making before the recommender engages with
any users. To this end, we propose an intuitive and effective method,
probability-based List Distribution uncertainty (LiDu). LiDu measures
uncertainty by determining the probability that a recommender will generate a
certain ranking list based on the prediction distributions of individual items.
We validate LiDu's ability to represent model self-awareness in two settings:
(1) with a matrix factorization model on a synthetic dataset, and (2) with
popular recommendation algorithms on real-world datasets. Experimental results
show that LiDu is more correlated with recommendation performance than a series
of label-free performance estimators. Additionally, LiDu provides valuable
insights into the dynamic inner states of models throughout training and
inference. This work establishes an empirical connection between recommendation
uncertainty and performance, framing it as a step towards more transparent and
self-evaluating recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal Relational Item Representation Learning for Inferring
  Substitutable and Complementary Items 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junting Wang, Chenghuan Guo, Jiao Yang, Yanhui Guo, Yan Gao, Hari Sundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel self-supervised multi-modal relational item
representation learning framework designed to infer substitutable and
complementary items. Existing approaches primarily focus on modeling item-item
associations deduced from user behaviors using graph neural networks (GNNs) or
leveraging item content information. However, these methods often overlook
critical challenges, such as noisy user behavior data and data sparsity due to
the long-tailed distribution of these behaviors. In this paper, we propose
MMSC, a self-supervised multi-modal relational item representation learning
framework to address these challenges. Specifically, MMSC consists of three
main components: (1) a multi-modal item representation learning module that
leverages a multi-modal foundational model and learns from item metadata, (2) a
self-supervised behavior-based representation learning module that denoises and
learns from user behavior data, and (3) a hierarchical representation
aggregation mechanism that integrates item representations at both the semantic
and task levels. Additionally, we leverage LLMs to generate augmented training
data, further enhancing the denoising process during training. We conduct
extensive experiments on five real-world datasets, showing that MMSC
outperforms existing baselines by 26.1% for substitutable recommendation and
39.2% for complementary recommendation. In addition, we empirically show that
MMSC is effective in modeling cold-start items.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cost-Effective, Low Latency Vector Search with Azure Cosmos DB 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nitish Upreti, Harsha Vardhan Simhadri, Hari Sudan Sundar, Krishnan Sundaram, Samer Boshra, Balachandar Perumalswamy, Shivam Atri, Martin Chisholm, Revti Raman Singh, Greg Yang, Tamara Hass, Nitesh Dudhey, Subramanyam Pattipaka, Mark Hildebrand, Magdalen Manohar, Jack Moffitt, Haiyang Xu, Naren Datha, Suryansh Gupta, Ravishankar Krishnaswamy, Prashant Gupta, Abhishek Sahu, Hemeswari Varada, Sudhanshu Barthwal, Ritika Mor, James Codella, Shaun Cooper, Kevin Pilch, Simon Moreno, Aayush Kataria, Santosh Kulkarni, Neil Deshpande, Amar Sagare, Dinesh Billa, Zishan Fu, Vipul Vishal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector indexing enables semantic search over diverse corpora and has become
an important interface to databases for both users and AI agents. Efficient
vector search requires deep optimizations in database systems. This has
motivated a new class of specialized vector databases that optimize for vector
search quality and cost. Instead, we argue that a scalable, high-performance,
and cost-efficient vector search system can be built inside a cloud-native
operational database like Azure Cosmos DB while leveraging the benefits of a
distributed database such as high availability, durability, and scale. We do
this by deeply integrating DiskANN, a state-of-the-art vector indexing library,
inside Azure Cosmos DB NoSQL. This system uses a single vector index per
partition stored in existing index trees, and kept in sync with underlying
data. It supports < 20ms query latency over an index spanning 10 million
vectors, has stable recall over updates, and offers approximately 43x and 12x
lower query cost compared to Pinecone and Zilliz serverless enterprise
products. It also scales out to billions of vectors via automatic partitioning.
This convergent design presents a point in favor of integrating vector indices
into operational databases in the context of recent debates on specialized
vector databases, and offers a template for vector indexing in other databases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rec<span class="highlight-title">GPT</span> Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Yi, Dian Chen, Gaoyang Guo, Jiakai Tang, Jian Wu, Jing Yu, Mao Zhang, Sunhao Dai, Wen Chen, Wenjun Yang, Yuning Jiang, Zhujin Gao, Bo Zheng, Chi Li, Dimin Wang, Dixuan Wang, Fan Li, Fan Zhang, Haibin Chen, Haozhuang Liu, Jialin Zhu, Jiamang Wang, Jiawei Wu, Jin Cui, Ju Huang, Kai Zhang, Kan Liu, Lang Tian, Liang Rao, Longbin Li, Lulu Zhao, Na He, Peiyang Wang, Qiqi Huang, Tao Luo, Wenbo Su, Xiaoxiao He, Xin Tong, Xu Chen, Xunke Xi, Yang Li, Yaxuan Wu, Yeqiu Yang, Yi Hu, Yinnan Song, Yuchen Li, Yujie Luo, Yujin Yuan, Yuliang Yan, Zhengyang Wang, Zhibo Xiao, Zhixin Ma, Zile Zhou, Ziqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are among the most impactful applications of artificial
intelligence, serving as critical infrastructure connecting users, merchants,
and platforms. However, most current industrial systems remain heavily reliant
on historical co-occurrence patterns and log-fitting objectives, i.e.,
optimizing for past user interactions without explicitly modeling user intent.
This log-fitting approach often leads to overfitting to narrow historical
preferences, failing to capture users' evolving and latent interests. As a
result, it reinforces filter bubbles and long-tail phenomena, ultimately
harming user experience and threatening the sustainability of the whole
recommendation ecosystem.
  To address these challenges, we rethink the overall design paradigm of
recommender systems and propose RecGPT, a next-generation framework that places
user intent at the center of the recommendation pipeline. By integrating large
language models (LLMs) into key stages of user interest mining, item retrieval,
and explanation generation, RecGPT transforms log-fitting recommendation into
an intent-centric process. To effectively align general-purpose LLMs to the
above domain-specific recommendation tasks at scale, RecGPT incorporates a
multi-stage training paradigm, which integrates reasoning-enhanced
pre-alignment and self-training evolution, guided by a Human-LLM cooperative
judge system. Currently, RecGPT has been fully deployed on the Taobao App.
Online experiments demonstrate that RecGPT achieves consistent performance
gains across stakeholders: users benefit from increased content diversity and
satisfaction, merchants and the platform gain greater exposure and conversions.
These comprehensive improvement results across all stakeholders validates that
LLM-driven, intent-centric design can foster a more sustainable and mutually
beneficial recommendation ecosystem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Ecosystem for Ontology Interoperability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.12311v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.12311v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ontology interoperability is one of the complicated issues that restricts the
use of ontologies in knowledge graphs (KGs). Different ontologies with
conflicting and overlapping concepts make it difficult to design, develop, and
deploy an interoperable ontology for downstream tasks. We propose an ecosystem
for ontology interoperability. The ecosystem employs three state-of-the-art
semantic techniques in different phases of the ontology engineering life cycle:
ontology design patterns (ODPs) in the design phase, ontology matching and
versioning (OM\&OV) in the develop phase, and ontology-compliant knowledge
graphs (OCKGs) in the deploy phase, to achieve better ontology interoperability
and data integration in real-world applications. A case study of sensor
observation in the building domain validates the usefulness of the proposed
ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Who's important? -- SUnSET: Synergistic Understanding of Stakeholder,
  Events and Time for Timeline Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21903v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21903v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiviatis Sim, Kaiwen Yang, Shen Xin, Kenji Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As news reporting becomes increasingly global and decentralized online,
tracking related events across multiple sources presents significant
challenges. Existing news summarization methods typically utilizes Large
Language Models and Graphical methods on article-based summaries. However, this
is not effective since it only considers the textual content of similarly dated
articles to understand the gist of the event. To counteract the lack of
analysis on the parties involved, it is essential to come up with a novel
framework to gauge the importance of stakeholders and the connection of related
events through the relevant entities involved. Therefore, we present SUnSET:
Synergistic Understanding of Stakeholder, Events and Time for the task of
Timeline Summarization (TLS). We leverage powerful Large Language Models (LLMs)
to build SET triplets and introduced the use of stakeholder-based ranking to
construct a $Relevancy$ metric, which can be extended into general situations.
Our experimental results outperform all prior baselines and emerged as the new
State-of-the-Art, highlighting the impact of stakeholder information within
news article.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KeyB2: Selecting Key Blocks is Also Important for Long Document Ranking
  with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06254v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06254v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghan Li, Eric Gaussier, Juntao Li, Guodong Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of large language models (LLMs) such as Llama has significantly
advanced neural information retrieval (IR). However, applying LLMs to long
document reranking remains computationally expensive and may be ineffective.
Moreover, the internal behavior of LLMs during document relevance judgment is
still underexplored. In this paper, we begin with an in-depth analysis of
decoder-only LLM attention patterns and find that several attention heads
consistently align with relevance signals, yet this alignment deteriorates as
irrelevant content increases. Motivated by this observation, we revisit and
extend the block selection paradigm, introducing KeyB2, a scalable reranking
framework that combines block pre-selection with powerful decoder-only LLMs.
KeyB2 generalizes the selection stage to support BM25, cross-encoder, and
bi-encoder, and adapts LLM to compute fine-grained relevance scores. We further
introduce a new bi-encoder strategy that performs strongly and efficiently.
Extensive experiments on TREC DL 2019/2023 document task, Robust04, and MLDR-zh
demonstrate that KeyB2 outperforms baselines including RankLLaMA,
RankLLaMA-MaxP/AvgP, and KeyB, achieving new state-of-the-art (SOTA) results on
TREC DL 2019 document reranking task. In addition, KeyB2 reduces reranking
latency compared with RankLLaMA by over 83% and memory usage by over 74%,
positioning it as a practical and effective solution for long document ranking
with LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAVine: Reality-Aligned Evaluation for Agentic Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilong Xu, Xiang Long, Zhi Zheng, Jinhua Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agentic search, as a more autonomous and adaptive paradigm of retrieval
augmentation, is driving the evolution of intelligent search systems. However,
existing evaluation frameworks fail to align well with the goals of agentic
search. First, the complex queries commonly used in current benchmarks often
deviate from realistic user search scenarios. Second, prior approaches tend to
introduce noise when extracting ground truth for end-to-end evaluations,
leading to distorted assessments at a fine-grained level. Third, most current
frameworks focus solely on the quality of final answers, neglecting the
evaluation of the iterative process inherent to agentic search. To address
these limitations, we propose RAVine -- a Reality-Aligned eValuation framework
for agentic LLMs with search. RAVine targets multi-point queries and long-form
answers that better reflect user intents, and introduces an attributable ground
truth construction strategy to enhance the accuracy of fine-grained evaluation.
Moreover, RAVine examines model's interaction with search tools throughout the
iterative process, and accounts for factors of efficiency. We benchmark a
series of models using RAVine and derive several insights, which we hope will
contribute to advancing the development of agentic search systems. The code and
datasets are available at https://github.com/SwordFaith/RAVine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transform Before You Query: A Privacy-Preserving Approach for Vector
  Retrieval with Embedding Space Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqi He, Zekun Fei, Jiaqi Li, Xinyuan Zhu, Biao Yi, Siyi Lv, Weijie Liu, Zheli Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector Database (VDB) can efficiently index and search high-dimensional
vector embeddings from unstructured data, crucially enabling fast semantic
similarity search essential for modern AI applications like generative AI and
recommendation systems. Since current VDB service providers predominantly use
proprietary black-box models, users are forced to expose raw query text to them
via API in exchange for the vector retrieval services. Consequently, if query
text involves confidential records from finance or healthcare domains, this
mechanism inevitably leads to critical leakage of user's sensitive information.
To address this issue, we introduce STEER (\textbf{S}ecure \textbf{T}ransformed
\textbf{E}mbedding v\textbf{E}ctor\textbf{ R}etrieval), a private vector
retrieval framework that leverages the alignment relationship between the
semantic spaces of different embedding models to derive approximate embeddings
for the query text. STEER performs the retrieval using the approximate
embeddings within the original VDB and requires no modifications to the server
side. Our theoretical and experimental analyses demonstrate that STEER
effectively safeguards query text privacy while maintaining the retrieval
accuracy. Even though approximate embeddings are approximations of the
embeddings from proprietary models, they still prevent the providers from
recovering the query text through Embedding Inversion Attacks (EIAs). Extensive
experimental results show that Recall@100 of STEER can basically achieve a
decrease of less than 5\%. Furthermore, even when searching within a text
corpus of millions of entries, STEER achieves a Recall@20 accuracy 20\% higher
than current baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TransFR: Transferable Federated Recommendation with Adapter Tuning on
  <span class="highlight-title">Pre-train</span>ed Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01124v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01124v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honglei Zhang, Zhiwei Li, Haoxuan Li, Xin Zhou, Jie Zhang, Yidong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated recommendations (FRs), facilitating multiple local clients to
collectively learn a global model without disclosing user private data, have
emerged as a prevalent on-device service. In conventional FRs, a dominant
paradigm is to utilize discrete identities to represent clients and items,
which are then mapped to domain-specific embeddings to participate in model
training. Despite considerable performance, we reveal three inherent
limitations that can not be ignored in federated settings, i.e.,
non-transferability across domains, ineffectiveness in cold-start settings, and
potential privacy violations during federated training. To this end, we propose
a transferable federated recommendation model, TransFR, which delicately
incorporates the general capabilities empowered by pre-trained models and the
personalized abilities by fine-tuning local private data. Specifically, it
first learns domain-agnostic representations of items by exploiting pre-trained
models with public textual corpora. To tailor for FR tasks, we further
introduce efficient federated adapter-tuning and test-time adaptation
mechanisms, which facilitate personalized local adapters for each client by
fitting their private data distributions. We theoretically prove the advantages
of incorporating adapter tuning in FRs regarding both effectiveness and
privacy. Through extensive experiments, we show that our TransFR model
surpasses several state-of-the-art FRs on transferability.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Phi-Ground Tech Report: Advancing Perception in GUI Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miaosen Zhang, Ziqiang Xu, Jialiang Zhu, Qi Dai, Kai Qiu, Yifan Yang, Chong Luo, Tianyi Chen, Justin Wagle, Tim Franklin, Baining Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of multimodal reasoning models, Computer Use Agents
(CUAs), akin to Jarvis from \textit{"Iron Man"}, are becoming a reality. GUI
grounding is a core component for CUAs to execute actual actions, similar to
mechanical control in robotics, and it directly leads to the success or failure
of the system. It determines actions such as clicking and typing, as well as
related parameters like the coordinates for clicks. Current end-to-end
grounding models still achieve less than 65\% accuracy on challenging
benchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from
being ready for deployment. % , as a single misclick can result in unacceptable
consequences. In this work, we conduct an empirical study on the training of
grounding models, examining details from data collection to model training.
Ultimately, we developed the \textbf{Phi-Ground} model family, which achieves
state-of-the-art performance across all five grounding benchmarks for models
under $10B$ parameters in agent settings. In the end-to-end model setting, our
model still achieves SOTA results with scores of \textit{\textbf{43.2}} on
ScreenSpot-pro and \textit{\textbf{27.2}} on UI-Vision. We believe that the
various details discussed in this paper, along with our successes and failures,
not only clarify the construction of grounding models but also benefit other
perception tasks. Project homepage:
\href{https://zhangmiaosen2000.github.io/Phi-Ground/}{https://zhangmiaosen2000.github.io/Phi-Ground/}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agency Among Agents: Designing with Hypertextual Friction in the
  Algorithmic Web 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophia Liu, Shm Garanganao Almeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's algorithm-driven interfaces, from recommendation feeds to GenAI
tools, often prioritize engagement and efficiency at the expense of user
agency. As systems take on more decision-making, users have less control over
what they see and how meaning or relationships between content are constructed.
This paper introduces "Hypertextual Friction," a conceptual design stance that
repositions classical hypertext principles--friction, traceability, and
structure--as actionable values for reclaiming agency in algorithmically
mediated environments. Through a comparative analysis of real-world
interfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image
tools--we examine how different systems structure user experience, navigation,
and authorship. We show that hypertext systems emphasize provenance,
associative thinking, and user-driven meaning-making, while algorithmic systems
tend to obscure process and flatten participation. We contribute: (1) a
comparative analysis of how interface structures shape agency in user-driven
versus agent-driven systems, and (2) a conceptual stance that offers
hypertextual values as design commitments for reclaiming agency in an
increasingly algorithmic web.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in: Adjunct Proceedings of the 36th ACM Conference on
  Hypertext and Social Media, Chicago, IL, USA, September 15-18, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid CNN-Mamba Enhancement Network for Robust Multimodal Sentiment
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Xianfu Cheng, Xiaoming Zhang, Zhoujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Sentiment Analysis (MSA) with missing modalities has recently
attracted increasing attention. Although existing research mainly focuses on
designing complex model architectures to handle incomplete data, it still faces
significant challenges in effectively aligning and fusing multimodal
information. In this paper, we propose a novel framework called the Hybrid
CNN-Mamba Enhancement Network (HCMEN) for robust multimodal sentiment analysis
under missing modality conditions. HCMEN is designed around three key
components: (1) hierarchical unimodal modeling, (2) cross-modal enhancement and
alignment, and (3) multimodal mix-up fusion. First, HCMEN integrates the
strengths of Convolutional Neural Network (CNN) for capturing local details and
the Mamba architecture for modeling global contextual dependencies across
different modalities. Furthermore, grounded in the principle of Mutual
Information Maximization, we introduce a cross-modal enhancement mechanism that
generates proxy modalities from mixed token-level representations and learns
fine-grained token-level correspondences between modalities. The enhanced
unimodal features are then fused and passed through the CNN-Mamba backbone,
enabling local-to-global cross-modal interaction and comprehensive multimodal
integration. Extensive experiments on two benchmark MSA datasets demonstrate
that HCMEN consistently outperforms existing state-of-the-art methods,
achieving superior performance across various missing modality scenarios. The
code will be released publicly in the near future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for
  Enhanced Visual Instruction Tuning <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.15621v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.15621v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Cocchi, Nicholas Moratelli, Davide Caffagni, Sara Sarto, Lorenzo Baraldi, Marcella Cornia, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in Multimodal Large Language Models (MLLMs) has highlighted
the critical roles of both the visual backbone and the underlying language
model. While prior work has primarily focused on scaling these components to
billions of parameters, the trade-offs between model size, architecture, and
performance remain underexplored. Additionally, inconsistencies in training
data and evaluation protocols have hindered direct comparisons, making it
difficult to derive optimal design choices. In this paper, we introduce
LLaVA-MORE, a new family of MLLMs that integrates recent language models with
diverse visual backbones. To ensure fair comparisons, we employ a unified
training protocol applied consistently across all architectures. Our analysis
systematically explores both small- and medium-scale LLMs -- including Phi-4,
LLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and
instruction following, while examining the relationship between model size and
performance. Beyond evaluating the LLM impact on final results, we conduct a
comprehensive study of various visual encoders, ranging from CLIP-based
architectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional
experiments investigate the effects of increased image resolution and
variations in pre-training datasets. Overall, our results provide insights into
the design of more effective MLLMs, offering a reproducible evaluation
framework that facilitates direct comparisons and can guide future model
development. Our source code and trained models are publicly available at:
https://github.com/aimagelab/LLaVA-MORE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025 Workshop on What is Next in Multimodal Foundation Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic
  Bidirectional Reconstruction <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10006v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10006v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Qin, Wei Yang, Yan Su, Yiran Zhu, Weizhen Li, Yunyue Pan, Chengchang Pan, Honggang Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In breast cancer HER2 assessment, clinical evaluation relies on combined H&E
and IHC images, yet acquiring both modalities is often hindered by clinical
constraints and cost. We propose an adaptive bimodal prediction framework that
flexibly supports single- or dual-modality inputs through two core innovations:
a dynamic branch selector activating modality completion or joint inference
based on input availability, and a cross-modal GAN (CM-GAN) enabling
feature-space reconstruction of missing modalities. This design dramatically
improves H&E-only accuracy from 71.44% to 94.25%, achieves 95.09% with full
dual-modality inputs, and maintains 90.28% reliability under single-modality
conditions. The "dual-modality preferred, single-modality compatible"
architecture delivers near-dual-modality accuracy without mandatory
synchronized acquisition, offering a cost-effective solution for
resource-limited regions and significantly improving HER2 assessment
accessibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages,6 figures,3 tables,accepted by the 33rd ACM International
  Conference on Multimedia(ACM MM 2025)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-07-30T00:00:00Z">2025-07-30</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AUV-Fusion: Cross-Modal Adversarial Fusion of User Interactions and
  Visual Perturbations Against VARS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Ling, Tianchi Wang, Xiaohao Liu, Zhulin Tao, Lifang Yang, Xianglin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Visual-Aware Recommender Systems (VARS) exploit the integration of
user interaction data and visual features to deliver personalized
recommendations with high precision. However, their robustness against
adversarial attacks remains largely underexplored, posing significant risks to
system reliability and security. Existing attack strategies suffer from notable
limitations: shilling attacks are costly and detectable, and visual-only
perturbations often fail to align with user preferences. To address these
challenges, we propose AUV-Fusion, a cross-modal adversarial attack framework
that adopts high-order user preference modeling and cross-modal adversary
generation. Specifically, we obtain robust user embeddings through multi-hop
user-item interactions and transform them via an MLP into semantically aligned
perturbations. These perturbations are injected onto the latent space of a
pre-trained VAE within the diffusion model. By synergistically integrating
genuine user interaction data with visually plausible perturbations, AUV-Fusion
eliminates the need for injecting fake user profiles and effectively mitigates
the challenge of insufficient user preference extraction inherent in
traditional visual-only attacks. Comprehensive evaluations on diverse VARS
architectures and real-world datasets demonstrate that AUV-Fusion significantly
enhances the exposure of target (cold-start) items compared to conventional
baseline methods. Moreover, AUV-Fusion maintains exceptional stealth under
rigorous scrutiny.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages,6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoOutageKG: A Multimodal Geospatiotemporal Knowledge Graph for
  Multiresolution Power Outage Analysis <span class="chip">ISWC 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Frakes, Yinghui Wu, Roger H. French, Mengjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting, analyzing, and predicting power outages is crucial for grid risk
assessment and disaster mitigation. Numerous outages occur each year,
exacerbated by extreme weather events such as hurricanes. Existing outage data
are typically reported at the county level, limiting their spatial resolution
and making it difficult to capture localized patterns. However, it offers
excellent temporal granularity. In contrast, nighttime light satellite image
data provides significantly higher spatial resolution and enables a more
comprehensive spatial depiction of outages, enhancing the accuracy of assessing
the geographic extent and severity of power loss after disaster events.
However, these satellite data are only available on a daily basis. Integrating
spatiotemporal visual and time-series data sources into a unified knowledge
representation can substantially improve power outage detection, analysis, and
predictive reasoning. In this paper, we propose GeoOutageKG, a multimodal
knowledge graph that integrates diverse data sources, including nighttime light
satellite image data, high-resolution spatiotemporal power outage maps, and
county-level timeseries outage reports in the U.S. We describe our method for
constructing GeoOutageKG by aligning source data with a developed ontology,
GeoOutageOnto. Currently, GeoOutageKG includes over 10.6 million individual
outage records spanning from 2014 to 2024, 300,000 NTL images spanning from
2012 to 2024, and 15,000 outage maps. GeoOutageKG is a novel, modular and
reusable semantic resource that enables robust multimodal data integration. We
demonstrate its use through multiresolution analysis of geospatiotemporal power
outages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 24th International Semantic Web Conference Resource
  Track (ISWC 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sustainability Evaluation Metrics for Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Felfernig, Damian Garber, Viet-Man Le, Sebastian Lubos, Thi Ngoc Trang Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sustainability-oriented evaluation metrics can help to assess the quality of
recommender systems beyond wide-spread metrics such as accuracy, precision,
recall, and satisfaction. Following the United Nations`s sustainable
development goals (SDGs), such metrics can help to analyse the impact of
recommender systems on environmental, social, and economic aspects. We discuss
different basic sustainability evaluation metrics for recommender systems and
analyze their applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roxana Petcu, Samarth Bhargav, Maarten de Rijke, Evangelos Kanoulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and solving complex reasoning tasks is vital for addressing the
information needs of a user. Although dense neural models learn contextualised
embeddings, they still underperform on queries containing negation. To
understand this phenomenon, we study negation in both traditional neural
information retrieval and LLM-based models. We (1) introduce a taxonomy of
negation that derives from philosophical, linguistic, and logical definitions;
(2) generate two benchmark datasets that can be used to evaluate the
performance of neural information retrieval models and to fine-tune models for
a more robust performance on negation; and (3) propose a logic-based
classification mechanism that can be used to analyze the performance of
retrieval models on existing datasets. Our taxonomy produces a balanced data
distribution over negation types, providing a better training setup that leads
to faster convergence on the NevIR dataset. Moreover, we propose a
classification schema that reveals the coverage of negation types in existing
datasets, offering insights into the factors that might affect the
generalization of fine-tuned models on negation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conversational LLMs Simplify Secure Clinical Data Access, Understanding,
  and Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafi Al Attrach, Pedro Moreira, Rajna Fani, Renato Umeton, Leo Anthony Celi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As ever-larger clinical datasets become available, they have the potential to
unlock unprecedented opportunities for medical research. Foremost among them is
Medical Information Mart for Intensive Care (MIMIC-IV), the world's largest
open-source EHR database. However, the inherent complexity of these datasets,
particularly the need for sophisticated querying skills and the need to
understand the underlying clinical settings, often presents a significant
barrier to their effective use. M3 lowers the technical barrier to
understanding and querying MIMIC-IV data. With a single command it retrieves
MIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the
hosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers
converse with the database in plain English. Ask a clinical question in natural
language; M3 uses a language model to translate it into SQL, executes the query
against the MIMIC-IV dataset, and returns structured results alongside the
underlying query for verifiability and reproducibility. Demonstrations show
that minutes of dialogue with M3 yield the kind of nuanced cohort analyses that
once demanded hours of handcrafted SQL and relied on understanding the
complexities of clinical workflows. By simplifying access, M3 invites the
broader research community to mine clinical critical-care data and accelerates
the translation of raw records into actionable insight.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Search for Stability: Learning Dynamics of Strategic Publishers with
  Initial Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16695v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16695v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omer Madmon, Idan Pipano, Itamar Reinman, Moshe Tennenholtz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a game-theoretic information retrieval model in which strategic
publishers aim to maximize their chances of being ranked first by the search
engine while maintaining the integrity of their original documents. We show
that the commonly used Probability Ranking Principle (PRP) ranking scheme
results in an unstable environment where games often fail to reach pure Nash
equilibrium. We propose two families of ranking functions that do not adhere to
the PRP principle. We provide both theoretical and empirical evidence that
these methods lead to a stable search ecosystem, by providing positive results
on the learning dynamics convergence. We also define the publishers' and users'
welfare, demonstrate a possible publisher-user trade-off, and provide means for
a search system designer to control it. Finally, we show how instability harms
long-term users' welfare.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the Journal of Artificial Intelligence Research 83
  (2025)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santosh Patapati, Trisanth Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles must react in milliseconds while reasoning about road
geometry and traffic intent to navigate complex situations. We introduce
NovaDrive, a single-branch vision-language architecture that processes
front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a
single branch. A lightweight, two-stage cross-attention block first aligns
waypoint tokens with the HD map, then refines attention over fine-grained image
and depth patches. Coupled with a novel smoothness loss that discourages abrupt
steering and speed changes, this design eliminates the need for recurrent
memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language
backbone, enabling real-time inference. On the nuScenes / Waymo subset of the
MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts
path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from
2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations
confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention
fusion each contribute the most to these gains. Beyond safety, NovaDrive's
shorter routes (resulting from the novel smoothness loss) translate to lower
fuel or battery usage, pointing toward leaner, more easily updated driving
stacks. NovaDrive can be extended to other embodied-AI domains as well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GestureHYDRA: Semantic Co-speech Gesture Synthesis via Hybrid Modality
  Diffusion <span class="highlight-title">Transformer</span> and Cascaded-Synchronized Retrieval-Augmented
  Generation <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quanwei Yang, Luying Huang, Kaisiyuan Wang, Jiazhi Guan, Shengyi He, Fengguo Li, Hang Zhou, Lingyun Yu, Yingying Li, Haocheng Feng, Hongtao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While increasing attention has been paid to co-speech gesture synthesis, most
previous works neglect to investigate hand gestures with explicit and essential
semantics. In this paper, we study co-speech gesture generation with an
emphasis on specific hand gesture activation, which can deliver more
instructional information than common body movements. To achieve this, we first
build a high-quality dataset of 3D human body movements including a set of
semantically explicit hand gestures that are commonly used by live streamers.
Then we present a hybrid-modality gesture generation system GestureHYDRA built
upon a hybrid-modality diffusion transformer architecture with novelly designed
motion-style injective transformer layers, which enables advanced gesture
modeling ability and versatile gesture operations. To guarantee these specific
hand gestures can be activated, we introduce a cascaded retrieval-augmented
generation strategy built upon a semantic gesture repository annotated for each
subject and an adaptive audio-gesture synchronization mechanism, which
substantially improves semantic gesture activation and production efficiency.
Quantitative and qualitative experiments demonstrate that our proposed approach
achieves superior performance over all the counterparts. The project page can
be found at https://mumuwei.github.io/GestureHYDRA/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Blind Bitstream-corrupted Video Recovery via a Visual Foundation
  Model-driven Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Liu, Kejun Wu, Chen Cai, Yi Wang, Kim-Hui Yap, Lap-Pui Chau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video signals are vulnerable in multimedia communication and storage systems,
as even slight bitstream-domain corruption can lead to significant pixel-domain
degradation. To recover faithful spatio-temporal content from corrupted inputs,
bitstream-corrupted video recovery has recently emerged as a challenging and
understudied task. However, existing methods require time-consuming and
labor-intensive annotation of corrupted regions for each corrupted video frame,
resulting in a large workload in practice. In addition, high-quality recovery
remains difficult as part of the local residual information in corrupted frames
may mislead feature completion and successive content recovery. In this paper,
we propose the first blind bitstream-corrupted video recovery framework that
integrates visual foundation models with a recovery model, which is adapted to
different types of corruption and bitstream-level prompts. Within the
framework, the proposed Detect Any Corruption (DAC) model leverages the rich
priors of the visual foundation model while incorporating bitstream and
corruption knowledge to enhance corruption localization and blind recovery.
Additionally, we introduce a novel Corruption-aware Feature Completion (CFC)
module, which adaptively processes residual contributions based on high-level
corruption understanding. With VFM-guided hierarchical feature augmentation and
high-level coordination in a mixture-of-residual-experts (MoRE) structure, our
method suppresses artifacts and enhances informative residuals. Comprehensive
evaluations show that the proposed method achieves outstanding performance in
bitstream-corrupted video recovery without requiring a manually labeled mask
sequence. The demonstrated effectiveness will help to realize improved user
experience, wider application scenarios, and more reliable multimedia
communication and storage systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, accepted by ACMMM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided
  LLM Representations and Multimodal Apparent Behaviors <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Li, Yichao He, Jiacheng Xu, Tianhao Luo, Zhenzhen Hu, Richang Hong, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and reliable personality assessment plays a vital role in many
fields, such as emotional intelligence, mental health diagnostics, and
personalized education. Unlike fleeting emotions, personality traits are
stable, often subconsciously leaked through language, facial expressions, and
body behaviors, with asynchronous patterns across modalities. It was hard to
model personality semantics with traditional superficial features and seemed
impossible to achieve effective cross-modal understanding. To address these
challenges, we propose a novel personality assessment framework called
\textit{\textbf{Traits Run Deep}}. It employs
\textit{\textbf{psychology-informed prompts}} to elicit high-level
personality-relevant semantic representations. Besides, it devises a
\textit{\textbf{Text-Centric Trait Fusion Network}} that anchors rich text
semantics to align and integrate asynchronous signals from other modalities. To
be specific, such fusion module includes a Chunk-Wise Projector to decrease
dimensionality, a Cross-Modal Connector and a Text Feature Enhancer for
effective modality fusion and an ensemble regression head to improve
generalization in data-scarce situations. To our knowledge, we are the first to
apply personality-specific prompts to guide large language models (LLMs) in
extracting personality-aware semantics for improved representation quality.
Furthermore, extracting and fusing audio-visual apparent behavior features
further improves the accuracy. Experimental results on the AVI validation set
have demonstrated the effectiveness of the proposed components, i.e.,
approximately a 45\% reduction in mean squared error (MSE). Final evaluations
on the test set of the AVI Challenge 2025 confirm our method's superiority,
ranking first in the Personality Assessment track. The source code will be made
available at https://github.com/MSA-LMC/TraitsRunDeep.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, ACM MM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAVFlow: Preserving Paralinguistic Elements with Conditional Flow
  Matching for Zero-Shot AV2AV Multilingual Translation <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11026v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11026v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungwoo Cho, Jeongsoo Choi, Sungnyun Kim, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in text-to-speech (TTS) models,
audio-visual-to-audio-visual (AV2AV) translation still faces a critical
challenge: maintaining speaker consistency between the original and translated
vocal and facial features. To address this issue, we propose a conditional flow
matching (CFM) zero-shot audio-visual renderer that utilizes strong dual
guidance from both audio and visual modalities. By leveraging multimodal
guidance with CFM, our model robustly preserves speaker-specific
characteristics and enhances zero-shot AV2AV translation abilities. For the
audio modality, we enhance the CFM process by integrating robust speaker
embeddings with x-vectors, which serve to bolster speaker consistency.
Additionally, we convey emotional nuances to the face rendering module. The
guidance provided by both audio and visual cues remains independent of semantic
or linguistic content, allowing our renderer to effectively handle zero-shot
translation tasks for monolingual speakers in different languages. We
empirically demonstrate that the inclusion of high-quality mel-spectrograms
conditioned on facial information not only enhances the quality of the
synthesized speech but also positively influences facial generation, leading to
overall performance improvements in LSE and FID score. Our code is available at
https://github.com/Peter-SungwooCho/MAVFlow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Talking-to-Build: How LLM-Assisted Interface Shapes Player Performance
  and Experience in Minecraft 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Sun, Lei Wang, Yue Li, Jie Li, Massimo Poesio, Julian Frommel, Koen Hinriks, Jiahuan Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With large language models (LLMs) on the rise, in-game interactions are
shifting from rigid commands to natural conversations. However, the impacts of
LLMs on player performance and game experience remain underexplored. This work
explores LLM's role as a co-builder during gameplay, examining its impact on
task performance, usability, and player experience. Using Minecraft as a
sandbox, we present an LLM-assisted interface that engages players through
natural language, aiming to facilitate creativity and simplify complex gaming
commands. We conducted a mixed-methods study with 30 participants, comparing
LLM-assisted and command-based interfaces across simple and complex game tasks.
Quantitative and qualitative analyses reveal that the LLM-assisted interface
significantly improves player performance, engagement, and overall game
experience. Additionally, task complexity has a notable effect on player
performance and experience across both interfaces. Our findings highlight the
potential of LLM-assisted interfaces to revolutionize virtual experiences,
emphasizing the importance of balancing intuitiveness with predictability,
transparency, and user agency in AI-driven, multimodal gaming environments.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-07-29T00:00:00Z">2025-07-29</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Recommendation with Semantic IDs: A Practitioner's Handbook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clark Mingxuan Ju, Liam Collins, Leonardo Neves, Bhuvesh Kumar, Louis Yufeng Wang, Tong Zhao, Neil Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative recommendation (GR) has gained increasing attention for its
promising performance compared to traditional models. A key factor contributing
to the success of GR is the semantic ID (SID), which converts continuous
semantic representations (e.g., from large language models) into discrete ID
sequences. This enables GR models with SIDs to both incorporate semantic
information and learn collaborative filtering signals, while retaining the
benefits of discrete decoding. However, varied modeling techniques,
hyper-parameters, and experimental setups in existing literature make direct
comparisons between GR proposals challenging. Furthermore, the absence of an
open-source, unified framework hinders systematic benchmarking and extension,
slowing model iteration. To address this challenge, our work introduces and
open-sources a framework for Generative Recommendation with semantic ID, namely
GRID, specifically designed for modularity to facilitate easy component
swapping and accelerate idea iteration. Using GRID, we systematically
experiment with and ablate different components of GR models with SIDs on
public benchmarks. Our comprehensive experiments with GRID reveal that many
overlooked architectural components in GR models with SIDs substantially impact
performance. This offers both novel insights and validates the utility of an
open-source platform for robust benchmarking and GR research advancement. GRID
is open-sourced at https://github.com/snap-research/GRID.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent-Aware Neural Query Reformulation for Behavior-Aligned Product
  Search <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayanth Yetukuri, Ishita Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and modeling buyer intent is a foundational challenge in
optimizing search query reformulation within the dynamic landscape of
e-commerce search systems. This work introduces a robust data pipeline designed
to mine and analyze large-scale buyer query logs, with a focus on extracting
fine-grained intent signals from both explicit interactions and implicit
behavioral cues. Leveraging advanced sequence mining techniques and supervised
learning models, the pipeline systematically captures patterns indicative of
latent purchase intent, enabling the construction of a high-fidelity,
intent-rich dataset. The proposed framework facilitates the development of
adaptive query rewrite strategies by grounding reformulations in inferred user
intent rather than surface-level lexical signals. This alignment between query
rewriting and underlying user objectives enhances both retrieval relevance and
downstream engagement metrics. Empirical evaluations across multiple product
verticals demonstrate measurable gains in precision-oriented relevance metrics,
underscoring the efficacy of intent-aware reformulation. Our findings highlight
the value of intent-centric modeling in bridging the gap between sparse user
inputs and complex product discovery goals, and establish a scalable foundation
for future research in user-aligned neural retrieval and ranking systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR eCom'25.
  https://sigir-ecom.github.io/eCom25Papers/paper_23.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not Here, Go There: Analyzing Redirection Patterns on the Web 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kritika Garg, Sawood Alam, Dietrich Ayala, Michele C. Weigle, Michael L. Nelson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  URI redirections are integral to web management, supporting structural
changes, SEO optimization, and security. However, their complexities affect
usability, SEO performance, and digital preservation. This study analyzed 11
million unique redirecting URIs, following redirections up to 10 hops per URI,
to uncover patterns and implications of redirection practices. Our findings
revealed that 50% of the URIs terminated successfully, while 50% resulted in
errors, including 0.06% exceeding 10 hops. Canonical redirects, such as HTTP to
HTTPS transitions, were prevalent, reflecting adherence to SEO best practices.
Non-canonical redirects, often involving domain or path changes, highlighted
significant web migrations, rebranding, and security risks. Notable patterns
included "sink" URIs, where multiple redirects converged, ranging from traffic
consolidation by global websites to deliberate "Rickrolling." The study also
identified 62,000 custom 404 URIs, almost half being soft 404s, which could
compromise SEO and user experience. These findings underscore the critical role
of URI redirects in shaping the web while exposing challenges such as outdated
URIs, server instability, and improper error handling. This research offers a
detailed analysis of URI redirection practices, providing insights into their
prevalence, types, and outcomes. By examining a large dataset, we highlight
inefficiencies in redirection chains and examine patterns such as the use of
"sink" URIs and custom error pages. This information can help webmasters,
researchers, and digital archivists improve web usability, optimize resource
allocation, and safeguard valuable online content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper accepted at the 2025 ACM Web Science
  Conference (WebSci 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Filtered Approximate Nearest Neighbor Search Algorithms on
  <span class="highlight-title">Transformer</span>-based Embedding Vectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Iff, Paul Bruegger, Marcin Chrapek, Maciej Besta, Torsten Hoefler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in embedding models for text, image, audio, and video drive progress
across multiple domains, including retrieval-augmented generation,
recommendation systems, vehicle/person reidentification, and face recognition.
Many applications in these domains require an efficient method to retrieve
items that are close to a given query in the embedding space while satisfying a
filter condition based on the item's attributes, a problem known as Filtered
Approximate Nearest Neighbor Search (FANNS). In this work, we present a
comprehensive survey and taxonomy of FANNS methods and analyze how they are
benchmarked in the literature. By doing so, we identify a key challenge in the
current FANNS landscape: the lack of diverse and realistic datasets,
particularly ones derived from the latest transformer-based text embedding
models. To address this, we introduce a novel dataset consisting of embedding
vectors for the abstracts of over 2.7 million research articles from the arXiv
repository, accompanied by 11 real-world attributes such as authors and
categories. We benchmark a wide range of FANNS methods on our novel dataset and
find that each method has distinct strengths and limitations; no single
approach performs best across all scenarios. ACORN, for example, supports
various filter types and performs reliably across dataset scales but is often
outperformed by more specialized methods. SeRF shows excellent performance for
range filtering on ordered attributes but cannot handle categorical attributes.
Filtered-DiskANN and UNG excel on the medium-scale dataset but fail on the
large-scale dataset, highlighting the challenge posed by transformer-based
embeddings, which are often more than an order of magnitude larger than earlier
embeddings. We conclude that no universally best method exists.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Curious Case of High-Dimensional Indexing as a File Structure: A
  Case Study of eCP-FS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Shahbaz Khan, Gylfi Þór Guðmundsson, Björn Þór Jónsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern analytical pipelines routinely deploy multiple deep learning and
retrieval models that rely on approximate nearest-neighbor (ANN) indexes to
support efficient similarity-based search. While many state-of-the-art
ANN-indexes are memory-based (e.g., HNSW and IVF), using multiple ANN indexes
creates a competition for limited GPU/CPU memory resources, which in turn
necessitates disk-based index structures (e.g., DiskANN or eCP). In typical
index implementations, the main component is a complex data structure that is
serialized to disk and is read either fully at startup time, for memory-based
indexes, or incrementally at query time, for disk-based indexes. To visualize
the index structure, or analyze its quality, complex coding is needed that is
either embedded in the index implementation or replicates the code that reads
the data structure. In this paper, we consider an alternative approach that
maps the data structure to a file structure, using a file library, making the
index easily readable for any programming language and even human-readable. The
disadvantage is that the serialized index is verbose, leading to overhead of
searching through the index. The question addressed in this paper is how severe
this performance penalty is. To that end, this paper presents eCP-FS, a
file-based implementation of eCP, a well-known disk-based ANN index. A
comparison with state-of-the-art indexes shows that while eCP-FS is slower, the
implementation is nevertheless somewhat competitive even when memory is not
constrained. In a memory-constrained scenario, eCP-FS offers a minimal memory
footprint, making it ideal for resource-constrained or multi-index
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploration on Demand: From Algorithmic Control to User Empowerment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo Bianchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems often struggle with over-specialization, which severely
limits users' exposure to diverse content and creates filter bubbles that
reduce serendipitous discovery. To address this fundamental limitation, this
paper introduces an adaptive clustering framework with user-controlled
exploration that effectively balances personalization and diversity in movie
recommendations. Our approach leverages sentence-transformer embeddings to
group items into semantically coherent clusters through an online algorithm
with dynamic thresholding, thereby creating a structured representation of the
content space. Building upon this clustering foundation, we propose a novel
exploration mechanism that empowers users to control recommendation diversity
by strategically sampling from less-engaged clusters, thus expanding their
content horizons while preserving relevance. Experiments on the MovieLens
dataset demonstrate the system's effectiveness, showing that exploration
significantly reduces intra-list similarity from 0.34 to 0.26 while
simultaneously increasing unexpectedness to 0.73. Furthermore, our Large
Language Model-based A/B testing methodology, conducted with 300 simulated
users, reveals that 72.7% of long-term users prefer exploratory recommendations
over purely exploitative ones, providing strong evidence for the system's
ability to promote meaningful content discovery without sacrificing user
satisfaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proposing a Semantic Movie Recommendation System Enhanced by Chat<span class="highlight-title">GPT</span>'s
  NLP Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Fallahi, Azam Bastanfard, Amineh Amini, Hadi Saboohi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of recommender systems on the web has grown, especially in the
movie industry, with a vast selection of options to watch. To assist users in
traversing available items and finding relevant results, recommender systems
analyze operational data and investigate users' tastes and habits. Providing
highly individualized suggestions can boost user engagement and satisfaction,
which is one of the fundamental goals of the movie industry, significantly in
online platforms. According to recent studies and research, using
knowledge-based techniques and considering the semantic ideas of the textual
data is a suitable way to get more appropriate results. This study provides a
new method for building a knowledge graph based on semantic information. It
uses the ChatGPT, as a large language model, to assess the brief descriptions
of movies and extract their tone of voice. Results indicated that using the
proposed method may significantly enhance accuracy rather than employing the
explicit genres supplied by the publishers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>May 2023, 6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh-Anh Nguyen, Bao Nguyen, Ha Lan N. T., Tuan Anh Hoang, Duc-Trong Le, Dung D. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation systems often suffer from data sparsity caused by limited
user-item interactions, which degrade their performance and amplify popularity
bias in real-world scenarios. This paper proposes a novel data augmentation
framework that leverages Large Language Models (LLMs) and item textual
descriptions to enrich interaction data. By few-shot prompting LLMs multiple
times to rerank items and aggregating the results via majority voting, we
generate high-confidence synthetic user-item interactions, supported by
theoretical guarantees based on the concentration of measure. To effectively
leverage the augmented data in the context of a graph recommendation system, we
integrate it into a graph contrastive learning framework to mitigate
distributional shift and alleviate popularity bias. Extensive experiments show
that our method improves accuracy and reduces popularity bias, outperforming
strong baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solution for Meta KDD Cup'25: A Comprehensive Three-Step Framework for
  Vision Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Zhang, Xiaocheng Zhang, Yang Zhou, Zhimin Lin, Peng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Large Language Models (VLLMs) have improved multi-modal understanding
and visual question answering (VQA), but still suffer from hallucinated
answers. Multi-modal Retrieval-Augmented Generation (RAG) helps address these
issues by incorporating external information, yet challenges remain in visual
context comprehension, multi-source retrieval, and multi-turn interactions. To
address these challenges, Meta constructed the CRAG-MM benchmark and launched
the CRAG-MM Challenge at KDD Cup 2025, which consists of three tasks. This
paper describes the solutions of all tasks in Meta KDD Cup'25 from BlackPearl
team. We use a single model for each task, with key methods including data
augmentation, RAG, reranking, and multi-task fine-tuning. Our solution achieve
automatic evaluation rankings of 3rd, 3rd, and 1st on the three tasks, and win
second place in Task3 after human evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conversations over Clicks: Impact of Chatbots on Information Search in
  Interdisciplinary Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Kim, Sergei L. Kosakovsky Pond, Stephen MacNeil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This full research paper investigates the impact of generative AI (GenAI) on
the learner experience, with a focus on how learners engage with and utilize
the information it provides. In e-learning environments, learners often need to
navigate a complex information space on their own. This challenge is further
compounded in interdisciplinary fields like bioinformatics, due to the varied
prior knowledge and backgrounds. In this paper, we studied how GenAI influences
information search in bioinformatics research: (1) How do interactions with a
GenAI chatbot influence learner orienteering behaviors?; and (2) How do
learners identify information scent in GenAI chatbot responses? We adopted an
autoethnographic approach to investigate these questions. GenAI was found to
support orienteering once a learning plan was established, but it was
counterproductive prior to that. Moreover, traditionally value-rich information
sources such as bullet points and related terms proved less effective when
applied to GenAI responses. Information scents were primarily recognized
through the presence or absence of prior knowledge of the domain. These
findings suggest that GenAI should be adopted into e-learning environments with
caution, particularly in interdisciplinary learning contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 tables, 3 figures, 2025 ASEE/IEEE Frontiers in Education
  (FIE) Conference preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hebbian Memory-Augmented Recurrent Networks: Engram Neurons in Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Szelogowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite success across diverse tasks, current artificial recurrent network
architectures rely primarily on implicit hidden-state memories, limiting their
interpretability and ability to model long-range dependencies. In contrast,
biological neural systems employ explicit, associative memory traces (i.e.,
engrams) strengthened through Hebbian synaptic plasticity and activated
sparsely during recall. Motivated by these neurobiological insights, we
introduce the Engram Neural Network (ENN), a novel recurrent architecture
incorporating an explicit, differentiable memory matrix with Hebbian plasticity
and sparse, attention-driven retrieval mechanisms. The ENN explicitly models
memory formation and recall through dynamic Hebbian traces, improving
transparency and interpretability compared to conventional RNN variants. We
evaluate the ENN architecture on three canonical benchmarks: MNIST digit
classification, CIFAR-10 image sequence modeling, and WikiText-103 language
modeling. Our empirical results demonstrate that the ENN achieves accuracy and
generalization performance broadly comparable to classical RNN, GRU, and LSTM
architectures, with all models converging to similar accuracy and perplexity on
the large-scale WikiText-103 task. At the same time, the ENN offers significant
enhancements in interpretability through observable memory dynamics. Hebbian
trace visualizations further reveal biologically plausible, structured memory
formation processes, validating the potential of neuroscience-inspired
mechanisms to inform the development of more interpretable and robust deep
learning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Data Retrieval and Comparative Bias Analysis of Recommendation
  Algorithms for YouTube Shorts and Long-Form Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Selimhan Dagtas, Mert Can Cakmak, Nitin Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing popularity of short-form video content, such as YouTube Shorts,
has transformed user engagement on digital platforms, raising critical
questions about the role of recommendation algorithms in shaping user
experiences. These algorithms significantly influence content consumption, yet
concerns about biases, echo chambers, and content diversity persist. This study
develops an efficient data collection framework to analyze YouTube's
recommendation algorithms for both short-form and long-form videos, employing
parallel computing and advanced scraping techniques to overcome limitations of
YouTube's API. The analysis uncovers distinct behavioral patterns in
recommendation algorithms across the two formats, with short-form videos
showing a more immediate shift toward engaging yet less diverse content
compared to long-form videos. Furthermore, a novel investigation into biases in
politically sensitive topics, such as the South China Sea dispute, highlights
the role of these algorithms in shaping narratives and amplifying specific
viewpoints. By providing actionable insights for designing equitable and
transparent recommendation systems, this research underscores the importance of
responsible AI practices in the evolving digital media landscape.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Modal Hypergraph Enhanced LLM Learning for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.10541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.10541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Guo, Tong Zhang, Yuanzhi Wang, Chenxu Wang, Fuyun Wang, Xudong Wang, Xiaoya Zhang, Xin Liu, Zhen Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The burgeoning presence of Large Language Models (LLM) is propelling the
development of personalized recommender systems. Most existing LLM-based
methods fail to sufficiently explore the multi-view graph structure
correlations inherent in recommendation scenarios. To this end, we propose a
novel framework, Hypergraph Enhanced LLM Learning for multimodal Recommendation
(HeLLM), designed to equip LLMs with the capability to capture intricate
higher-order semantic correlations by fusing graph-level contextual signals
with sequence-level behavioral patterns. In the recommender pre-training phase,
we design a user hypergraph to uncover shared interest preferences among users
and an item hypergraph to capture correlations within multimodal similarities
among items. The hypergraph convolution and synergistic contrastive learning
mechanism are introduced to enhance the distinguishability of learned
representations. In the LLM fine-tuning phase, we inject the learned
graph-structured embeddings directly into the LLM's architecture and integrate
sequential features capturing each user's chronological behavior. This process
enables hypergraphs to leverage graph-structured information as global context,
enhancing the LLM's ability to perceive complex relational patterns and
integrate multimodal information, while also modeling local temporal dynamics.
Extensive experiments demonstrate the superiority of our proposed method over
state-of-the-art baselines, confirming the advantages of fusing
hypergraph-based context with sequential user behavior in LLMs for
recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures, submitted to IEEE Transactions on Knowledge and
  Data Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaborative filtering based on nonnegative/binary matrix factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10381v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10381v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukino Terui, Yuka Inoue, Yohei Hamakawa, Kosuke Tatsumura, Kazue Kudo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering generates recommendations by exploiting user-item
similarities based on rating data, which often contains numerous unrated items.
To predict scores for unrated items, matrix factorization techniques such as
nonnegative matrix factorization (NMF) are often employed. Nonnegative/binary
matrix factorization (NBMF), which is an extension of NMF, approximates a
nonnegative matrix as the product of nonnegative and binary matrices. While
previous studies have applied NBMF primarily to dense data such as images, this
paper proposes a modified NBMF algorithm tailored for collaborative filtering
with sparse data. In the modified method, unrated entries in the rating matrix
are masked, enhancing prediction accuracy. Furthermore, utilizing a low-latency
Ising machine in NBMF is advantageous in terms of the computation time, making
the proposed method beneficial.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RANa: Retrieval-Augmented Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.03524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.03524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca Monaci, Rafael S. Rezende, Romain Deffayet, Gabriela Csurka, Guillaume Bono, Hervé Déjean, Stéphane Clinchant, Christian Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods for navigation based on large-scale learning typically treat each
episode as a new problem, where the agent is spawned with a clean memory in an
unknown environment. While these generalization capabilities to an unknown
environment are extremely important, we claim that, in a realistic setting, an
agent should have the capacity of exploiting information collected during
earlier robot operations. We address this by introducing a new
retrieval-augmented agent, trained with RL, capable of querying a database
collected from previous episodes in the same environment and learning how to
integrate this additional context information. We introduce a unique agent
architecture for the general navigation task, evaluated on ImageNav,
Instance-ImageNav and ObjectNav. Our retrieval and context encoding methods are
data-driven and employ vision foundation models (FM) for both semantic and
geometric understanding. We propose new benchmarks for these settings and we
show that retrieval allows zero-shot transfer across tasks and environments
while significantly improving performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Scalable and High Availability Solution for Recommending Resolutions
  to Problem Tickets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.19846v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.19846v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harish Saragadam, Chetana K Nayak, Joy Bose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resolution of incidents or problem tickets is a common theme in service
industries in any sector, including billing and charging systems in telecom
domain. Machine learning can help to identify patterns and suggest resolutions
for the problem tickets, based on patterns in the historical data of the
tickets. However, this process may be complicated due to a variety of phenomena
such as data drift and issues such as missing data, lack of data pertaining to
resolutions of past incidents, too many similar sounding resolutions due to
free text and similar sounding text. This paper proposes a robust ML-driven
solution employing clustering, supervised learning, and advanced NLP models to
tackle these challenges effectively. Building on previous work, we demonstrate
clustering-based resolution identification, supervised classification with LDA,
Siamese networks, and One-shot learning, Index embedding. Additionally, we
present a real-time dashboard and a highly available Kubernetes-based
production deployment. Our experiments with both the open-source Bitext
customer-support dataset and proprietary telecom datasets demonstrate high
prediction accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Ghost: Investigating Ranking Bias Hidden in AI-Generated
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowen Gao, Liang Pang, Shicheng Xu, Leigang Qu, Tat-Seng Chua, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of AI-generated content (AIGC), the creation of
high-quality AI-generated videos has become faster and easier, resulting in the
Internet being flooded with all kinds of video content. However, the impact of
these videos on the content ecosystem remains largely unexplored. Video
information retrieval remains a fundamental approach for accessing video
content. Building on the observation that retrieval models often favor
AI-generated content in ad-hoc and image retrieval tasks, we investigate
whether similar biases emerge in the context of challenging video retrieval,
where temporal and visual factors may further influence model behavior. To
explore this, we first construct a comprehensive benchmark dataset containing
both real and AI-generated videos, along with a set of fair and rigorous
metrics to assess bias. This benchmark consists of 13,000 videos generated by
two state-of-the-art open-source video generation models. We meticulously
design a suite of rigorous metrics to accurately measure this preference,
accounting for potential biases arising from the limited frame rate and
suboptimal quality of AIGC videos. We then applied three off-the-shelf video
retrieval models to perform retrieval tasks on this hybrid dataset. Our
findings reveal a clear preference for AI-generated videos in retrieval.
Further investigation shows that incorporating AI-generated videos into the
training set of retrieval models exacerbates this bias. Unlike the preference
observed in image modalities, we find that video retrieval bias arises from
both unseen visual and temporal information, making the root causes of video
bias a complex interplay of these two factors. To mitigate this bias, we
fine-tune the retrieval models using a contrastive learning approach. The
results of this study highlight the potential implications of AI-generated
videos on retrieval systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, Accepted at ACMMM2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Spectral Knowledge Interrogation: A Reliable
  Retrieval-Augmented Generative Framework on Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11557v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11557v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiheng Liang, Zujie Xie, Ziru Yu, Xiangyang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM) has demonstrated significant success in a range of
natural language processing (NLP) tasks within general domain. The emergence of
LLM has introduced innovative methodologies across diverse fields, including
the natural sciences. Researchers aim to implement automated, concurrent
process driven by LLM to supplant conventional manual, repetitive and
labor-intensive work. In the domain of spectral analysis and detection, it is
imperative for researchers to autonomously acquire pertinent knowledge across
various research objects, which encompasses the spectroscopic techniques and
the chemometric methods that are employed in experiments and analysis.
Paradoxically, despite the recognition of spectroscopic detection as an
effective analytical method, the fundamental process of knowledge retrieval
remains both time-intensive and repetitive. In response to this challenge, we
first introduced the Spectral Detection and Analysis Based Paper(SDAAP)
dataset, which is the first open-source textual knowledge dataset for spectral
analysis and detection and contains annotated literature data as well as
corresponding knowledge instruction data. Subsequently, we also designed an
automated Q\&A framework based on the SDAAP dataset, which can retrieve
relevant knowledge and generate high-quality responses by extracting entities
in the input as retrieval parameters. It is worth noting that: within this
framework, LLM is only used as a tool to provide generalizability, while RAG
technique is used to accurately capture the source of the knowledge.This
approach not only improves the quality of the generated responses, but also
ensures the traceability of the knowledge. Experimental results show that our
framework generates responses with more reliable expertise compared to the
baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages,10 figures,3 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Runtime Failure Hunting for Physics Engine Based Software Systems: How
  Far Can We Go? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuqing Li, Qiang Chen, Xiaoxue Ren, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics Engines (PEs) are fundamental software frameworks that simulate
physical interactions in applications ranging from entertainment to
safety-critical systems. Despite their importance, PEs suffer from physics
failures, deviations from expected physical behaviors that can compromise
software reliability, degrade user experience, and potentially cause critical
failures in autonomous vehicles or medical robotics. Current testing approaches
for PE-based software are inadequate, typically requiring white-box access and
focusing on crash detection rather than semantically complex physics failures.
This paper presents the first large-scale empirical study characterizing
physics failures in PE-based software. We investigate three research questions
addressing the manifestations of physics failures, the effectiveness of
detection techniques, and developer perceptions of current detection practices.
Our contributions include: (1) a taxonomy of physics failure manifestations;
(2) a comprehensive evaluation of detection methods including deep learning,
prompt-based techniques, and large multimodal models; and (3) actionable
insights from developer experiences for improving detection approaches. To
support future research, we release PhysiXFails, code, and other materials at
https://sites.google.com/view/physics-failure-detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Sub-pixel Motion Compensation in Learned Video Codecs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Théo Ladune, Thomas Leguay, Pierrick Philippe, Gordon Clare, Félix Henry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion compensation is a key component of video codecs. Conventional codecs
(HEVC and VVC) have carefully refined this coding step, with an important focus
on sub-pixel motion compensation. On the other hand, learned codecs achieve
sub-pixel motion compensation through simple bilinear filtering. This paper
offers to improve learned codec motion compensation by drawing inspiration from
conventional codecs. It is shown that the usage of more advanced interpolation
filters, block-based motion information and finite motion accuracy lead to
better compression performance and lower decoding complexity. Experimental
results are provided on the Cool-chic video codec, where we demonstrate a rate
decrease of more than 10% and a lowering of motion-related decoding complexity
from 391 MAC per pixel to 214 MAC per pixel. All contributions are made
open-source at https://github.com/Orange-OpenSource/Cool-Chic
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAGE: Multimodal Alignment and Generation Enhancement via Bridging
  Visual and Semantic Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaojun E, Yuchen Yang, Jiaheng Wu, Yan Zhang, Tiejun Zhao, Ziyan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the latest advancements in multimodal learning, effectively addressing the
spatial and semantic losses of visual data after encoding remains a critical
challenge. This is because the performance of large multimodal models is
positively correlated with the coupling between visual encoders and large
language models. Existing approaches often face issues such as vector gaps or
semantic disparities, resulting in information loss during the propagation
process. To address these issues, we propose MAGE (Multimodal Alignment and
Generation Enhancement), a novel framework that bridges the semantic spaces of
vision and text through an innovative alignment mechanism. By introducing the
Intelligent Alignment Network (IAN), MAGE achieves dimensional and semantic
alignment. To reduce the gap between synonymous heterogeneous data, we employ a
training strategy that combines cross-entropy and mean squared error,
significantly enhancing the alignment effect. Moreover, to enhance MAGE's
"Any-to-Any" capability, we developed a fine-tuning dataset for multimodal
tool-calling instructions to expand the model's output capability boundaries.
Finally, our proposed multimodal large model architecture, MAGE, achieved
significantly better performance compared to similar works across various
evaluation benchmarks, including MME, MMBench, and SEED. Complete code and
appendix are available at: https://github.com/GTCOM-NLP/MAGE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PC-JND: Subjective Study and <span class="highlight-title">Dataset</span> on Just Noticeable Difference for
  Point Clouds in 6DoF Virtual Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunling Fan, Yun Zhang, Dietmar Saupe, Raouf Hamzaoui, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Just Noticeable Difference (JND) accounts for the minimum distortion at
which humans can perceive a difference between a pristine stimulus and its
distorted version. The JND concept has been widely applied in visual signal
processing tasks, including coding, transmission, rendering, and quality
assessment, to optimize human-centric media experiences. A point cloud is a
mainstream volumetric data representation consisting of both geometry
information and attributes (e.g. color). Point clouds are used for advanced
immersive 3D media such as Virtual Reality (VR). However, the JND
characteristics of viewing point clouds in VR have not been explored before. In
this paper, we study the point cloud-wise JND (PCJND) characteristics in a Six
Degrees of Freedom (6DoF) VR environment using a head-mounted display. Our
findings reveal that the texture PCJND of human eyes is smaller than the
geometry PCJND for most point clouds. Furthermore, we identify a correlation
between colorfulness and texture PCJND. However, there is no significant
correlation between colorfulness and the geometry PCJND, nor between the number
of points and neither the texture or geometry PCJND. To support future research
in JND prediction and perception-driven signal processing, we introduce PC-JND,
a novel point cloud-based JND dataset. This dataset will be made publicly
available to facilitate advancements in perceptual optimization for immersive
media.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures, Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly
  Grounding and Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shibo Gao, Peipei Yang, Yangyang Liu, Yi Chen, Han Zhu, Xuyao Zhang, Linlin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Anomaly Detection (VAD) aims to identify anomalous events in videos and
accurately determine their time intervals. Current VAD methods mainly fall into
two categories: traditional DNN-based approaches that focus on temporal
localization, and LLM-based approaches that emphasize semantic understanding.
Both anomaly understanding and grounding are essential for comprehensive video
anomaly detection and can complement each other. However, no existing model or
dataset supports both tasks simultaneously. To address this, we introduce VAGU
(Video Anomaly Grounding and Understanding), the first benchmark to integrate
both tasks. Each VAGU instance includes annotations for anomaly category,
semantic explanation, precise temporal grounding and Video QA. We also provide
multiple-choice Video QA for objective evaluation. Based on this dataset, we
propose Glance then Scrutinize (GtS), a training-free framework guided by
textual prompts. The framework first enables coarse localization of
high-probability anomalous regions, followed by detailed anomaly interpretation
and temporal boundary refinement. Additionally, we propose the JeAUG metric,
which jointly evaluates semantic interpretability and temporal precision,
overcoming the limitations of traditional metrics. Extensive experiments verify
the effectiveness of our benchmark, framework, and evaluation metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 19 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sync-TVA: A Graph-Attention Framework for Multimodal Emotion Recognition
  with Cross-Modal Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Deng, Yanhui Lu, Jiashu Liao, Shuang Wu, Chongfeng Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal emotion recognition (MER) is crucial for enabling emotionally
intelligent systems that perceive and respond to human emotions. However,
existing methods suffer from limited cross-modal interaction and imbalanced
contributions across modalities. To address these issues, we propose Sync-TVA,
an end-to-end graph-attention framework featuring modality-specific dynamic
enhancement and structured cross-modal fusion. Our design incorporates a
dynamic enhancement module for each modality and constructs heterogeneous
cross-modal graphs to model semantic relations across text, audio, and visual
features. A cross-attention fusion mechanism further aligns multimodal cues for
robust emotion inference. Experiments on MELD and IEMOCAP demonstrate
consistent improvements over state-of-the-art models in both accuracy and
weighted F1 score, especially under class-imbalanced conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embodied Web Agents: Bridging Physical-Digital Realms for Integrated
  Agent Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15677v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15677v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yining Hong, Rui Sun, Bingxuan Li, Xingcheng Yao, Maxine Wu, Alexander Chien, Da Yin, Ying Nian Wu, Zhecan James Wang, Kai-Wei Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI agents today are mostly siloed - they either retrieve and reason over vast
amount of digital information and knowledge obtained online; or interact with
the physical world through embodied perception, planning and action - but
rarely both. This separation limits their ability to solve tasks that require
integrated physical and digital intelligence, such as cooking from online
recipes, navigating with dynamic map data, or interpreting real-world landmarks
using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI
agents that fluidly bridge embodiment and web-scale reasoning. To
operationalize this concept, we first develop the Embodied Web Agents task
environments, a unified simulation platform that tightly integrates realistic
3D indoor and outdoor environments with functional web interfaces. Building
upon this platform, we construct and release the Embodied Web Agents Benchmark,
which encompasses a diverse suite of tasks including cooking, navigation,
shopping, tourism, and geolocation - all requiring coordinated reasoning across
physical and digital realms for systematic assessment of cross-domain
intelligence. Experimental results reveal significant performance gaps between
state-of-the-art AI systems and human capabilities, establishing both
challenges and opportunities at the intersection of embodied cognition and
web-scale knowledge access. All datasets, codes and websites are publicly
available at our project page https://embodied-web-agent.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Positive-Augmented Contrastive Learning for Vision-and-Language
  Evaluation and Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Sarto, Nicholas Moratelli, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in caption generation, existing evaluation
metrics often fail to capture the full quality or fine-grained details of
captions. This is mainly due to their reliance on non-specific human-written
references or noisy pre-training data. Still, finding an effective metric is
crucial not only for captions evaluation but also for the generation phase.
Metrics can indeed play a key role in the fine-tuning stage of captioning
models, ultimately enhancing the quality of the generated captions. In this
paper, we propose PAC-S++, a learnable metric that leverages the CLIP model,
pre-trained on both web-collected and cleaned data and regularized through
additional pairs of generated visual and textual positive samples. Exploiting
this stronger and curated pre-training, we also apply PAC-S++ as a reward in
the Self-Critical Sequence Training (SCST) stage typically employed to
fine-tune captioning models. Extensive experiments on different image and video
datasets highlight the effectiveness of PAC-S++ compared to popular metrics for
the task, including its sensitivity to object hallucinations. Furthermore, we
show that integrating PAC-S++ into the fine-tuning stage of a captioning model
results in semantically richer captions with fewer repetitions and grammatical
errors. Evaluations on out-of-domain benchmarks further demonstrate the
efficacy of our fine-tuning approach in enhancing model capabilities. Source
code and trained models are publicly available at:
https://github.com/aimagelab/pacscore.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Journal of Computer Vision (2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ISDrama: Immersive Spatial Drama Generation through Multimodal <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.20630v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.20630v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Tao Jin, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal immersive spatial drama generation focuses on creating continuous
multi-speaker binaural speech with dramatic prosody based on multimodal
prompts, with potential applications in AR, VR, and others. This task requires
simultaneous modeling of spatial information and dramatic prosody based on
multimodal inputs, with high data collection costs. To the best of our
knowledge, our work is the first attempt to address these challenges. We
construct MRSDrama, the first multimodal recorded spatial drama dataset,
containing binaural drama audios, scripts, videos, geometric poses, and textual
prompts. Then, we propose ISDrama, the first immersive spatial drama generation
model through multimodal prompting. ISDrama comprises these primary components:
1) Multimodal Pose Encoder, based on contrastive learning, considering the
Doppler effect caused by moving speakers to extract unified pose information
from multimodal prompts. 2) Immersive Drama Transformer, a flow-based
mamba-transformer model that generates high-quality drama, incorporating
Drama-MOE to select proper experts for enhanced prosody and pose control. We
also design a context-consistent classifier-free guidance strategy to
coherently generate complete drama. Experimental results show that ISDrama
outperforms baseline models on objective and subjective metrics. The demos are
available at https://aaronz345.github.io/ISDramaDemo. We provide the dataset
and the evaluation code at https://huggingface.co/datasets/AaronZ345/MRSDrama
and https://github.com/AaronZ345/ISDrama.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Multimedia 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Music-Aligned Holistic 3D Dance Generation via Hierarchical Motion
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojie Li, Ronghui Li, Shukai Fang, Shuzhao Xie, Xiaoyang Guo, Jiaqing Zhou, Junkun Peng, Zhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Well-coordinated, music-aligned holistic dance enhances emotional
expressiveness and audience engagement. However, generating such dances remains
challenging due to the scarcity of holistic 3D dance datasets, the difficulty
of achieving cross-modal alignment between music and dance, and the complexity
of modeling interdependent motion across the body, hands, and face. To address
these challenges, we introduce SoulDance, a high-precision music-dance paired
dataset captured via professional motion capture systems, featuring
meticulously annotated holistic dance movements. Building on this dataset, we
propose SoulNet, a framework designed to generate music-aligned, kinematically
coordinated holistic dance sequences. SoulNet consists of three principal
components: (1) Hierarchical Residual Vector Quantization, which models
complex, fine-grained motion dependencies across the body, hands, and face; (2)
Music-Aligned Generative Model, which composes these hierarchical motion units
into expressive and coordinated holistic dance; (3) Music-Motion Retrieval
Module, a pre-trained cross-modal model that functions as a music-dance
alignment prior, ensuring temporal synchronization and semantic coherence
between generated dance and input music throughout the generation process.
Extensive experiments demonstrate that SoulNet significantly surpasses existing
approaches in generating high-quality, music-coordinated, and well-aligned
holistic 3D dance sequences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Swap Joint Diffusion for 2D Long-Form Latent Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05130v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05130v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusheng Dai, Chenxi Wang, Chang Li, Chen Wang, Jun Du, Kewei Li, Ruoyu Wang, Jiefeng Ma, Lei Sun, Jianqing Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Swap Forward (SaFa), a modality-agnostic and efficient
method to generate seamless and coherence long spectrum and panorama through
latent swap joint diffusion across multi-views. We first investigate the
spectrum aliasing problem in spectrum-based audio generation caused by existing
joint diffusion methods. Through a comparative analysis of the VAE latent
representation of Mel-spectra and RGB images, we identify that the failure
arises from excessive suppression of high-frequency components during the
spectrum denoising process due to the averaging operator. To address this
issue, we propose Self-Loop Latent Swap, a frame-level bidirectional swap
applied to the overlapping region of adjacent views. Leveraging stepwise
differentiated trajectories of adjacent subviews, this swap operator adaptively
enhances high-frequency components and avoid spectrum distortion. Furthermore,
to improve global cross-view consistency in non-overlapping regions, we
introduce Reference-Guided Latent Swap, a unidirectional latent swap operator
that provides a centralized reference trajectory to synchronize subview
diffusions. By refining swap timing and intervals, we can achieve a cross-view
similarity-diversity balance in a forward-only manner. Quantitative and
qualitative experiments demonstrate that SaFa significantly outperforms
existing joint diffusion methods and even training-based methods in audio
generation using both U-Net and DiT models, along with effective longer length
adaptation. It also adapts well to panorama generation, achieving comparable
performance with 2 $\sim$ 20 $\times$ faster speed and greater model
generalizability. More generation demos are available at
https://swapforward.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EEmo-Bench: A Benchmark for Multi-modal Large Language Models on Image
  Evoked Emotion Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.16405v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.16405v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lancheng Gao, Ziheng Jia, Yunhao Zeng, Wei Sun, Yiming Zhang, Wei Zhou, Guangtao Zhai, Xiongkuo Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The furnishing of multi-modal large language models (MLLMs) has led to the
emergence of numerous benchmark studies, particularly those evaluating their
perception and understanding capabilities. Among these, understanding
image-evoked emotions aims to enhance MLLMs' empathy, with significant
applications such as human-machine interaction and advertising recommendations.
However, current evaluations of this MLLM capability remain coarse-grained, and
a systematic and comprehensive assessment is still lacking. To this end, we
introduce EEmo-Bench, a novel benchmark dedicated to the analysis of the evoked
emotions in images across diverse content categories. Our core contributions
include: 1) Regarding the diversity of the evoked emotions, we adopt an emotion
ranking strategy and employ the Valence-Arousal-Dominance (VAD) as emotional
attributes for emotional assessment. In line with this methodology, 1,960
images are collected and manually annotated. 2) We design four tasks to
evaluate MLLMs' ability to capture the evoked emotions by single images and
their associated attributes: Perception, Ranking, Description, and Assessment.
Additionally, image-pairwise analysis is introduced to investigate the model's
proficiency in performing joint and comparative analysis. In total, we collect
6,773 question-answer pairs and perform a thorough assessment on 19
commonly-used MLLMs. The results indicate that while some proprietary and
large-scale open-source MLLMs achieve promising overall performance, the
analytical capabilities in certain evaluation dimensions remain suboptimal. Our
EEmo-Bench paves the path for further research aimed at enhancing the
comprehensive perceiving and understanding capabilities of MLLMs concerning
image-evoked emotions, which is crucial for machine-centric emotion perception
and understanding.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-07-28T00:00:00Z">2025-07-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StructText: A Synthetic Table-to-Text Approach for Benchmark Generation
  with Multi-Dimensional Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satyananda Kashyap, Sola Shirai, Nandana Mihindukulasooriya, Horst Samulowitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting structured information from text, such as key-value pairs that
could augment tabular data, is quite useful in many enterprise use cases.
Although large language models (LLMs) have enabled numerous automated pipelines
for converting natural language into structured formats, there is still a lack
of benchmarks for evaluating their extraction quality, especially in specific
domains or focused documents specific to a given organization. Building such
benchmarks by manual annotations is labour-intensive and limits the size and
scalability of the benchmarks. In this work, we present StructText, an
end-to-end framework for automatically generating high-fidelity benchmarks for
key-value extraction from text using existing tabular data. It uses available
tabular data as structured ground truth, and follows a two-stage
``plan-then-execute'' pipeline to synthetically generate corresponding
natural-language text. To ensure alignment between text and structured source,
we introduce a multi-dimensional evaluation strategy that combines (a)
LLM-based judgments on factuality, hallucination, and coherence and (b)
objective extraction metrics measuring numeric and temporal accuracy. We
evaluated the proposed method on 71,539 examples across 49 datasets. Results
reveal that while LLMs achieve strong factual accuracy and avoid hallucination,
they struggle with narrative coherence in producing extractable text. Notably,
models presume numerical and temporal information with high fidelity yet this
information becomes embedded in narratives that resist automated extraction. We
release a framework, including datasets, evaluation tools, and baseline
extraction systems, to support continued research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Data available:
  https://huggingface.co/datasets/ibm-research/struct-text and code available
  at: https://github.com/ibm/struct-text</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling User Behavior from Adaptive <span class="highlight-title">Survey</span>s with Supplemental Context <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Shukla, Daniel Patrick Scantlebury, Rishabh Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling user behavior is critical across many industries where understanding
preferences, intent, or decisions informs personalization, targeting, and
strategic outcomes. Surveys have long served as a classical mechanism for
collecting such behavioral data due to their interpretability, structure, and
ease of deployment. However, surveys alone are inherently limited by user
fatigue, incomplete responses, and practical constraints on their length making
them insufficient for capturing user behavior. In this work, we present LANTERN
(Late-Attentive Network for Enriched Response Modeling), a modular architecture
for modeling user behavior by fusing adaptive survey responses with
supplemental contextual signals. We demonstrate the architectural value of
maintaining survey primacy through selective gating, residual connections and
late fusion via cross-attention, treating survey data as the primary signal
while incorporating external modalities only when relevant. LANTERN outperforms
strong survey-only baselines in multi-label prediction of survey responses. We
further investigate threshold sensitivity and the benefits of selective
modality reliance through ablation and rare/frequent attribute analysis.
LANTERN's modularity supports scalable integration of new encoders and evolving
datasets. This work provides a practical and extensible blueprint for behavior
modeling in survey-centric applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Best Paper, NewInML @ ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Watermarking Large Language Model-based Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Yuan, Chaoqun Yang, Yu Xing, Tong Chen, Nguyen Quoc Viet Hung, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model-based Time Series Forecasting (LLMTS) has shown
remarkable promise in handling complex and diverse temporal data, representing
a significant step toward foundation models for time series analysis. However,
this emerging paradigm introduces two critical challenges. First, the
substantial commercial potential and resource-intensive development raise
urgent concerns about intellectual property (IP) protection. Second, their
powerful time series forecasting capabilities may be misused to produce
misleading or fabricated deepfake time series data. To address these concerns,
we explore watermarking the outputs of LLMTS models, that is, embedding
imperceptible signals into the generated time series data that remain
detectable by specialized algorithms. We propose a novel post-hoc watermarking
framework, Waltz, which is broadly compatible with existing LLMTS models. Waltz
is inspired by the empirical observation that time series patch embeddings are
rarely aligned with a specific set of LLM tokens, which we term ``cold
tokens''. Leveraging this insight, Waltz embeds watermarks by rewiring the
similarity statistics between patch embeddings and cold token embeddings, and
detects watermarks using similarity z-scores. To minimize potential side
effects, we introduce a similarity-based embedding position identification
strategy and employ projected gradient descent to constrain the watermark noise
within a defined boundary. Extensive experiments using two popular LLMTS models
across seven benchmark datasets demonstrate that Waltz achieves high watermark
detection accuracy with minimal impact on the quality of the generated time
series.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Industry Insights from Comparing Deep Learning and GBDT Models for
  E-Commerce Learning-to-Rank <span class="chip">RecSys 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunus Lutz, Timo Wilm, Philipp Duwe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In e-commerce recommender and search systems, tree-based models, such as
LambdaMART, have set a strong baseline for Learning-to-Rank (LTR) tasks.
Despite their effectiveness and widespread adoption in industry, the debate
continues whether deep neural networks (DNNs) can outperform traditional
tree-based models in this domain. To contribute to this discussion, we
systematically benchmark DNNs against our production-grade LambdaMART model. We
evaluate multiple DNN architectures and loss functions on a proprietary dataset
from OTTO and validate our findings through an 8-week online A/B test. The
results show that a simple DNN architecture outperforms a strong tree-based
baseline in terms of total clicks and revenue, while achieving parity in total
units sold.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work was accepted for publication in the 19th ACM Conference on
  Recommender Systems (RecSys 2025). The final published version will be
  available at the ACM Digital Library</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Interactions: Node-Level Graph Generation for Knowledge-Free
  Augmentation in Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyan Wang, Hyunjun Ahn, In-Young Ko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in recommender systems rely on external resources such as
knowledge graphs or large language models to enhance recommendations, which
limit applicability in real-world settings due to data dependency and
computational overhead. Although knowledge-free models are able to bolster
recommendations by direct edge operations as well, the absence of augmentation
primitives drives them to fall short in bridging semantic and structural gaps
as high-quality paradigm substitutes. Unlike existing diffusion-based works
that remodel user-item interactions, this work proposes NodeDiffRec, a
pioneering knowledge-free augmentation framework that enables fine-grained
node-level graph generation for recommendations and expands the scope of
restricted augmentation primitives via diffusion. By synthesizing pseudo-items
and corresponding interactions that align with the underlying distribution for
injection, and further refining user preferences through a denoising preference
modeling process, NodeDiffRec dramatically enhances both semantic diversity and
structural connectivity without external knowledge. Extensive experiments
across diverse datasets and recommendation algorithms demonstrate the
superiority of NodeDiffRec, achieving State-of-the-Art (SOTA) performance, with
maximum average performance improvement 98.6% in Recall@5 and 84.0% in NDCG@5
over selected baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and <span class="highlight-title">Prompt</span>-Guided
  Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duc-Tai Dinh, Duc Anh Khoa Dinh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ZSE-Cap (Zero-Shot Ensemble for Captioning), our 4th place system
in Event-Enriched Image Analysis (EVENTA) shared task on article-grounded image
retrieval and captioning. Our zero-shot approach requires no finetuning on the
competition's data. For retrieval, we ensemble similarity scores from CLIP,
SigLIP, and DINOv2. For captioning, we leverage a carefully engineered prompt
to guide the Gemma 3 model, enabling it to link high-level events from the
article to the visual content in the image. Our system achieved a final score
of 0.42002, securing a top-4 position on the private test set, demonstrating
the effectiveness of combining foundation models through ensembling and
prompting. Our code is available at https://github.com/ductai05/ZSE-Cap.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Community Detection in Academic Networks by Handling
  Publication Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Asaduzzaman Noor, John Sheppard, Jason Clark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding potential research collaborators is a challenging task, especially in
today's fast-growing and interdisciplinary research landscape. While
traditional methods often rely on observable relationships such as
co-authorships and citations to construct the research network, in this work,
we focus solely on publication content to build a topic-based research network
using BERTopic with a fine-tuned SciBERT model that connects and recommends
researchers across disciplines based on shared topical interests. A major
challenge we address is publication imbalance, where some researchers publish
much more than others, often across several topics. Without careful handling,
their less frequent interests are hidden under dominant topics, limiting the
network's ability to detect their full research scope. To tackle this, we
introduce a cloning strategy that clusters a researcher's publications and
treats each cluster as a separate node. This allows researchers to be part of
multiple communities, improving the detection of interdisciplinary links.
Evaluation on the proposed method shows that the cloned network structure leads
to more meaningful communities and uncovers a broader set of collaboration
opportunities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is an extended version of a work accepted at ASONAM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model Powered Decision Support for a Metal Additive
  Manufacturing Knowledge Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Tayyab Khan, Lequn Chen, Wenhe Feng, Seung Ki Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metal additive manufacturing (AM) involves complex interdependencies among
processes, materials, feedstock, and post-processing steps. However, the
underlying relationships and domain knowledge remain fragmented across
literature and static databases that often require expert-level queries,
limiting their applicability in design and planning. To address these
limitations, we develop a novel and structured knowledge graph (KG),
representing 53 distinct metals and alloys across seven material categories,
nine AM processes, four feedstock types, and corresponding post-processing
requirements. A large language model (LLM) interface, guided by a few-shot
prompting strategy, enables natural language querying without the need for
formal query syntax. The system supports a range of tasks, including
compatibility evaluation, constraint-based filtering, and design for AM (DfAM)
guidance. User queries in natural language are normalized, translated into
Cypher, and executed on the KG, with results returned in a structured format.
This work introduces the first interactive system that connects a
domain-specific metal AM KG with an LLM interface, delivering accessible and
explainable decision support for engineers and promoting human-centered tools
in manufacturing knowledge systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted at 11th International Conference of Asian
  Society for Precision Engineering and Nanotechnology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Virtual Quantum Markov Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Ao Chen, Chengkai Zhu, Keming He, Mingrui Jing, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum Markov chains generalize classical Markov chains for random variables
to the quantum realm and exhibit unique inherent properties, making them an
important feature in quantum information theory. In this work, we propose the
concept of virtual quantum Markov chains (VQMCs), focusing on scenarios where
subsystems retain classical information about global systems from measurement
statistics. As a generalization of quantum Markov chains, VQMCs characterize
states where arbitrary global shadow information can be recovered from
subsystems through local quantum operations and measurements. We present an
algebraic characterization for virtual quantum Markov chains and show that the
virtual quantum recovery is fully determined by the block matrices of a quantum
state on its subsystems. Notably, we find a distinction between two classes of
tripartite entanglement by showing that the W state is a VQMC while the GHZ
state is not. Furthermore, we introduce the virtual non-Markovianity to
quantify the non-Markovianity of a given quantum state, which also assesses the
optimal sampling overhead for virtually recovering this state. Our findings
elucidate distinctions between quantum Markov chains and virtual quantum Markov
chains, extending our understanding of quantum recovery to scenarios
prioritizing classical information from measurement statistics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages including appendix, 6 figures, v2: results and references
  updated</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Music Arena: Live Evaluation for Text-to-Music 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghyun Kim, Wayne Chi, Anastasios N. Angelopoulos, Wei-Lin Chiang, Koichi Saito, Shinji Watanabe, Yuki Mitsufuji, Chris Donahue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Music Arena, an open platform for scalable human preference
evaluation of text-to-music (TTM) models. Soliciting human preferences via
listening studies is the gold standard for evaluation in TTM, but these studies
are expensive to conduct and difficult to compare, as study protocols may
differ across systems. Moreover, human preferences might help researchers align
their TTM systems or improve automatic evaluation metrics, but an open and
renewable source of preferences does not currently exist. We aim to fill these
gaps by offering *live* evaluation for TTM. In Music Arena, real-world users
input text prompts of their choosing and compare outputs from two TTM systems,
and their preferences are used to compile a leaderboard. While Music Arena
follows recent evaluation trends in other AI domains, we also design it with
key features tailored to music: an LLM-based routing system to navigate the
heterogeneous type signatures of TTM systems, and the collection of *detailed*
preferences including listening data and natural language feedback. We also
propose a rolling data release policy with user privacy guarantees, providing a
renewable source of preference data and increasing platform transparency.
Through its standardized evaluation protocol, transparent data access policies,
and music-specific features, Music Arena not only addresses key challenges in
the TTM ecosystem but also demonstrates how live evaluation can be thoughtfully
adapted to unique characteristics of specific AI domains.
  Music Arena is available at: https://music-arena.org
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regularizing Subspace Redundancy of Low-Rank Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Zhu, Haiwen Diao, Shang Gao, Jiazuo Yu, Jiawen Zhu, Yunzhi Zhuge, Shuai Hao, Xu Jia, Lu Zhang, Ying Zhang, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) and its variants have delivered strong capability
in Parameter-Efficient Transfer Learning (PETL) by minimizing trainable
parameters and benefiting from reparameterization. However, their projection
matrices remain unrestricted during training, causing high representation
redundancy and diminishing the effectiveness of feature adaptation in the
resulting subspaces. While existing methods mitigate this by manually adjusting
the rank or implicitly applying channel-wise masks, they lack flexibility and
generalize poorly across various datasets and architectures. Hence, we propose
ReSoRA, a method that explicitly models redundancy between mapping subspaces
and adaptively Regularizes Subspace redundancy of Low-Rank Adaptation.
Specifically, it theoretically decomposes the low-rank submatrices into
multiple equivalent subspaces and systematically applies de-redundancy
constraints to the feature distributions across different projections.
Extensive experiments validate that our proposed method consistently
facilitates existing state-of-the-art PETL methods across various backbones and
datasets in vision-language retrieval and standard visual classification
benchmarks. Besides, as a training supervision, ReSoRA can be seamlessly
integrated into existing approaches in a plug-and-play manner, with no
additional inference costs. Code is publicly available at:
https://github.com/Lucenova/ReSoRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, Accepted by ACMMM2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dark Side of Modalities: Reinforced Multimodal Distillation for
  Multimodal Knowledge Graph Reasoning <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhao, Ying Zhang, Xuhui Sui, Baohang Zhou, Haoze Zhu, Jeff Z. Pan, Xiaojie Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The multimodal knowledge graph reasoning (MKGR) task aims to predict the
missing facts in the incomplete MKGs by leveraging auxiliary images and
descriptions of entities. Existing approaches are trained with single-target
objectives, which neglect the probabilistic correlations of entity labels,
especially in non-target entities. Moreover, previous studies incorporate all
modalities statically or adaptively, overlooking the negative impacts of
irrelevant or misleading information in the incompetent modalities. To address
these issues, we introduce a novel Reinforced Multimodal Distillation
framework, exploiting the Dark Side of Modalities (DSoM) from two perspectives:
(1) Dark knowledge from non-target entities: We propose to train a unimodal KGR
model through logit distillation to mimic the multimodal soft labels provided
by pre-trained multimodal teacher models. The multimodal soft labels could
provide rich supervision signals with subtle correlations among both target and
non-target entities from multiple perspectives. We further decouple logits into
neighbor entities and non-neighbor entities to divide into two types of
correlations. (2) Dark side in unhelpful modalities: To exclude the adverse
effects of unhelpful modalities, we introduce a reinforced teacher combination
mechanism that dynamically selects the optimal set of multimodal teachers for
each triple. The agent is trained to maximize the rewards, which are only
assigned to the beneficial multimodal combination strategies for the student
model. Comprehensive experiments demonstrate the effectiveness of DSoM
framework on 5 MKGR datasets. Codes are available at github.com/OreOZhao/DSoM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vocalize: Lead Acquisition and User Engagement through Gamified Voice
  Competitions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edvin Teskeredzic, Muamer Paric, Adna Sestic, Petra Fribert, Anamarija Lukac, Hadzem Hadzic, Kemal Altwlkany, Emanuel Lacic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the prospect of creating engaging user experiences and
collecting leads through an interactive and gamified platform. We introduce
Vocalize, an end-to-end system for increasing user engagement and lead
acquisition through gamified voice competitions. Using audio processing
techniques and LLMs, we create engaging and interactive experiences that have
the potential to reach a wide audience, foster brand recognition, and increase
customer loyalty. We describe the system from a technical standpoint and report
results from launching Vocalize at 4 different live events. Our user study
shows that Vocalize is capable of generating significant user engagement, which
shows potential for gamified audio campaigns in marketing and similar
verticals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Hypertext 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controllable Video-to-Music Generation with Multiple Time-Varying
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junxian Wu, Weitao You, Heda Zuo, Dengming Zhang, Pei Chen, Lingyun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music enhances video narratives and emotions, driving demand for automatic
video-to-music (V2M) generation. However, existing V2M methods relying solely
on visual features or supplementary textual inputs generate music in a
black-box manner, often failing to meet user expectations. To address this
challenge, we propose a novel multi-condition guided V2M generation framework
that incorporates multiple time-varying conditions for enhanced control over
music generation. Our method uses a two-stage training strategy that enables
learning of V2M fundamentals and audiovisual temporal synchronization while
meeting users' needs for multi-condition control. In the first stage, we
introduce a fine-grained feature selection module and a progressive temporal
alignment attention mechanism to ensure flexible feature alignment. For the
second stage, we develop a dynamic conditional fusion module and a
control-guided decoder module to integrate multiple conditions and accurately
guide the music composition process. Extensive experiments demonstrate that our
method outperforms existing V2M pipelines in both subjective and objective
evaluations, significantly enhancing control and alignment with user
expectations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 33rd ACM International Conference on Multimedia
  (ACMMM 2025). The project page is available at
  https://kita-wjx.github.io/MCV2M/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T2VParser: Adaptive Decomposition Tokens for Partial Alignment in Text
  to Video Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yili Li, Gang Xiong, Gaopeng Gou, Xiangyan Qu, Jiamin Zhuang, Zhen Li, Junzheng Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video retrieval essentially aims to train models to align visual
content with textual descriptions accurately. Due to the impressive general
multimodal knowledge demonstrated by image-text pretrained models such as CLIP,
existing work has primarily focused on extending CLIP knowledge for video-text
tasks. However, videos typically contain richer information than images. In
current video-text datasets, textual descriptions can only reflect a portion of
the video content, leading to partial misalignment in video-text matching.
Therefore, directly aligning text representations with video representations
can result in incorrect supervision, ignoring the inequivalence of information.
In this work, we propose T2VParser to extract multiview semantic
representations from text and video, achieving adaptive semantic alignment
rather than aligning the entire representation. To extract corresponding
representations from different modalities, we introduce Adaptive Decomposition
Tokens, which consist of a set of learnable tokens shared across modalities.
The goal of T2VParser is to emphasize precise alignment between text and video
while retaining the knowledge of pretrained models. Experimental results
demonstrate that T2VParser achieves accurate partial alignment through
effective cross-modal content decomposition. The code is available at
https://github.com/Lilidamowang/T2VParser.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaXsive: High-Capacity and Robust Training-Free Generative Image
  Watermarking in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.21195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.21195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-Yuan Mao, Cheng-Chang Tsai, Chun-Shien Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The great success of the diffusion model in image synthesis led to the
release of gigantic commercial models, raising the issue of copyright
protection and inappropriate content generation. Training-free diffusion
watermarking provides a low-cost solution for these issues. However, the prior
works remain vulnerable to rotation, scaling, and translation (RST) attacks.
Although some methods employ meticulously designed patterns to mitigate this
issue, they often reduce watermark capacity, which can result in identity (ID)
collusion. To address these problems, we propose MaXsive, a training-free
diffusion model generative watermarking technique that has high capacity and
robustness. MaXsive best utilizes the initial noise to watermark the diffusion
model. Moreover, instead of using a meticulously repetitive ring pattern, we
propose injecting the X-shape template to recover the RST distortions. This
design significantly increases robustness without losing any capacity, making
ID collusion less likely to happen. The effectiveness of MaXsive has been
verified on two well-known watermarking benchmarks under the scenarios of
verification and identification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PUMA: Layer-Pruned Language Model for Efficient Unified Multimodal
  Retrieval with Modality-Adaptive Learning <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.08064v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.08064v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Lyu, Rui Shao, Gongwei Chen, Yijie Zhu, Weili Guan, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As multimedia content expands, the demand for unified multimodal retrieval
(UMR) in real-world applications increases. Recent work leverages multimodal
large language models (MLLMs) to tackle this task. However, their large
parameter size results in high training costs and low inference efficiency. To
address this, we propose PUMA: a Layer-Pruned Language Model for Efficient
Unified Multimodal Retrieval with Modality-Adaptive Learning. Our approach
improves UMR from both structural and learning perspectives. (1) Structurally,
we propose Layer-Pruned Self-Distillation, which prunes MLLMs by keeping only
shallow layers while distilling features from dropped deep layers as teacher
signals. This reduces parameters and preserves representation capability. (2)
On the learning side, we introduce Modality-Adaptive Contrastive Learning Loss
(MAC-Loss), which separates in-batch negatives into harder intra-modality and
easier inter-modality groups based on the target modality, assigning different
temperature strategies to enhance learning efficiency. Experiments show our
method significantly reduces resource usage while maintaining strong
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstructFLIP: Exploring Unified Vision-Language Model for Face
  Anti-spoofing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.12060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.12060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun-Hsiang Lin, Yu-Wen Tseng, Kang-Yang Huang, Jhih-Ciang Wu, Wen-Huang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face anti-spoofing (FAS) aims to construct a robust system that can withstand
diverse attacks. While recent efforts have concentrated mainly on cross-domain
generalization, two significant challenges persist: limited semantic
understanding of attack types and training redundancy across domains. We
address the first by integrating vision-language models (VLMs) to enhance the
perception of visual input. For the second challenge, we employ a meta-domain
strategy to learn a unified model that generalizes well across multiple
domains. Our proposed InstructFLIP is a novel instruction-tuned framework that
leverages VLMs to enhance generalization via textual guidance trained solely on
a single domain. At its core, InstructFLIP explicitly decouples instructions
into content and style components, where content-based instructions focus on
the essential semantics of spoofing, and style-based instructions consider
variations related to the environment and camera characteristics. Extensive
experiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA
models in accuracy and substantially reducing training redundancy across
diverse domains in FAS. Project website is available at
https://kunkunlin1221.github.io/InstructFLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MM'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Multimodal Reconstruction for Misinformation Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.06010v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.06010v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefanos-Iordanis Papadopoulos, Christos Koutlis, Symeon Papadopoulos, Panagiotis C. Petrantonakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal misinformation, such as miscaptioned images, where captions
misrepresent an image's origin, context, or meaning, poses a growing challenge
in the digital age. To support fact-checkers, researchers have focused on
developing datasets and methods for multimodal misinformation detection (MMD).
Due to the scarcity of large-scale annotated MMD datasets, recent approaches
rely on synthetic training data created via out-of-context pairings or named
entity manipulations (e.g., altering names, dates, or locations). However,
these often yield simplistic examples that lack real-world complexity, limiting
model robustness. Meanwhile, Large Vision-Language Models (LVLMs) remain
underexplored for generating diverse and realistic synthetic data for MMD. To
address, we introduce "Miscaption This!", a collection of LVLM-generated
miscaptioned image datasets. Additionally, we introduce "Latent Multimodal
Reconstruction" (LAMAR), a network trained to reconstruct the embeddings of
truthful captions, providing a strong auxiliary signal to guide detection. We
explore various training strategies (end-to-end vs. large-scale pre-training)
and integration mechanisms (direct, mask, gate, and attention). Extensive
experiments show that models trained on "MisCaption This!" generalize better to
real-world misinformation while LAMAR achieves new state-of-the-art on both
NewsCLIPpings and VERITE benchmarks; highlighting the value of LVLM-generated
data and reconstruction-based networks for advancing MMD. Our code is available
at https://github.com/stevejpapad/miscaptioned-image-reconstruction
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MagicMotion: Controllable Video Generation with Dense-to-Sparse
  Trajectory Guidance <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quanhao Li, Zhen Xing, Rui Wang, Hui Zhang, Qi Dai, Zuxuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in video generation have led to remarkable improvements in
visual quality and temporal coherence. Upon this, trajectory-controllable video
generation has emerged to enable precise object motion control through
explicitly defined spatial paths. However, existing methods struggle with
complex object movements and multi-object motion control, resulting in
imprecise trajectory adherence, poor object consistency, and compromised visual
quality. Furthermore, these methods only support trajectory control in a single
format, limiting their applicability in diverse scenarios. Additionally, there
is no publicly available dataset or benchmark specifically tailored for
trajectory-controllable video generation, hindering robust training and
systematic evaluation. To address these challenges, we introduce MagicMotion, a
novel image-to-video generation framework that enables trajectory control
through three levels of conditions from dense to sparse: masks, bounding boxes,
and sparse boxes. Given an input image and trajectories, MagicMotion seamlessly
animates objects along defined trajectories while maintaining object
consistency and visual quality. Furthermore, we present MagicData, a
large-scale trajectory-controlled video dataset, along with an automated
pipeline for annotation and filtering. We also introduce MagicBench, a
comprehensive benchmark that assesses both video quality and trajectory control
accuracy across different numbers of objects. Extensive experiments demonstrate
that MagicMotion outperforms previous methods across various metrics. Our
project page are publicly available at
https://quanhaol.github.io/magicmotion-site.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-08-05T05:41:38.262748515Z">
            2025-08-05 05:41:38 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
