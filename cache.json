{"2025-08-04T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2508.02635v1","updated":"2025-08-04T17:22:08Z","published":"2025-08-04T17:22:08Z","title":"Test Set Quality in Multilingual LLM Evaluation","summary":"  Several multilingual benchmark datasets have been developed in a\nsemi-automatic manner in the recent past to measure progress and understand the\nstate-of-the-art in the multilingual capabilities of Large Language Models.\nHowever, there is not a lot of attention paid to the quality of the datasets\nthemselves, despite the existence of previous work in identifying errors in\neven fully human-annotated test sets. In this paper, we manually analyze recent\nmultilingual evaluation sets in two languages - French and Telugu, identifying\nseveral errors in the process. We compare the performance difference across\nseveral LLMs with the original and revised versions of the datasets and\nidentify large differences (almost 10% in some cases) in both languages). Based\non these results, we argue that test sets should not be considered immutable\nand should be revisited, checked for correctness, and potentially versioned. We\nend with some recommendations for both the dataset creators as well as\nconsumers on addressing the dataset quality issues.\n","authors":["Kranti Chalamalasetti","Gabriel Bernier-Colborne","Yvan Gauthier","Sowmya Vajjala"],"pdf_url":"https://arxiv.org/pdf/2508.02635v1.pdf","comment":"Accepted at the 1st Workshop on Multilingual Data Quality Signals,\n  COLM 2025, Short paper. 10 pages in total"},{"id":"http://arxiv.org/abs/2508.02631v1","updated":"2025-08-04T17:19:56Z","published":"2025-08-04T17:19:56Z","title":"Pointer: Linear-Complexity Long-Range Modeling without Pre-training","summary":"  We introduce Pointer, a novel architecture that achieves linear $O(NK)$\ncomplexity for long-range sequence modeling while maintaining superior\nperformance without requiring pre-training. Unlike standard attention\nmechanisms that compute $O(N^2)$ pairwise interactions, our approach uses\nlayer-wise pointer chaining where each layer's pointer selection depends on\nprevious layer's pointer positions, creating explicit long-distance connections\nthrough pointer chains. We demonstrate that this architecture achieves\n$2$--$10\\times$ speedup on long sequences compared to standard transformers,\nmaintains $>95\\%$ accuracy on copy tasks at distances up to 2048 tokens, and\nlearns interpretable pointer patterns that reveal structured dependency\nmodeling. Our experiments on efficiency benchmarks, long-range dependency\ntasks, and interpretability analysis show that Pointer offers a compelling\nalternative to attention mechanisms for scenarios requiring efficient\nlong-range modeling without pre-training dependencies.\n","authors":["Zixi Li"],"pdf_url":"https://arxiv.org/pdf/2508.02631v1.pdf","comment":"Submitted to Nordic AI Meet 2025"},{"id":"http://arxiv.org/abs/2508.02629v1","updated":"2025-08-04T17:18:14Z","published":"2025-08-04T17:18:14Z","title":"HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and\n  Decision in Embodied Agents","summary":"  Recent advances in multimodal large language models (MLLMs) have enabled\nricher perceptual grounding for code policy generation in embodied agents.\nHowever, most existing systems lack effective mechanisms to adaptively monitor\npolicy execution and repair codes during task completion. In this work, we\nintroduce HyCodePolicy, a hybrid language-based control framework that\nsystematically integrates code synthesis, geometric grounding, perceptual\nmonitoring, and iterative repair into a closed-loop programming cycle for\nembodied agents. Technically, given a natural language instruction, our system\nfirst decomposes it into subgoals and generates an initial executable program\ngrounded in object-centric geometric primitives. The program is then executed\nin simulation, while a vision-language model (VLM) observes selected\ncheckpoints to detect and localize execution failures and infer failure\nreasons. By fusing structured execution traces capturing program-level events\nwith VLM-based perceptual feedback, HyCodePolicy infers failure causes and\nrepairs programs. This hybrid dual feedback mechanism enables self-correcting\nprogram synthesis with minimal human supervision. Our results demonstrate that\nHyCodePolicy significantly improves the robustness and sample efficiency of\nrobot manipulation policies, offering a scalable strategy for integrating\nmultimodal reasoning into autonomous decision-making pipelines.\n","authors":["Yibin Liu","Zhixuan Liang","Zanxin Chen","Tianxing Chen","Mengkang Hu","Wanxi Dong","Congsheng Xu","Zhaoming Han","Yusen Qin","Yao Mu"],"pdf_url":"https://arxiv.org/pdf/2508.02629v1.pdf","comment":"Accepted to ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic\n  Intelligence"},{"id":"http://arxiv.org/abs/2508.02622v1","updated":"2025-08-04T17:10:08Z","published":"2025-08-04T17:10:08Z","title":"Noosemia: toward a Cognitive and Phenomenological Account of\n  Intentionality Attribution in Human-Generative AI Interaction","summary":"  This paper introduces and formalizes Noosemia, a novel\ncognitive-phenomenological phenomenon emerging from human interaction with\ngenerative AI systems, particularly those enabling dialogic or multimodal\nexchanges. We propose a multidisciplinary framework to explain how, under\ncertain conditions, users attribute intentionality, agency, and even\ninteriority to these systems - a process grounded not in physical resemblance,\nbut in linguistic performance, epistemic opacity, and emergent technological\ncomplexity. By linking an LLM declination of meaning holism to our technical\nnotion of the LLM Contextual Cognitive Field, we clarify how LLMs construct\nmeaning relationally and how coherence and a simulacrum of agency arise at the\nhuman-AI interface. The analysis situates noosemia alongside pareidolia,\nanimism, the intentional stance and the uncanny valley, distinguishing its\nunique characteristics. We also introduce a-noosemia to describe the\nphenomenological withdrawal of such projections. The paper concludes with\nreflections on the broader philosophical, epistemological, and social\nimplications of noosemic dynamics and directions for future research.\n","authors":["Enrico De Santis","Antonello Rizzi"],"pdf_url":"https://arxiv.org/pdf/2508.02622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02621v1","updated":"2025-08-04T17:08:47Z","published":"2025-08-04T17:08:47Z","title":"HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous\n  Healthcare Research","summary":"  The efficacy of AI agents in healthcare research is hindered by their\nreliance on static, predefined strategies. This creates a critical limitation:\nagents can become better tool-users but cannot learn to become better strategic\nplanners, a crucial skill for complex domains like healthcare. We introduce\nHealthFlow, a self-evolving AI agent that overcomes this limitation through a\nnovel meta-level evolution mechanism. HealthFlow autonomously refines its own\nhigh-level problem-solving policies by distilling procedural successes and\nfailures into a durable, strategic knowledge base. To anchor our research and\nfacilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark\nfeaturing complex, realistic health data analysis tasks derived from\npeer-reviewed clinical research. Our comprehensive experiments demonstrate that\nHealthFlow's self-evolving approach significantly outperforms state-of-the-art\nagent frameworks. This work marks a necessary shift from building better\ntool-users to designing smarter, self-evolving task-managers, paving the way\nfor more autonomous and effective AI for scientific discovery.\n","authors":["Yinghao Zhu","Yifan Qi","Zixiang Wang","Lei Gu","Dehao Sui","Haoran Hu","Xichen Zhang","Ziyi He","Liantao Ma","Lequan Yu"],"pdf_url":"https://arxiv.org/pdf/2508.02621v1.pdf","comment":"Code: https://github.com/yhzhu99/HealthFlow"},{"id":"http://arxiv.org/abs/2508.02618v1","updated":"2025-08-04T17:06:23Z","published":"2025-08-04T17:06:23Z","title":"Mitigating Attention Hacking in Preference-Based Reward Modeling via\n  Interaction Distillation","summary":"  The reward model (RM), as the core component of reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs), responsible for\nproviding reward signals to generated responses. However, mainstream preference\nmodeling in RM is inadequate in terms of token-level interaction, making its\njudgment signals vulnerable to being hacked by misallocated attention to\ncontext. This stems from two fundamental limitations: (1) Current preference\nmodeling employs decoder-only architectures, where the unidirectional causal\nattention mechanism leads to forward-decaying intra-sequence attention within\nthe prompt-response sequence. (2) The independent Siamese-encoding paradigm\ninduces the absence of token-level inter-sequence attention between chosen and\nrejected sequences. To address this \"attention hacking\", we propose\n\"Interaction Distillation\", a novel training framework for more adequate\npreference modeling through attention-level optimization. The method introduces\nan interaction-based natural language understanding model as the teacher to\nprovide sophisticated token interaction patterns via comprehensive attention,\nand guides the preference modeling to simulate teacher model's interaction\npattern through an attentional alignment objective. Through extensive\nexperiments, interaction distillation has demonstrated its ability to provide\nmore stable and generalizable reward signals compared to state-of-the-art RM\noptimization methods that target data noise, highlighting the attention hacking\nconstitute a more fundamental limitation in RM.\n","authors":["Jianxiang Zang","Meiling Ning","Shihan Dou","Jiazheng Zhang","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2508.02618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09251v2","updated":"2025-08-04T16:57:32Z","published":"2025-06-10T21:22:51Z","title":"Extrapolation by Association: Length Generalization Transfer in\n  Transformers","summary":"  Transformer language models have demonstrated impressive generalization\ncapabilities in natural language domains, yet we lack a fine-grained\nunderstanding of how such generalization arises. In this paper, we investigate\nlength generalization--the ability to extrapolate from shorter to longer\ninputs--through the lens of \\textit{task association}. We find that length\ngeneralization can be \\textit{transferred} across related tasks. That is,\ntraining a model with a longer and related auxiliary task can lead it to\ngeneralize to unseen and longer inputs from some other target task. We\ndemonstrate this length generalization transfer across diverse algorithmic\ntasks, including arithmetic operations, string transformations, and maze\nnavigation. Our results show that transformer models can inherit generalization\ncapabilities from similar tasks when trained jointly. Moreover, we observe\nsimilar transfer effects in pretrained language models, suggesting that\npretraining equips models with reusable computational scaffolding that\nfacilitates extrapolation in downstream settings. Finally, we provide initial\nmechanistic evidence that length generalization transfer correlates with the\nre-use of the same attention heads between the tasks. Together, our findings\ndeepen our understanding of how transformers generalize to out-of-distribution\ninputs and highlight the compositional reuse of inductive structure across\ntasks.\n","authors":["Ziyang Cai","Nayoung Lee","Avi Schwarzschild","Samet Oymak","Dimitris Papailiopoulos"],"pdf_url":"https://arxiv.org/pdf/2506.09251v2.pdf","comment":"23 pages, 20 figures"},{"id":"http://arxiv.org/abs/2508.02591v1","updated":"2025-08-04T16:46:15Z","published":"2025-08-04T16:46:15Z","title":"CharBench: Evaluating the Role of Tokenization in Character-Level Tasks","summary":"  Tasks that require character-level reasoning, such as counting or locating\ncharacters within words, remain challenging for contemporary language models. A\ncommon conjecture is that language models' reliance on subword units, rather\nthan characters, contributes to their struggles with character-level tasks, yet\nrecent studies offer conflicting conclusions about the role of tokenization,\nleaving its impact unclear. To address this gap, we introduce CharBench, a\ncomprehensive benchmark of character-level tasks that is two orders of\nmagnitude larger than existing alternatives. We evaluate a diverse range of\nleading open-weight and proprietary models on CharBench and find that it\npresents a significant challenge to modern LLMs, with an average accuracy of\n43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic\nproperties of words and their segmentations into tokens correspond to model\nperformance. For counting tasks, we find that tokenization properties are\nweakly correlated with correctness, while the length of the queried word and\nthe actual character count play a more significant part. In contrast, for tasks\nrequiring intra-word positional understanding, performance is negatively\ncorrelated with the length of the token containing the queried character,\nsuggesting that longer tokens obscure character position information for LLMs.\nWe encourage future work to build on the benchmark and evaluation methodology\nintroduced here as tools for improving model performance on such tasks.\n","authors":["Omri Uzan","Yuval Pinter"],"pdf_url":"https://arxiv.org/pdf/2508.02591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02587v1","updated":"2025-08-04T16:43:09Z","published":"2025-08-04T16:43:09Z","title":"Parameter-Efficient Routed Fine-Tuning: Mixture-of-Experts Demands\n  Mixture of Adaptation Modules","summary":"  Mixture-of-Experts (MoE) benefits from a dynamic routing mechanism among\ntheir specialized experts, which existing Parameter- Efficient Fine-Tuning\n(PEFT) strategies fail to leverage. This motivates us to investigate whether\nadaptation modules themselves should incorporate routing mechanisms to align\nwith MoE's multi-expert architecture. We analyze dynamics of core components\nwhen applying PEFT to MoE language models and examine how different routing\nstrategies affect adaptation effectiveness. Extensive experiments adapting\nOLMoE-1B-7B and Mixtral-8x7B on various commonsense and math reasoning tasks\nvalidate the performance and efficiency of our routed approach. We identify the\noptimal configurations for different scenarios and provide empirical analyses\nwith practical insights to facilitate better PEFT and MoE applications.\n","authors":["Yilun Liu","Yunpu Ma","Yuetian Lu","Shuo Chen","Zifeng Ding","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2508.02587v1.pdf","comment":"This paper is a preprint under review. arXiv admin note: text overlap\n  with arXiv:2411.08212"},{"id":"http://arxiv.org/abs/2508.02584v1","updated":"2025-08-04T16:40:02Z","published":"2025-08-04T16:40:02Z","title":"MArgE: Meshing Argumentative Evidence from Multiple Large Language\n  Models for Justifiable Claim Verification","summary":"  Leveraging outputs from multiple large language models (LLMs) is emerging as\na method for harnessing their power across a wide range of tasks while\nmitigating their capacity for making errors, e.g., hallucinations. However,\ncurrent approaches to combining insights from multiple LLMs often involve\nunstructured interactions (e.g., free debate), resulting in model generations\nthat are not faithfully justifiable. In this work, we introduce MArgE, a novel\nframework to provide formal structure to the evidence from each LLM, in the\nform of a tree of extracted arguments, for the task of claim verification. We\nuse a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks\nand semantics from the field of computational argumentation, to construct\nstructured argument trees for given claims. This process creates an inspectable\npathway from the initial arguments to the final claim verification decisions,\nproviding a faithful justification thereof. We show experimentally that MArgE\ncan significantly outperform single LLMs, including three open-source models\n(4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior\nmethods for unstructured multi-LLM debates. We thus demonstrate the advantages\nof incorporating formal, argumentative reasoning mechanisms when combining\nmultiple LLM outputs.\n","authors":["Ming Pok Ng","Junqi Jiang","Gabriel Freedman","Antonio Rago","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2508.02584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07927v3","updated":"2025-08-04T16:37:00Z","published":"2025-01-14T08:30:49Z","title":"Gandalf the Red: Adaptive Security for LLMs","summary":"  Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and expresses the security-utility in an\noptimizable form. We further address the shortcomings in existing evaluations\nby introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed\nto generate realistic, adaptive attack. Using Gandalf, we collect and release a\ndataset of 279k prompt attacks. Complemented by benign user data, our analysis\nreveals the interplay between security and utility, showing that defenses\nintegrated in the LLM (e.g., system prompts) can degrade usability even without\nblocking requests. We demonstrate that restricted application domains,\ndefense-in-depth, and adaptive defenses are effective strategies for building\nsecure and useful LLM applications.\n","authors":["Niklas Pfister","Václav Volhejn","Manuel Knott","Santiago Arias","Julia Bazińska","Mykhailo Bichurin","Alan Commike","Janet Darling","Peter Dienes","Matthew Fiedler","David Haber","Matthias Kraft","Marco Lancini","Max Mathys","Damián Pascual-Ortiz","Jakub Podolak","Adrià Romero-López","Kyriacos Shiarlis","Andreas Signer","Zsolt Terek","Athanasios Theocharis","Daniel Timbrell","Samuel Trautwein","Samuel Watts","Yun-Han Wu","Mateo Rojas-Carulla"],"pdf_url":"https://arxiv.org/pdf/2501.07927v3.pdf","comment":"Niklas Pfister, V\\'aclav Volhejn and Manuel Knott contributed equally"},{"id":"http://arxiv.org/abs/2508.02574v1","updated":"2025-08-04T16:28:58Z","published":"2025-08-04T16:28:58Z","title":"EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based\n  Sentiment Analysis in Healthcare","summary":"  Arabic-language patient feedback remains under-analysed because dialect\ndiversity and scarce aspect-level sentiment labels hinder automated assessment.\nTo address this gap, we introduce EHSAN, a data-centric hybrid pipeline that\nmerges ChatGPT pseudo-labelling with targeted human review to build the first\nexplainable Arabic aspect-based sentiment dataset for healthcare. Each sentence\nis annotated with an aspect and sentiment label (positive, negative, or\nneutral), forming a pioneering Arabic dataset aligned with healthcare themes,\nwith ChatGPT-generated rationales provided for each label to enhance\ntransparency. To evaluate the impact of annotation quality on model\nperformance, we created three versions of the training data: a fully supervised\nset with all labels reviewed by humans, a semi-supervised set with 50% human\nreview, and an unsupervised set with only machine-generated labels. We\nfine-tuned two transformer models on these datasets for both aspect and\nsentiment classification. Experimental results show that our Arabic-specific\nmodel achieved high accuracy even with minimal human supervision, reflecting\nonly a minor performance drop when using ChatGPT-only labels. Reducing the\nnumber of aspect classes notably improved classification metrics across the\nboard. These findings demonstrate an effective, scalable approach to Arabic\naspect-based sentiment analysis (SA) in healthcare, combining large language\nmodel annotation with human expertise to produce a robust and explainable\ndataset. Future directions include generalisation across hospitals, prompt\nrefinement, and interpretable data-driven modelling.\n","authors":["Eman Alamoudi","Ellis Solaiman"],"pdf_url":"https://arxiv.org/pdf/2508.02574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02573v1","updated":"2025-08-04T16:27:56Z","published":"2025-08-04T16:27:56Z","title":"Guess or Recall? Training CNNs to Classify and Localize Memorization in\n  LLMs","summary":"  Verbatim memorization in Large Language Models (LLMs) is a multifaceted\nphenomenon involving distinct underlying mechanisms. We introduce a novel\nmethod to analyze the different forms of memorization described by the existing\ntaxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the\nattention weights of the LLM and evaluate the alignment between this taxonomy\nand the attention weights involved in decoding.\n  We find that the existing taxonomy performs poorly and fails to reflect\ndistinct mechanisms within the attention blocks. We propose a new taxonomy that\nmaximizes alignment with the attention weights, consisting of three categories:\nmemorized samples that are guessed using language modeling abilities, memorized\nsamples that are recalled due to high duplication in the training set, and\nnon-memorized samples. Our results reveal that few-shot verbatim memorization\ndoes not correspond to a distinct attention mechanism. We also show that a\nsignificant proportion of extractable samples are in fact guessed by the model\nand should therefore be studied separately. Finally, we develop a custom visual\ninterpretability technique to localize the regions of the attention weights\ninvolved in each form of memorization.\n","authors":["Jérémie Dentan","Davide Buscaldi","Sonia Vanier"],"pdf_url":"https://arxiv.org/pdf/2508.02573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02558v1","updated":"2025-08-04T16:14:03Z","published":"2025-08-04T16:14:03Z","title":"Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction","summary":"  Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.\n","authors":["Yuerong Song","Xiaoran Liu","Ruixiao Li","Zhigeng Liu","Zengfeng Huang","Qipeng Guo","Ziwei He","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2508.02558v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2508.02556v1","updated":"2025-08-04T16:08:49Z","published":"2025-08-04T16:08:49Z","title":"Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU\n  Neural Networks","summary":"  Automated annotation of clinical text with standardized medical concepts is\ncritical for enabling structured data extraction and decision support. SNOMED\nCT provides a rich ontology for labeling clinical entities, but manual\nannotation is labor-intensive and impractical at scale. This study introduces a\nneural sequence labeling approach for SNOMED CT concept recognition using a\nBidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text\nwith domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences\ninto overlapping 19-token chunks enriched with contextual, syntactic, and\nmorphological features. The Bi-GRU model assigns IOB tags to identify concept\nspans and achieves strong performance with a 90 percent F1-score on the\nvalidation set. These results surpass traditional rule-based systems and match\nor exceed existing neural models. Qualitative analysis shows effective handling\nof ambiguous terms and misspellings. Our findings highlight that lightweight\nRNN-based architectures can deliver high-quality clinical concept annotation\nwith significantly lower computational cost than transformer-based models,\nmaking them well-suited for real-world deployment.\n","authors":["Ali Noori","Pratik Devkota","Somya Mohanty","Prashanti Manda"],"pdf_url":"https://arxiv.org/pdf/2508.02556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02555v1","updated":"2025-08-04T16:05:36Z","published":"2025-08-04T16:05:36Z","title":"Building and Aligning Comparable Corpora","summary":"  Comparable corpus is a set of topic aligned documents in multiple languages,\nwhich are not necessarily translations of each other. These documents are\nuseful for multilingual natural language processing when there is no parallel\ntext available in some domains or languages. In addition, comparable documents\nare informative because they can tell what is being said about a topic in\ndifferent languages. In this paper, we present a method to build comparable\ncorpora from Wikipedia encyclopedia and EURONEWS website in English, French and\nArabic languages. We further experiment a method to automatically align\ncomparable documents using cross-lingual similarity measures. We investigate\ntwo cross-lingual similarity measures to align comparable documents. The first\nmeasure is based on bilingual dictionary, and the second measure is based on\nLatent Semantic Indexing (LSI). Experiments on several corpora show that the\nCross-Lingual LSI (CL-LSI) measure outperforms the dictionary based measure.\nFinally, we collect English and Arabic news documents from the British\nBroadcast Corporation (BBC) and from ALJAZEERA (JSC) news website respectively.\nThen we use the CL-LSI similarity measure to automatically align comparable\ndocuments of BBC and JSC. The evaluation of the alignment shows that CL-LSI is\nnot only able to align cross-lingual documents at the topic level, but also it\nis able to do this at the event level.\n","authors":["Motaz Saad","David Langlois","Kamel Smaili"],"pdf_url":"https://arxiv.org/pdf/2508.02555v1.pdf","comment":"27 pages, 11 figures"},{"id":"http://arxiv.org/abs/2508.02546v1","updated":"2025-08-04T15:59:15Z","published":"2025-08-04T15:59:15Z","title":"What are you sinking? A geometric approach on attention sink","summary":"  Attention sink (AS) is a consistent pattern in transformer attention maps\nwhere certain tokens (often special tokens or positional anchors)\ndisproportionately attract attention from other tokens. We show that in\ntransformers, AS is not an architectural artifact, but it is the manifestation\nof a fundamental geometric principle: the establishment of reference frames\nthat anchor representational spaces. We analyze several architectures and\nidentify three distinct reference frame types, centralized, distributed, and\nbidirectional, that correlate with the attention sink phenomenon. We show that\nthey emerge during the earliest stages of training as optimal solutions to the\nproblem of establishing stable coordinate systems in high-dimensional spaces.\nWe show the influence of architecture components, particularly position\nencoding implementations, on the specific type of reference frame. This\nperspective transforms our understanding of transformer attention mechanisms\nand provides insights for both architecture design and the relationship with\nAS.\n","authors":["Valeria Ruscio","Umberto Nanni","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2508.02546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18182v2","updated":"2025-08-04T15:53:52Z","published":"2025-07-24T08:28:17Z","title":"SCOPE: Stochastic and Counterbiased Option Placement for Evaluating\n  Large Language Models","summary":"  Large Language Models (LLMs) can achieve inflated scores on multiple-choice\ntasks by exploiting inherent biases in option positions or labels, rather than\ndemonstrating genuine understanding. This study introduces SCOPE, an evaluation\nframework designed to measure and mitigate such selection bias in a\ndataset-independent manner. By repeatedly invoking a null prompt that lacks\nsemantic content, SCOPE estimates each model's unique position-bias\ndistribution. It then redistributes the answer slot according to the\ninverse-bias distribution, thereby equalizing the lucky-rate, the probability\nof selecting the correct answer by chance. Furthermore, it prevents\nsemantically similar distractors from being placed adjacent to the answer,\nthereby blocking near-miss guesses based on superficial proximity cues. Across\nmultiple benchmark experiments, SCOPE consistently outperformed existing\ndebiasing methods in terms of stable performance improvements and showed\nclearer confidence distributions over correct options. This framework thus\noffers a new standard for enhancing the fairness and reliability of LLM\nevaluations.\n","authors":["Wonjun Jeong","Dongseok Kim","Taegkeun Whangbo"],"pdf_url":"https://arxiv.org/pdf/2507.18182v2.pdf","comment":"Comments: 34 pages, 1 figure. v2: All \"Consequence.\" statements in\n  the Theoretical Analysis section relabeled as \"Corollary.\"; duplicated values\n  in Table 20 (previously identical to Table 15) corrected"},{"id":"http://arxiv.org/abs/2507.03336v2","updated":"2025-08-04T15:48:55Z","published":"2025-07-04T06:49:02Z","title":"Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky","summary":"  Large language models (LLMs) are increasingly tasked with invoking enterprise\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\nintent or when required arguments are left underspecified. We introduce\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\npersona-driven, multi-turn dialogues in which the assistant must distinguish\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\nreal-world readiness via a dynamic suite that redeploys each model in a live\nagentic loop and reports end-to-end goal completion alongside conventional\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\nrelease an open corpus of 5000 production-grade enterprise API specifications\npaired with rigorously validated, disambiguation-focused dialogues, offering a\npractical blueprint for building reliable, enterprise-ready tool-calling\nagents.\n","authors":["Ashutosh Hathidara","Julien Yu","Sebastian Schreiber"],"pdf_url":"https://arxiv.org/pdf/2507.03336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02540v1","updated":"2025-08-04T15:47:17Z","published":"2025-08-04T15:47:17Z","title":"What's in the News? Towards Identification of Bias by Commission,\n  Omission, and Source Selection (COSS)","summary":"  In a world overwhelmed with news, determining which information comes from\nreliable sources or how neutral is the reported information in the news\narticles poses a challenge to news readers. In this paper, we propose a\nmethodology for automatically identifying bias by commission, omission, and\nsource selection (COSS) as a joint three-fold objective, as opposed to the\nprevious work separately addressing these types of bias. In a pipeline concept,\nwe describe the goals and tasks of its steps toward bias identification and\nprovide an example of a visualization that leverages the extracted features and\npatterns of text reuse.\n","authors":["Anastasia Zhukova","Terry Ruas","Felix Hamborg","Karsten Donnay","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2508.02540v1.pdf","comment":"published in the Proceedings of the 2023 ACM/IEEE Joint Conference on\n  Digital Libraries"},{"id":"http://arxiv.org/abs/2508.02532v1","updated":"2025-08-04T15:41:35Z","published":"2025-08-04T15:41:35Z","title":"Contextual Graph Transformer: A Small Language Model for Enhanced\n  Engineering Document Information Extraction","summary":"  Standard transformer-based language models, while powerful for general text,\noften struggle with the fine-grained syntax and entity relationships in complex\ntechnical, engineering documents. To address this, we propose the Contextual\nGraph Transformer (CGT), a hybrid neural architecture that combines Graph\nNeural Networks (GNNs) and Transformers for domain-specific question answering.\nCGT constructs a dynamic graph over input tokens using sequential, skip-gram,\nand semantic similarity edges, which is processed by GATv2Conv layers for local\nstructure learning. These enriched embeddings are then passed to a Transformer\nencoder to capture global dependencies. Unlike generic large models, technical\ndomains often require specialized language models with stronger\ncontextualization and structure awareness. CGT offers a parameter-efficient\nsolution for such use cases. Integrated into a Retrieval-Augmented Generation\n(RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7%\nhigher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from\nCGTs ability to jointly model structural token interactions and long-range\nsemantic coherence. The model is trained from scratch using a two-phase\napproach: pretraining on general text followed by fine-tuning on\ndomain-specific manuals. This highlights CGTs adaptability to technical\nlanguage, enabling better grounding, entity tracking, and retrieval-augmented\nresponses in real-world applications.\n","authors":["Karan Reddy","Mayukha Pal"],"pdf_url":"https://arxiv.org/pdf/2508.02532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02527v1","updated":"2025-08-04T15:36:51Z","published":"2025-08-04T15:36:51Z","title":"I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic\n  Representations in LLaMA 3.2","summary":"  Large language models demonstrate proficiency on phonetic tasks, such as\nrhyming, without explicit phonetic or auditory grounding. In this work, we\ninvestigate how \\verb|Llama-3.2-1B-Instruct| represents token-level phonetic\ninformation. Our results suggest that Llama uses a rich internal model of\nphonemes to complete phonetic tasks. We provide evidence for high-level\norganization of phoneme representations in its latent space. In doing so, we\nalso identify a ``phoneme mover head\" which promotes phonetic information\nduring rhyming tasks. We visualize the output space of this head and find that,\nwhile notable differences exist, Llama learns a model of vowels similar to the\nstandard IPA vowel chart for humans, despite receiving no direct supervision to\ndo so.\n","authors":["Jack Merullo","Arjun Khurana","Oliver McLaughlin"],"pdf_url":"https://arxiv.org/pdf/2508.02527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02515v1","updated":"2025-08-04T15:19:22Z","published":"2025-08-04T15:19:22Z","title":"PoeTone: A Framework for Constrained Generation of Structured Chinese\n  Songci with LLMs","summary":"  This paper presents a systematic investigation into the constrained\ngeneration capabilities of large language models (LLMs) in producing Songci, a\nclassical Chinese poetry form characterized by strict structural, tonal, and\nrhyme constraints defined by Cipai templates. We first develop a comprehensive,\nmulti-faceted evaluation framework that includes: (i) a formal conformity\nscore, (ii) automated quality assessment using LLMs, (iii) human evaluation,\nand (iv) classification-based probing tasks. Using this framework, we evaluate\nthe generative performance of 18 LLMs, including 3 proprietary models and 15\nopen-source models across four families, under five prompting strategies:\nzero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.\nFinally, we propose a Generate-Critic architecture in which the evaluation\nframework functions as an automated critic. Leveraging the critic's feedback as\na reward signal, we fine-tune three lightweight open-source LLMs via supervised\nfine-tuning (SFT), resulting in improvements of up to 5.88% in formal\nconformity. Our findings offer new insights into the generative strengths and\nlimitations of LLMs in producing culturally significant and formally\nconstrained literary texts.\n","authors":["Zhan Qu","Shuzhou Yuan","Michael Färber"],"pdf_url":"https://arxiv.org/pdf/2508.02515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02513v1","updated":"2025-08-04T15:18:41Z","published":"2025-08-04T15:18:41Z","title":"Modular Arithmetic: Language Models Solve Math Digit by Digit","summary":"  While recent work has begun to uncover the internal strategies that Large\nLanguage Models (LLMs) employ for simple arithmetic tasks, a unified\nunderstanding of their underlying mechanisms is still lacking. We extend recent\nfindings showing that LLMs represent numbers in a digit-wise manner and present\nevidence for the existence of digit-position-specific circuits that LLMs use to\nperform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that\noperate independently on different digit positions (units, tens, hundreds).\nNotably, such circuits exist independently of model size and of tokenization\nstrategy, i.e. both for models that encode longer numbers digit-by-digit and as\none token. Using Feature Importance and Causal Interventions, we identify and\nvalidate the digit-position-specific circuits, revealing a compositional and\ninterpretable structure underlying the solving of arithmetic problems in LLMs.\nOur interventions selectively alter the model's prediction at targeted digit\npositions, demonstrating the causal role of digit-position circuits in solving\narithmetic tasks.\n","authors":["Tanja Baeumel","Daniil Gurgurov","Yusser al Ghussin","Josef van Genabith","Simon Ostermann"],"pdf_url":"https://arxiv.org/pdf/2508.02513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02511v1","updated":"2025-08-04T15:17:13Z","published":"2025-08-04T15:17:13Z","title":"Test-time Prompt Intervention","summary":"  Test-time compute has led to remarkable success in the large language model\n(LLM) community, particularly for complex tasks, where longer chains of thought\n(CoTs) are generated to enhance reasoning capabilities. However, growing\nevidence reveals that such reasoning models often produce CoTs plagued by\nexcessive redundancy, including unnecessary verification steps and repetitive\nreasoning shifts. The root cause lies in post-training of them that overly rely\non outcome reward paradigms, as the data of process reward paradigms, which\nregulate intermediate reasoning steps, is difficult to construct at scale. To\naddress this, we propose PI, a novel framework for Test-time Prompt\nIntervention. PI provides an interface to dynamically guide and regulate\nreasoning paths during inference through timely (When module) and proper (How\nmodule) interventions and post-intervention sampling (Which module). This\nallows human problem-solving expertise and cognitive science principles to be\nseamlessly integrated into LLMs' reasoning processes, enhancing controllability\nand interpretability. Extensive experiments across multiple models and datasets\ndemonstrate that PI significantly shortens CoTs while reducing hallucination,\nyielding more concise and reliable reasoning.\n","authors":["Chenxu Yang","Qingyi Si","Mz Dai","Dingyu Yao","Mingyu Zheng","Minghui Chen","Zheng Lin","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2508.02511v1.pdf","comment":"23 pages, 16 figures, under review"},{"id":"http://arxiv.org/abs/2508.02503v1","updated":"2025-08-04T15:11:51Z","published":"2025-08-04T15:11:51Z","title":"OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical\n  Modeling","summary":"  LLM-based solvers have emerged as a promising means of automating problem\nmodeling and solving. However, they remain unreliable and often depend on\niterative repair loops that result in significant latency. We introduce\nOptiHive, an LLM-based framework that produces high-quality solvers for\noptimization problems from natural-language descriptions without iterative\nself-correction. OptiHive uses a single batched LLM query to generate diverse\ncomponents (solvers, problem instances, and validation tests) and filters out\nerroneous components to ensure fully interpretable outputs. Taking into account\nthe imperfection of the generated components, we employ a statistical model to\ninfer their true performance, enabling principled uncertainty quantification\nand solver selection. On tasks ranging from traditional optimization problems\nto challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive\nsignificantly outperforms baselines, increasing the optimality rate from 5\\% to\n92\\% on the most complex problems.\n","authors":["Maxime Bouscary","Saurabh Amin"],"pdf_url":"https://arxiv.org/pdf/2508.02503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.03165v2","updated":"2025-08-04T15:10:59Z","published":"2025-04-04T04:43:13Z","title":"Efficient Dynamic Clustering-Based Document Compression for\n  Retrieval-Augmented-Generation","summary":"  Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge injection during large language model (LLM) inference in recent\nyears. However, due to their limited ability to exploit fine-grained\ninter-document relationships, current RAG implementations face challenges in\neffectively addressing the retrieved noise and redundancy content, which may\ncause error in the generation results. To address these limitations, we propose\nan Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG)\nthat utilizes latent inter-document relationships while simultaneously removing\nirrelevant information and redundant content. We validate our approach, built\nupon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and\nHallucination-Detection datasets. Experimental results show that our method\nachieves consistent performance improvements across various scenarios and\nexperimental settings, demonstrating strong robustness and applicability. Our\ncode and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG.\n","authors":["Weitao Li","Kaiming Liu","Xiangyu Zhang","Xuanyu Lei","Weizhi Ma","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2504.03165v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02502v1","updated":"2025-08-04T15:10:44Z","published":"2025-08-04T15:10:44Z","title":"From Monolingual to Bilingual: Investigating Language Conditioning in\n  Large Language Models for Psycholinguistic Tasks","summary":"  Large Language Models (LLMs) exhibit strong linguistic capabilities, but\nlittle is known about how they encode psycholinguistic knowledge across\nlanguages. We investigate whether and how LLMs exhibit human-like\npsycholinguistic responses under different linguistic identities using two\ntasks: sound symbolism and word valence. We evaluate two models,\nLlama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and\nbilingual prompting in English, Dutch, and Chinese. Behaviorally, both models\nadjust their outputs based on prompted language identity, with Qwen showing\ngreater sensitivity and sharper distinctions between Dutch and Chinese. Probing\nanalysis reveals that psycholinguistic signals become more decodable in deeper\nlayers, with Chinese prompts yielding stronger and more stable valence\nrepresentations than Dutch. Our results demonstrate that language identity\nconditions both output behavior and internal representations in LLMs, providing\nnew insights into their application as models of cross-linguistic cognition.\n","authors":["Shuzhou Yuan","Zhan Qu","Mario Tawfelis","Michael Färber"],"pdf_url":"https://arxiv.org/pdf/2508.02502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02498v1","updated":"2025-08-04T15:07:38Z","published":"2025-08-04T15:07:38Z","title":"Monsoon Uprising in Bangladesh: How Facebook Shaped Collective Identity","summary":"  This study investigates how Facebook shaped collective identity during the\nJuly 2024 pro-democracy uprising in Bangladesh, known as the Monsoon Uprising.\nDuring government repression, protesters turned to Facebook as a central space\nfor resistance, where multimodal expressions, images, memes, videos, hashtags,\nand satirical posts played an important role in unifying participants. Using a\nqualitative approach, this research analyzes visual rhetoric, verbal discourse,\nand digital irony to reveal how shared symbols, protest art, and slogans built\na sense of solidarity. Key elements included the symbolic use of red, the\nironic metaphorical use of the term \"Razakar\", and the widespread sharing of\nvisuals representing courage, injustice, and resistance. The findings show that\nthe combination of visual and verbal strategies on Facebook not only mobilized\npublic sentiment, but also built a strong collective identity that challenged\nauthoritarian narratives. This study tries to demonstrate how online platforms\ncan serve as powerful tools for identity construction and political\nmobilization in the digital age.\n","authors":["Md Tasin Abir","Arpita Chowdhury","Ashfia Rahman"],"pdf_url":"https://arxiv.org/pdf/2508.02498v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.20243v3","updated":"2025-08-04T14:52:16Z","published":"2025-05-26T17:21:26Z","title":"It's High Time: A Survey of Temporal Question Answering","summary":"  Time plays a critical role in how information is generated, retrieved, and\ninterpreted. In this survey, we provide a comprehensive overview of Temporal\nQuestion Answering (TQA), a research area that focuses on answering questions\ninvolving temporal constraints or context. As the amount of time-stamped\ncontent from sources like news articles, web archives, and knowledge bases\nincreases, systems must address challenges such as detecting temporal intent,\nnormalizing time expressions, ordering events, and reasoning over evolving or\nambiguous facts. We focus on recent advances in TQA enabled by neural\narchitectures, especially transformer-based models and Large Language Models\n(LLMs), highlighting progress in temporal language modeling,\nretrieval-augmented generation (RAG), and temporal reasoning. We also discuss\nbenchmark datasets and evaluation strategies designed to test temporal\nrobustness, recency awareness, and generalization.\n","authors":["Bhawna Piryani","Abdelrahman Abdallah","Jamshid Mozafari","Avishek Anand","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2505.20243v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11336v2","updated":"2025-08-04T14:42:02Z","published":"2025-05-16T15:02:19Z","title":"XtraGPT: Context-Aware and Controllable Academic Paper Revision via\n  Human-AI Collaboration","summary":"  Despite the growing adoption of large language models (LLMs) in academic\nworkflows, their capabilities remain limited when it comes to supporting\nhigh-quality scientific writing. Most existing systems are designed for\ngeneral-purpose scientific text generation and fail to meet the sophisticated\ndemands of research communication beyond surface-level polishing, such as\nconceptual coherence across sections. Furthermore, academic writing is\ninherently iterative and revision-driven, a process not well supported by\ndirect prompting-based paradigms. To address these scenarios, we propose a\nhuman-AI collaboration framework for academic paper revision. We first\nintroduce a comprehensive dataset of 7,040 research papers from top-tier venues\nannotated with over 140,000 instruction-response pairs that reflect realistic,\nsection-level scientific revisions. Building on the dataset, we develop\nXtraGPT, the first suite of open-source LLMs, designed to provide\ncontext-aware, instruction-guided writing assistance, ranging from 1.5B to 14B\nparameters. Extensive experiments validate that XtraGPT significantly\noutperforms same-scale baselines and approaches the quality of proprietary\nsystems. Both automated preference assessments and human evaluations confirm\nthe effectiveness of our models in improving scientific drafts.\n","authors":["Nuo Chen","Andre Lin HuiKai","Jiaying Wu","Junyi Hou","Zining Zhang","Qian Wang","Xidong Wang","Bingsheng He"],"pdf_url":"https://arxiv.org/pdf/2505.11336v2.pdf","comment":"Preprint. The model report is available at\n  https://arxiv.org/abs/2505.11336v1"},{"id":"http://arxiv.org/abs/2508.02470v1","updated":"2025-08-04T14:36:31Z","published":"2025-08-04T14:36:31Z","title":"AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language\n  and Multi-Agent Collaboration","summary":"  While many tools are available for designing AI, non-experts still face\nchallenges in clearly expressing their intent and managing system complexity.\nWe introduce AIAP, a no-code platform that integrates natural language input\nwith visual workflows. AIAP leverages a coordinated multi-agent system to\ndecompose ambiguous user instructions into modular, actionable steps, hidden\nfrom users behind a unified interface. A user study involving 32 participants\nshowed that AIAP's AI-generated suggestions, modular workflows, and automatic\nidentification of data, actions, and context significantly improved\nparticipants' ability to develop services intuitively. These findings highlight\nthat natural language-based visual programming significantly reduces barriers\nand enhances user experience in AI service design.\n","authors":["Hyunjn An","Yongwon Kim","Wonduk Seo","Joonil Park","Daye Kang","Changhoon Oh","Dokyun Kim","Seunghyun Lee"],"pdf_url":"https://arxiv.org/pdf/2508.02470v1.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2508.02452v1","updated":"2025-08-04T14:17:29Z","published":"2025-08-04T14:17:29Z","title":"LatentPrompt: Optimizing Promts in Latent Space","summary":"  Recent advances have shown that optimizing prompts for Large Language Models\n(LLMs) can significantly improve task performance, yet many optimization\ntechniques rely on heuristics or manual exploration. We present LatentPrompt, a\nmodel-agnostic framework for prompt optimization that leverages latent semantic\nspace to automatically generate, evaluate, and refine candidate prompts without\nrequiring hand-crafted rules. Beginning with a set of seed prompts, our method\nembeds them in a continuous latent space and systematically explores this space\nto identify prompts that maximize task-specific performance. In a\nproof-of-concept study on the Financial PhraseBank sentiment classification\nbenchmark, LatentPrompt increased classification accuracy by approximately 3\npercent after a single optimization cycle. The framework is broadly applicable,\nrequiring only black-box access to an LLM and an automatic evaluation metric,\nmaking it suitable for diverse domains and tasks.\n","authors":["Mateusz Bystroński","Grzegorz Piotrowski","Nitesh V. Chawla","Tomasz Kajdanowicz"],"pdf_url":"https://arxiv.org/pdf/2508.02452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12601v3","updated":"2025-08-04T14:11:45Z","published":"2024-10-16T14:21:52Z","title":"CCSBench: Evaluating Compositional Controllability in LLMs for\n  Scientific Document Summarization","summary":"  To broaden the dissemination of scientific knowledge to diverse audiences, it\nis desirable for scientific document summarization systems to simultaneously\ncontrol multiple attributes such as length and empirical focus. However,\nexisting research typically focuses on controlling single attributes, leaving\nthe compositional control of multiple attributes underexplored. To address this\ngap, we introduce CCSBench, the first evaluation benchmark for compositional\ncontrollable summarization in the scientific domain. Our benchmark enables\nfine-grained control over both explicit attributes (e.g., length), which are\nobjective and straightforward, and implicit attributes (e.g., conceptual or\nempirical focus), which are more subjective and abstract. We conduct extensive\nexperiments using various large language models (LLMs) under various settings,\nincluding in-context learning, parameter-efficient fine-tuning, and two-stage\nmodular methods for balancing control over different attributes. Our findings\nreveal significant limitations in LLMs capabilities in balancing trade-offs\nbetween control attributes, especially implicit ones that require deeper\nunderstanding and abstract reasoning.\n","authors":["Yixi Ding","Jiaying Wu","Tongyao Zhu","Yanxia Qin","Qian Liu","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2410.12601v3.pdf","comment":"Accepted to KDD 2025 SciSoc LLM Workshop: Large Language Models for\n  Scientific and Societal Advances"},{"id":"http://arxiv.org/abs/2405.18937v2","updated":"2025-08-04T13:54:40Z","published":"2024-05-29T09:43:48Z","title":"Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description","summary":"  In this paper, we introduce Part-Aware Point Grounded Description (PaPGD), a\nchallenging task aimed at advancing 3D multimodal learning for fine-grained,\npart-aware segmentation grounding and detailed explanation of 3D objects.\nExisting 3D datasets largely focus on either vision-only part segmentation or\nvision-language scene segmentation, lacking the fine-grained multimodal\nsegmentation needed for robotic navigation and interaction in real-world\nenvironments. To address this gap, we present the 3DCoMPaT Grounded\nInstructions (3DCoMPaT-GrIn) Dataset, a comprehensive resource that pairs rich\npoint cloud descriptions with corresponding part-level segmentation masks. This\ndataset encompasses extensive samples designed for both PaPGD and fine-grained\nsingle-part grounding tasks. To tackle the inherent challenges of grounding\nobjects and generating grounded descriptions at the part level, we propose\nKestrel, a part-aware 3D multimodal large language model that integrates an\nadvanced language model for nuanced language comprehension with multi-level\npoint feature propagation and query refinement mechanism to enhance spatial\nreasoning at the part level. The extensive experiments demonstrate that Kestrel\neffectively bridges the gap between part-aware language understanding and 3D\nsegmentation grounding, paving the way for more robust and interpretable 3D\nobject comprehension that meets the demands of real-world robotic applications.\nProject page at https://feielysia.github.io/Kestrel.github.io/\n","authors":["Mahmoud Ahmed","Junjie Fei","Jian Ding","Eslam Mohamed Bakr","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2405.18937v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02430v1","updated":"2025-08-04T13:49:30Z","published":"2025-08-04T13:49:30Z","title":"AI-Based Measurement of Innovation: Mapping Expert Insight into Large\n  Language Model Applications","summary":"  Measuring innovation often relies on context-specific proxies and on expert\nevaluation. Hence, empirical innovation research is often limited to settings\nwhere such data is available. We investigate how large language models (LLMs)\ncan be leveraged to overcome the constraints of manual expert evaluations and\nassist researchers in measuring innovation. We design an LLM framework that\nreliably approximates domain experts' assessment of innovation from\nunstructured text data. We demonstrate the performance and broad applicability\nof this framework through two studies in different contexts: (1) the\ninnovativeness of software application updates and (2) the originality of\nuser-generated feedback and improvement ideas in product reviews. We compared\nthe performance (F1-score) and reliability (consistency rate) of our LLM\nframework against alternative measures used in prior innovation studies, and to\nstate-of-the-art machine learning- and deep learning-based models. The LLM\nframework achieved higher F1-scores than the other approaches, and its results\nare highly consistent (i.e., results do not change across runs). This article\nequips R&D personnel in firms, as well as researchers, reviewers, and editors,\nwith the knowledge and tools to effectively use LLMs for measuring innovation\nand evaluating the performance of LLM-based innovation measures. In doing so,\nwe discuss, the impact of important design decisions-including model selection,\nprompt engineering, training data size, training data distribution, and\nparameter settings-on performance and reliability. Given the challenges\ninherent in using human expert evaluation and existing text-based measures, our\nframework has important implications for harnessing LLMs as reliable,\nincreasingly accessible, and broadly applicable research tools for measuring\ninnovation.\n","authors":["Robin Nowak","Patrick Figge","Carolin Haeussler"],"pdf_url":"https://arxiv.org/pdf/2508.02430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02426v1","updated":"2025-08-04T13:46:33Z","published":"2025-08-04T13:46:33Z","title":"Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding","summary":"  Since knowledge graphs (KG) will continue to evolve in real scenarios,\ntraditional KGE models are only suitable for static knowledge graphs.\nTherefore, continual knowledge graph embedding (CKGE) has attracted the\nattention of researchers. Currently, a key challenge facing CKGE is that the\nmodel is prone to \"catastrophic forgetting\", resulting in the loss of\npreviously learned knowledge. In order to effectively alleviate this problem,\nwe propose a new CKGE model BAKE. First, we note that the Bayesian posterior\nupdate principle provides a natural continual learning strategy that is\ninsensitive to data order and can theoretically effectively resist the\nforgetting of previous knowledge during data evolution. Different from the\nexisting CKGE method, BAKE regards each batch of new data as a Bayesian update\nof the model prior. Under this framework, as long as the posterior distribution\nof the model is maintained, the model can better preserve the knowledge of\nearly snapshots even after evolving through multiple time snapshots. Secondly,\nwe propose a continual clustering method for CKGE, which further directly\ncombats knowledge forgetting by constraining the evolution difference (or\nchange amplitude) between new and old knowledge between different snapshots. We\nconduct extensive experiments on BAKE on multiple datasets, and the results\nshow that BAKE significantly outperforms existing baseline models.\n","authors":["Linyu Li","Zhi Jin","Yuanpeng He","Dongming Jin","Yichi Zhang","Haoran Duan","Nyima Tash"],"pdf_url":"https://arxiv.org/pdf/2508.02426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01281v4","updated":"2025-08-04T13:46:22Z","published":"2024-11-02T15:23:28Z","title":"Arena-Lite: Efficient and Reliable Large Language Model Evaluation via\n  Tournament-Based Direct Comparisons","summary":"  As Large Language Models (LLMs) expand across domains, LLM judges have become\nessential for systems evaluation. Current benchmarks typically compare system\noutputs against baselines. This baseline-mediated approach, though convenient,\nyields lower reliability than direct comparison between systems. We propose\nArena-Lite which integrates tournament structure on top of head-to-head\ncomparison. The application of a tournament structure and direct comparison\neliminates the need for baseline outputs, reduces the number of required\ncomparisons, and allows higher reliability in system rankings. We conducted two\nexperiments: (1) controlled stochastic modeling and (2) empirical validation\nwith a real LLM judge. Those experiments collectively demonstrate that\nArena-Lite consistently achieves higher reliability with fewer comparisons,\neven with smaller datasets or weaker judges. We release an easy-to-use web\ndemonstration and code to foster adoption of Arena-Lite, streamlining model\nselection across research and industry communities. Arena-Lite demo and code\nare available on\n\\href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}\n","authors":["Seonil Son","Ju-Min Oh","Heegon Jin","Cheolhun Jang","Jeongbeom Jeong","Kuntae Kim"],"pdf_url":"https://arxiv.org/pdf/2411.01281v4.pdf","comment":"8 pages for main body, 19 pages in total"},{"id":"http://arxiv.org/abs/2506.16792v2","updated":"2025-08-04T13:42:52Z","published":"2025-06-20T07:16:47Z","title":"MIST: Jailbreaking Black-box Large Language Models via Iterative\n  Semantic Tuning","summary":"  Despite efforts to align large language models (LLMs) with societal and moral\nvalues, these models remain susceptible to jailbreak attacks -- methods\ndesigned to elicit harmful responses. Jailbreaking black-box LLMs is considered\nchallenging due to the discrete nature of token inputs, restricted access to\nthe target LLM, and limited query budget. To address the issues above, we\npropose an effective method for jailbreaking black-box large language Models\nvia Iterative Semantic Tuning, named MIST. MIST enables attackers to\niteratively refine prompts that preserve the original semantic intent while\ninducing harmful content. Specifically, to balance semantic similarity with\ncomputational efficiency, MIST incorporates two key strategies: sequential\nsynonym search, and its advanced version -- order-determining optimization. We\nconduct extensive experiments on two datasets using two open-source and four\nclosed-source models. Results show that MIST achieves competitive attack\nsuccess rate, relatively low query count, and fair transferability,\noutperforming or matching state-of-the-art jailbreak methods. Additionally, we\nconduct analysis on computational efficiency to validate the practical\nviability of MIST.\n","authors":["Muyang Zheng","Yuanzhi Yao","Changting Lin","Rui Wang","Caihong Kai"],"pdf_url":"https://arxiv.org/pdf/2506.16792v2.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2508.02419v1","updated":"2025-08-04T13:40:59Z","published":"2025-08-04T13:40:59Z","title":"Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination\n  via Attention Lens","summary":"  Large vision-language models (LVLMs) have demonstrated remarkable multimodal\ncomprehension and reasoning capabilities, but they still suffer from severe\nobject hallucination. Previous studies primarily attribute the flaw to\nlinguistic prior caused by the scale mismatch between visual encoders and large\nlanguage models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon\nLLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,\ngenerating descriptions inconsistent with visual cues. However, through an\nin-depth investigation of the hallucinated mechanisms, we empirically reveal a\npreviously overlooked phenomenon: LVLMs may ignore not only visual information\nbut also textual modality during hallucination, a behavior termed as modality\nbias, which indicates that LVLMs struggle to simultaneously attend to both\nvisual and textual modalities, leading to fragmented understanding of\nuser-provided instructions. Based on this observation, we propose a simple yet\neffective training-free method to mitigate object hallucination. Concretely, we\nintervene and adjust the attention weights of textual and visual tokens,\nbalancing cross-modal compatibility for better alignment with user intentions.\nFurthermore, we adopt a contrastive decoding strategy to reduce the LVLM's\noverreliance on its parametric knowledge, synergistically enhancing our\nattention manipulation. Extensive experiments confirm the widespread presence\nof modality bias in LVLMs. Notably, our method effectively mitigates\nhallucination across multiple open-source LVLMs and benchmarks, highlighting\nits generalizability and efficacy.\n","authors":["Haohan Zheng","Zhenguo Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.02419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05026v3","updated":"2025-08-04T13:38:49Z","published":"2025-05-08T08:00:32Z","title":"Do MLLMs Capture How Interfaces Guide User Behavior? A Benchmark for\n  Multimodal UI/UX Design Understanding","summary":"  User interface (UI) design goes beyond visuals, guiding user behavior and\noverall user experience (UX). Strategically crafted interfaces, for example,\ncan boost sign-ups and drive business sales, underscoring the shift toward\nUI/UX as a unified design concept. While recent studies have explored UI\nquality evaluation using Multimodal Large Language Models (MLLMs), they largely\nfocus on surface-level features, overlooking behavior-oriented aspects. To fill\nthis gap, we introduce WiserUI-Bench, a novel benchmark for assessing models'\nmultimodal understanding of UI/UX design. It includes 300 diverse real-world UI\nimage pairs, each consisting of two design variants A/B-tested at scale by\nactual companies, where one was empirically validated to steer more user\nactions than the other. Each pair is accompanied one or more of 684\nexpert-curated rationales that capture key factors behind each winning design's\neffectiveness, spanning diverse cognitive dimensions of UX. Our benchmark\nsupports two core tasks: (1) selecting the more effective UI/UX design by\npredicting the A/B test verified winner and (2) assessing how well a model,\ngiven the winner, can explain its effectiveness in alignment with expert\nreasoning. Experiments across several MLLMs show that current models exhibit\nlimited nuanced reasoning about UI/UX design and its behavioral impact. We\nbelieve our work will foster research in UI/UX understanding and enable broader\napplications such as behavior-aware interface optimization.\n","authors":["Jaehyun Jeon","Min Soo Kim","Jang Han Yoon","Sumin Shim","Yejin Choi","Hanbin Kim","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2505.05026v3.pdf","comment":"26 pages, 25 figures, Our code and dataset:\n  https://github.com/jeochris/wiserui-bench"},{"id":"http://arxiv.org/abs/2508.02401v1","updated":"2025-08-04T13:26:16Z","published":"2025-08-04T13:26:16Z","title":"CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation","summary":"  Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.\n","authors":["Xiaolin Lin","Jingcun Wang","Olga Kondrateva","Yiyu Shi","Bing Li","Grace Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.02401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11130v2","updated":"2025-08-04T13:14:13Z","published":"2024-06-17T01:21:28Z","title":"Dynamic Order Template Prediction for Generative Aspect-Based Sentiment\n  Analysis","summary":"  Aspect-based sentiment analysis (ABSA) assesses sentiments towards specific\naspects within texts, resulting in detailed sentiment tuples. Previous ABSA\nmodels often use static templates to predict all of the elements in the tuples,\nand these models often fail to accurately capture dependencies between\nelements. Multi-view prompting method improves the performance of ABSA by\npredicting tuples with various templates and then ensembling the results.\nHowever, this method suffers from inefficiencies and out-of-distribution\nerrors. In this paper, we propose a Dynamic Order Template (DOT) method for\nABSA, which dynamically generates necessary views for each instance based on\ninstance-level entropy. Ensuring the diverse and relevant view generation, our\nproposed method improves F1-scores on ASQP and ACOS datasets while\nsignificantly reducing inference time.\n","authors":["Yonghyun Jun","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2406.11130v2.pdf","comment":"ACL 2025 Main"},{"id":"http://arxiv.org/abs/2507.20957v2","updated":"2025-08-04T13:06:03Z","published":"2025-07-28T16:09:38Z","title":"Your AI, Not Your View: The Bias of LLMs in Investment Analysis","summary":"  In finance, Large Language Models (LLMs) face frequent knowledge conflicts\ndue to discrepancies between pre-trained parametric knowledge and real-time\nmarket data. These conflicts become particularly problematic when LLMs are\ndeployed in real-world investment services, where misalignment between a\nmodel's embedded preferences and those of the financial institution can lead to\nunreliable recommendations. Yet little research has examined what investment\nviews LLMs actually hold. We propose an experimental framework to investigate\nsuch conflicts, offering the first quantitative analysis of confirmation bias\nin LLM-based investment analysis. Using hypothetical scenarios with balanced\nand imbalanced arguments, we extract the latent preferences of models and\nmeasure their persistence. Focusing on sector, size, and momentum, our analysis\nreveals distinct, model-specific tendencies. In particular, we observe a\nconsistent preference for large-cap stocks and contrarian strategies across\nmost models. These preferences often harden into confirmation bias, with models\nclinging to initial judgments despite counter-evidence.\n","authors":["Hoyoung Lee","Junhyuk Seo","Suhwan Park","Junhyeong Lee","Wonbin Ahn","Chanyeol Choi","Alejandro Lopez-Lira","Yongjae Lee"],"pdf_url":"https://arxiv.org/pdf/2507.20957v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02371v1","updated":"2025-08-04T13:01:09Z","published":"2025-08-04T13:01:09Z","title":"Six Guidelines for Trustworthy, Ethical and Responsible Automation\n  Design","summary":"  Calibrated trust in automated systems (Lee and See 2004) is critical for\ntheir safe and seamless integration into society. Users should only rely on a\nsystem recommendation when it is actually correct and reject it when it is\nfactually wrong. One requirement to achieve this goal is an accurate\ntrustworthiness assessment, ensuring that the user's perception of the system's\ntrustworthiness aligns with its actual trustworthiness, allowing users to make\ninformed decisions about the extent to which they can rely on the system\n(Schlicker et al. 2022). We propose six design guidelines to help designers\noptimize for accurate trustworthiness assessments, thus fostering ethical and\nresponsible human-automation interactions. The proposed guidelines are derived\nfrom existing literature in various fields, such as human-computer interaction,\ncognitive psychology, automation research, user-experience design, and ethics.\nWe are incorporating key principles from the field of pragmatics, specifically\nthe cultivation of common ground (H. H. Clark 1996) and Gricean communication\nmaxims (Grice 1975). These principles are essential for the design of automated\nsystems because the user's perception of the system's trustworthiness is shaped\nby both environmental contexts, such as organizational culture or societal\nnorms, and by situational context, including the specific circumstances or\nscenarios in which the interaction occurs (Hoff and Bashir 2015). Our proposed\nguidelines provide actionable insights for designers to create automated\nsystems that make relevant trustworthiness cues available. This would ideally\nfoster calibrated trust and more satisfactory, productive, and safe\ninteractions between humans and automated systems. Furthermore, the proposed\nheuristics might work as a tool for evaluating to what extent existing systems\nenable users to accurately assess a system's trustworthiness.\n","authors":["Matouš Jelínek","Nadine Schlicker","Ewart de Visser"],"pdf_url":"https://arxiv.org/pdf/2508.02371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05386v2","updated":"2025-08-04T12:55:00Z","published":"2025-06-03T12:59:52Z","title":"Leaps Beyond the Seen: Reinforced Reasoning Augmented Generation for\n  Clinical Notes","summary":"  Clinical note generation aims to produce free-text summaries of a patient's\ncondition and diagnostic process, with discharge instructions being a\nrepresentative long-form example. While recent LLM-based methods pre-trained on\ngeneral clinical corpora show promise in clinical text generation, they fall\nshort in producing long-form notes from limited patient information. In this\npaper, we propose ReinRAG, a reinforced reasoning augmented generation (RAG)\nfor long-form discharge instructions based on pre-admission information.\nReinRAG retrieves reasoning paths from a medical knowledge graph to provide\nexplicit semantic guidance to the LLM. To bridge the information gap, we\npropose group-based retriever optimization (GRO) which improves retrieval\nquality with group-normalized rewards, encouraging reasoning leaps for deeper\ninference by the LLM. Comprehensive experiments on the real-world dataset show\nthat ReinRAG outperforms baselines in both clinical efficacy and natural\nlanguage generation metrics. Further analysis reveals that ReinRAG fills\nsemantic gaps in sparse input scenarios, and retrieved reasoning paths help\nLLMs avoid clinical misinterpretation by focusing on key evidence and following\ncoherent reasoning.\n","authors":["Lo Pang-Yun Ting","Chengshuai Zhao","Yu-Hua Zeng","Yuan Jee Lim","Kun-Ta Chuang","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2506.05386v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02366v1","updated":"2025-08-04T12:52:11Z","published":"2025-08-04T12:52:11Z","title":"Language Model Guided Reinforcement Learning in Quantitative Trading","summary":"  Algorithmic trading requires short-term decisions aligned with long-term\nfinancial goals. While reinforcement learning (RL) has been explored for such\ntactical decisions, its adoption remains limited by myopic behavior and opaque\npolicy rationale. In contrast, large language models (LLMs) have recently\ndemonstrated strategic reasoning and multi-modal financial signal\ninterpretation when guided by well-designed prompts.\n  We propose a hybrid system where LLMs generate high-level trading strategies\nto guide RL agents in their actions. We evaluate (i) the rationale of\nLLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and\nMaximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results\nshow improved return and risk metrics over standard RL.\n","authors":["Adam Darmanin","Vince Vella"],"pdf_url":"https://arxiv.org/pdf/2508.02366v1.pdf","comment":"12 pages (4 pages appendix and references), 6 figures, preprint under\n  review for FLLM 2025 conference"},{"id":"http://arxiv.org/abs/2412.15113v2","updated":"2025-08-04T12:51:56Z","published":"2024-12-19T17:55:42Z","title":"Associative memory inspires improvements for in-context learning using a\n  novel attention residual stream architecture","summary":"  Large language models (LLMs) demonstrate an impressive ability to utilise\ninformation within the context of their input sequences to appropriately\nrespond to data unseen by the LLM during its training procedure. This ability\nis known as in-context learning (ICL). Humans and non-human animals demonstrate\nsimilar abilities, however their neural architectures differ substantially from\nLLMs. Despite this, a critical component within LLMs, the attention mechanism,\nresembles modern associative memory models, widely used in and influenced by\nthe computational neuroscience community to model biological memory systems.\nUsing this connection, we introduce an associative memory model capable of\nperforming ICL. We use this as inspiration for a novel residual stream\narchitecture which allows information to directly flow between attention heads.\nWe test this architecture during training within a two-layer Transformer and\nshow its ICL abilities manifest more quickly than without this modification. We\nthen apply our architecture in small language models with 8 million and 1\nbillion parameters, focusing on attention head values, with results also\nindicating improved performance at these larger and more naturalistic scales.\n","authors":["Thomas F Burns","Tomoki Fukai","Christopher J Earls"],"pdf_url":"https://arxiv.org/pdf/2412.15113v2.pdf","comment":"35 pages, 14 figures, 6 tables; accepted and published in TMLR"},{"id":"http://arxiv.org/abs/2508.02360v1","updated":"2025-08-04T12:49:10Z","published":"2025-08-04T12:49:10Z","title":"Understanding and Mitigating Political Stance Cross-topic Generalization\n  in Large Language Models","summary":"  Fine-tuning Large Language Models on a political topic will significantly\nmanipulate their political stance on various issues and unintentionally affect\ntheir stance on unrelated topics. While previous studies have proposed this\nissue, there is still a lack of understanding regarding the internal\nrepresentations of these stances and the mechanisms that lead to unintended\ncross-topic generalization. In this paper, we systematically explore the\ninternal mechanisms underlying this phenomenon from a neuron-level perspective\nand how to mitigate the cross-topic generalization of political fine-tuning.\nFirstly, we propose Political Neuron Localization through Activation\nContrasting (PNLAC) to identify two distinct types of political neurons:\ngeneral political neurons, which govern stance across multiple political\ntopics, and topic-specific neurons} that affect the model's political stance on\nindividual topics. We find the existence of these political neuron types across\nfour models and datasets through activation patching experiments. Leveraging\nthese insights, we introduce InhibitFT, an inhibition-based fine-tuning method,\neffectively mitigating the cross-topic stance generalization. Experimental\nresults demonstrate the robustness of identified neuron types across various\nmodels and datasets, and show that InhibitFT significantly reduces the\ncross-topic stance generalization by 20% on average, while preserving\ntopic-specific performance. Moreover, we demonstrate that selectively\ninhibiting only 5% of neurons is sufficient to effectively mitigate the\ncross-topic stance generalization.\n","authors":["Jiayi Zhang","Shu Yang","Junchao Wu","Derek F. Wong","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2508.02360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13626v2","updated":"2025-08-04T12:34:26Z","published":"2025-04-18T11:07:19Z","title":"Thought Manipulation: External Thought Can Be Efficient for Large\n  Reasoning Models","summary":"  Recent advancements in large reasoning models (LRMs) have demonstrated the\neffectiveness of scaling test-time computation to enhance reasoning\ncapabilities on various tasks. However, LRMs often suffer from an\n``overthinking'' problem, where the model generates excessively redundant\nreasoning steps with limited performance gains. In this work, we empirically\nreveal an important characteristic of LRM behaviors that placing external CoTs\ngenerated by smaller models between the thinking token (\\texttt{<think>} and\n\\texttt{</think>}) can effectively manipulate the model to generate fewer\nthoughts. Building on this finding, we propose a simple yet efficient pipeline,\n\\Method, to enable LRMs to bypass unnecessary intermediate steps, thereby\nsignificantly reducing computational costs. We conduct extensive experiments to\nevaluate the utility and efficiency of \\Method. For instance, when applied to\nQwQ-32B on the LiveBench/Code dataset, \\Method keeps the original performance\nwhile reducing output token counts by approximately 30\\%, with minimal overhead\nintroduced by the CoT generator. Furthermore, we identify two suboptimal modes,\nblindly following flawed external thoughts and unnecessary rethinking, and show\nthat simple mitigations, such as difficulty-aware fallbacks, can further\nimprove performance. Overall, \\Method offers a practical, general, and\nefficient way to optimize LRM inference, making powerful reasoning models more\naccessible and scalable for real-world applications.\n","authors":["Yule Liu","Jingyi Zheng","Zhen Sun","Zifan Peng","Wenhan Dong","Zeyang Sha","Shiwen Cui","Weiqiang Wang","Xinlei He"],"pdf_url":"https://arxiv.org/pdf/2504.13626v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15442v2","updated":"2025-08-04T12:03:51Z","published":"2025-05-21T12:23:32Z","title":"On the Generalization vs Fidelity Paradox in Knowledge Distillation","summary":"  Knowledge distillation (KD) is a key technique for compressing large language\nmodels into smaller ones while preserving performance. Despite the recent\ntraction of KD research, its effectiveness for smaller language models (LMs)\nand the mechanisms driving knowledge transfer remain underexplored. In this\nwork, we present the first large-scale empirical and statistical analysis of KD\nacross models ranging from 0.5B to 7B parameters on 14 complex reasoning tasks\nin a zero-shot setting. Our findings reveal that KD can improve the average\nperformance of smaller models by up to $10\\%$, with a peak task specific gain\nof $22\\%$, while providing only marginal benefits ($\\sim 1.3\\%$) for larger\nmodels. Surprisingly, teacher performance has a minimal impact on student\noutcomes, while teacher task expertise impacts KD effectiveness. A correlation\nstudy indicates that smaller LMs benefit more from KD, whereas larger LMs show\ndiminished gains. Additionally, we uncover a misalignment between improvements\nin student performance and reasoning fidelity, suggesting that while KD\nenhances accuracy, it does not always maintain the structured decision-making\nprocesses of the teacher. Our ablation study further highlights the importance\nof teacher signals and logit smoothing in influencing students' performance\nafter distillation. Overall, our study offers a comprehensive empirical and\nstatistical assessment of KD, highlighting both its benefits and trade-offs\nwhen distilling knowledge from larger to smaller LMs.\n","authors":["Suhas Kamasetty Ramesh","Ayan Sengupta","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2505.15442v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02328v1","updated":"2025-08-04T11:56:47Z","published":"2025-08-04T11:56:47Z","title":"Understanding User Preferences for Interaction Styles in Conversational\n  Recommender Systems: The Predictive Role of System Qualities, User\n  Experience, and Traits","summary":"  Conversational Recommender Systems (CRSs) deliver personalised\nrecommendations through multi-turn natural language dialogue and increasingly\nsupport both task-oriented and exploratory interactions. Yet, the factors\nshaping user interaction preferences remain underexplored. In this\nwithin-subjects study (\\(N = 139\\)), participants experienced two scripted CRS\ndialogues, rated their experiences, and indicated the importance of eight\nsystem qualities. Logistic regression revealed that preference for the\nexploratory interaction was predicted by enjoyment, usefulness, novelty, and\nconversational quality. Unexpectedly, perceived effectiveness was also\nassociated with exploratory preference. Clustering uncovered five latent user\nprofiles with distinct dialogue style preferences. Moderation analyses\nindicated that age, gender, and control preference significantly influenced\nthese choices. These findings integrate affective, cognitive, and trait-level\npredictors into CRS user modelling and inform autonomy-sensitive,\nvalue-adaptive dialogue design. The proposed predictive and adaptive framework\napplies broadly to conversational AI systems seeking to align dynamically with\nevolving user needs.\n","authors":["Raj Mahmud","Shlomo Berkovsky","Mukesh Prasad","A. Baki Kocaballi"],"pdf_url":"https://arxiv.org/pdf/2508.02328v1.pdf","comment":"Accepted at OZCHI 2025. 21 pages, 9 figures, 8 tables"},{"id":"http://arxiv.org/abs/2507.02833v2","updated":"2025-08-04T11:54:59Z","published":"2025-07-03T17:44:33Z","title":"Generalizing Verifiable Instruction Following","summary":"  A crucial factor for successful human and AI interaction is the ability of\nlanguage models or chatbots to follow human instructions precisely. A common\nfeature of instructions are output constraints like ``only answer with yes or\nno\" or ``mention the word `abrakadabra' at least 3 times\" that the user adds to\ncraft a more useful answer. Even today's strongest models struggle with\nfulfilling such constraints. We find that most models strongly overfit on a\nsmall set of verifiable constraints from the benchmarks that test these\nabilities, a skill called precise instruction following, and are not able to\ngeneralize well to unseen output constraints. We introduce a new benchmark,\nIFBench, to evaluate precise instruction following generalization on 58 new,\ndiverse, and challenging verifiable out-of-domain constraints. In addition, we\nperform an extensive analysis of how and on what data models can be trained to\nimprove precise instruction following generalization. Specifically, we\ncarefully design constraint verification modules and show that reinforcement\nlearning with verifiable rewards (RLVR) significantly improves instruction\nfollowing. In addition to IFBench, we release 29 additional new hand-annotated\ntraining constraints and verification functions, RLVR training prompts, and\ncode.\n","authors":["Valentina Pyatkin","Saumya Malik","Victoria Graf","Hamish Ivison","Shengyi Huang","Pradeep Dasigi","Nathan Lambert","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2507.02833v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2508.02322v1","updated":"2025-08-04T11:42:48Z","published":"2025-08-04T11:42:48Z","title":"CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert\n  Redundancy Analysis","summary":"  Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.\n","authors":["Yuzhuang Xu","Xu Han","Yuanchi Zhang","Yixuan Wang","Yijun Liu","Shiyu Ji","Qingfu Zhu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2508.02322v1.pdf","comment":"16 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2508.02317v1","updated":"2025-08-04T11:33:04Z","published":"2025-08-04T11:33:04Z","title":"VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo","summary":"  Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. % We present \\veomni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. \\veomni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. % Using \\veomni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.\n","authors":["Qianli Ma","Yaowei Zheng","Zhelun Shi","Zhongkai Zhao","Bin Jia","Ziyue Huang","Zhiqi Lin","Youjie Li","Jiacheng Yang","Yanghua Peng","Zhi Zhang","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2508.02317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13422v2","updated":"2025-08-04T11:27:55Z","published":"2025-02-19T04:45:05Z","title":"Towards Question Answering over Large Semi-structured Tables","summary":"  Table Question Answering (TableQA) attracts strong interests due to the\nprevalence of web information presented in the form of semi-structured tables.\nDespite many efforts, TableQA over large tables remains an open challenge. This\nis because large tables may overwhelm models that try to comprehend them in\nfull to locate question answers. Recent studies reduce input table size by\ndecomposing tables into smaller, question-relevant sub-tables via generating\nprograms to parse the tables. However, such solutions are subject to program\ngeneration and execution errors and are difficult to ensure decomposition\nquality. To address this issue, we propose TaDRe, a TableQA model that\nincorporates both pre- and post-table decomposition refinements to ensure table\ndecomposition quality, hence achieving highly accurate TableQA results. To\nevaluate TaDRe, we construct two new large-table TableQA benchmarks via\nLLM-driven table expansion and QA pair generation. Extensive experiments on\nboth the new and public benchmarks show that TaDRe achieves state-of-the-art\nperformance on large-table TableQA tasks.\n","authors":["Yuxiang Wang","Junhao Gan","Jianzhong Qi"],"pdf_url":"https://arxiv.org/pdf/2502.13422v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.22676v2","updated":"2025-08-04T11:22:43Z","published":"2025-07-30T13:37:06Z","title":"Listening to the Unspoken: Exploring \"365\" Aspects of Multimodal\n  Interview Performance Assessment","summary":"  Interview performance assessment is essential for determining candidates'\nsuitability for professional positions. To ensure holistic and fair\nevaluations, we propose a novel and comprehensive framework that explores\n``365'' aspects of interview performance by integrating \\textit{three}\nmodalities (video, audio, and text), \\textit{six} responses per candidate, and\n\\textit{five} key evaluation dimensions. The framework employs\nmodality-specific feature extractors to encode heterogeneous data streams and\nsubsequently fused via a Shared Compression Multilayer Perceptron. This module\ncompresses multimodal embeddings into a unified latent space, facilitating\nefficient feature interaction. To enhance prediction robustness, we incorporate\na two-level ensemble learning strategy: (1) independent regression heads\npredict scores for each response, and (2) predictions are aggregated across\nresponses using a mean-pooling mechanism to produce final scores for the five\ntarget dimensions. By listening to the unspoken, our approach captures both\nexplicit and implicit cues from multimodal data, enabling comprehensive and\nunbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our\nframework secured first place in the AVI Challenge 2025, demonstrating its\neffectiveness and robustness in advancing automated and multimodal interview\nperformance assessment. The full implementation is available at\nhttps://github.com/MSA-LMC/365Aspects.\n","authors":["Jia Li","Yang Wang","Wenhao Qian","Zhenzhen Hu","Richang Hong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2507.22676v2.pdf","comment":"8 pages, 4 figures, ACM MM 2025.\n  github:https://github.com/MSA-LMC/365Aspects"},{"id":"http://arxiv.org/abs/2508.02308v1","updated":"2025-08-04T11:22:13Z","published":"2025-08-04T11:22:13Z","title":"LaMPE: Length-aware Multi-grained Position Encoding for Adaptive\n  Long-context Scaling Without Training","summary":"  Large language models (LLMs) experience significant performance degradation\nwhen the input exceeds the pretraining context window, primarily due to the\nout-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent\nstudies mitigate this problem by remapping OOD positions into the\nin-distribution range with fixed mapping strategies, ignoring the dynamic\nrelationship between input length and the model's effective context window. To\nthis end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a\ntraining-free method that fully utilizes the model's effective context window\nfor adaptive long-context scaling in LLMs. Motivated by the left-skewed\nfrequency distribution of relative positions, LaMPE establishes a dynamic\nrelationship between mapping length and input length through a parametric\nscaled sigmoid function to adaptively allocate positional capacity across\nvarying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention\nmechanism that strategically allocates positional resolution across different\nsequence regions to capture both fine-grained locality and long-range\ndependencies. Our method can be seamlessly applied to a wide range of\nRoPE-based LLMs without training. Extensive experiments on three representative\nLLMs across five mainstream long-context benchmarks demonstrate that LaMPE\nachieves significant performance improvements compared to existing length\nextrapolation methods. The code will be released at\nhttps://github.com/scar-on/LaMPE.\n","authors":["Sikui Zhang","Guangze Gao","Ziyun Gan","Chunfeng Yuan","Zefeng Lin","Houwen Peng","Bing Li","Weiming Hu"],"pdf_url":"https://arxiv.org/pdf/2508.02308v1.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2508.02298v1","updated":"2025-08-04T11:06:08Z","published":"2025-08-04T11:06:08Z","title":"CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative\n  Credit Assignment","summary":"  Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback, helping to mitigate reward hacking. However, current RLVR methods\ntypically treat whole responses as single actions, assigning the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies and inefficient learning.\nMethods like PPO provide credit assignment through value estimation, but often\nyield inaccurate and unverifiable signals due to limited sampling. On the other\nhand, methods using Process Reward Models can provide step-by-step judgments\nfor each reasoning step, but they require high-quality process supervision\nlabels and are time-consuming when applied in online reinforcement learning\n(RL). To overcome these limitations, we introduce a simple but efficient method\nCredit Assignment Policy Optimization (CAPO). Given a reasoning response\nrollout from the policy model, CAPO directly leverages an off-the-shelf,\ngeneral-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to\ngenerate all step-wise critique by one pass, thereby providing verifiable\ntoken-level rewards to refine the tokens that were originally assigned\nidentical rule-based rewards. This enables more fine-grained credit assignment\nin an effective way. Furthermore, to enhance the accuracy and robustness of\nCAPO, we employ voting mechanisms that scale with the number of generated\ncritiques. Extensive experiments using different backbones like Llama and Qwen\nmodels and in different sizes show that CAPO consistently outperforms\nsupervised learning-based and RL-based fine-tuning methods across six\nchallenging mathematical benchmarks and three out-of-domain benchmarks.\n","authors":["Guofu Xie","Yunsheng Shi","Hongtao Tian","Ting Yao","Xiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.02298v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2508.02296v1","updated":"2025-08-04T11:04:54Z","published":"2025-08-04T11:04:54Z","title":"Simple Methods Defend RAG Systems Well Against Real-World Attacks","summary":"  Ensuring safety and in-domain responses for Retrieval-Augmented Generation\n(RAG) systems is paramount in safety-critical applications, yet remains a\nsignificant challenge. To address this, we evaluate four methodologies for\nOut-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal\nComponent Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG\nsystem only responds to queries confined to the system's knowledge base.\nSpecifically, our evaluation explores two novel dimensionality reduction and\nfeature separation strategies: \\textit{PCA}, where top components are selected\nusing explained variance or OOD separability, and an adaptation of\n\\textit{Neural Collapse Feature Separation}. We validate our approach on\nstandard datasets (StackExchange and MSMARCO) and real-world applications\n(Substance Use and COVID-19), including tests against LLM-simulated and actual\nattacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations\nof response correctness and relevance, we confirm that an external OOD detector\nis crucial for maintaining response relevance.\n","authors":["Ilias Triantafyllopoulos","Renyi Qu","Salvatore Giorgi","Brenda Curtis","Lyle H. Ungar","João Sedoc"],"pdf_url":"https://arxiv.org/pdf/2508.02296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02290v1","updated":"2025-08-04T10:57:54Z","published":"2025-08-04T10:57:54Z","title":"A French Version of the OLDI Seed Corpus","summary":"  We present the first French partition of the OLDI Seed Corpus, our submission\nto the WMT 2025 Open Language Data Initiative (OLDI) shared task. We detail its\ncreation process, which involved using multiple machine translation systems and\na custom-built interface for post-editing by qualified native speakers. We also\nhighlight the unique translation challenges presented by the source data, which\ncombines highly technical, encyclopedic terminology with the stylistic\nirregularities characteristic of user-generated content taken from Wikipedia.\nThis French corpus is not an end in itself, but is intended as a crucial pivot\nresource to facilitate the collection of parallel corpora for the\nunder-resourced regional languages of France.\n","authors":["Malik Marmonier","Benoît Sagot","Rachel Bawden"],"pdf_url":"https://arxiv.org/pdf/2508.02290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02279v1","updated":"2025-08-04T10:49:01Z","published":"2025-08-04T10:49:01Z","title":"Dialogue Systems Engineering: A Survey and Future Directions","summary":"  This paper proposes to refer to the field of software engineering related to\nthe life cycle of dialogue systems as Dialogue Systems Engineering, and surveys\nthis field while also discussing its future directions. With the advancement of\nlarge language models, the core technologies underlying dialogue systems have\nsignificantly progressed. As a result, dialogue system technology is now\nexpected to be applied to solving various societal issues and in business\ncontexts. To achieve this, it is important to build, operate, and continuously\nimprove dialogue systems correctly and efficiently. Accordingly, in addition to\napplying existing software engineering knowledge, it is becoming increasingly\nimportant to evolve software engineering tailored specifically to dialogue\nsystems. In this paper, we enumerate the knowledge areas of dialogue systems\nengineering based on those of software engineering, as defined in the Software\nEngineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based\non this survey, we identify unexplored topics in each area and discuss the\nfuture direction of dialogue systems engineering.\n","authors":["Mikio Nakano","Hironori Takeuchi","Sadahiro Yoshikawa","Yoichi Matsuyama","Kazunori Komatani"],"pdf_url":"https://arxiv.org/pdf/2508.02279v1.pdf","comment":"18 pages, 2 figures"},{"id":"http://arxiv.org/abs/2508.02276v1","updated":"2025-08-04T10:43:31Z","published":"2025-08-04T10:43:31Z","title":"CellForge: Agentic Design of Virtual Cell Models","summary":"  Virtual cell modeling represents an emerging frontier at the intersection of\nartificial intelligence and biology, aiming to predict quantities such as\nresponses to diverse perturbations quantitatively. However, autonomously\nbuilding computational models for virtual cells is challenging due to the\ncomplexity of biological systems, the heterogeneity of data modalities, and the\nneed for domain-specific expertise across multiple disciplines. Here, we\nintroduce CellForge, an agentic system that leverages a multi-agent framework\nthat transforms presented biological datasets and research objectives directly\ninto optimized computational models for virtual cells. More specifically, given\nonly raw single-cell multi-omics data and task descriptions as input, CellForge\noutputs both an optimized model architecture and executable code for training\nvirtual cell models and inference. The framework integrates three core modules:\nTask Analysis for presented dataset characterization and relevant literature\nretrieval, Method Design, where specialized agents collaboratively develop\noptimized modeling strategies, and Experiment Execution for automated\ngeneration of code. The agents in the Design module are separated into experts\nwith differing perspectives and a central moderator, and have to\ncollaboratively exchange solutions until they achieve a reasonable consensus.\nWe demonstrate CellForge's capabilities in single-cell perturbation prediction,\nusing six diverse datasets that encompass gene knockouts, drug treatments, and\ncytokine stimulations across multiple modalities. CellForge consistently\noutperforms task-specific state-of-the-art methods. Overall, CellForge\ndemonstrates how iterative interaction between LLM agents with differing\nperspectives provides better solutions than directly addressing a modeling\nchallenge. Our code is publicly available at\nhttps://github.com/gersteinlab/CellForge.\n","authors":["Xiangru Tang","Zhuoyun Yu","Jiapeng Chen","Yan Cui","Daniel Shao","Weixu Wang","Fang Wu","Yuchen Zhuang","Wenqi Shi","Zhi Huang","Arman Cohan","Xihong Lin","Fabian Theis","Smita Krishnaswamy","Mark Gerstein"],"pdf_url":"https://arxiv.org/pdf/2508.02276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02271v1","updated":"2025-08-04T10:30:42Z","published":"2025-08-04T10:30:42Z","title":"Dynaword: From One-shot to Continuously Developed Datasets","summary":"  Large-scale datasets are foundational for research and development in natural\nlanguage processing. However, current approaches face three key challenges: (1)\nreliance on ambiguously licensed sources restricting use, sharing, and\nderivative works; (2) static dataset releases that prevent community\ncontributions and diminish longevity; and (3) quality assurance processes\nrestricted to publishing teams rather than leveraging community expertise.\n  To address these limitations, we introduce two contributions: the Dynaword\napproach and Danish Dynaword. The Dynaword approach is a framework for creating\nlarge-scale, open datasets that can be continuously updated through community\ncollaboration. Danish Dynaword is a concrete implementation that validates this\napproach and demonstrates its potential. Danish Dynaword contains over four\ntimes as many tokens as comparable releases, is exclusively openly licensed,\nand has received multiple contributions across industry and research. The\nrepository includes light-weight tests to ensure data formatting, quality, and\ndocumentation, establishing a sustainable framework for ongoing community\ncontributions and dataset evolution.\n","authors":["Kenneth Enevoldsen","Kristian Nørgaard Jensen","Jan Kostkan","Balázs Szabó","Márton Kardos","Kirten Vad","Andrea Blasi Núñez","Gianluca Barmina","Jacob Nielsen","Rasmus Larsen","Peter Vahlstrup","Per Møldrup Dalum","Desmond Elliott","Lukas Galke","Peter Schneider-Kamp","Kristoffer Nielbo"],"pdf_url":"https://arxiv.org/pdf/2508.02271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10939v2","updated":"2025-08-04T10:29:40Z","published":"2025-05-16T07:23:59Z","title":"GenKnowSub: Improving Modularity and Reusability of LLMs through General\n  Knowledge Subtraction","summary":"  Large language models often struggle with zero-shot generalization, and\nseveral modular approaches have been proposed to address this challenge. Yet,\nwe hypothesize that a key limitation remains: the entanglement of general\nknowledge and task-specific adaptations. To overcome this, we propose a modular\nframework that disentangles these components by constructing a library of\ntask-specific LoRA modules alongside a general-domain LoRA. By subtracting this\ngeneral knowledge component from each task-specific module, we obtain residual\nmodules that focus more exclusively on task-relevant information, a method we\ncall general knowledge subtraction (GenKnowSub). Leveraging the refined\ntask-specific modules and the Arrow routing algorithm\n\\citep{ostapenko2024towards}, we dynamically select and combine modules for new\ninputs without additional training. Our studies on the Phi-3 model and standard\nArrow as baselines reveal that using general knowledge LoRAs derived from\ndiverse languages, including English, French, and German, yields consistent\nperformance gains in both monolingual and cross-lingual settings across a wide\nset of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub\ngeneralizes to weaker LLMs. The complete code and data are available at\nhttps://github.com/saharsamr/Modular-LLM.\n","authors":["Mohammadtaha Bagherifard","Sahar Rajabi","Ali Edalat","Yadollah Yaghoobzadeh"],"pdf_url":"https://arxiv.org/pdf/2505.10939v2.pdf","comment":"Accepted to ACL 2025 (main conference, short paper), 10 pages"},{"id":"http://arxiv.org/abs/2508.02268v1","updated":"2025-08-04T10:21:11Z","published":"2025-08-04T10:21:11Z","title":"SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic\n  Bidirectional Machine Translation System","summary":"  The rich linguistic landscape of the Arab world is characterized by a\nsignificant gap between Modern Standard Arabic (MSA), the language of formal\ncommunication, and the diverse regional dialects used in everyday life. This\ndiglossia presents a formidable challenge for natural language processing,\nparticularly machine translation. This paper introduces \\textbf{SHAMI-MT}, a\nbidirectional machine translation system specifically engineered to bridge the\ncommunication gap between MSA and the Syrian dialect. We present two\nspecialized models, one for MSA-to-Shami and another for Shami-to-MSA\ntranslation, both built upon the state-of-the-art AraT5v2-base-1024\narchitecture. The models were fine-tuned on the comprehensive Nabra dataset and\nrigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami\nmodel achieved an outstanding average quality score of \\textbf{4.01 out of 5.0}\nwhen judged by OPENAI model GPT-4.1, demonstrating its ability to produce\ntranslations that are not only accurate but also dialectally authentic. This\nwork provides a crucial, high-fidelity tool for a previously underserved\nlanguage pair, advancing the field of dialectal Arabic translation and offering\nsignificant applications in content localization, cultural heritage, and\nintercultural communication.\n","authors":["Serry Sibaee","Omer Nacar","Yasser Al-Habashi","Adel Ammar","Wadii Boulila"],"pdf_url":"https://arxiv.org/pdf/2508.02268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02260v1","updated":"2025-08-04T10:08:10Z","published":"2025-08-04T10:08:10Z","title":"Decomposing the Entropy-Performance Exchange: The Missing Keys to\n  Unlocking Effective Reinforcement Learning","summary":"  Recently, reinforcement learning with verifiable rewards (RLVR) has been\nwidely used for enhancing the reasoning abilities of large language models\n(LLMs). A core challenge in RLVR involves managing the exchange between entropy\nand performance of policies. Despite the importance of this exchange, a\nfine-grained understanding of when and how this exchange operates most\neffectively remains limited. To bridge this gap, we conduct a systematic\nempirical analysis of the entropy-performance exchange mechanism of RLVR across\ndifferent levels of granularity. Specifically, we first divide the training\nprocess into two distinct stages based on entropy dynamics, i.e., rising stage\nand plateau stage, and then systematically investigate how this mechanism\nvaries across stage-level, instance-level, and token-level granularitiess. Our\nanalysis reveals that, in the rising stage, entropy reduction in negative\nsamples facilitates the learning of effective reasoning patterns, which in turn\ndrives rapid performance gains. Moreover, in the plateau stage, learning\nefficiency strongly correlates with high-entropy tokens present in\nlow-perplexity samples and those located at the end of sequences. Motivated by\nthese findings, we propose two methods that dynamically adjust the reward\nsignal using perplexity and positional information to focus RL updates on\ntokens that exhibit high learning potential, achieving improvements compared to\nthe baseline methods on various LLMs.\n","authors":["Jia Deng","Jie Chen","Zhipeng Chen","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2508.02260v1.pdf","comment":"7 pages, 20 figures"},{"id":"http://arxiv.org/abs/2507.20343v2","updated":"2025-08-04T10:03:09Z","published":"2025-07-27T16:19:46Z","title":"DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech\n  Movement Patterns","summary":"  We present DYNARTmo, a dynamic articulatory model designed to visualize\nspeech articulation processes in a two-dimensional midsagittal plane. The model\nbuilds upon the UK-DYNAMO framework and integrates principles of articulatory\nunderspecification, segmental and gestural control, and coarticulation.\nDYNARTmo simulates six key articulators based on ten continuous and six\ndiscrete control parameters, allowing for the generation of both vocalic and\nconsonantal articulatory configurations. The current implementation is embedded\nin a web-based application (SpeechArticulationTrainer) that includes sagittal,\nglottal, and palatal views, making it suitable for use in phonetics education\nand speech therapy. While this paper focuses on the static modeling aspects,\nfuture work will address dynamic movement generation and integration with\narticulatory-acoustic modules.\n","authors":["Bernd J. Kröger"],"pdf_url":"https://arxiv.org/pdf/2507.20343v2.pdf","comment":"10 pages, 29 references, 2 figures, supplementary material. V2:\n  Discussion of the tongue-palate contact pattern for /t/"},{"id":"http://arxiv.org/abs/2508.02256v1","updated":"2025-08-04T10:02:19Z","published":"2025-08-04T10:02:19Z","title":"Interference Matrix: Quantifying Cross-Lingual Interference in\n  Transformer Encoders","summary":"  In this paper, we present a comprehensive study of language interference in\nencoder-only Transformer models across 83 languages. We construct an\ninterference matrix by training and evaluating small BERT-like models on all\npossible language pairs, providing a large-scale quantification of\ncross-lingual interference. Our analysis reveals that interference between\nlanguages is asymmetrical and that its patterns do not align with traditional\nlinguistic characteristics, such as language family, nor with proxies like\nembedding similarity, but instead better relate to script. Finally, we\ndemonstrate that the interference matrix effectively predicts performance on\ndownstream tasks, serving as a tool to better design multilingual models to\nobtain optimal performance.\n","authors":["Belen Alastruey","João Maria Janeiro","Alexandre Allauzen","Maha Elbayad","Loïc Barrault","Marta R. Costa-jussà"],"pdf_url":"https://arxiv.org/pdf/2508.02256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02241v1","updated":"2025-08-04T09:41:10Z","published":"2025-08-04T09:41:10Z","title":"Isolating Culture Neurons in Multilingual Large Language Models","summary":"  Language and culture are deeply intertwined, yet it is so far unclear how and\nwhere multilingual large language models encode culture. Here, we extend upon\nan established methodology for identifying language-specific neurons and extend\nit to localize and isolate culture-specific neurons, carefully disentangling\ntheir overlap and interaction with language-specific neurons. To facilitate our\nexperiments, we introduce MUREL, a curated dataset of 85.2 million tokens\nspanning six different cultures. Our localization and intervention experiments\nshow that LLMs encode different cultures in distinct neuron populations,\npredominantly in upper layers, and that these culture neurons can be modulated\nindependently from language-specific neurons or those specific to other\ncultures. These findings suggest that cultural knowledge and propensities in\nmultilingual language models can be selectively isolated and edited - promoting\nfairness, inclusivity, and alignment. Code and data is available at\nhttps://github.com/namazifard/Culture_Neurons .\n","authors":["Danial Namazifard","Lukas Galke"],"pdf_url":"https://arxiv.org/pdf/2508.02241v1.pdf","comment":"18 pages, 13 figures"},{"id":"http://arxiv.org/abs/2508.02215v1","updated":"2025-08-04T09:08:43Z","published":"2025-08-04T09:08:43Z","title":"LeanK: Learnable K Cache Channel Pruning for Efficient Decoding","summary":"  Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.\n","authors":["Yike Zhang","Zhiyuan He","Huiqiang Jiang","Chengruidong Zhang","Yuqing Yang","Jianyong Wang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2508.02215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02208v1","updated":"2025-08-04T08:59:36Z","published":"2025-08-04T08:59:36Z","title":"Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for\n  Proof-Centric Problems","summary":"  Evaluating the mathematical capability of Large Language Models (LLMs) is a\ncritical yet challenging frontier. Existing benchmarks fall short, particularly\nfor proof-centric problems, as manual creation is unscalable and costly,\nleaving the true mathematical abilities of LLMs largely unassessed. To overcome\nthese barriers, we propose Proof2Hybrid, the first fully automated framework\nthat synthesizes high-quality, proof-centric benchmarks from natural language\nmathematical corpora. The key novelty of our solution is Proof2X, a roadmap of\nconverting mathematical proofs into various kinds of questions that are easy to\nverify. Instructed by this roadmap, we propose a new type of hybrid-formatted\nquestions, named ``$m$-out-of-$n$ multiple judge questions'', specifically\ndesigned to enable robust, automatic evaluation while being resilient to\nguessing and superficial pattern matching inherent in traditional formats. As a\ndemonstration of our framework, we introduce AlgGeoTest, a benchmark for\nalgebraic geometry--a frontier domain of modern mathematics--comprising 456\nchallenging items. Our extensive evaluations on state-of-the-art LLMs using\nAlgGeoTest reveal profound deficits in their comprehension of algebraic\ngeometry, providing a more precise measure of their true mathematical\ncapabilities. Our framework and benchmark pave the way for a new wave of\nin-depth research into the mathematical intelligence of AI systems.\n","authors":["Yebo Peng","Zixiang Liu","Yaoming Li","Zhizhuo Yang","Xinye Xu","Bowen Ye","Weijun Yuan","Zihan Wang","Tong Yang"],"pdf_url":"https://arxiv.org/pdf/2508.02208v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2508.02193v1","updated":"2025-08-04T08:43:01Z","published":"2025-08-04T08:43:01Z","title":"Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed\n  Inference","summary":"  We present Seed Diffusion Preview, a large-scale language model based on\ndiscrete-state diffusion, offering remarkably fast inference speed. Thanks to\nnon-sequential, parallel generation, discrete diffusion models provide a\nnotable speedup to mitigate the inherent latency of token-by-token decoding, as\ndemonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion\nPreview achieves an inference speed of 2,146 token/s over H20 GPUs while\nmaintaining competitive performance across a sweep of standard code evaluation\nbenchmarks, significantly faster than contemporary Mercury and Gemini\nDiffusion, establishing new state of the art on the speed-quality Pareto\nfrontier for code models.\n","authors":["Yuxuan Song","Zheng Zhang","Cheng Luo","Pengyang Gao","Fan Xia","Hao Luo","Zheng Li","Yuehang Yang","Hongli Yu","Xingwei Qu","Yuwei Fu","Jing Su","Ge Zhang","Wenhao Huang","Mingxuan Wang","Lin Yan","Xiaoying Jia","Jingjing Liu","Wei-Ying Ma","Ya-Qin Zhang","Yonghui Wu","Hao Zhou"],"pdf_url":"https://arxiv.org/pdf/2508.02193v1.pdf","comment":"Demo is available at https://studio.seed.ai/exp/seed_diffusion/;\n  Project page is https://seed.bytedance.com/seed_diffusion"},{"id":"http://arxiv.org/abs/2508.02189v1","updated":"2025-08-04T08:34:30Z","published":"2025-08-04T08:34:30Z","title":"Learning Dynamics of Meta-Learning in Small Model Pretraining","summary":"  Large language models are powerful but costly. We ask whether meta-learning\ncan make the pretraining of small language models not only better but also more\ninterpretable. We integrate first-order MAML with subset-masked LM pretraining,\nproducing four LLama-style decoder-only models (11M-570M params), and evaluate\nit on a fundamental NLP task with many settings and real-world applications.\nCompared with vanilla training, our model (i) reaches the same loss up to 1.6x\nsooner, (ii) improves F1 on multilingual Universal NER under equal compute, and\n(iii) makes the training dynamics easy to read: first the network's\nrepresentations fan out (\"diversify\") and later they collapse into a smaller,\nshared subspace (\"compress\"). This two-stage shift shows up as a rise-and-fall\nin both effective-rank curves and attention-head entropy. The same curves\npinpoint which layers specialise earliest and which later reconverge, giving a\ncompact, interpretable signature of meta-adaptation. Code, checkpoints and\nWandB logs are released.\n","authors":["David Demitri Africa","Yuval Weiss","Paula Buttery","Richard Diehl Martinez"],"pdf_url":"https://arxiv.org/pdf/2508.02189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.19906v2","updated":"2025-08-04T08:19:26Z","published":"2025-07-26T10:34:53Z","title":"CaliDrop: KV Cache Compression with Calibration","summary":"  Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods.\n","authors":["Yi Su","Quantong Qiu","Yuechi Zhou","Juntao Li","Qingrong Xia","Ping Li","Xinyu Duan","Zhefeng Wang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.19906v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02175v1","updated":"2025-08-04T08:15:16Z","published":"2025-08-04T08:15:16Z","title":"Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through\n  Latent Acoustic Pattern Triggers","summary":"  As Audio Large Language Models (ALLMs) emerge as powerful tools for speech\nprocessing, their safety implications demand urgent attention. While\nconsiderable research has explored textual and vision safety, audio's distinct\ncharacteristics present significant challenges. This paper first investigates:\nIs ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In\nresponse to this issue, we introduce Hidden in the Noise (HIN), a novel\nbackdoor attack framework designed to exploit subtle, audio-specific features.\nHIN applies acoustic modifications to raw audio waveforms, such as alterations\nto temporal dynamics and strategic injection of spectrally tailored noise.\nThese changes introduce consistent patterns that an ALLM's acoustic feature\nencoder captures, embedding robust triggers within the audio stream. To\nevaluate ALLM robustness against audio-feature-based triggers, we develop the\nAudioSafe benchmark, assessing nine distinct risk types. Extensive experiments\non AudioSafe and three established safety datasets reveal critical\nvulnerabilities in existing ALLMs: (I) audio features like environment noise\nand speech rate variations achieve over 90% average attack success rate. (II)\nALLMs exhibit significant sensitivity differences across acoustic features,\nparticularly showing minimal response to volume as a trigger, and (III)\npoisoned sample inclusion causes only marginal loss curve fluctuations,\nhighlighting the attack's stealth.\n","authors":["Liang Lin","Miao Yu","Kaiwen Luo","Yibo Zhang","Lilan Peng","Dexian Wang","Xuehai Tang","Yuanhe Zhang","Xikang Yang","Zhenhong Zhou","Kun Wang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2508.02175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02165v1","updated":"2025-08-04T08:05:18Z","published":"2025-08-04T08:05:18Z","title":"Subject or Style: Adaptive and Training-Free Mixture of LoRAs","summary":"  Fine-tuning models via Low-Rank Adaptation (LoRA) demonstrates remarkable\nperformance in subject-driven or style-driven generation tasks. Studies have\nexplored combinations of different LoRAs to jointly generate learned styles and\ncontent. However, current methods struggle to balance the original subject and\nstyle, and often require additional training. Recently, K-LoRA proposed a\ntraining-free LoRA fusion method. But it involves multiple hyperparameters,\nmaking it difficult to adapt to all styles and subjects. In this paper, we\npropose EST-LoRA, a training-free adaptive LoRA fusion method. It\ncomprehensively considers three critical factors: \\underline{E}nergy of matrix,\n\\underline{S}tyle discrepancy scores and \\underline{T}ime steps. Analogous to\nthe Mixture of Experts (MoE) architecture, the model adaptively selects between\nsubject LoRA and style LoRA within each attention layer. This integrated\nselection mechanism ensures balanced contributions from both components during\nthe generation process. Experimental results show that EST-LoRA outperforms\nstate-of-the-art methods in both qualitative and quantitative evaluations and\nachieves faster generation speed compared to other efficient fusion approaches.\nOur code is publicly available at:\nhttps://anonymous.4open.science/r/EST-LoRA-F318.\n","authors":["Jia-Chen Zhang","Yu-Jie Xiong"],"pdf_url":"https://arxiv.org/pdf/2508.02165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.19467v2","updated":"2025-08-04T07:19:31Z","published":"2025-06-24T09:49:26Z","title":"Can Reasoning Help Large Language Models Capture Human Annotator\n  Disagreement?","summary":"  Variation in human annotation (i.e., disagreements) is common in NLP, often\nreflecting important information like task subjectivity and sample ambiguity.\nModeling this variation is important for applications that are sensitive to\nsuch information. Although RLVR-style reasoning (Reinforcement Learning with\nVerifiable Rewards) has improved Large Language Model (LLM) performance on many\ntasks, it remains unclear whether such reasoning enables LLMs to capture\ninformative variation in human annotation. In this work, we evaluate the\ninfluence of different reasoning settings on LLM disagreement modeling. We\nsystematically evaluate each reasoning setting across model sizes, distribution\nexpression methods, and steering methods, resulting in 60 experimental setups\nacross 3 tasks. Surprisingly, our results show that RLVR-style reasoning\ndegrades performance in disagreement modeling, while naive Chain-of-Thought\n(CoT) reasoning improves the performance of RLHF LLMs (RL from human feedback).\nThese findings underscore the potential risk of replacing human annotators with\nreasoning LLMs, especially when disagreements are important.\n","authors":["Jingwei Ni","Yu Fan","Vilém Zouhar","Donya Rooein","Alexander Hoyle","Mrinmaya Sachan","Markus Leippold","Dirk Hovy","Elliott Ash"],"pdf_url":"https://arxiv.org/pdf/2506.19467v2.pdf","comment":"Preprint Under Review"},{"id":"http://arxiv.org/abs/2508.02124v1","updated":"2025-08-04T07:05:15Z","published":"2025-08-04T07:05:15Z","title":"Trainable Dynamic Mask Sparse Attention","summary":"  In large language models, the demand for modeling long contexts is constantly\nincreasing, but the quadratic complexity of the standard self-attention\nmechanism often becomes a bottleneck. Although existing sparse attention\nmechanisms have improved efficiency, they may still encounter issues such as\nstatic patterns or information loss. We introduce a trainable dynamic mask\nsparse attention mechanism, Dynamic Mask Attention, which effectively utilizes\ncontent-aware and position-aware sparsity. DMA achieves this through two key\ninnovations: First, it dynamically generates content-aware sparse masks from\nvalue representations, enabling the model to identify and focus on critical\ninformation adaptively. Second, it implements position-aware sparse attention\ncomputation that effectively skips unnecessary calculation regions. This\ndual-sparsity design allows the model to significantly reduce the computational\ncomplexity of important information while retaining complete information,\nachieving an excellent balance between information fidelity and computational\nefficiency. We have verified the performance of DMA through comprehensive\nexperiments. Comparative studies show that DMA outperforms multi-head\nattention, sliding window attention, multi-head latent attention, and native\nsparse attention in terms of perplexity under Chinchilla Scaling Law settings.\nMoreover, in challenging multi-query associative recall tasks, DMA also\ndemonstrates superior performance and efficiency compared to these methods.\nCrucially, in the evaluation of a 1.7B parameter model, DMA significantly\noutperforms multi-head attention in both standard benchmark performance and the\nchallenging needle-in-a-haystack task. These experimental results highlight its\ncapability to balance model efficiency and long-context modeling ability\neffectively.\n","authors":["Jingze Shi","Yifan Wu","Bingheng Wu","Yiran Peng","Liangdong Wang","Guang Liu","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2508.02124v1.pdf","comment":"8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2506.08625v2","updated":"2025-08-04T07:04:27Z","published":"2025-06-10T09:40:53Z","title":"RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval","summary":"  Scientific reasoning requires not only long-chain reasoning processes, but\nalso knowledge of domain-specific terminologies and adaptation to updated\nfindings. To deal with these challenges for scientific reasoning, we introduce\nRAISE, a step-by-step retrieval-augmented framework which retrieves logically\nrelevant documents from in-the-wild corpus. RAISE is divided into three steps:\nproblem decomposition, logical query generation, and logical retrieval. We\nobserve that RAISE consistently outperforms other baselines on scientific\nreasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves\ndocuments that are not only similar in terms of the domain knowledge, but also\ndocuments logically more relevant.\n","authors":["Minhae Oh","Jeonghye Kim","Nakyung Lee","Donggeon Seo","Taeuk Kim","Jungwoo Lee"],"pdf_url":"https://arxiv.org/pdf/2506.08625v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.16628v2","updated":"2025-08-04T06:34:17Z","published":"2025-04-23T11:35:57Z","title":"ParetoHqD: Fast Offline Multiobjective Alignment of Large Language\n  Models using Pareto High-quality Data","summary":"  Aligning large language models with multiple human expectations and values is\ncrucial for ensuring that they adequately serve a variety of user needs. To\nthis end, offline multiobjective alignment algorithms such as the\nRewards-in-Context algorithm have shown strong performance and efficiency.\nHowever, inappropriate preference representations and training with imbalanced\nreward scores limit the performance of such algorithms. In this work, we\nintroduce ParetoHqD that addresses the above issues by representing human\npreferences as preference directions in the objective space and regarding data\nnear the Pareto front as ''high-quality'' data. For each preference, ParetoHqD\nfollows a two-stage supervised fine-tuning process, where each stage uses an\nindividual Pareto high-quality training set that best matches its preference\ndirection. The experimental results have demonstrated the superiority of\nParetoHqD over five baselines on two multiobjective alignment tasks.\n","authors":["Haoran Gu","Handing Wang","Yi Mei","Mengjie Zhang","Yaochu Jin"],"pdf_url":"https://arxiv.org/pdf/2504.16628v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07167v2","updated":"2025-08-04T06:22:49Z","published":"2025-05-12T01:26:50Z","title":"One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and\n  Usability in Large Language Models","summary":"  Large Language Models (LLMs) have been extensively used across diverse\ndomains, including virtual assistants, automated code generation, and\nscientific research. However, they remain vulnerable to jailbreak attacks,\nwhich manipulate the models into generating harmful responses despite safety\nalignment. Recent studies have shown that current safety-aligned LLMs often\nundergo the shallow safety alignment, where the first few tokens largely\ndetermine whether the response will be harmful. Through comprehensive\nobservations, we find that safety-aligned LLMs and various defense strategies\ngenerate highly similar initial tokens in their refusal responses, which we\ndefine as safety trigger tokens. Building on this insight, we propose\n\\texttt{D-STT}, a simple yet effective defense algorithm that identifies and\nexplicitly decodes safety trigger tokens of the given safety-aligned LLM to\ntrigger the model's learned safety patterns. In this process, the safety\ntrigger is constrained to a single token, which effectively preserves model\nusability by introducing minimum intervention in the decoding process.\nExtensive experiments across diverse jailbreak attacks and benign prompts\ndemonstrate that \\ours significantly reduces output harmfulness while\npreserving model usability and incurring negligible response time overhead,\noutperforming ten baseline methods.\n","authors":["Haoran Gu","Handing Wang","Yi Mei","Mengjie Zhang","Yaochu Jin"],"pdf_url":"https://arxiv.org/pdf/2505.07167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.16150v2","updated":"2025-08-04T06:15:54Z","published":"2025-06-19T09:06:27Z","title":"PRISON: Unmasking the Criminal Potential of Large Language Models","summary":"  As large language models (LLMs) advance, concerns about their misconduct in\ncomplex social contexts intensify. Existing research overlooked the systematic\nunderstanding and assessment of their criminal capability in realistic\ninteractions. We propose a unified framework PRISON, to quantify LLMs' criminal\npotential across five traits: False Statements, Frame-Up, Psychological\nManipulation, Emotional Disguise, and Moral Disengagement. Using structured\ncrime scenarios adapted from classic films grounded in reality, we evaluate\nboth criminal potential and anti-crime ability of LLMs. Results show that\nstate-of-the-art LLMs frequently exhibit emergent criminal tendencies, such as\nproposing misleading statements or evasion tactics, even without explicit\ninstructions. Moreover, when placed in a detective role, models recognize\ndeceptive behavior with only 44% accuracy on average, revealing a striking\nmismatch between conducting and detecting criminal behavior. These findings\nunderscore the urgent need for adversarial robustness, behavioral alignment,\nand safety mechanisms before broader LLM deployment.\n","authors":["Xinyi Wu","Geng Hong","Pei Chen","Yueyue Chen","Xudong Pan","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2506.16150v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14037v3","updated":"2025-08-04T06:13:07Z","published":"2025-02-19T19:00:02Z","title":"DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation","summary":"  Despite their growing capabilities, language models still frequently\nreproduce content from their training data, generate repetitive text, and favor\ncommon grammatical patterns and vocabulary. A possible cause is the decoding\nstrategy: the most common strategies either consider only the most probable\ntokens, which reduces output diversity, or increase the likelihood of unlikely\ntokens, compromising output accuracy and correctness. In this paper, we propose\nDiffSampling, a new decoding method that leverages a mathematical analysis of\nthe token probability distribution to ensure the generation of contextually\nappropriate text. In particular, the difference between consecutive, sorted\nprobabilities can be used to truncate incorrect tokens. In addition, we also\npropose two variations of the proposed method that aim to correct the subtle\ninconsistencies of common sampling strategies. Experiments involving four\ndifferent text-generation tasks demonstrate that our approach consistently\nperforms at least on par with the existing methods it builds upon in terms of\nquality, while potentially improving output diversity.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2502.14037v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02094v1","updated":"2025-08-04T06:05:36Z","published":"2025-08-04T06:05:36Z","title":"\"Harmless to You, Hurtful to Me!\": Investigating the Detection of Toxic\n  Languages Grounded in the Perspective of Youth","summary":"  Risk perception is subjective, and youth's understanding of toxic content\ndiffers from that of adults. Although previous research has conducted extensive\nstudies on toxicity detection in social media, the investigation of youth's\nunique toxicity, i.e., languages perceived as nontoxic by adults but toxic as\nyouth, is ignored. To address this gap, we aim to explore: 1) What are the\nfeatures of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing\ntoxicity detection techniques accurately detect these languages (RQ2). For\nthese questions, we took Chinese youth as the research target, constructed the\nfirst Chinese ``youth-toxicity'' dataset, and then conducted extensive\nanalysis. Our results suggest that youth's perception of these is associated\nwith several contextual factors, like the source of an utterance and\ntext-related features. Incorporating these meta information into current\ntoxicity detection methods significantly improves accuracy overall. Finally, we\npropose several insights into future research on youth-centered toxicity\ndetection.\n","authors":["Yaqiong Li","Peng Zhang","Lin Wang","Hansu Gu","Siyuan Qiao","Ning Gu","Tun Lu"],"pdf_url":"https://arxiv.org/pdf/2508.02094v1.pdf","comment":"Accepted at the 20th International AAAI Conference on Web and Social\n  Media (ICWSM 2026)"},{"id":"http://arxiv.org/abs/2508.02091v1","updated":"2025-08-04T05:57:46Z","published":"2025-08-04T05:57:46Z","title":"CRINN: Contrastive Reinforcement Learning for Approximate Nearest\n  Neighbor Search","summary":"  Approximate nearest-neighbor search (ANNS) algorithms have become\nincreasingly critical for recent AI applications, particularly in\nretrieval-augmented generation (RAG) and agent-based LLM applications. In this\npaper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS\noptimization as a reinforcement learning problem where execution speed serves\nas the reward signal. This approach enables the automatic generation of\nprogressively faster ANNS implementations while maintaining accuracy\nconstraints. Our experimental evaluation demonstrates CRINN's effectiveness\nacross six widely-used NNS benchmark datasets. When compared against\nstate-of-the-art open-source ANNS algorithms, CRINN achieves best performance\non three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and\nGloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean\nand GloVe-25-angular). The implications of CRINN's success reach well beyond\nANNS optimization: It validates that LLMs augmented with reinforcement learning\ncan function as an effective tool for automating sophisticated algorithmic\noptimizations that demand specialized knowledge and labor-intensive manual\nrefinement.Code can be found at https://github.com/deepreinforce-ai/CRINN\n","authors":["Xiaoya Li","Xiaofei Sun","Albert Wang","Chris Shum","Jiwei Li"],"pdf_url":"https://arxiv.org/pdf/2508.02091v1.pdf","comment":"Preprint Version"},{"id":"http://arxiv.org/abs/2501.00571v5","updated":"2025-08-04T05:56:52Z","published":"2024-12-31T17:58:36Z","title":"KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation\n  Extraction with Comprehensive Reasoning Abilities","summary":"  Document-level relation extraction (Doc-RE) aims to extract relations between\nentities across multiple sentences. Therefore, Doc-RE requires more\ncomprehensive reasoning abilities like humans, involving complex cross-sentence\ninteractions between entities, contexts, and external general knowledge,\ncompared to the sentence-level RE. However, most existing Doc-RE methods focus\non optimizing single reasoning ability, but lack the ability to utilize\nexternal knowledge for comprehensive reasoning on long documents. To solve\nthese problems, a knowledge retrieval augmented method, named KnowRA, was\nproposed with comprehensive reasoning to autonomously determine whether to\naccept external knowledge to assist DocRE. Firstly, we constructed a document\ngraph for semantic encoding and integrated the co-reference resolution model to\naugment the co-reference reasoning ability. Then, we expanded the document\ngraph into a document knowledge graph by retrieving the external knowledge base\nfor common-sense reasoning and a novel knowledge filtration method was\npresented to filter out irrelevant knowledge. Finally, we proposed the axis\nattention mechanism to build direct and indirect associations with intermediary\nentities for achieving cross-sentence logical reasoning. Extensive experiments\nconducted on two datasets verified the effectiveness of our method compared to\nthe state-of-the-art baselines. Our code is available at\nhttps://anonymous.4open.science/r/KnowRA.\n","authors":["Chengcheng Mai","Yuxiang Wang","Ziyu Gong","Hanxiang Wang","Yihua Huang"],"pdf_url":"https://arxiv.org/pdf/2501.00571v5.pdf","comment":"This work has been accepted by IJCAI 2025 (CCF A)"},{"id":"http://arxiv.org/abs/2508.02087v1","updated":"2025-08-04T05:55:06Z","published":"2025-08-04T05:55:06Z","title":"When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy\n  in Large Language Models","summary":"  Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing\nwith user-stated opinions even when those contradict factual knowledge. While\nprior work has documented this tendency, the internal mechanisms that enable\nsuch behavior remain poorly understood. In this paper, we provide a mechanistic\naccount of how sycophancy arises within LLMs. We first systematically study how\nuser opinions induce sycophancy across different model families. We find that\nsimple opinion statements reliably induce sycophancy, whereas user expertise\nframing has a negligible impact. Through logit-lens analysis and causal\nactivation patching, we identify a two-stage emergence of sycophancy: (1) a\nlate-layer output preference shift and (2) deeper representational divergence.\nWe also verify that user authority fails to influence behavior because models\ndo not encode it internally. In addition, we examine how grammatical\nperspective affects sycophantic behavior, finding that first-person prompts\n(``I believe...'') consistently induce higher sycophancy rates than\nthird-person framings (``They believe...'') by creating stronger\nrepresentational perturbations in deeper layers. These findings highlight that\nsycophancy is not a surface-level artifact but emerges from a structural\noverride of learned knowledge in deeper layers, with implications for alignment\nand truthful AI systems.\n","authors":["Jin Li","Keyu Wang","Shu Yang","Zhuoran Zhang","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2508.02087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15672v3","updated":"2025-08-04T05:48:35Z","published":"2024-09-24T02:24:48Z","title":"Language-based Audio Moment Retrieval","summary":"  In this paper, we propose and design a new task called audio moment retrieval\n(AMR). Unlike conventional language-based audio retrieval tasks that search for\nshort audio clips from an audio database, AMR aims to predict relevant moments\nin untrimmed long audio based on a text query. Given the lack of prior work in\nAMR, we first build a dedicated dataset, Clotho-Moment, consisting of\nlarge-scale simulated audio recordings with moment annotations. We then propose\na DETR-based model, named Audio Moment DETR (AM-DETR), as a fundamental\nframework for AMR tasks. This model captures temporal dependencies within audio\nfeatures, inspired by similar video moment retrieval tasks, thus surpassing\nconventional clip-level audio retrieval methods. Additionally, we provide\nmanually annotated datasets to properly measure the effectiveness and\nrobustness of our methods on real data. Experimental results show that AM-DETR,\ntrained with Clotho-Moment, outperforms a baseline model that applies a\nclip-level audio retrieval method with a sliding window on all metrics,\nparticularly improving Recall1@0.7 by 9.00 points. Our datasets and code are\npublicly available in\nhttps://h-munakata.github.io/Language-based-Audio-Moment-Retrieval.\n","authors":["Hokuto Munakata","Taichi Nishimura","Shota Nakada","Tatsuya Komatsu"],"pdf_url":"https://arxiv.org/pdf/2409.15672v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13207v2","updated":"2025-08-04T05:39:29Z","published":"2025-02-18T19:00:01Z","title":"Thinking Outside the (Gray) Box: A Context-Based Score for Assessing\n  Value and Originality in Neural Text Generation","summary":"  Despite the increasing use of large language models for creative tasks, their\noutputs often lack diversity. Common solutions, such as sampling at higher\ntemperatures, can compromise the quality of the results. Dealing with this\ntrade-off is still an open challenge in designing AI systems for creativity.\nDrawing on information theory, we propose a context-based score to\nquantitatively evaluate value and originality. This score incentivizes accuracy\nand adherence to the request while fostering divergence from the learned\ndistribution. We show that our score can be used as a reward in a reinforcement\nlearning framework to fine-tune large language models for maximum performance.\nWe validate our strategy through experiments considering a variety of creative\ntasks, such as poetry generation and math problem solving, demonstrating that\nit enhances the value and originality of the generated solutions.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2502.13207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02075v1","updated":"2025-08-04T05:31:09Z","published":"2025-08-04T05:31:09Z","title":"Human Capital Visualization using Speech Amount during Meetings","summary":"  In recent years, many companies have recognized the importance of human\nresources and are investing in human capital to revitalize their organizations\nand enhance internal communication, thereby fostering innovation. However,\nconventional quantification methods have mainly focused on readily measurable\nindicators without addressing the fundamental role of conversations in human\ncapital. This study focuses on routine meetings and proposes strategies to\nvisualize human capital by analyzing speech amount during these meetings. We\nemploy conversation visualization technology, which operates effectively, to\nquantify speech. We then measure differences in speech amount by attributes\nsuch as gender and job post, changes in speech amount depending on whether\ncertain participants are present, and correlations between speech amount and\ncontinuous attributes. To verify the effectiveness of our proposed methods, we\nanalyzed speech amounts by departmental affiliation during weekly meetings at\nsmall to medium enterprises.\n","authors":["Ekai Hashimoto","Takeshi Mizumoto","Kohei Nagira","Shun Shiramatsu"],"pdf_url":"https://arxiv.org/pdf/2508.02075v1.pdf","comment":"This paper has been accepted for presentation at the 26th Annual\n  Meeting of the Special Interest Group on Discourse and Dialogue(SIGDIAL\n  2025). It represents the author's version of the work"},{"id":"http://arxiv.org/abs/2508.02074v1","updated":"2025-08-04T05:29:17Z","published":"2025-08-04T05:29:17Z","title":"The SMeL Test: A simple benchmark for media literacy in language models","summary":"  The internet is rife with unattributed, deliberately misleading, or otherwise\nuntrustworthy content. Though large language models (LLMs) are often tasked\nwith autonomous web browsing, the extent to which they have learned the simple\nheuristics human researchers use to navigate this noisy environment is not\ncurrently known. In this paper, we introduce the Synthetic Media Literacy Test\n(SMeL Test), a minimal benchmark that tests the ability of language models to\nactively filter out untrustworthy information in context. We benchmark a\nvariety of commonly used instruction-tuned LLMs, including reasoning models,\nand find that no model consistently trusts more reliable sources; while\nreasoning in particular is associated with higher scores, even the best API\nmodel we test hallucinates up to 70% of the time. Remarkably, larger and more\ncapable models do not necessarily outperform their smaller counterparts. We\nhope our work sheds more light on this important form of hallucination and\nguides the development of new methods to combat it.\n","authors":["Gustaf Ahdritz","Anat Kleiman"],"pdf_url":"https://arxiv.org/pdf/2508.02074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02066v1","updated":"2025-08-04T05:10:11Z","published":"2025-08-04T05:10:11Z","title":"MolReasoner: Toward Effective and Interpretable Reasoning for Molecular\n  LLMs","summary":"  Large Language Models(LLMs) have demonstrated remarkable performance across\nvarious domains, yet their capabilities in molecular reasoning remain\ninsufficiently explored. Current approaches tend to rely heavily on\ngeneral-purpose prompting, which lacks domain-specific molecular semantics,\nwhile those that use fine-tuning strategies often face challenges with\ninterpretability and reasoning depth. To address these issues, we introduce\nMolReasoner, a two-stage framework designed to transition LLMs from\nmemorization towards chemical reasoning. First, we propose Mol-SFT, which\ninitializes the model's reasoning abilities via synthetic Chain-of-Thought(CoT)\nsamples generated by GPT-4o and verified for chemical accuracy. Subsequently,\nMol-RL applies reinforcement learning with specialized reward functions\ndesigned explicitly to align chemical structures with linguistic descriptions,\nthereby enhancing molecular reasoning capabilities. Our approach notably\nenhances interpretability, improving the model 's molecular understanding and\nenabling better generalization. Extensive experiments demonstrate that\nMolReasoner outperforms existing methods, and marking a significant shift from\nmemorization-based outputs to robust chemical reasoning.\n","authors":["Guojiang Zhao","Sihang Li","Zixiang Lu","Zheng Cheng","Haitao Lin","Lirong Wu","Hanchen Xia","Hengxing Cai","Wentao Guo","Hongshuai Wang","Mingjun Xu","Siyu Zhu","Guolin Ke","Linfeng Zhang","Zhifeng Gao"],"pdf_url":"https://arxiv.org/pdf/2508.02066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.16383v5","updated":"2025-08-04T04:51:04Z","published":"2025-06-19T15:12:58Z","title":"Large Language Models in Argument Mining: A Survey","summary":"  Argument Mining (AM), a critical subfield of Natural Language Processing\n(NLP), focuses on extracting argumentative structures from text. The advent of\nLarge Language Models (LLMs) has profoundly transformed AM, enabling advanced\nin-context learning, prompt-based generation, and robust cross-domain\nadaptability. This survey systematically synthesizes recent advancements in\nLLM-driven AM. We provide a concise review of foundational theories and\nannotation frameworks, alongside a meticulously curated catalog of datasets. A\nkey contribution is our comprehensive taxonomy of AM subtasks, elucidating how\ncontemporary LLM techniques -- such as prompting, chain-of-thought reasoning,\nand retrieval augmentation -- have reconfigured their execution. We further\ndetail current LLM architectures and methodologies, critically assess\nevaluation practices, and delineate pivotal challenges including long-context\nreasoning, interpretability, and annotation bottlenecks. Conclusively, we\nhighlight emerging trends and propose a forward-looking research agenda for\nLLM-based computational argumentation, aiming to strategically guide\nresearchers in this rapidly evolving domain.\n","authors":["Hao Li","Viktor Schlegel","Yizheng Sun","Riza Batista-Navarro","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2506.16383v5.pdf","comment":"Work draft"},{"id":"http://arxiv.org/abs/2508.02053v1","updated":"2025-08-04T04:44:43Z","published":"2025-08-04T04:44:43Z","title":"ProCut: LLM Prompt Compression via Attribution Estimation","summary":"  In large-scale industrial LLM systems, prompt templates often expand to\nthousands of tokens as teams iteratively incorporate sections such as task\ninstructions, few-shot examples, and heuristic rules to enhance robustness and\ncoverage. This expansion leads to bloated prompts that are difficult to\nmaintain and incur significant inference latency and serving costs. To address\nthis, we introduce Prompt Compression via Attribution Estimation (ProCut), a\nflexible, LLM-agnostic, training-free framework that compresses prompts through\nattribution analysis. ProCut segments prompt templates into semantically\nmeaningful units, quantifies their impact on task performance, and prunes\nlow-utility components. Through extensive experiments on five public benchmark\ndatasets and real-world industrial prompts, we show that ProCut achieves\nsubstantial prompt size reductions (78% fewer tokens in production) while\nmaintaining or even slightly improving task performance (up to 62% better than\nalternative methods). We further introduce an LLM-driven attribution estimator\nthat reduces compression latency by over 50%, and demonstrate that ProCut\nintegrates seamlessly with existing prompt-optimization frameworks to produce\nconcise, high-performing prompts.\n","authors":["Zhentao Xu","Fengyi Li","Albert Chen","Xiaofeng Wang"],"pdf_url":"https://arxiv.org/pdf/2508.02053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02045v1","updated":"2025-08-04T04:27:06Z","published":"2025-08-04T04:27:06Z","title":"Harnessing Temporal Databases for Systematic Evaluation of Factual\n  Time-Sensitive Question-Answering in Large Language Models","summary":"  Facts evolve over time, making it essential for Large Language Models (LLMs)\nto handle time-sensitive factual knowledge accurately and reliably. While\nfactual Time-Sensitive Question-Answering (TSQA) tasks have been widely\nstudied, existing benchmarks often rely on manual curation or a small, fixed\nset of predefined templates, which restricts scalable and comprehensive TSQA\nevaluation. To address these challenges, we propose TDBench, a new benchmark\nthat systematically constructs TSQA pairs by harnessing temporal databases and\ndatabase techniques such as temporal SQL and functional dependencies. We also\nintroduce a fine-grained evaluation metric called time accuracy, which assesses\nthe validity of time references in model explanations alongside traditional\nanswer accuracy to enable a more reliable TSQA evaluation. Extensive\nexperiments on contemporary LLMs show how \\ours{} enables scalable and\ncomprehensive TSQA evaluation while reducing the reliance on human labor,\ncomplementing existing Wikipedia/Wikidata-based TSQA evaluation approaches by\nenabling LLM evaluation on application-specific data and seamless multi-hop\nquestion generation. Code and data are publicly available at:\nhttps://github.com/ssoy0701/tdbench.git.\n","authors":["Soyeon Kim","Jindong Wang","Xing Xie","Steven Euijong Whang"],"pdf_url":"https://arxiv.org/pdf/2508.02045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12937v2","updated":"2025-08-04T04:22:09Z","published":"2025-03-17T08:51:44Z","title":"R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization","summary":"  Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods.\n","authors":["Jingyi Zhang","Jiaxing Huang","Huanjin Yao","Shunyu Liu","Xikun Zhang","Shijian Lu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2503.12937v2.pdf","comment":"ICCV 2025 Camera Ready"},{"id":"http://arxiv.org/abs/2408.16493v3","updated":"2025-08-04T04:11:24Z","published":"2024-08-29T12:44:01Z","title":"Learning from Negative Samples in Biomedical Generative Entity Linking","summary":"  Generative models have become widely used in biomedical entity linking\n(BioEL) due to their excellent performance and efficient memory usage. However,\nthese models are usually trained only with positive samples, i.e., entities\nthat match the input mention's identifier, and do not explicitly learn from\nhard negative samples, which are entities that look similar but have different\nmeanings. To address this limitation, we introduce ANGEL (Learning from\nNegative Samples in Biomedical Generative Entity Linking), the first framework\nthat trains generative BioEL models using negative samples. Specifically, a\ngenerative model is initially trained to generate positive entity names from\nthe knowledge base for given input entities. Subsequently, both correct and\nincorrect outputs are gathered from the model's top-k predictions. Finally, the\nmodel is updated to prioritize the correct predictions through preference\noptimization. Our models outperform the previous best baseline models by up to\nan average top-1 accuracy of 1.4% on five benchmarks. When incorporating our\nframework into pre-training, the performance improvement increases further to\n1.7%, demonstrating its effectiveness in both the pre-training and fine-tuning\nstages. The code and model weights are available at\nhttps://github.com/dmis-lab/ANGEL.\n","authors":["Chanhwi Kim","Hyunjae Kim","Sihyeon Park","Jiwoo Lee","Mujeen Sung","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2408.16493v3.pdf","comment":"ACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2508.02038v1","updated":"2025-08-04T04:08:22Z","published":"2025-08-04T04:08:22Z","title":"Marco-Voice Technical Report","summary":"  This paper presents a multifunctional speech synthesis system that integrates\nvoice cloning and emotion control speech synthesis within a unified framework.\nThe goal of this work is to address longstanding challenges in achieving highly\nexpressive, controllable, and natural speech generation that faithfully\npreserves speaker identity across diverse linguistic and emotional contexts.\nOur approach introduces an effective speaker-emotion disentanglement mechanism\nwith in-batch contrastive learning, enabling independent manipulation of\nspeaker identity and eemotional style, as well as rotational emotional\nembedding integration method for smooth emotion control. To support\ncomprehensive training and evaluation, we construct CSEMOTIONS, a high-quality\nemotional speech dataset containing 10 hours of Mandarin speech from six\nprofessional speakers across seven emotional categories. Extensive experiments\ndemonstrate that our system, Marco-Voice, achieves substantial improvements in\nboth objective and subjective metrics. Comprehensive evaluations and analysis\nwere conducted, results show that MarcoVoice delivers competitive performance\nin terms of speech clarity and emotional richness, representing a substantial\nadvance in the field of expressive neural speech synthesis.\n","authors":["Fengping Tian","Chenyang Lyu","Xuanfan Ni","Haoqin Sun","Qingjuan Li","Zhiqiang Qian","Haijun Li","Longyue Wang","Zhao Xu","Weihua Luo","Kaifu Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.02038v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2508.02037v1","updated":"2025-08-04T04:06:34Z","published":"2025-08-04T04:06:34Z","title":"Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a\n  Time","summary":"  Large Language Models (LLMs) perform well on reasoning benchmarks but often\nfail when inputs alter slightly, raising concerns about the extent to which\ntheir success relies on memorization. This issue is especially acute in\nChain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger\nintermediate errors that cascade into incorrect final answers. We introduce\nSTIM, a novel framework for Source-aware Token-level Identification of\nMemorization, which attributes each token in a reasoning chain to one of\nmultiple memorization sources - local, mid-range, or long-range - based on\ntheir statistical co-occurrence with the token in the pretraining corpus. Our\ntoken-level analysis across tasks and distributional settings reveals that\nmodels rely more on memorization in complex or long-tail cases, and that local\nmemorization is often the dominant driver of errors, leading to up to 67% of\nwrong tokens. We also show that memorization scores from STIM can be effective\nin predicting the wrong tokens in the wrong reasoning step. STIM offers a\npowerful tool for diagnosing and improving model reasoning and can generalize\nto other structured step-wise generation tasks.\n","authors":["Huihan Li","You Chen","Siyuan Wang","Yixin He","Ninareh Mehrabi","Rahul Gupta","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2508.02037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12465v3","updated":"2025-08-04T03:37:34Z","published":"2024-12-17T01:54:08Z","title":"Core Context Aware Transformers for Long Context Language Modeling","summary":"  Transformer-based Large Language Models (LLMs) have exhibited remarkable\nsuccess in extensive tasks primarily attributed to self-attention mechanism,\nwhich requires a token to consider all preceding tokens as its context to\ncompute attention. However, when the context length L becomes very large (e.g.,\n128K), the amount of potentially redundant information in the context tends to\nincrease. The redundant context not only hampers the modeling representation\nperformance but also incurs unnecessary computational and storage overhead. In\nthis paper, we propose a plug-and-play Core Context Aware (CCA) Attention for\nefficient long-context modeling, comprising two complementary modules: 1)\nGlobality-aware pooling module groups input tokens and dynamically compresses\neach group into one core token based on their significance. In this way, our\nmethod automatically focuses and strengthens core context while diminishing\nredundancy during the learning process, leading to effective long-term\ndependency modeling. 2) Locality-preserving module incorporates neighboring\ntokens to preserve local context for detailed representation. Notably, our\nCCA-Attention is able to replace the self-attention module in existing LLMs\nwith minimal fine-tuning cost. Extensive experimental results show the\nsuperiority of our method in both long-context modeling and computational\nefficiency over state-of-the-art methods.\n","authors":["Yaofo Chen","Zeng You","Shuhai Zhang","Haokun Li","Yirui Li","Yaowei Wang","Mingkui Tan"],"pdf_url":"https://arxiv.org/pdf/2412.12465v3.pdf","comment":"Accepted for publication at ICML 2025"},{"id":"http://arxiv.org/abs/2505.12842v3","updated":"2025-08-04T03:30:31Z","published":"2025-05-19T08:29:05Z","title":"GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in\n  GUI Agents","summary":"  Graphical user interface (GUI) agents have recently emerged as an intriguing\nparadigm for human-computer interaction, capable of automatically executing\nuser instructions to operate intelligent terminal devices. However, when\nencountering out-of-distribution (OOD) instructions that violate environmental\nconstraints or exceed the current capabilities of agents, GUI agents may suffer\ntask breakdowns or even pose security threats. Therefore, effective OOD\ndetection for GUI agents is essential. Traditional OOD detection methods\nperform suboptimally in this domain due to the complex embedding space and\nevolving GUI environments. In this work, we observe that the in-distribution\ninput semantic space of GUI agents exhibits a clustering pattern with respect\nto the distance from the centroid. Based on the finding, we propose GEM, a\nnovel method based on fitting a Gaussian mixture model over input embedding\ndistances extracted from the GUI agent that reflect its capability boundary.\nEvaluated on eight datasets spanning smartphones, computers, and web browsers,\nour method achieves an average accuracy improvement of 23.70\\% over the\nbest-performing baseline while only increasing training time by 4.9\\% and\ntesting time by 6.5\\%. We also experimentally demonstrate that GEM can improve\nthe step-wise success rate by 9.40\\% by requesting assistance from the cloud\nmodel when encountering OOD samples. Analysis verifies the generalization\nability of our method through experiments on nine different backbones. The\ncodes are available at https://github.com/Wuzheng02/GEM-OODforGUIagents.\n","authors":["Zheng Wu","Pengzhou Cheng","Zongru Wu","Lingzhong Dong","Zhuosheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.12842v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06258v3","updated":"2025-08-04T03:29:30Z","published":"2025-02-10T08:48:10Z","title":"Emergent Response Planning in LLMs","summary":"  In this work, we argue that large language models (LLMs), though trained to\npredict only the next token, exhibit emergent planning behaviors:\n$\\textbf{their hidden representations encode future outputs beyond the next\ntoken}$. Through simple probing, we demonstrate that LLM prompt representations\nencode global attributes of their entire responses, including\n$\\textit{structure attributes}$ (e.g., response length, reasoning steps),\n$\\textit{content attributes}$ (e.g., character choices in storywriting,\nmultiple-choice answers at the end of response), and $\\textit{behavior\nattributes}$ (e.g., answer confidence, factual consistency). In addition to\nidentifying response planning, we explore how it scales with model size across\ntasks and how it evolves during generation. The findings that LLMs plan ahead\nfor the future in their hidden representations suggest potential applications\nfor improving transparency and generation control.\n","authors":["Zhichen Dong","Zhanhui Zhou","Zhixuan Liu","Chao Yang","Chaochao Lu"],"pdf_url":"https://arxiv.org/pdf/2502.06258v3.pdf","comment":"Published at ICML 2025. Code available at:\n  https://github.com/niconi19/Emergent-Response-Planning-in-LLMs"},{"id":"http://arxiv.org/abs/2508.02018v1","updated":"2025-08-04T03:28:04Z","published":"2025-08-04T03:28:04Z","title":"SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models","summary":"  Large audio-language models (LALMs) have achieved near-human performance in\nsentence-level transcription and emotion recognition. However, existing\nevaluations focus mainly on surface-level perception, leaving the capacity of\nmodels for contextual and inference-driven reasoning in speech-based scenarios\ninsufficiently examined. To address this gap, we introduce SpeechR, a unified\nbenchmark for evaluating reasoning over speech in large audio-language models.\nSpeechR evaluates models along three key dimensions: factual retrieval,\nprocedural inference, and normative judgment. It includes three distinct\nevaluation formats. The multiple-choice version measures answer selection\naccuracy. The generative version assesses the coherence and logical consistency\nof reasoning chains. The acoustic-feature version investigates whether\nvariations in stress and emotion affect reasoning performance. Evaluations on\neleven state-of-the-art LALMs reveal that high transcription accuracy does not\ntranslate into strong reasoning capabilities. SpeechR establishes a structured\nbenchmark for evaluating reasoning in spoken language, enabling more targeted\nanalysis of model capabilities across diverse dialogue-based tasks.\n","authors":["Wanqi Yang","Yanda Li","Yunchao Wei","Meng Fang","Ling Chen"],"pdf_url":"https://arxiv.org/pdf/2508.02018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02013v1","updated":"2025-08-04T03:18:36Z","published":"2025-08-04T03:18:36Z","title":"SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech\n  Role-Playing Agents","summary":"  Recently, role-playing agents have emerged as a promising paradigm for\nachieving personalized interaction and emotional resonance. Existing research\nprimarily focuses on the textual modality, neglecting the critical dimension of\nspeech in realistic interactive scenarios. In particular, there is a lack of\nsystematic evaluation for Speech Role-Playing Agents (SRPAs). To address this\ngap, we construct SpeechRole-Data, a large-scale, high-quality dataset that\ncomprises 98 diverse roles and 112k speech-based single-turn and multi-turn\nconversations. Each role demonstrates distinct vocal characteristics, including\ntimbre and prosody, thereby enabling more sophisticated speech role-playing.\nFurthermore, we propose SpeechRole-Eval, a multidimensional evaluation\nbenchmark that systematically assesses SRPAs performance in key aspects such as\nfundamental interaction ability, speech expressiveness, and role-playing\nfidelity. Experimental results reveal the advantages and challenges of both\ncascaded and end-to-end speech role-playing agents in maintaining vocal style\nconsistency and role coherence. We release all data, code, and baseline models\nto provide a solid foundation for speech-driven multimodal role-playing\nresearch and to foster further developments in this field.\n","authors":["Changhao Jiang","Jiajun Sun","Yifei Cao","Jiabao Zhuang","Hui Li","Xiaoran Fan","Ming Zhang","Junjie Ye","Shihan Dou","Zhiheng Xi","Jingqi Tong","Yilong Wu","Baoyu Fan","Zhen Wang","Tao Liang","Zhihui Fei","Mingyang Wan","Guojun Ma","Tao Ji","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2508.02013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04066v4","updated":"2025-08-04T03:05:01Z","published":"2025-02-06T13:23:53Z","title":"Beyond Scaling: Measuring and Predicting the Upper Bound of Knowledge\n  Retention in Language Model Pre-Training","summary":"  The GPT-4 technical report highlights the possibility of predicting model\nperformance on downstream tasks using only pre-training signals, though\ndetailed methodologies are absent. Such predictive capabilities are essential\nfor resource-efficient pre-training and the construction of task-aligned\ndatasets. In this paper, we aim to predict performance in closed-book question\nanswering (QA), a vital downstream task that directly reflects a model's\ninternalized knowledge without the help of external tools. We address three\nprimary challenges: (1) limited access to and understanding of pre-training\ncorpora, (2) limitations of current evaluation methods for pre-trained models,\nand (3) limitations of frequency-based metrics in predicting model performance.\nIn response, we conduct large-scale retrieval and semantic analysis across the\npre-training corpora of 21 publicly available and 3 custom-trained large\nlanguage models. We then develop a multi-template QA evaluation framework\nincorporating paraphrased question variants. Building on these foundations, we\npropose Size-dependent Mutual Information (SMI), an information-theoretic\nmetric that linearly correlates pre-training data characteristics, model size,\nand QA accuracy, without requiring additional training. Experimental results\nshow that SMI outperforms co-occurrence-based baselines, achieving $R^2 > 0.75$\non models with over one billion parameters. Theoretical analysis further\nsuggests an upper bound of around 80% QA accuracy under optimal pre-training,\nreflecting intrinsic memory limitations and motivating the use of retrieval or\nfew-shot methods in later stages.\n","authors":["Changhao Jiang","Ming Zhang","Junjie Ye","Xiaoran Fan","Yifei Cao","Jiajun Sun","Zhiheng Xi","Shihan Dou","Yi Dong","Yujiong Shen","Jingqi Tong","Baoyu Fan","Zhen Wang","Tao Liang","Zhihui Fei","Mingyang Wan","Guojun Ma","Qi Zhang","Tao Gui","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2502.04066v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18458v3","updated":"2025-08-04T02:46:22Z","published":"2025-03-24T09:02:51Z","title":"StableGS: A Floater-Free Framework for 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) reconstructions are plagued by stubborn\n``floater\" artifacts that degrade their geometric and visual fidelity. We are\nthe first to reveal the root cause: a fundamental conflict in the 3DGS\noptimization process where the opacity gradients of floaters vanish when their\nblended color reaches a pseudo-equilibrium of canceling errors against the\nbackground, trapping them in a spurious local minimum. To resolve this, we\npropose StableGS, a novel framework that decouples geometric regularization\nfrom final appearance rendering. Its core is a Dual Opacity architecture that\ncreates two separate rendering paths: a ``Geometric Regularization Path\" to\nbear strong depth-based constraints for structural correctness, and an\n``Appearance Refinement Path\" to generate high-fidelity details upon this\nstable foundation. We complement this with a synergistic set of geometric\nconstraints: a self-supervised depth consistency loss and an external geometric\nprior enabled by our efficient global scale optimization algorithm. Experiments\non multiple benchmarks show StableGS not only eliminates floaters but also\nresolves the common blur-artifact trade-off, achieving state-of-the-art\ngeometric accuracy and visual quality.\n","authors":["Luchao Wang","Qian Ren","Kaimin Liao","Hua Wang","Zhi Chen","Yaohua Tang"],"pdf_url":"https://arxiv.org/pdf/2503.18458v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01999v1","updated":"2025-08-04T02:39:16Z","published":"2025-08-04T02:39:16Z","title":"Prompting Large Language Models to Detect Dementia Family Caregivers","summary":"  Social media, such as Twitter, provides opportunities for caregivers of\ndementia patients to share their experiences and seek support for a variety of\nreasons. Availability of this information online also paves the way for the\ndevelopment of internet-based interventions in their support. However, for this\npurpose, tweets written by caregivers of dementia patients must first be\nidentified. This paper demonstrates our system for the SMM4H 2025 shared task\n3, which focuses on detecting tweets posted by individuals who have a family\nmember with dementia. The task is outlined as a binary classification problem,\ndifferentiating between tweets that mention dementia in the context of a family\nmember and those that do not. Our solution to this problem explores large\nlanguage models (LLMs) with various prompting methods. Our results show that a\nsimple zero-shot prompt on a fine-tuned model yielded the best results. Our\nfinal system achieved a macro F1-score of 0.95 on the validation set and the\ntest set. Our full code is available on GitHub.\n","authors":["Md Badsha Biswas","Özlem Uzuner"],"pdf_url":"https://arxiv.org/pdf/2508.01999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.02252v2","updated":"2025-08-04T02:17:56Z","published":"2024-12-03T08:29:27Z","title":"Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity","summary":"  The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression.\n","authors":["Da Ma","Lu Chen","Situo Zhang","Yuxun Miao","Su Zhu","Zhi Chen","Hongshen Xu","Hanqi Li","Shuai Fan","Lei Pan","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2412.02252v2.pdf","comment":"14 pages, 7 figures, 7 tables"},{"id":"http://arxiv.org/abs/2501.05727v2","updated":"2025-08-04T02:14:13Z","published":"2025-01-10T05:51:52Z","title":"Self-Evolving Critique Abilities in Large Language Models","summary":"  Despite their remarkable performance, Large Language Models (LLMs) face a\ncritical challenge: providing feedback for tasks where human evaluation is\ndifficult or where LLMs potentially outperform humans. In such scenarios,\nleveraging the critique ability of LLMs themselves - identifying and correcting\nflaws - shows considerable promise. This paper explores enhancing critique\nabilities of LLMs, noting that current approaches rely on human annotations or\nmore powerful models, leaving the challenge of improving critique abilities\nwithout external supervision unresolved. We introduce SCRIT (Self-evolving\nCRITic), a framework that trains LLMs with self-generated data to evolve their\ncritique abilities. To address the low quality of naively generated data, we\npropose a contrastive-critic approach that uses reference solutions during data\nsynthesis to enhance the model's understanding of key concepts, and\nincorporates a self-validation scheme to ensure data quality. The final trained\nmodel operates without any reference solutions at inference time. Implemented\nwith Qwen2.5-72B-Instruct, a leading LLM, SCRIT demonstrates consistent\nimprovements across a wide range of benchmarks spanning both mathematical and\nscientific reasoning: achieving a 10.0\\% relative gain in critique-correction\naccuracy and a 19.0\\% relative improvement in error identification F1-score.\nOur analysis reveals that SCRIT's performance scales positively with data and\nmodel size and enables continuous improvement through multi-round iterations.\n","authors":["Zhengyang Tang","Ziniu Li","Zhenyang Xiao","Tian Ding","Ruoyu Sun","Benyou Wang","Dayiheng Liu","Fei Huang","Tianyu Liu","Bowen Yu","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2501.05727v2.pdf","comment":"Accepted by COLM 2025"},{"id":"http://arxiv.org/abs/2508.01990v1","updated":"2025-08-04T02:14:07Z","published":"2025-08-04T02:14:07Z","title":"Contextually Aware E-Commerce Product Question Answering using RAG","summary":"  E-commerce product pages contain a mix of structured specifications,\nunstructured reviews, and contextual elements like personalized offers or\nregional variants. Although informative, this volume can lead to cognitive\noverload, making it difficult for users to quickly and accurately find the\ninformation they need. Existing Product Question Answering (PQA) systems often\nfail to utilize rich user context and diverse product information effectively.\nWe propose a scalable, end-to-end framework for e-commerce PQA using Retrieval\nAugmented Generation (RAG) that deeply integrates contextual understanding. Our\nsystem leverages conversational history, user profiles, and product attributes\nto deliver relevant and personalized answers. It adeptly handles objective,\nsubjective, and multi-intent queries across heterogeneous sources, while also\nidentifying information gaps in the catalog to support ongoing content\nimprovement. We also introduce novel metrics to measure the framework's\nperformance which are broadly applicable for RAG system evaluations.\n","authors":["Praveen Tangarajan","Anand A. Rajasekar","Manish Rathi","Vinay Rao Dandin","Ozan Ersoy"],"pdf_url":"https://arxiv.org/pdf/2508.01990v1.pdf","comment":"6 pages, 1 figure, 5 tables. Preprint under review"},{"id":"http://arxiv.org/abs/2503.18288v5","updated":"2025-08-04T01:35:59Z","published":"2025-03-24T02:17:41Z","title":"TIB-STC: A Large-Scale Structured Tibetan Benchmark for Low-Resource\n  Language Modeling","summary":"  Advancement of large language models (LLMs) has brought transformative\ncapabilities to NLP, but such progress remains unevenly distributed, especially\nfor low-resource and culturally rich languages like Tibetan. In this paper, we\npresent TIB-STC, the first large-scale, expert-curated, and multi-domain\ndataset specifically designed to support the development and evaluation of LLMs\nfor the Tibetan language. Spanning over 11 billion tokens across literature,\nreligion, medicine, law, and daily communication, TIB-STC preserves traditional\ngrammar and stylistic richness. To validate its utility, we train a reference\nmodel, Sun-Shine, on TIB-STC through a three-stage pipeline involving\npretraining, supervised fine-tuning, and preference optimization. Evaluation on\nTLUE Benchmark for Tibetan-specific tasks, including Ti-MMLU and\nTi-SafetyBench, demonstrates the TIB-STC's effectiveness in enabling robust\ninstruction-following and culturally aligned generation. We release TIB-STC to\nadvance research in low-resource language modeling and promote inclusivity in\nmultilingual NLP. All data are available:\nhttps://github.com/Vicentvankor/sun-shine.\n","authors":["Cheng Huang","Fan Gao","Yutong Liu","Nyima Tashi","Xiangxiang Wang","Thupten Tsering","Ban Ma-bao","Renzeg Duojie","Gadeng Luosang","Rinchen Dongrub","Dorje Tashi","Xiao Feng","Hao Wang","Yongbin Yu"],"pdf_url":"https://arxiv.org/pdf/2503.18288v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01977v1","updated":"2025-08-04T01:32:58Z","published":"2025-08-04T01:32:58Z","title":"TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought\n  Reasoning in Language Models","summary":"  To address the severe data scarcity in Tibetan, a low-resource language\nspoken by over six million people, we introduce TIBSTC-CoT, the large-scale,\nmulti-domain Tibetan dataset automatically constructed via chain-of-thought\nprompting with large language models (LLMs). TIBSTC-CoT establishes a scalable\nand reproducible framework for dataset creation in low-resource settings,\ncovering diverse domains and reasoning patterns essential for language\nunderstanding and generation. Building on this dataset, we develop the\nSunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with\nchain-of-thought capabilities. Trained entirely on TIBSTC-CoT,\nSunshine-thinking has demonstrated strong reasoning and generation performance,\ncomparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a\nsignificant step toward inclusive AI by enabling high-quality Tibetan language\nprocessing through both resource creation and model innovation. All data are\navailable: https://github.com/Vicentvankor/sun-shine.\n","authors":["Fan Gao","Cheng Huang","Nyima Tashi","Yutong Liu","Xiangxiang Wang","Thupten Tsering","Ban Ma-bao","Renzeg Duojie","Gadeng Luosang","Rinchen Dongrub","Dorje Tashi","Xiao Feng","Hao Wang","Yongbin Yu"],"pdf_url":"https://arxiv.org/pdf/2508.01977v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09506v2","updated":"2025-08-04T01:12:15Z","published":"2025-07-13T06:17:53Z","title":"Ref-Long: Benchmarking the Long-context Referencing Capability of\n  Long-context Language Models","summary":"  Long-context language models (LCLMs) have exhibited impressive capabilities\nin long-context understanding tasks. Among these, long-context referencing -- a\ncrucial task that requires LCLMs to attribute items of interest to specific\nparts of long-context data -- remains underexplored. To bridge this gap, this\npaper proposes Referencing Evaluation for Long-context Language Models\n(Ref-Long), a novel benchmark designed to assess the long-context referencing\ncapability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the\nindexes of documents that reference a specific key, emphasizing contextual\nrelationships between the key and the documents over simple retrieval. Based on\nthe task design, we construct three subsets ranging from synthetic to realistic\nscenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs\nreveal significant shortcomings in long-context referencing, even among\nadvanced models like GPT-4o. To further investigate these challenges, we\nconduct comprehensive analyses, including human evaluations, task format\nadjustments, fine-tuning experiments, and error analyses, leading to several\nkey insights. Our data and code can be found in https://github.\ncom/wujunjie1998/Ref-Long.\n","authors":["Junjie Wu","Gefei Gu","Yanan Zheng","Dit-Yan Yeung","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2507.09506v2.pdf","comment":"ACL 2025 Main Conference. First 2 authors contributed equally.\n  Project webpage: https://wujunjie1998.github.io/Ref-Long-website/"},{"id":"http://arxiv.org/abs/2506.23463v3","updated":"2025-08-04T01:06:29Z","published":"2025-06-30T02:03:23Z","title":"What to Keep and What to Drop: Adaptive Table Filtering Framework","summary":"  Large language models (LLMs) for table-based reasoning often struggle with\nlarge tables due to input length limits. We propose ATF (Adaptive Table\nFiltering Framework), a modular and question-aware filtering pipeline that\nprunes uninformative columns and rows using LLM-generated column descriptions,\nclustering, and sparse-dense alignment scores. ATF integrates seamlessly with\nexisting models (e.g., TAPAS, TAPEX) without retraining. Experiments show that\nATF reduces table cells by 70%, boosting performance on out-of-domain TableQA\ntasks while causing slight performance drops on Table Fact Verification, where\nfull-table context is more critical. These results highlight ATF's ability to\nadaptively balance informativeness and minimalism across tasks. Our code\navailable at:\nhttps://github.com/torijune/ATF-Adaptive-Table-Filtering-Framework\n","authors":["WonJune Jang"],"pdf_url":"https://arxiv.org/pdf/2506.23463v3.pdf","comment":"26 pages, 9 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2508.02671v1","updated":"2025-08-04T17:59:56Z","published":"2025-08-04T17:59:56Z","title":"Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on\n  Vision-Language Models","summary":"  For CLIP-based prompt tuning, introducing more data as additional knowledge\nfor enhancing fine-tuning process is proved to be an effective approach.\nExisting data amplification strategies for prompt tuning typically rely on\nexternal knowledge (e.g., large language models or pre-structured knowledge\nbases), resulting in higher costs for data collection and processing, while\ngenerally ignoring further utilization of features in image modality. To\naddress this, we propose Augmentation-driven Prompt Tuning (AugPT), a\nself-contained distillation-based prompt tuning approach using only internal\naugmentation on raw dataset to better exploit known features. Specifically,\nAugPT employs self-supervised augmentation on unlabeled images in the training\nset, and introduces a novel gating mechanism based on consensus test, reusing\nthe pre-trained prompt tuning backbone model to spontaneously filter noisy\nsamples, further enhancing the quality of augmented views. Extensive\nexperiments validate that AugPT simultaneously enhances model performance and\ngeneralization capability without using appended external knowledge. The code\nof AugPT is available at: https://github.com/JREion/AugPT .\n","authors":["Haoyang Li","Liang Wang","Chao Wang","Siyu Zhou","Jing Jiang","Yan Peng","Guodong Long"],"pdf_url":"https://arxiv.org/pdf/2508.02671v1.pdf","comment":"16 pages, 6 figures, 15 tables"},{"id":"http://arxiv.org/abs/2508.02669v1","updated":"2025-08-04T17:59:38Z","published":"2025-08-04T17:59:38Z","title":"MedVLThinker: Simple Baselines for Multimodal Medical Reasoning","summary":"  Large Reasoning Models (LRMs) have introduced a new paradigm in AI by\nenabling models to ``think before responding\" via chain-of-thought reasoning.\nHowever, the absence of open and reproducible recipes for building\nreasoning-centric medical LMMs hinders community-wide research, analysis, and\ncomparison. In this paper, we present MedVLThinker, a suite of simple yet\nstrong baselines. Our fully open recipe consists of: (1) systematic data\ncuration for both text-only and image-text medical data, filtered according to\nvarying levels of reasoning difficulty, and (2) two training paradigms:\nSupervised Fine-Tuning (SFT) on distilled reasoning traces and Reinforcement\nLearning with Verifiable Rewards (RLVR) based on final answer correctness.\nAcross extensive experiments on the Qwen2.5-VL model family (3B, 7B) and six\nmedical QA benchmarks, we find that RLVR consistently and significantly\noutperforms SFT. Additionally, under the RLVR framework, a key,\ncounter-intuitive finding is that training on our curated text-only reasoning\ndata provides a more substantial performance boost than training on multimodal\nimage-text data. Our best open 7B model, trained using the RLVR recipe on\ntext-only data, establishes a new state-of-the-art on existing public VQA\nbenchmarks, surpassing all previous open-source medical LMMs. Furthermore,\nscaling our model to 32B achieves performance on par with the proprietary\nGPT-4o. We release all curated data, models, and code to provide the community\nwith a strong, open foundation for future research in multimodal medical\nreasoning.\n","authors":["Xiaoke Huang","Juncheng Wu","Hui Liu","Xianfeng Tang","Yuyin Zhou"],"pdf_url":"https://arxiv.org/pdf/2508.02669v1.pdf","comment":"Project page and code: https://ucsc-vlaa.github.io/MedVLThinker/"},{"id":"http://arxiv.org/abs/2508.02660v1","updated":"2025-08-04T17:49:37Z","published":"2025-08-04T17:49:37Z","title":"PMGS: Reconstruction of Projectile Motion across Large Spatiotemporal\n  Spans via 3D Gaussian Splatting","summary":"  Modeling complex rigid motion across large spatiotemporal spans remains an\nunresolved challenge in dynamic reconstruction. Existing paradigms are mainly\nconfined to short-term, small-scale deformation and offer limited consideration\nfor physical consistency. This study proposes PMGS, focusing on reconstructing\nProjectile Motion via 3D Gaussian Splatting. The workflow comprises two stages:\n1) Target Modeling: achieving object-centralized reconstruction through dynamic\nscene decomposition and an improved point density control; 2) Motion Recovery:\nrestoring full motion sequences by learning per-frame SE(3) poses. We introduce\nan acceleration consistency constraint to bridge Newtonian mechanics and pose\nestimation, and design a dynamic simulated annealing strategy that adaptively\nschedules learning rates based on motion states. Futhermore, we devise a Kalman\nfusion scheme to optimize error accumulation from multi-source observations to\nmitigate disturbances. Experiments show PMGS's superior performance in\nreconstructing high-speed nonlinear rigid motion compared to mainstream dynamic\nmethods.\n","authors":["Yijun Xu","Jingrui Zhang","Yuhan Chen","Dingwen Wang","Lei Yu","Chu He"],"pdf_url":"https://arxiv.org/pdf/2508.02660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02645v1","updated":"2025-08-04T17:37:13Z","published":"2025-08-04T17:37:13Z","title":"Evaluating Variance in Visual Question Answering Benchmarks","summary":"  Multimodal large language models (MLLMs) have emerged as powerful tools for\nvisual question answering (VQA), enabling reasoning and contextual\nunderstanding across visual and textual modalities. Despite their advancements,\nthe evaluation of MLLMs on VQA benchmarks often relies on point estimates,\noverlooking the significant variance in performance caused by factors such as\nstochastic model outputs, training seed sensitivity, and hyperparameter\nconfigurations. This paper critically examines these issues by analyzing\nvariance across 14 widely used VQA benchmarks, covering diverse tasks such as\nvisual reasoning, text understanding, and commonsense reasoning. We\nsystematically study the impact of training seed, framework non-determinism,\nmodel scale, and extended instruction finetuning on performance variability.\nAdditionally, we explore Cloze-style evaluation as an alternate assessment\nstrategy, studying its effectiveness in reducing stochasticity and improving\nreliability across benchmarks. Our findings highlight the limitations of\ncurrent evaluation practices and advocate for variance-aware methodologies to\nfoster more robust and reliable development of MLLMs.\n","authors":["Nikitha SR"],"pdf_url":"https://arxiv.org/pdf/2508.02645v1.pdf","comment":"Accepted in ICCV 2025 Workshop on What's Next in Multimodal\n  Foundational Models"},{"id":"http://arxiv.org/abs/2508.02605v1","updated":"2025-08-04T16:56:35Z","published":"2025-08-04T16:56:35Z","title":"ReMoMask: Retrieval-Augmented Masked Motion Generation","summary":"  Text-to-Motion (T2M) generation aims to synthesize realistic and semantically\naligned human motion sequences from natural language descriptions. However,\ncurrent approaches face dual challenges: Generative models (e.g., diffusion\nmodels) suffer from limited diversity, error accumulation, and physical\nimplausibility, while Retrieval-Augmented Generation (RAG) methods exhibit\ndiffusion inertia, partial-mode collapse, and asynchronous artifacts. To\naddress these limitations, we propose ReMoMask, a unified framework integrating\nthree key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples\nnegative sample scale from batch size via momentum queues, substantially\nimproving cross-modal retrieval precision; 2) A Semantic Spatio-temporal\nAttention mechanism enforces biomechanical constraints during part-level fusion\nto eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates\nminor unconditional generation to enhance generalization. Built upon MoMask's\nRVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal\nsteps. Extensive experiments on standard benchmarks demonstrate the\nstate-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97%\nimprovement in FID scores on HumanML3D and KIT-ML, respectively, compared to\nthe previous SOTA method RAG-T2M. Code:\nhttps://github.com/AIGeeksGroup/ReMoMask. Website:\nhttps://aigeeksgroup.github.io/ReMoMask.\n","authors":["Zhengdao Li","Siheng Wang","Zeyu Zhang","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2508.02605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.22967v2","updated":"2025-08-04T16:54:44Z","published":"2025-06-28T17:57:58Z","title":"ActAlign: Zero-Shot Fine-Grained Video Classification via\n  Language-Guided Sequence Alignment","summary":"  We address the task of zero-shot video classification for extremely\nfine-grained actions (e.g., Windmill Dunk in basketball), where no video\nexamples or temporal annotations are available for unseen classes. While\nimage-language models (e.g., CLIP, SigLIP) show strong open-set recognition,\nthey lack temporal modeling needed for video understanding. We propose\nActAlign, a truly zero-shot, training-free method that formulates video\nclassification as a sequence alignment problem, preserving the generalization\nstrength of pretrained image-language models. For each class, a large language\nmodel (LLM) generates an ordered sequence of sub-actions, which we align with\nvideo frames using Dynamic Time Warping (DTW) in a shared embedding space.\nWithout any video-text supervision or fine-tuning, ActAlign achieves 30.5%\naccuracy on ActionAtlas--the most diverse benchmark of fine-grained actions\nacross multiple sports--where human performance is only 61.6%. ActAlign\noutperforms billion-parameter video-language models while using 8x fewer\nparameters. Our approach is model-agnostic and domain-general, demonstrating\nthat structured language priors combined with classical alignment methods can\nunlock the open-set recognition potential of image-language models for\nfine-grained video understanding.\n","authors":["Amir Aghdam","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2506.22967v2.pdf","comment":"Preprint manuscript - Project page:\n  https://amir-aghdam.github.io/act-align/"},{"id":"http://arxiv.org/abs/2506.23711v2","updated":"2025-08-04T16:51:20Z","published":"2025-06-30T10:36:49Z","title":"Subjective Camera 0.1: Bridging Human Cognition and Visual\n  Reconstruction through Sequence-Aware Sketch-Guided Diffusion","summary":"  We introduce the concept of a subjective camera to reconstruct meaningful\nmoments that physical cameras fail to capture. We propose Subjective Camera\n0.1, a framework for reconstructing real-world scenes from readily accessible\nsubjective readouts, i.e., textual descriptions and progressively drawn rough\nsketches. Built on optimization-based alignment of diffusion models, our\napproach avoids large-scale paired training data and mitigates generalization\nissues. To address the challenge of integrating multiple abstract concepts in\nreal-world scenarios, we design a Sequence-Aware Sketch-Guided Diffusion\nframework with three loss terms for concept-wise sequential optimization,\nfollowing the natural order of subjective readouts. Experiments on two datasets\ndemonstrate that our method achieves state-of-the-art performance in image\nquality as well as spatial and semantic alignment with target scenes. User\nstudies with 40 participants further confirm that our approach is consistently\npreferred.Our project page is at: subjective-camera.github.io\n","authors":["Haoyang Chen","Dongfang Sun","Caoyuan Ma","Shiqin Wang","Kewei Zhang","Zheng Wang","Zhixiang Wang"],"pdf_url":"https://arxiv.org/pdf/2506.23711v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17317v2","updated":"2025-08-04T16:47:17Z","published":"2025-05-22T22:11:05Z","title":"Optimizing Image Capture for Computer Vision-Powered Taxonomic\n  Identification and Trait Recognition of Biodiversity Specimens","summary":"  1) Biological collections house millions of specimens with digital images\nincreasingly available through open-access platforms. However, most imaging\nprotocols were developed for human interpretation without considering automated\nanalysis requirements. As computer vision applications revolutionize taxonomic\nidentification and trait extraction, a critical gap exists between current\ndigitization practices and computational analysis needs. This review provides\nthe first comprehensive practical framework for optimizing biological specimen\nimaging for computer vision applications. 2) Through interdisciplinary\ncollaboration between taxonomists, collection managers, ecologists, and\ncomputer scientists, we synthesized evidence-based recommendations addressing\nfundamental computer vision concepts and practical imaging considerations. We\nprovide immediately actionable implementation guidance while identifying\ncritical areas requiring community standards development. 3) Our framework\nencompasses ten interconnected considerations for optimizing image capture for\ncomputer vision-powered taxonomic identification and trait extraction. We\ntranslate these into practical implementation checklists, equipment selection\nguidelines, and a roadmap for community standards development including\nfilename conventions, pixel density requirements, and cross-institutional\nprotocols. 4)By bridging biological and computational disciplines, this\napproach unlocks automated analysis potential for millions of existing\nspecimens and guides future digitization efforts toward unprecedented\nanalytical capabilities.\n","authors":["Alyson East","Elizabeth G. Campolongo","Luke Meyers","S M Rayeed","Samuel Stevens","Iuliia Zarubiieva","Isadora E. Fluck","Jennifer C. Girón","Maximiliane Jousse","Scott Lowe","Kayla I Perry","Isabelle Betancourt","Noah Charney","Evan Donoso","Nathan Fox","Kim J. Landsbergen","Ekaterina Nepovinnykh","Michelle Ramirez","Parkash Singh","Khum Thapa-Magar","Matthew Thompson","Evan Waite","Tanya Berger-Wolf","Hilmar Lapp","Paula Mabee","Charles Stewart","Graham Taylor","Sydne Record"],"pdf_url":"https://arxiv.org/pdf/2505.17317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17289v4","updated":"2025-08-04T16:19:21Z","published":"2025-02-24T16:20:25Z","title":"A novel approach to navigate the taxonomic hierarchy to address the\n  Open-World Scenarios in Medicinal Plant Classification","summary":"  In this article, we propose a novel approach for plant hierarchical taxonomy\nclassification by posing the problem as an open class problem. It is observed\nthat existing methods for medicinal plant classification often fail to perform\nhierarchical classification and accurately identifying unknown species,\nlimiting their effectiveness in comprehensive plant taxonomy classification.\nThus we address the problem of unknown species classification by assigning it\nbest hierarchical labels. We propose a novel method, which integrates\nDenseNet121, Multi-Scale Self-Attention (MSSA) and cascaded classifiers for\nhierarchical classification. The approach systematically categorizes medicinal\nplants at multiple taxonomic levels, from phylum to species, ensuring detailed\nand precise classification. Using multi scale space attention, the model\ncaptures both local and global contextual information from the images,\nimproving the distinction between similar species and the identification of new\nones. It uses attention scores to focus on important features across multiple\nscales. The proposed method provides a solution for hierarchical\nclassification, showcasing superior performance in identifying both known and\nunknown species. The model was tested on two state-of-art datasets with and\nwithout background artifacts and so that it can be deployed to tackle real word\napplication. We used unknown species for testing our model. For unknown species\nthe model achieved an average accuracy of 83.36%, 78.30%, 60.34% and 43.32% for\npredicting correct phylum, class, order and family respectively. Our proposed\nmodel size is almost four times less than the existing state of the art methods\nmaking it easily deploy able in real world application.\n","authors":["Soumen Sinha","Tanisha Rana","Susmita Ghosh","Rahul Roy"],"pdf_url":"https://arxiv.org/pdf/2502.17289v4.pdf","comment":"We want to do some modifications and add more experiments"},{"id":"http://arxiv.org/abs/2508.02560v1","updated":"2025-08-04T16:14:15Z","published":"2025-08-04T16:14:15Z","title":"Explainable AI Methods for Neuroimaging: Systematic Failures of Common\n  Tools, the Need for Domain-Specific Validation, and a Proposal for Safe\n  Application","summary":"  Trustworthy interpretation of deep learning models is critical for\nneuroimaging applications, yet commonly used Explainable AI (XAI) methods lack\nrigorous validation, risking misinterpretation. We performed the first\nlarge-scale, systematic comparison of XAI methods on ~45,000 structural brain\nMRIs using a novel XAI validation framework. This framework establishes\nverifiable ground truth by constructing prediction tasks with known signal\nsources - from localized anatomical features to subject-specific clinical\nlesions - without artificially altering input images. Our analysis reveals\nsystematic failures in two of the most widely used methods: GradCAM\nconsistently failed to localize predictive features, while Layer-wise Relevance\nPropagation generated extensive, artifactual explanations that suggest\nincompatibility with neuroimaging data characteristics. Our results indicate\nthat these failures stem from a domain mismatch, where methods with design\nprinciples tailored to natural images require substantial adaptation for\nneuroimaging data. In contrast, the simpler, gradient-based method SmoothGrad,\nwhich makes fewer assumptions about data structure, proved consistently\naccurate, suggesting its conceptual simplicity makes it more robust to this\ndomain shift. These findings highlight the need for domain-specific adaptation\nand validation of XAI methods, suggest that interpretations from prior\nneuroimaging studies using standard XAI methodology warrant re-evaluation, and\nprovide urgent guidance for practical application of XAI in neuroimaging.\n","authors":["Nys Tjade Siegel","James H. Cole","Mohamad Habes","Stefan Haufe","Kerstin Ritter","Marc-André Schulz"],"pdf_url":"https://arxiv.org/pdf/2508.02560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.17349v2","updated":"2025-08-04T16:14:00Z","published":"2025-04-24T08:10:10Z","title":"DRC: Enhancing Personalized Image Generation via Disentangled\n  Representation Composition","summary":"  Personalized image generation has emerged as a promising direction in\nmultimodal content creation. It aims to synthesize images tailored to\nindividual style preferences (e.g., color schemes, character appearances,\nlayout) and semantic intentions (e.g., emotion, action, scene contexts) by\nleveraging user-interacted history images and multimodal instructions. Despite\nnotable progress, existing methods -- whether based on diffusion models, large\nlanguage models, or Large Multimodal Models (LMMs) -- struggle to accurately\ncapture and fuse user style preferences and semantic intentions. In particular,\nthe state-of-the-art LMM-based method suffers from the entanglement of visual\nfeatures, leading to Guidance Collapse, where the generated images fail to\npreserve user-preferred styles or reflect the specified semantics.\n  To address these limitations, we introduce DRC, a novel personalized image\ngeneration framework that enhances LMMs through Disentangled Representation\nComposition. DRC explicitly extracts user style preferences and semantic\nintentions from history images and the reference image, respectively, to form\nuser-specific latent instructions that guide image generation within LMMs.\nSpecifically, it involves two critical learning stages: 1) Disentanglement\nlearning, which employs a dual-tower disentangler to explicitly separate style\nand semantic features, optimized via a reconstruction-driven paradigm with\ndifficulty-aware importance sampling; and 2) Personalized modeling, which\napplies semantic-preserving augmentations to effectively adapt the disentangled\nrepresentations for robust personalized generation. Extensive experiments on\ntwo benchmarks demonstrate that DRC shows competitive performance while\neffectively mitigating the guidance collapse issue, underscoring the importance\nof disentangled representation learning for controllable and effective\npersonalized image generation.\n","authors":["Yiyan Xu","Wuqiang Zheng","Wenjie Wang","Fengbin Zhu","Xinting Hu","Yang Zhang","Fuli Feng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2504.17349v2.pdf","comment":"Accepted for publication in ACM MM'25"},{"id":"http://arxiv.org/abs/2508.02557v1","updated":"2025-08-04T16:12:06Z","published":"2025-08-04T16:12:06Z","title":"RL-U$^2$Net: A Dual-Branch UNet with Reinforcement Learning-Assisted\n  Multimodal Feature Fusion for Accurate 3D Whole-Heart Segmentation","summary":"  Accurate whole-heart segmentation is a critical component in the precise\ndiagnosis and interventional planning of cardiovascular diseases. Integrating\ncomplementary information from modalities such as computed tomography (CT) and\nmagnetic resonance imaging (MRI) can significantly enhance segmentation\naccuracy and robustness. However, existing multi-modal segmentation methods\nface several limitations: severe spatial inconsistency between modalities\nhinders effective feature fusion; fusion strategies are often static and lack\nadaptability; and the processes of feature alignment and segmentation are\ndecoupled and inefficient. To address these challenges, we propose a\ndual-branch U-Net architecture enhanced by reinforcement learning for feature\nalignment, termed RL-U$^2$Net, designed for precise and efficient multi-modal\n3D whole-heart segmentation. The model employs a dual-branch U-shaped network\nto process CT and MRI patches in parallel, and introduces a novel RL-XAlign\nmodule between the encoders. The module employs a cross-modal attention\nmechanism to capture semantic correspondences between modalities and a\nreinforcement-learning agent learns an optimal rotation strategy that\nconsistently aligns anatomical pose and texture features. The aligned features\nare then reconstructed through their respective decoders. Finally, an\nensemble-learning-based decision module integrates the predictions from\nindividual patches to produce the final segmentation result. Experimental\nresults on the publicly available MM-WHS 2017 dataset demonstrate that the\nproposed RL-U$^2$Net outperforms existing state-of-the-art methods, achieving\nDice coefficients of 93.1% on CT and 87.0% on MRI, thereby validating the\neffectiveness and superiority of the proposed approach.\n","authors":["Jierui Qu","Jianchun Zhao"],"pdf_url":"https://arxiv.org/pdf/2508.02557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02549v1","updated":"2025-08-04T16:01:30Z","published":"2025-08-04T16:01:30Z","title":"MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming","summary":"  Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth\ninputs to provide rich spatial cues for action planning, but these sensors can\nbe costly or less accessible in real-world deployments. Recent approaches based\non Vision-Language Action (VLA) models achieve strong results with monocular\ninput, yet they still lag behind methods using panoramic RGB-D information. We\npresent MonoDream, a lightweight VLA framework that enables monocular agents to\nlearn a Unified Navigation Representation (UNR). This shared feature\nrepresentation jointly aligns navigation-relevant visual semantics (e.g.,\nglobal layout, depth, and future cues) and language-grounded action intent,\nenabling more reliable action prediction. MonoDream further introduces Latent\nPanoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to\npredict latent features of panoramic RGB and depth observations at both current\nand future steps based on only monocular input. Experiments on multiple VLN\nbenchmarks show that MonoDream consistently improves monocular navigation\nperformance and significantly narrows the gap with panoramic-based agents.\n","authors":["Shuo Wang","Yongcai Wang","Wanting Li","Yucheng Wang","Maiyue Chen","Kaihui Wang","Zhizhong Su","Xudong Cai","Yeying Jin","Deying Li","Zhaoxin Fan"],"pdf_url":"https://arxiv.org/pdf/2508.02549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10342v2","updated":"2025-08-04T15:56:54Z","published":"2025-06-12T04:35:39Z","title":"UrbanSense:A Framework for Quantitative Analysis of Urban Streetscapes\n  leveraging Vision Large Language Models","summary":"  Urban cultures and architectural styles vary significantly across cities due\nto geographical, chronological, historical, and socio-political factors.\nUnderstanding these differences is essential for anticipating how cities may\nevolve in the future. As representative cases of historical continuity and\nmodern innovation in China, Beijing and Shenzhen offer valuable perspectives\nfor exploring the transformation of urban streetscapes. However, conventional\napproaches to urban cultural studies often rely on expert interpretation and\nhistorical documentation, which are difficult to standardize across different\ncontexts. To address this, we propose a multimodal research framework based on\nvision-language models, enabling automated and scalable analysis of urban\nstreetscape style differences. This approach enhances the objectivity and\ndata-driven nature of urban form research. The contributions of this study are\nas follows: First, we construct UrbanDiffBench, a curated dataset of urban\nstreetscapes containing architectural images from different periods and\nregions. Second, we develop UrbanSense, the first vision-language-model-based\nframework for urban streetscape analysis, enabling the quantitative generation\nand comparison of urban style representations. Third, experimental results show\nthat Over 80% of generated descriptions pass the t-test (p less than 0.05).\nHigh Phi scores (0.912 for cities, 0.833 for periods) from subjective\nevaluations confirm the method's ability to capture subtle stylistic\ndifferences. These results highlight the method's potential to quantify and\ninterpret urban style evolution, offering a scientifically grounded lens for\nfuture design.\n","authors":["Jun Yin","Jing Zhong","Peilin Li","Ruolin Pan","Pengyu Zeng","Miao Zhang","Shuai Lu"],"pdf_url":"https://arxiv.org/pdf/2506.10342v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02533v1","updated":"2025-08-04T15:41:52Z","published":"2025-08-04T15:41:52Z","title":"Precision-Aware Video Compression for Reducing Bandwidth Requirements in\n  Video Communication for Vehicle Detection-Based Applications","summary":"  Computer vision has become a popular tool in intelligent transportation\nsystems (ITS), enabling various applications through roadside traffic cameras\nthat capture video and transmit it in real time to computing devices within the\nsame network. The efficiency of this video transmission largely depends on the\navailable bandwidth of the communication system. However, limited bandwidth can\nlead to communication bottlenecks, hindering the real-time performance of ITS\napplications. To mitigate this issue, lossy video compression techniques can be\nused to reduce bandwidth requirements, at the cost of degrading video quality.\nThis degradation can negatively impact the accuracy of applications that rely\non real-time vehicle detection. Additionally, vehicle detection accuracy is\ninfluenced by environmental factors such as weather and lighting conditions,\nsuggesting that compression levels should be dynamically adjusted in response\nto these variations. In this work, we utilize a framework called\nPrecision-Aware Video Compression (PAVC), where a roadside video camera\ncaptures footage of vehicles on roadways, compresses videos, and then transmits\nthem to a processing unit, running a vehicle detection algorithm for\nsafety-critical applications, such as real-time collision risk assessment. The\nsystem dynamically adjusts the video compression level based on current weather\nand lighting conditions to maintain vehicle detection accuracy while minimizing\nbandwidth usage. Our results demonstrate that PAVC improves vehicle detection\naccuracy by up to 13% and reduces communication bandwidth requirements by up to\n8.23x in areas with moderate bandwidth availability. Moreover, in locations\nwith severely limited bandwidth, PAVC reduces bandwidth requirements by up to\n72x while preserving vehicle detection performance.\n","authors":["Abyad Enan","Jon C Calhoun","Mashrur Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2508.02533v1.pdf","comment":"This work has been submitted to the Transportation Research Record:\n  Journal of the Transportation Research Board for possible publication"},{"id":"http://arxiv.org/abs/2508.02530v1","updated":"2025-08-04T15:40:03Z","published":"2025-08-04T15:40:03Z","title":"Understanding the Risks of Asphalt Art on the Reliability of\n  Surveillance Perception Systems","summary":"  Artistic crosswalks featuring asphalt art, introduced by different\norganizations in recent years, aim to enhance the visibility and safety of\npedestrians. However, their visual complexity may interfere with surveillance\nsystems that rely on vision-based object detection models. In this study, we\ninvestigate the impact of asphalt art on pedestrian detection performance of a\npretrained vision-based object detection model. We construct realistic\ncrosswalk scenarios by compositing various street art patterns into a fixed\nsurveillance scene and evaluate the model's performance in detecting\npedestrians on asphalt-arted crosswalks under both benign and adversarial\nconditions. A benign case refers to pedestrian crosswalks painted with existing\nnormal asphalt art, whereas an adversarial case involves digitally crafted or\naltered asphalt art perpetrated by an attacker. Our results show that while\nsimple, color-based designs have minimal effect, complex artistic patterns,\nparticularly those with high visual salience, can significantly degrade\npedestrian detection performance. Furthermore, we demonstrate that\nadversarially crafted asphalt art can be exploited to deliberately obscure real\npedestrians or generate non-existent pedestrian detections. These findings\nhighlight a potential vulnerability in urban vision-based pedestrian\nsurveillance systems and underscore the importance of accounting for\nenvironmental visual variations when designing robust pedestrian perception\nmodels.\n","authors":["Jin Ma","Abyad Enan","Long Cheng","Mashrur Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2508.02530v1.pdf","comment":"J. Ma and A. Enan are co-first authors; they have contributed\n  equally. This work has been submitted to the Transportation Research Record:\n  Journal of the Transportation Research Board for possible publication"},{"id":"http://arxiv.org/abs/2507.18661v3","updated":"2025-08-04T15:39:40Z","published":"2025-07-23T16:58:44Z","title":"Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by\n  Reinforcement Learning from Visual Map Feed Back","summary":"  Next Location Prediction is a fundamental task in the study of human\nmobility, with wide-ranging applications in transportation planning, urban\ngovernance, and epidemic forecasting. In practice, when humans attempt to\npredict the next location in a trajectory, they often visualize the trajectory\non a map and reason based on road connectivity and movement trends. However,\nthe vast majority of existing next-location prediction models do not reason\nover maps \\textbf{in the way that humans do}. Fortunately, the recent\ndevelopment of Vision-Language Models (VLMs) has demonstrated strong\ncapabilities in visual perception and even visual reasoning. This opens up a\nnew possibility: by rendering both the road network and trajectory onto an\nimage and leveraging the reasoning abilities of VLMs, we can enable models to\nperform trajectory inference in a human-like manner. To explore this idea, we\nfirst propose a method called Vision-Guided Location Search (VGLS), which\nevaluates whether a general-purpose VLM is capable of trajectory-based\nreasoning without modifying any of its internal parameters. Based on insights\nfrom the VGLS results, we further propose our main approach: VLMLocPredictor,\nwhich is composed of two stages: In the first stage, we design two Supervised\nFine-Tuning (SFT) tasks that help the VLM understand road network and\ntrajectory structures and acquire basic reasoning ability on such visual\ninputs. In the second stage, we introduce Reinforcement Learning from Visual\nMap Feedback, enabling the model to self-improve its next-location prediction\nability through interaction with the environment. Experiments conducted on\ndatasets from four different cities show that our method achieves\nstate-of-the-art (SOTA) performance and exhibits superior cross-city\ngeneralization compared to other LLM-based approaches.\n","authors":["Ruixing Zhang","Yang Zhang","Tongyu Zhu","Leilei Sun","Weifeng Lv"],"pdf_url":"https://arxiv.org/pdf/2507.18661v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02528v1","updated":"2025-08-04T15:36:58Z","published":"2025-08-04T15:36:58Z","title":"From Pixels to Pathology: Restoration Diffusion for\n  Diagnostic-Consistent Virtual IHC","summary":"  Hematoxylin and eosin (H&E) staining is the clinical standard for assessing\ntissue morphology, but it lacks molecular-level diagnostic information. In\ncontrast, immunohistochemistry (IHC) provides crucial insights into biomarker\nexpression, such as HER2 status for breast cancer grading, but remains costly\nand time-consuming, limiting its use in time-sensitive clinical workflows. To\naddress this gap, virtual staining from H&E to IHC has emerged as a promising\nalternative, yet faces two core challenges: (1) Lack of fair evaluation of\nsynthetic images against misaligned IHC ground truths, and (2) preserving\nstructural integrity and biological variability during translation. To this\nend, we present an end-to-end framework encompassing both generation and\nevaluation in this work. We introduce Star-Diff, a structure-aware staining\nrestoration diffusion model that reformulates virtual staining as an image\nrestoration task. By combining residual and noise-based generation pathways,\nStar-Diff maintains tissue structure while modeling realistic biomarker\nvariability. To evaluate the diagnostic consistency of the generated IHC\npatches, we propose the Semantic Fidelity Score (SFS), a\nclinical-grading-task-driven metric that quantifies class-wise semantic\ndegradation based on biomarker classification accuracy. Unlike pixel-level\nmetrics such as SSIM and PSNR, SFS remains robust under spatial misalignment\nand classifier uncertainty. Experiments on the BCI dataset demonstrate that\nStar-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity\nand diagnostic relevance. With rapid inference and strong clinical alignment,it\npresents a practical solution for applications such as intraoperative virtual\nIHC synthesis.\n","authors":["Jingsong Liu","Xiaofeng Deng","Han Li","Azar Kazemi","Christian Grashei","Gesa Wilkens","Xin You","Tanja Groll","Nassir Navab","Carolin Mogler","Peter J. Schüffler"],"pdf_url":"https://arxiv.org/pdf/2508.02528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02521v1","updated":"2025-08-04T15:31:13Z","published":"2025-08-04T15:31:13Z","title":"Towards Reliable Audio Deepfake Attribution and Model Recognition: A\n  Multi-Level Autoencoder-Based Framework","summary":"  The proliferation of audio deepfakes poses a growing threat to trust in\ndigital communications. While detection methods have advanced, attributing\naudio deepfakes to their source models remains an underexplored yet crucial\nchallenge. In this paper we introduce LAVA (Layered Architecture for Voice\nAttribution), a hierarchical framework for audio deepfake detection and model\nrecognition that leverages attention-enhanced latent representations extracted\nby a convolutional autoencoder trained solely on fake audio. Two specialized\nclassifiers operate on these features: Audio Deepfake Attribution (ADA), which\nidentifies the generation technology, and Audio Deepfake Model Recognition\n(ADMR), which recognize the specific generative model instance. To improve\nrobustness under open-set conditions, we incorporate confidence-based rejection\nthresholds. Experiments on ASVspoof2021, FakeOrReal, and CodecFake show strong\nperformance: the ADA classifier achieves F1-scores over 95% across all\ndatasets, and the ADMR module reaches 96.31% macro F1 across six classes.\nAdditional tests on unseen attacks from ASVpoof2019 LA and error propagation\nanalysis confirm LAVA's robustness and reliability. The framework advances the\nfield by introducing a supervised approach to deepfake attribution and model\nrecognition under open-set conditions, validated on public benchmarks and\naccompanied by publicly released models and code. Models and code are available\nat https://www.github.com/adipiz99/lava-framework.\n","authors":["Andrea Di Pierno","Luca Guarnera","Dario Allegra","Sebastiano Battiato"],"pdf_url":"https://arxiv.org/pdf/2508.02521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00443v2","updated":"2025-08-04T15:30:18Z","published":"2025-08-01T09:00:48Z","title":"SDMatte: Grafting Diffusion Models for Interactive Matting","summary":"  Recent interactive matting methods have shown satisfactory performance in\ncapturing the primary regions of objects, but they fall short in extracting\nfine-grained details in edge regions. Diffusion models trained on billions of\nimage-text pairs, demonstrate exceptional capability in modeling highly complex\ndata distributions and synthesizing realistic texture details, while exhibiting\nrobust text-driven interaction capabilities, making them an attractive solution\nfor interactive matting. To this end, we propose SDMatte, a diffusion-driven\ninteractive matting model, with three key contributions. First, we exploit the\npowerful priors of diffusion models and transform the text-driven interaction\ncapability into visual prompt-driven interaction capability to enable\ninteractive matting. Second, we integrate coordinate embeddings of visual\nprompts and opacity embeddings of target objects into U-Net, enhancing\nSDMatte's sensitivity to spatial position information and opacity information.\nThird, we propose a masked self-attention mechanism that enables the model to\nfocus on areas specified by visual prompts, leading to better performance.\nExtensive experiments on multiple datasets demonstrate the superior performance\nof our method, validating its effectiveness in interactive matting. Our code\nand model are available at https://github.com/vivoCameraResearch/SDMatte.\n","authors":["Longfei Huang","Yu Liang","Hao Zhang","Jinwei Chen","Wei Dong","Lunde Chen","Wanyu Liu","Bo Li","Peng-Tao Jiang"],"pdf_url":"https://arxiv.org/pdf/2508.00443v2.pdf","comment":"Accepted at ICCV 2025, 11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2508.02516v1","updated":"2025-08-04T15:21:29Z","published":"2025-08-04T15:21:29Z","title":"Engagement Prediction of Short Videos with Large Multimodal Models","summary":"  The rapid proliferation of user-generated content (UGC) on short-form video\nplatforms has made video engagement prediction increasingly important for\noptimizing recommendation systems and guiding content creation. However, this\ntask remains challenging due to the complex interplay of factors such as\nsemantic content, visual quality, audio characteristics, and user background.\nPrior studies have leveraged various types of features from different\nmodalities, such as visual quality, semantic content, background sound, etc.,\nbut often struggle to effectively model their cross-feature and cross-modality\ninteractions. In this work, we empirically investigate the potential of large\nmultimodal models (LMMs) for video engagement prediction. We adopt two\nrepresentative LMMs: VideoLLaMA2, which integrates audio, visual, and language\nmodalities, and Qwen2.5-VL, which models only visual and language modalities.\nSpecifically, VideoLLaMA2 jointly processes key video frames, text-based\nmetadata, and background sound, while Qwen2.5-VL utilizes only key video frames\nand text-based metadata. Trained on the SnapUGC dataset, both models\ndemonstrate competitive performance against state-of-the-art baselines,\nshowcasing the effectiveness of LMMs in engagement prediction. Notably,\nVideoLLaMA2 consistently outperforms Qwen2.5-VL, highlighting the importance of\naudio features in engagement prediction. By ensembling two types of models, our\nmethod achieves first place in the ICCV VQualA 2025 EVQA-SnapUGC Challenge on\nshort-form video engagement prediction. The code is available at\nhttps://github.com/sunwei925/LMM-EVQA.git.\n","authors":["Wei Sun","Linhan Cao","Yuqin Cao","Weixia Zhang","Wen Wen","Kaiwei Zhang","Zijian Chen","Fangfang Lu","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2508.02516v1.pdf","comment":"The proposed method achieves first place in the ICCV VQualA 2025\n  EVQA-SnapUGC Challenge on short-form video engagement prediction"},{"id":"http://arxiv.org/abs/2508.02512v1","updated":"2025-08-04T15:18:01Z","published":"2025-08-04T15:18:01Z","title":"QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots","summary":"  Panoramic cameras, capturing comprehensive 360-degree environmental data, are\nsuitable for quadruped robots in surrounding perception and interaction with\ncomplex environments. However, the scarcity of high-quality panoramic training\ndata-caused by inherent kinematic constraints and complex sensor calibration\nchallenges-fundamentally limits the development of robust perception systems\ntailored to these embodied platforms. To address this issue, we propose\nQuaDreamer-the first panoramic data generation engine specifically designed for\nquadruped robots. QuaDreamer focuses on mimicking the motion paradigm of\nquadruped robots to generate highly controllable, realistic panoramic videos,\nproviding a data source for downstream tasks. Specifically, to effectively\ncapture the unique vertical vibration characteristics exhibited during\nquadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts\ncontrollable vertical signals through frequency-domain feature filtering and\nprovides high-quality prompts. To facilitate high-quality panoramic video\ngeneration under jitter signal control, we propose a Scene-Object Controller\n(SOC) that effectively manages object motion and boosts background jitter\ncontrol through the attention mechanism. To address panoramic distortions in\nwide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream\narchitecture that synergizes frequency-texture refinement for local detail\nenhancement with spatial-structure correction for global geometric consistency.\nWe further demonstrate that the generated video sequences can serve as training\ndata for the quadruped robot's panoramic visual perception model, enhancing the\nperformance of multi-object tracking in 360-degree scenes. The source code and\nmodel weights will be publicly available at\nhttps://github.com/losehu/QuaDreamer.\n","authors":["Sheng Wu","Fei Teng","Hao Shi","Qi Jiang","Kai Luo","Kaiwei Wang","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2508.02512v1.pdf","comment":"Accepted to CoRL 2025. The source code and model weights will be\n  publicly available at https://github.com/losehu/QuaDreamer"},{"id":"http://arxiv.org/abs/2503.12172v3","updated":"2025-08-04T15:15:16Z","published":"2025-03-15T15:29:05Z","title":"SEAL: Semantic Aware Image Watermarking","summary":"  Generative models have rapidly evolved to generate realistic outputs.\nHowever, their synthetic outputs increasingly challenge the clear distinction\nbetween natural and AI-generated content, necessitating robust watermarking\ntechniques. Watermarks are typically expected to preserve the integrity of the\ntarget image, withstand removal attempts, and prevent unauthorized replication\nonto unrelated images. To address this need, recent methods embed persistent\nwatermarks into images produced by diffusion models using the initial noise.\nYet, to do so, they either distort the distribution of generated images or rely\non searching through a long dictionary of used keys for detection.\n  In this paper, we propose a novel watermarking method that embeds semantic\ninformation about the generated image directly into the watermark, enabling a\ndistortion-free watermark that can be verified without requiring a database of\nkey patterns. Instead, the key pattern can be inferred from the semantic\nembedding of the image using locality-sensitive hashing. Furthermore,\nconditioning the watermark detection on the original image content improves\nrobustness against forgery attacks. To demonstrate that, we consider two\nlargely overlooked attack strategies: (i) an attacker extracting the initial\nnoise and generating a novel image with the same pattern; (ii) an attacker\ninserting an unrelated (potentially harmful) object into a watermarked image,\npossibly while preserving the watermark. We empirically validate our method's\nincreased robustness to these attacks. Taken together, our results suggest that\ncontent-aware watermarks can mitigate risks arising from image-generative\nmodels.\n","authors":["Kasra Arabi","R. Teal Witter","Chinmay Hegde","Niv Cohen"],"pdf_url":"https://arxiv.org/pdf/2503.12172v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02507v1","updated":"2025-08-04T15:14:47Z","published":"2025-08-04T15:14:47Z","title":"Rethinking Transparent Object Grasping: Depth Completion with Monocular\n  Depth Estimation and Instance Mask","summary":"  Due to the optical properties, transparent objects often lead depth cameras\nto generate incomplete or invalid depth data, which in turn reduces the\naccuracy and reliability of robotic grasping. Existing approaches typically\ninput the RGB-D image directly into the network to output the complete depth,\nexpecting the model to implicitly infer the reliability of depth values.\nHowever, while effective in training datasets, such methods often fail to\ngeneralize to real-world scenarios, where complex light interactions lead to\nhighly variable distributions of valid and invalid depth data. To address this,\nwe propose ReMake, a novel depth completion framework guided by an instance\nmask and monocular depth estimation. By explicitly distinguishing transparent\nregions from non-transparent ones, the mask enables the model to concentrate on\nlearning accurate depth estimation in these areas from RGB-D input during\ntraining. This targeted supervision reduces reliance on implicit reasoning and\nimproves generalization to real-world scenarios. Additionally, monocular depth\nestimation provides depth context between the transparent object and its\nsurroundings, enhancing depth prediction accuracy. Extensive experiments show\nthat our method outperforms existing approaches on both benchmark datasets and\nreal-world scenarios, demonstrating superior accuracy and generalization\ncapability. Code and videos are available at\nhttps://chengyaofeng.github.io/ReMake.github.io/.\n","authors":["Yaofeng Cheng","Xinkai Gao","Sen Zhang","Chao Zeng","Fusheng Zha","Lining Sun","Chenguang Yang"],"pdf_url":"https://arxiv.org/pdf/2508.02507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18545v2","updated":"2025-08-04T15:06:39Z","published":"2025-01-30T18:13:29Z","title":"UDC-VIT: A Real-World Video Dataset for Under-Display Cameras","summary":"  Even though an Under-Display Camera (UDC) is an advanced imaging system, the\ndisplay panel significantly degrades captured images or videos, introducing low\ntransmittance, blur, noise, and flare issues. Tackling such issues is\nchallenging because of the complex degradation of UDCs, including diverse flare\npatterns. However, no dataset contains videos of real-world UDC degradation. In\nthis paper, we propose a real-world UDC video dataset called UDC-VIT. Unlike\nexisting datasets, UDC-VIT exclusively includes human motions for facial\nrecognition. We propose a video-capturing system to acquire clean and\nUDC-degraded videos of the same scene simultaneously. Then, we align a pair of\ncaptured videos frame by frame, using discrete Fourier transform (DFT). We\ncompare UDC-VIT with six representative UDC still image datasets and two\nexisting UDC video datasets. Using six deep-learning models, we compare UDC-VIT\nand an existing synthetic UDC video dataset. The results indicate the\nineffectiveness of models trained on earlier synthetic UDC video datasets, as\nthey do not reflect the actual characteristics of UDC-degraded videos. We also\ndemonstrate the importance of effective UDC restoration by evaluating face\nrecognition accuracy concerning PSNR, SSIM, and LPIPS scores. UDC-VIT is\navailable at our official GitHub repository.\n","authors":["Kyusu Ahn","JiSoo Kim","Sangik Lee","HyunGyu Lee","Byeonghyun Ko","Chanwoo Park","Jaejin Lee"],"pdf_url":"https://arxiv.org/pdf/2501.18545v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02495v1","updated":"2025-08-04T15:05:27Z","published":"2025-08-04T15:05:27Z","title":"Clinical Expert Uncertainty Guided Generalized Label Smoothing for\n  Medical Noisy Label Learning","summary":"  Many previous studies have proposed extracting image labels from clinical\nnotes to create large-scale medical image datasets at a low cost. However,\nthese approaches inherently suffer from label noise due to uncertainty from the\nclinical experts. When radiologists and physicians analyze medical images to\nmake diagnoses, they often include uncertainty-aware notes such as ``maybe'' or\n``not excluded''. Unfortunately, current text-mining methods overlook these\nnuances, resulting in the creation of noisy labels. Existing methods for\nhandling noisy labels in medical image analysis, which typically address the\nproblem through post-processing techniques, have largely ignored the important\nissue of expert-driven uncertainty contributing to label noise. To better\nincorporate the expert-written uncertainty in clinical notes into medical image\nanalysis and address the label noise issue, we first examine the impact of\nclinical expert uncertainty on label noise. We then propose a clinical expert\nuncertainty-aware benchmark, along with a label smoothing method, which\nsignificantly improves performance compared to current state-of-the-art\napproaches.\n","authors":["Kunyu Zhang","Lin Gu","Liangchen Liu","Yingke Chen","Bingyang Wang","Jin Yan","Yingying Zhu"],"pdf_url":"https://arxiv.org/pdf/2508.02495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02493v1","updated":"2025-08-04T15:03:56Z","published":"2025-08-04T15:03:56Z","title":"Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian\n  Splatting","summary":"  3D Gaussian Splatting (3DGS) is a powerful and computationally efficient\nrepresentation for 3D reconstruction. Despite its strengths, 3DGS often\nproduces floating artifacts, which are erroneous structures detached from the\nactual geometry and significantly degrade visual fidelity. The underlying\nmechanisms causing these artifacts, particularly in low-quality initialization\nscenarios, have not been fully explored. In this paper, we investigate the\norigins of floating artifacts from a frequency-domain perspective and identify\nunder-optimized Gaussians as the primary source. Based on our analysis, we\npropose \\textit{Eliminating-Floating-Artifacts} Gaussian Splatting (EFA-GS),\nwhich selectively expands under-optimized Gaussians to prioritize accurate\nlow-frequency learning. Additionally, we introduce complementary depth-based\nand scale-based strategies to dynamically refine Gaussian expansion,\neffectively mitigating detail erosion. Extensive experiments on both synthetic\nand real-world datasets demonstrate that EFA-GS substantially reduces floating\nartifacts while preserving high-frequency details, achieving an improvement of\n1.68 dB in PSNR over baseline method on our RWLQ dataset. Furthermore, we\nvalidate the effectiveness of our approach in downstream 3D editing tasks. Our\nimplementation will be released on GitHub.\n","authors":["Jianchao Wang","Peng Zhou","Cen Li","Rong Quan","Jie Qin"],"pdf_url":"https://arxiv.org/pdf/2508.02493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15969v3","updated":"2025-08-04T15:00:28Z","published":"2025-03-20T09:13:31Z","title":"Beyond the Visible: Multispectral Vision-Language Learning for Earth\n  Observation","summary":"  Vision-language models for Earth observation (EO) typically rely on the\nvisual spectrum of data as the only model input, thus failing to leverage the\nrich spectral information available in the multispectral channels recorded by\nsatellites. Therefore, we introduce Llama3-MS-CLIP, the first vision-language\nmodel pre-trained with contrastive learning on a large-scale multispectral\ndataset and report on the performance gains due to the extended spectral range.\nFurthermore, we present the largest-to-date image-caption dataset for\nmultispectral data, consisting of one million Sentinel-2 samples and\ncorresponding textual descriptions generated using Llama3-LLaVA-Next and\nOverture Maps data. We develop a scalable captioning pipeline, which is\nvalidated by domain experts. We evaluate Llama3-MS-CLIP on multispectral\nzero-shot image classification and retrieval using three datasets of varying\ncomplexity. Our results demonstrate that Llama3-MS-CLIP significantly\noutperforms other RGB-based approaches, improving classification accuracy by\n+6.77% on average and retrieval performance by +4.63% mAP compared to the\nsecond-best model. Our results emphasize the relevance of multispectral\nvision-language learning. The image-caption dataset, code, and model weights\nare available at https://github.com/IBM/MS-CLIP.\n","authors":["Clive Tinashe Marimo","Benedikt Blumenstiel","Maximilian Nitsche","Johannes Jakubik","Thomas Brunschwiler"],"pdf_url":"https://arxiv.org/pdf/2503.15969v3.pdf","comment":"Machine Learning and Knowledge Discovery in Databases. Research Track\n  - European Conference, ECML PKDD 2025"},{"id":"http://arxiv.org/abs/2503.12049v2","updated":"2025-08-04T14:59:28Z","published":"2025-03-15T08:47:45Z","title":"TACO: Taming Diffusion for in-the-wild Video Amodal Completion","summary":"  Humans can infer complete shapes and appearances of objects from limited\nvisual cues, relying on extensive prior knowledge of the physical world.\nHowever, completing partially observable objects while ensuring consistency\nacross video frames remains challenging for existing models, especially for\nunstructured, in-the-wild videos. This paper tackles the task of Video Amodal\nCompletion (VAC), which aims to generate the complete object consistently\nthroughout the video given a visual prompt specifying the object of interest.\nLeveraging the rich, consistent manifolds learned by pre-trained video\ndiffusion models, we propose a conditional diffusion model, TACO, that\nrepurposes these manifolds for VAC. To enable its effective and robust\ngeneralization to challenging in-the-wild scenarios, we curate a large-scale\nsynthetic dataset with multiple difficulty levels by systematically imposing\nocclusions onto un-occluded videos. Building on this, we devise a progressive\nfine-tuning paradigm that starts with simpler recovery tasks and gradually\nadvances to more complex ones. We demonstrate TACO's versatility on a wide\nrange of in-the-wild videos from Internet, as well as on diverse, unseen\ndatasets commonly used in autonomous driving, robotic manipulation, and scene\nunderstanding. Moreover, we show that TACO can be effectively applied to\nvarious downstream tasks like object reconstruction and pose estimation,\nhighlighting its potential to facilitate physical world understanding and\nreasoning. Our project page is available at https://jason-aplp.github.io/TACO.\n","authors":["Ruijie Lu","Yixin Chen","Yu Liu","Jiaxiang Tang","Junfeng Ni","Diwen Wan","Gang Zeng","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2503.12049v2.pdf","comment":"Accepted by ICCV 2025.Project page: https://jason-aplp.github.io/TACO"},{"id":"http://arxiv.org/abs/2505.24739v3","updated":"2025-08-04T14:50:47Z","published":"2025-05-30T15:58:14Z","title":"Contrast-Invariant Self-supervised Segmentation for Quantitative\n  Placental MRI","summary":"  Accurate placental segmentation is essential for quantitative analysis of the\nplacenta. However, this task is particularly challenging in T2*-weighted\nplacental imaging due to: (1) weak and inconsistent boundary contrast across\nindividual echoes; (2) the absence of manual ground truth annotations for all\necho times; and (3) motion artifacts across echoes caused by fetal and maternal\nmovement. In this work, we propose a contrast-augmented segmentation framework\nthat leverages complementary information across multi-echo T2*-weighted MRI to\nlearn robust, contrast-invariant representations. Our method integrates: (i)\nmasked autoencoding (MAE) for self-supervised pretraining on unlabeled\nmulti-echo slices; (ii) masked pseudo-labeling (MPL) for unsupervised domain\nadaptation across echo times; and (iii) global-local collaboration to align\nfine-grained features with global anatomical context. We further introduce a\nsemantic matching loss to encourage representation consistency across echoes of\nthe same subject. Experiments on a clinical multi-echo placental MRI dataset\ndemonstrate that our approach generalizes effectively across echo times and\noutperforms both single-echo and naive fusion baselines. To our knowledge, this\nis the first work to systematically exploit multi-echo T2*-weighted MRI for\nplacental segmentation.\n","authors":["Xinliu Zhong","Ruiying Liu","Emily S. Nichols","Xuzhe Zhang","Andrew F. Laine","Emma G. Duerden","Yun Wang"],"pdf_url":"https://arxiv.org/pdf/2505.24739v3.pdf","comment":"12 pages, 20 figures"},{"id":"http://arxiv.org/abs/2505.22011v2","updated":"2025-08-04T14:48:23Z","published":"2025-05-28T06:19:37Z","title":"Prototype Embedding Optimization for Human-Object Interaction Detection\n  in Livestreaming","summary":"  Livestreaming often involves interactions between streamers and objects,\nwhich is critical for understanding and regulating web content. While\nhuman-object interaction (HOI) detection has made some progress in\ngeneral-purpose video downstream tasks, when applied to recognize the\ninteraction behaviors between a streamer and different objects in\nlivestreaming, it tends to focuses too much on the objects and neglects their\ninteractions with the streamer, which leads to object bias. To solve this\nissue, we propose a prototype embedding optimization for human-object\ninteraction detection (PeO-HOI). First, the livestreaming is preprocessed using\nobject detection and tracking techniques to extract features of the\nhuman-object (HO) pairs. Then, prototype embedding optimization is adopted to\nmitigate the effect of object bias on HOI. Finally, after modelling the\nspatio-temporal context between HO pairs, the HOI detection results are\nobtained by the prediction head. The experimental results show that the\ndetection accuracy of the proposed PeO-HOI method has detection accuracies of\n37.19%@full, 51.42%@non-rare, 26.20%@rare on the publicly available dataset\nVidHOI, 45.13%@full, 62.78%@non-rare and 30.37%@rare on the self-built dataset\nBJUT-HOI, which effectively improves the HOI detection performance in\nlivestreaming.\n","authors":["Menghui Zhang","Jing Zhang","Lin Chen","Li Zhuo"],"pdf_url":"https://arxiv.org/pdf/2505.22011v2.pdf","comment":"Accepted by IEEE MMSP 2025"},{"id":"http://arxiv.org/abs/2508.02480v1","updated":"2025-08-04T14:47:17Z","published":"2025-08-04T14:47:17Z","title":"MindShot: Multi-Shot Video Reconstruction from fMRI with LLM Decoding","summary":"  Reconstructing dynamic videos from fMRI is important for understanding visual\ncognition and enabling vivid brain-computer interfaces. However, current\nmethods are critically limited to single-shot clips, failing to address the\nmulti-shot nature of real-world experiences. Multi-shot reconstruction faces\nfundamental challenges: fMRI signal mixing across shots, the temporal\nresolution mismatch between fMRI and video obscuring rapid scene changes, and\nthe lack of dedicated multi-shot fMRI-video datasets. To overcome these\nlimitations, we propose a novel divide-and-decode framework for multi-shot fMRI\nvideo reconstruction. Our core innovations are: (1) A shot boundary predictor\nmodule explicitly decomposing mixed fMRI signals into shot-specific segments.\n(2) Generative keyframe captioning using LLMs, which decodes robust textual\ndescriptions from each segment, overcoming temporal blur by leveraging\nhigh-level semantics. (3) Novel large-scale data synthesis (20k samples) from\nexisting datasets. Experimental results demonstrate our framework outperforms\nstate-of-the-art methods in multi-shot reconstruction fidelity. Ablation\nstudies confirm the critical role of fMRI decomposition and semantic\ncaptioning, with decomposition significantly improving decoded caption CLIP\nsimilarity by 71.8%. This work establishes a new paradigm for multi-shot fMRI\nreconstruction, enabling accurate recovery of complex visual narratives through\nexplicit decomposition and semantic prompting.\n","authors":["Wenwen Zeng","Yonghuang Wu","Yifan Chen","Xuan Xie","Chengqian Zhao","Feiyu Yin","Guoqing Wu","Jinhua Yu"],"pdf_url":"https://arxiv.org/pdf/2508.02480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02479v1","updated":"2025-08-04T14:46:59Z","published":"2025-08-04T14:46:59Z","title":"Fine-grained Multiple Supervisory Network for Multi-modal Manipulation\n  Detecting and Grounding","summary":"  The task of Detecting and Grounding Multi-Modal Media Manipulation (DGM$^4$)\nis a branch of misinformation detection. Unlike traditional binary\nclassification, it includes complex subtasks such as forgery content\nlocalization and forgery method classification. Consider that existing methods\nare often limited in performance due to neglecting the erroneous interference\ncaused by unreliable unimodal data and failing to establish comprehensive\nforgery supervision for mining fine-grained tampering traces. In this paper, we\npresent a Fine-grained Multiple Supervisory (FMS) network, which incorporates\nmodality reliability supervision, unimodal internal supervision and cross-modal\nsupervision to provide comprehensive guidance for DGM$^4$ detection. For\nmodality reliability supervision, we propose the Multimodal Decision Supervised\nCorrection (MDSC) module. It leverages unimodal weak supervision to correct the\nmulti-modal decision-making process. For unimodal internal supervision, we\npropose the Unimodal Forgery Mining Reinforcement (UFMR) module. It amplifies\nthe disparity between real and fake information within unimodal modality from\nboth feature-level and sample-level perspectives. For cross-modal supervision,\nwe propose the Multimodal Forgery Alignment Reasoning (MFAR) module. It\nutilizes soft-attention interactions to achieve cross-modal feature perception\nfrom both consistency and inconsistency perspectives, where we also design the\ninteraction constraints to ensure the interaction quality. Extensive\nexperiments demonstrate the superior performance of our FMS compared to\nstate-of-the-art methods.\n","authors":["Xinquan Yu","Wei Lu","Xiangyang Luo"],"pdf_url":"https://arxiv.org/pdf/2508.02479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02477v1","updated":"2025-08-04T14:44:40Z","published":"2025-08-04T14:44:40Z","title":"Multi-class Image Anomaly Detection for Practical Applications:\n  Requirements and Robust Solutions","summary":"  Recent advances in image anomaly detection have extended unsupervised\nlearning-based models from single-class settings to multi-class frameworks,\naiming to improve efficiency in training time and model storage. When a single\nmodel is trained to handle multiple classes, it often underperforms compared to\nclass-specific models in terms of per-class detection accuracy. Accordingly,\nprevious studies have primarily focused on narrowing this performance gap.\nHowever, the way class information is used, or not used, remains a relatively\nunderstudied factor that could influence how detection thresholds are defined\nin multi-class image anomaly detection. These thresholds, whether\nclass-specific or class-agnostic, significantly affect detection outcomes. In\nthis study, we identify and formalize the requirements that a multi-class image\nanomaly detection model must satisfy under different conditions, depending on\nwhether class labels are available during training and evaluation. We then\nre-examine existing methods under these criteria. To meet these challenges, we\npropose Hierarchical Coreset (HierCore), a novel framework designed to satisfy\nall defined requirements. HierCore operates effectively even without class\nlabels, leveraging a hierarchical memory bank to estimate class-wise decision\ncriteria for anomaly detection. We empirically validate the applicability and\nrobustness of existing methods and HierCore under four distinct scenarios,\ndetermined by the presence or absence of class labels in the training and\nevaluation phases. The experimental results demonstrate that HierCore\nconsistently meets all requirements and maintains strong, stable performance\nacross all settings, highlighting its practical potential for real-world\nmulti-class anomaly detection tasks.\n","authors":["Jaehyuk Heo","Pilsung Kang"],"pdf_url":"https://arxiv.org/pdf/2508.02477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.18463v2","updated":"2025-08-04T14:42:54Z","published":"2025-06-23T10:01:14Z","title":"DIP: Unsupervised Dense In-Context Post-training of Visual\n  Representations","summary":"  We introduce DIP, a novel unsupervised post-training method designed to\nenhance dense image representations in large-scale pretrained vision encoders\nfor in-context scene understanding. Unlike prior approaches that rely on\ncomplex self-distillation architectures, our method trains the vision encoder\nusing pseudo-tasks that explicitly simulate downstream in-context scenarios,\ninspired by meta-learning principles. To enable post-training on unlabeled\ndata, we propose an automatic mechanism for generating in-context tasks that\ncombines a pretrained diffusion model and the vision encoder itself. DIP is\nsimple, unsupervised, and computationally efficient, requiring less than 9\nhours on a single A100 GPU. By learning dense representations through pseudo\nin-context tasks, it achieves strong performance across a wide variety of\ndownstream real-world in-context scene understanding tasks. It outperforms both\nthe initial vision encoder and prior methods, offering a practical and\neffective solution for improving dense representations. Code available here:\nhttps://github.com/sirkosophia/DIP\n","authors":["Sophia Sirko-Galouchenko","Spyros Gidaris","Antonin Vobecky","Andrei Bursuc","Nicolas Thome"],"pdf_url":"https://arxiv.org/pdf/2506.18463v2.pdf","comment":"Accepted to ICCV 2025"},{"id":"http://arxiv.org/abs/2505.03510v2","updated":"2025-08-04T14:37:42Z","published":"2025-05-06T13:20:04Z","title":"From Neurons to Computation: Biological Reservoir Computing for Pattern\n  Recognition","summary":"  In this paper, we introduce a paradigm for reservoir computing (RC) that\nleverages a pool of cultured biological neurons as the reservoir substrate,\ncreating a biological reservoir computing (BRC). This system operates similarly\nto an echo state network (ESN), with the key distinction that the neural\nactivity is generated by a network of cultured neurons, rather than being\nmodeled by traditional artificial computational units. The neuronal activity is\nrecorded using a multi-electrode array (MEA), which enables high-throughput\nrecording of neural signals. In our approach, inputs are introduced into the\nnetwork through a subset of the MEA electrodes, while the remaining electrodes\ncapture the resulting neural activity. This generates a nonlinear mapping of\nthe input data to a high-dimensional biological feature space, where\ndistinguishing between data becomes more efficient and straightforward,\nallowing a simple linear classifier to perform pattern recognition tasks\neffectively. To evaluate the performance of our proposed system, we present an\nexperimental study that includes various input patterns, such as positional\ncodes, bars with different orientations, and a digit recognition task. The\nresults demonstrate the feasibility of using biological neural networks to\nperform tasks traditionally handled by artificial neural networks, paving the\nway for further exploration of biologically-inspired computing systems, with\npotential applications in neuromorphic engineering and bio-hybrid computing.\n","authors":["Ludovico Iannello","Luca Ciampi","Gabriele Lagani","Fabrizio Tonelli","Eleonora Crocco","Lucio Maria Calcagnile","Angelo Di Garbo","Federico Cremisi","Giuseppe Amato"],"pdf_url":"https://arxiv.org/pdf/2505.03510v2.pdf","comment":"Accepted at ICONIP 2025 Conference"},{"id":"http://arxiv.org/abs/2409.00029v2","updated":"2025-08-04T14:34:44Z","published":"2024-08-17T12:46:53Z","title":"Attack Anything: Blind DNNs via Universal Background Adversarial Attack","summary":"  It has been widely substantiated that deep neural networks (DNNs) are\nsusceptible and vulnerable to adversarial perturbations. Existing studies\nmainly focus on performing attacks by corrupting targeted objects (physical\nattack) or images (digital attack), which is intuitively acceptable and\nunderstandable in terms of the attack's effectiveness. In contrast, our focus\nlies in conducting background adversarial attacks in both digital and physical\ndomains, without causing any disruptions to the targeted objects themselves.\nSpecifically, an effective background adversarial attack framework is proposed\nto attack anything, by which the attack efficacy generalizes well between\ndiverse objects, models, and tasks. Technically, we approach the background\nadversarial attack as an iterative optimization problem, analogous to the\nprocess of DNN learning. Besides, we offer a theoretical demonstration of its\nconvergence under a set of mild but sufficient conditions. To strengthen the\nattack efficacy and transferability, we propose a new ensemble strategy\ntailored for adversarial perturbations and introduce an improved smooth\nconstraint for the seamless connection of integrated perturbations. We conduct\ncomprehensive and rigorous experiments in both digital and physical domains\nacross various objects, models, and tasks, demonstrating the effectiveness of\nattacking anything of the proposed method. The findings of this research\nsubstantiate the significant discrepancy between human and machine vision on\nthe value of background variations, which play a far more critical role than\npreviously recognized, necessitating a reevaluation of the robustness and\nreliability of DNNs. The code will be publicly available at\nhttps://github.com/JiaweiLian/Attack_Anything\n","authors":["Jiawei Lian","Shaohui Mei","Xiaofei Wang","Yi Wang","Lefan Wang","Yingjie Lu","Mingyang Ma","Lap-Pui Chau"],"pdf_url":"https://arxiv.org/pdf/2409.00029v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02464v1","updated":"2025-08-04T14:31:11Z","published":"2025-08-04T14:31:11Z","title":"SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with\n  Vision Foundation Models","summary":"  Foundation models like Segment Anything Model (SAM) excel in promptable\nsegmentation but suffer from an intent gap: they segment only explicitly\nprompted objects, failing to generalize to semantically related instances\nimplicitly desired by users. This limitation is critical in domains with dense\nhomogeneous objects (e.g., biomedical nuclei segmentation), where sparse visual\nprompts typically yield incomplete results, rendering dense annotations\nimpractical due to prohibitive cost. To bridge this gap, we introduce SAMPO\n(Segment Anything Model with Preference Optimization), a novel framework that\nteaches visual foundation models to infer high-level categorical intent from\nsparse visual interactions. Unlike conventional pixel-level fine-tuning, SAMPO\noptimizes models to implicitly capture target-class characteristics through\npreference optimization. This approach, which operates without dependency on\nlanguage models, enables robust multi-object segmentation even under sparse\nprompting and demonstrates superior data efficiency during fine-tuning.\nValidated on three medical segmentation tasks, SAMPO achieves state-of-the-art\nperformance: on challenging tasks like PanNuke-T2, our method, when fine-tuned\nwith only 10% of the training data, significantly outperforms all existing\nmethods trained on the full 100% dataset, achieving an improvement of over 9\npercentage points compared to the best baseline. Our work establishes a new\nparadigm for intent-aware alignment in visual foundation models, removing\ndependencies on auxiliary prompt generators or language-model-assisted\npreference learning.\n","authors":["Yonghuang Wu","Wenwen Zeng","Xuan Xie","Chengqian Zhao","Guoqing Wu","Jinhua Yu"],"pdf_url":"https://arxiv.org/pdf/2508.02464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02662v3","updated":"2025-08-04T14:30:43Z","published":"2025-03-04T14:25:51Z","title":"10K is Enough: An Ultra-Lightweight Binarized Network for Infrared\n  Small-Target Detection","summary":"  The widespread deployment of Infrared Small-Target Detection (IRSTD)\nalgorithms on edge devices necessitates the exploration of model compression\ntechniques. Binarized neural networks (BNNs) are distinguished by their\nexceptional efficiency in model compression. However, the small size of\ninfrared targets introduces stringent precision requirements for the IRSTD\ntask, while the inherent precision loss during binarization presents a\nsignificant challenge. To address this, we propose the Binarized Infrared\nSmall-Target Detection Network (BiisNet), which preserves the core operations\nof binarized convolutions while integrating full-precision features into the\nnetwork's information flow. Specifically, we propose the Dot Binary\nConvolution, which retains fine-grained semantic information in feature maps\nwhile still leveraging the binarized convolution operations. In addition, we\nintroduce a smooth and adaptive Dynamic Softsign function, which provides more\ncomprehensive and progressively finer gradient during backpropagation,\nenhancing model stability and promoting an optimal weight distribution.\nExperimental results demonstrate that BiisNet not only significantly\noutperforms other binary architectures but also has strong competitiveness\namong state-of-the-art full-precision models.\n","authors":["Biqiao Xin","Qianchen Mao","Bingshu Wang","Jiangbin Zheng","Yong Zhao","C. L. Philip Chen"],"pdf_url":"https://arxiv.org/pdf/2503.02662v3.pdf","comment":"We found the paper has insufficient workload after review. No\n  substitute manuscript can be ready soon. To ensure academic quality, we\n  withdraw it and plan to resubmit when improved"},{"id":"http://arxiv.org/abs/2508.02460v1","updated":"2025-08-04T14:27:01Z","published":"2025-08-04T14:27:01Z","title":"InfoSyncNet: Information Synchronization Temporal Convolutional Network\n  for Visual Speech Recognition","summary":"  Estimating spoken content from silent videos is crucial for applications in\nAssistive Technology (AT) and Augmented Reality (AR). However, accurately\nmapping lip movement sequences in videos to words poses significant challenges\ndue to variability across sequences and the uneven distribution of information\nwithin each sequence. To tackle this, we introduce InfoSyncNet, a non-uniform\nsequence modeling network enhanced by tailored data augmentation techniques.\nCentral to InfoSyncNet is a non-uniform quantization module positioned between\nthe encoder and decoder, enabling dynamic adjustment to the network's focus and\neffectively handling the natural inconsistencies in visual speech data.\nAdditionally, multiple training strategies are incorporated to enhance the\nmodel's capability to handle variations in lighting and the speaker's\norientation. Comprehensive experiments on the LRW and LRW1000 datasets confirm\nthe superiority of InfoSyncNet, achieving new state-of-the-art accuracies of\n92.0% and 60.7% Top-1 ACC. The code is available for download (see comments).\n","authors":["Junxiao Xue","Xiaozhen Liu","Xuecheng Wu","Fei Yu","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2508.02460v1.pdf","comment":"https://github.com/liuxiaozhen123/InfoSyncNet"},{"id":"http://arxiv.org/abs/2410.14334v3","updated":"2025-08-04T14:10:44Z","published":"2024-10-18T09:44:35Z","title":"Evaluating the evaluators: Towards human-aligned metrics for missing\n  markers reconstruction","summary":"  Animation data is often obtained through optical motion capture systems,\nwhich utilize a multitude of cameras to establish the position of optical\nmarkers. However, system errors or occlusions can result in missing markers,\nthe manual cleaning of which can be time-consuming. This has sparked interest\nin machine learning-based solutions for missing marker reconstruction in the\nacademic community. Most academic papers utilize a simplistic mean square error\nas the main metric. In this paper, we show that this metric does not correlate\nwith subjective perception of the fill quality. Additionally, we introduce and\nevaluate a set of better-correlated metrics that can drive progress in the\nfield.\n","authors":["Taras Kucherenko","Derek Peristy","Judith Bütepage"],"pdf_url":"https://arxiv.org/pdf/2410.14334v3.pdf","comment":"Accepted at the ACM International Conference on Multimedia 2025 (ACM\n  MM'25)"},{"id":"http://arxiv.org/abs/2411.14017v2","updated":"2025-08-04T14:05:44Z","published":"2024-11-21T11:01:03Z","title":"Automatic brain tumor segmentation in 2D intra-operative ultrasound\n  images using magnetic resonance imaging tumor annotations","summary":"  Automatic segmentation of brain tumors in intra-operative ultrasound (iUS)\nimages could facilitate localization of tumor tissue during resection surgery.\nThe lack of large annotated datasets limits the current models performances. In\nthis paper, we investigated the use of tumor annotations in magnetic resonance\nimaging (MRI) scans, which are more accessible than annotations in iUS images,\nfor training of deep learning models for iUS brain tumor segmentation. We used\n180 annotated MRI scans with corresponding unannotated iUS images, and 29\nannotated iUS images. Image registration was performed to transfer the MRI\nannotations to the corresponding iUS images before training the nnU-Net model\nwith different configurations of the data and label origins. The results showed\nno significant difference in Dice score for a model trained with only MRI\nannotated tumors compared to models trained with only iUS annotations and both,\nand to expert annotations, indicating that MRI tumor annotations can be used as\na substitute for iUS tumor annotations to train a deep learning model for\nautomatic brain tumor segmentation in iUS images. The best model obtained an\naverage Dice score of $0.62\\pm0.31$, compared to $0.67\\pm0.25$ for an expert\nneurosurgeon, where the performance on larger tumors were similar, but lower\nfor the models on smaller tumors. In addition, the results showed that removing\nsmaller tumors from the training sets improved the results. The main models are\navailable here:\nhttps://github.com/mathildefaanes/us_brain_tumor_segmentation/tree/main\n","authors":["Mathilde Faanes","Ragnhild Holden Helland","Ole Solheim","Sébastien Muller","Ingerid Reinertsen"],"pdf_url":"https://arxiv.org/pdf/2411.14017v2.pdf","comment":"14, 5figures. This work has been submitted to the IEEE for possible\n  publication"},{"id":"http://arxiv.org/abs/2504.12970v2","updated":"2025-08-04T14:05:05Z","published":"2025-04-17T14:22:27Z","title":"MathPhys-Guided Coarse-to-Fine Anomaly Synthesis with SQE-Driven\n  Bi-Level Optimization for Anomaly Detection","summary":"  Currently, industrial anomaly detection suffers from two bottlenecks: (i) the\nrarity of real-world defect images and (ii) the opacity of sample quality when\nsynthetic data are used. Existing synthetic strategies (e.g., cut-and-paste)\noverlook the underlying physical causes of defects, leading to inconsistent,\nlow-fidelity anomalies that hamper model generalization to real-world\ncomplexities. In this paper, we introduce a novel and lightweight pipeline that\ngenerates synthetic anomalies through Math-Phys model guidance, refines them\nvia a Coarse-to-Fine approach and employs a bi-level optimization strategy with\na Synthesis Quality Estimator (SQE). By combining physical modeling of the\nthree most typical physics-driven defect mechanisms: Fracture Line (FL),\nPitting Loss (PL), and Plastic Warpage (PW), our method produces realistic\ndefect masks, which are subsequently enhanced in two phases. The first stage\n(npcF) enforces a PDE-based consistency to achieve a globally coherent anomaly\nstructure, while the second stage (npcF++) further improves local fidelity.\nAdditionally, we leverage SQE-driven weighting, ensuring that high-quality\nsynthetic samples receive greater emphasis during training. To validate our\nmethod, we conduct experiments on three anomaly detection benchmarks: MVTec AD,\nVisA, and BTAD. Across these datasets, our method achieves state-of-the-art\nresults in both image- and pixel-AUROC, confirming the effectiveness of our\nMaPhC2F dataset and BiSQAD method. All code will be released.\n","authors":["Long Qian","Bingke Zhu","Yingying Chen","Ming Tang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2504.12970v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02443v1","updated":"2025-08-04T14:02:20Z","published":"2025-08-04T14:02:20Z","title":"Uncertainty Estimation for Novel Views in Gaussian Splatting from\n  Primitive-Based Representations of Error and Visibility","summary":"  In this work, we present a novel method for uncertainty estimation (UE) in\nGaussian Splatting. UE is crucial for using Gaussian Splatting in critical\napplications such as robotics and medicine. Previous methods typically estimate\nthe variance of Gaussian primitives and use the rendering process to obtain\npixel-wise uncertainties. Our method establishes primitive representations of\nerror and visibility of trainings views, which carries meaningful uncertainty\ninformation. This representation is obtained by projection of training error\nand visibility onto the primitives. Uncertainties of novel views are obtained\nby rendering the primitive representations of uncertainty for those novel\nviews, yielding uncertainty feature maps. To aggregate these uncertainty\nfeature maps of novel views, we perform a pixel-wise regression on holdout\ndata. In our experiments, we analyze the different components of our method,\ninvestigating various combinations of uncertainty feature maps and regression\nmodels. Furthermore, we considered the effect of separating splatting into\nforeground and background. Our UEs show high correlations to true errors,\noutperforming state-of-the-art methods, especially on foreground objects. The\ntrained regression models show generalization capabilities to new scenes,\nallowing uncertainty estimation without the need for holdout data.\n","authors":["Thomas Gottwald","Edgar Heinert","Matthias Rottmann"],"pdf_url":"https://arxiv.org/pdf/2508.02443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02439v1","updated":"2025-08-04T13:59:57Z","published":"2025-08-04T13:59:57Z","title":"Glioblastoma Overall Survival Prediction With Vision Transformers","summary":"  Glioblastoma is one of the most aggressive and common brain tumors, with a\nmedian survival of 10-15 months. Predicting Overall Survival (OS) is critical\nfor personalizing treatment strategies and aligning clinical decisions with\npatient outcomes. In this study, we propose a novel Artificial Intelligence\n(AI) approach for OS prediction using Magnetic Resonance Imaging (MRI) images,\nexploiting Vision Transformers (ViTs) to extract hidden features directly from\nMRI images, eliminating the need of tumor segmentation. Unlike traditional\napproaches, our method simplifies the workflow and reduces computational\nresource requirements.\n  The proposed model was evaluated on the BRATS dataset, reaching an accuracy\nof 62.5% on the test set, comparable to the top-performing methods.\nAdditionally, it demonstrated balanced performance across precision, recall,\nand F1 score, overcoming the best model in these metrics. The dataset size\nlimits the generalization of the ViT which typically requires larger datasets\ncompared to convolutional neural networks. This limitation in generalization is\nobserved across all the cited studies. This work highlights the applicability\nof ViTs for downsampled medical imaging tasks and establishes a foundation for\nOS prediction models that are computationally efficient and do not rely on\nsegmentation.\n","authors":["Yin Lin","iccardo Barbieri","Domenico Aquino","Giuseppe Lauria","Marina Grisoli","Elena De Momi","Alberto Redaelli","Simona Ferrante"],"pdf_url":"https://arxiv.org/pdf/2508.02439v1.pdf","comment":"4 pages, 4 figures, EMBC2025"},{"id":"http://arxiv.org/abs/2412.17804v3","updated":"2025-08-04T13:59:08Z","published":"2024-12-23T18:58:17Z","title":"GausSim: Foreseeing Reality by Gaussian Simulator for Elastic Objects","summary":"  We introduce GausSim, a novel neural network-based simulator designed to\ncapture the dynamic behaviors of real-world elastic objects represented through\nGaussian kernels. We leverage continuum mechanics and treat each kernel as a\nCenter of Mass System (CMS) that represents continuous piece of matter,\naccounting for realistic deformations without idealized assumptions. To improve\ncomputational efficiency and fidelity, we employ a hierarchical structure that\nfurther organizes kernels into CMSs with explicit formulations, enabling a\ncoarse-to-fine simulation approach. This structure significantly reduces\ncomputational overhead while preserving detailed dynamics. In addition, GausSim\nincorporates explicit physics constraints, such as mass and momentum\nconservation, ensuring interpretable results and robust, physically plausible\nsimulations. To validate our approach, we present a new dataset, READY,\ncontaining multi-view videos of real-world elastic deformations. Experimental\nresults demonstrate that GausSim achieves superior performance compared to\nexisting physics-driven baselines, offering a practical and accurate solution\nfor simulating complex dynamic behaviors. Code and model are available at our\nproject page: https://www.mmlab-ntu.com/project/gausim/index.html .\n","authors":["Yidi Shao","Mu Huang","Chen Change Loy","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2412.17804v3.pdf","comment":"Accepted by ICCV2025. Project page:\n  https://www.mmlab-ntu.com/project/gausim/index.html"},{"id":"http://arxiv.org/abs/2405.18937v2","updated":"2025-08-04T13:54:40Z","published":"2024-05-29T09:43:48Z","title":"Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description","summary":"  In this paper, we introduce Part-Aware Point Grounded Description (PaPGD), a\nchallenging task aimed at advancing 3D multimodal learning for fine-grained,\npart-aware segmentation grounding and detailed explanation of 3D objects.\nExisting 3D datasets largely focus on either vision-only part segmentation or\nvision-language scene segmentation, lacking the fine-grained multimodal\nsegmentation needed for robotic navigation and interaction in real-world\nenvironments. To address this gap, we present the 3DCoMPaT Grounded\nInstructions (3DCoMPaT-GrIn) Dataset, a comprehensive resource that pairs rich\npoint cloud descriptions with corresponding part-level segmentation masks. This\ndataset encompasses extensive samples designed for both PaPGD and fine-grained\nsingle-part grounding tasks. To tackle the inherent challenges of grounding\nobjects and generating grounded descriptions at the part level, we propose\nKestrel, a part-aware 3D multimodal large language model that integrates an\nadvanced language model for nuanced language comprehension with multi-level\npoint feature propagation and query refinement mechanism to enhance spatial\nreasoning at the part level. The extensive experiments demonstrate that Kestrel\neffectively bridges the gap between part-aware language understanding and 3D\nsegmentation grounding, paving the way for more robust and interpretable 3D\nobject comprehension that meets the demands of real-world robotic applications.\nProject page at https://feielysia.github.io/Kestrel.github.io/\n","authors":["Mahmoud Ahmed","Junjie Fei","Jian Ding","Eslam Mohamed Bakr","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2405.18937v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02431v1","updated":"2025-08-04T13:50:00Z","published":"2025-08-04T13:50:00Z","title":"Identifying actionable driver mutations in lung cancer using an\n  efficient Asymmetric Transformer Decoder","summary":"  Identifying actionable driver mutations in non-small cell lung cancer (NSCLC)\ncan impact treatment decisions and significantly improve patient outcomes.\nDespite guideline recommendations, broader adoption of genetic testing remains\nchallenging due to limited availability and lengthy turnaround times. Machine\nLearning (ML) methods for Computational Pathology (CPath) offer a potential\nsolution; however, research often focuses on only one or two common mutations,\nlimiting the clinical value of these tools and the pool of patients who can\nbenefit from them. This study evaluates various Multiple Instance Learning\n(MIL) techniques to detect six key actionable NSCLC driver mutations: ALK,\nBRAF, EGFR, ERBB2, KRAS, and MET ex14. Additionally, we introduce an Asymmetric\nTransformer Decoder model that employs queries and key-values of varying\ndimensions to maintain a low query dimensionality. This approach efficiently\nextracts information from patch embeddings and minimizes overfitting risks,\nproving highly adaptable to the MIL setting. Moreover, we present a method to\ndirectly utilize tissue type in the model, addressing a typical MIL limitation\nwhere either all regions or only some specific regions are analyzed, neglecting\nbiological relevance. Our method outperforms top MIL models by an average of\n3%, and over 4% when predicting rare mutations such as ERBB2 and BRAF, moving\nML-based tests closer to being practical alternatives to standard genetic\ntesting.\n","authors":["Biagio Brattoli","Jack Shi","Jongchan Park","Taebum Lee","Donggeun Yoo","Sergio Pereira"],"pdf_url":"https://arxiv.org/pdf/2508.02431v1.pdf","comment":"Accepted at MICCAI 2025 Workshop COMPAYL"},{"id":"http://arxiv.org/abs/2508.02419v1","updated":"2025-08-04T13:40:59Z","published":"2025-08-04T13:40:59Z","title":"Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination\n  via Attention Lens","summary":"  Large vision-language models (LVLMs) have demonstrated remarkable multimodal\ncomprehension and reasoning capabilities, but they still suffer from severe\nobject hallucination. Previous studies primarily attribute the flaw to\nlinguistic prior caused by the scale mismatch between visual encoders and large\nlanguage models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon\nLLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,\ngenerating descriptions inconsistent with visual cues. However, through an\nin-depth investigation of the hallucinated mechanisms, we empirically reveal a\npreviously overlooked phenomenon: LVLMs may ignore not only visual information\nbut also textual modality during hallucination, a behavior termed as modality\nbias, which indicates that LVLMs struggle to simultaneously attend to both\nvisual and textual modalities, leading to fragmented understanding of\nuser-provided instructions. Based on this observation, we propose a simple yet\neffective training-free method to mitigate object hallucination. Concretely, we\nintervene and adjust the attention weights of textual and visual tokens,\nbalancing cross-modal compatibility for better alignment with user intentions.\nFurthermore, we adopt a contrastive decoding strategy to reduce the LVLM's\noverreliance on its parametric knowledge, synergistically enhancing our\nattention manipulation. Extensive experiments confirm the widespread presence\nof modality bias in LVLMs. Notably, our method effectively mitigates\nhallucination across multiple open-source LVLMs and benchmarks, highlighting\nits generalizability and efficacy.\n","authors":["Haohan Zheng","Zhenguo Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.02419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13053v2","updated":"2025-08-04T13:40:46Z","published":"2025-03-17T10:56:30Z","title":"Uncertainty-Aware Knowledge Distillation for Compact and Efficient 6DoF\n  Pose Estimation","summary":"  Compact and efficient 6DoF object pose estimation is crucial in applications\nsuch as robotics, augmented reality, and space autonomous navigation systems,\nwhere lightweight models are critical for real-time accurate performance. This\npaper introduces a novel uncertainty-aware end-to-end Knowledge Distillation\n(KD) framework focused on keypoint-based 6DoF pose estimation. Keypoints\npredicted by a large teacher model exhibit varying levels of uncertainty that\ncan be exploited within the distillation process to enhance the accuracy of the\nstudent model while ensuring its compactness. To this end, we propose a\ndistillation strategy that aligns the student and teacher predictions by\nadjusting the knowledge transfer based on the uncertainty associated with each\nteacher keypoint prediction. Additionally, the proposed KD leverages this\nuncertainty-aware alignment of keypoints to transfer the knowledge at key\nlocations of their respective feature maps. Experiments on the widely-used\nLINEMOD benchmark demonstrate the effectiveness of our method, achieving\nsuperior 6DoF object pose estimation with lightweight models compared to\nstate-of-the-art approaches. Further validation on the SPEED+ dataset for\nspacecraft pose estimation highlights the robustness of our approach under\ndiverse 6DoF pose estimation scenarios.\n","authors":["Nassim Ali Ousalah","Anis Kacem","Enjie Ghorbel","Emmanuel Koumandakis","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2503.13053v2.pdf","comment":"Accepted to the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2508.02411v1","updated":"2025-08-04T13:33:28Z","published":"2025-08-04T13:33:28Z","title":"HGTS-Former: Hierarchical HyperGraph Transformer for Multivariate Time\n  Series Analysis","summary":"  Multivariate time series analysis has long been one of the key research\ntopics in the field of artificial intelligence. However, analyzing complex time\nseries data remains a challenging and unresolved problem due to its high\ndimensionality, dynamic nature, and complex interactions among variables.\nInspired by the strong structural modeling capability of hypergraphs, this\npaper proposes a novel hypergraph-based time series transformer backbone\nnetwork, termed HGTS-Former, to address the multivariate coupling in time\nseries data. Specifically, given the multivariate time series signal, we first\nnormalize and embed each patch into tokens. Then, we adopt the multi-head\nself-attention to enhance the temporal representation of each patch. The\nhierarchical hypergraphs are constructed to aggregate the temporal patterns\nwithin each channel and fine-grained relations between different variables.\nAfter that, we convert the hyperedge into node features through the EdgeToNode\nmodule and adopt the feed-forward network to further enhance the output\nfeatures. Extensive experiments conducted on two multivariate time series tasks\nand eight datasets fully validated the effectiveness of our proposed\nHGTS-Former. The source code will be released on\nhttps://github.com/Event-AHU/Time_Series_Analysis.\n","authors":["Xiao Wang","Hao Si","Fan Zhang","Xiaoya Zhou","Dengdi Sun","Wanli Lyu","Qingquan Yang","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2508.02411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02409v1","updated":"2025-08-04T13:33:06Z","published":"2025-08-04T13:33:06Z","title":"Hydra: Accurate Multi-Modal Leaf Wetness Sensing with mm-Wave and Camera\n  Fusion","summary":"  Leaf Wetness Duration (LWD), the time that water remains on leaf surfaces, is\ncrucial in the development of plant diseases. Existing LWD detection lacks\nstandardized measurement techniques, and variations across different plant\ncharacteristics limit its effectiveness. Prior research proposes diverse\napproaches, but they fail to measure real natural leaves directly and lack\nresilience in various environmental conditions. This reduces the precision and\nrobustness, revealing a notable practical application and effectiveness gap in\nreal-world agricultural settings. This paper presents Hydra, an innovative\napproach that integrates millimeter-wave (mm-Wave) radar with camera technology\nto detect leaf wetness by determining if there is water on the leaf. We can\nmeasure the time to determine the LWD based on this detection. Firstly, we\ndesign a Convolutional Neural Network (CNN) to selectively fuse multiple\nmm-Wave depth images with an RGB image to generate multiple feature images.\nThen, we develop a transformer-based encoder to capture the inherent connection\namong the multiple feature images to generate a feature map, which is further\nfed to a classifier for detection. Moreover, we augment the dataset during\ntraining to generalize our model. Implemented using a frequency-modulated\ncontinuous-wave (FMCW) radar within the 76 to 81 GHz band, Hydra's performance\nis meticulously evaluated on plants, demonstrating the potential to classify\nleaf wetness with up to 96% accuracy across varying scenarios. Deploying Hydra\nin the farm, including rainy, dawn, or poorly light nights, it still achieves\nan accuracy rate of around 90%.\n","authors":["Yimeng Liu","Maolin Gan","Huaili Zeng","Li Liu","Younsuk Dong","Zhichao Cao"],"pdf_url":"https://arxiv.org/pdf/2508.02409v1.pdf","comment":"In Proceedings of ACM MobiCom (2024)"},{"id":"http://arxiv.org/abs/2508.02408v1","updated":"2025-08-04T13:31:42Z","published":"2025-08-04T13:31:42Z","title":"GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT\n  Reconstruction","summary":"  3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT\nreconstruction. However, existing methods rely on the average gradient\nmagnitude of points within the view, often leading to severe needle-like\nartifacts under sparse-view conditions. To address this challenge, we propose\nGR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses\nneedle-like artifacts and improves reconstruction accuracy under sparse-view\nconditions. Our framework introduces two key innovations: (1) a Denoised Point\nCloud Initialization Strategy that reduces initialization errors and\naccelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that\nrefines gradient computation using graph-based density differences, improving\nsplitting accuracy and density representation. Experiments on X-3D and\nreal-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR\nimprovements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These\nresults highlight the applicability of GR-Gaussian for accurate CT\nreconstruction under challenging sparse-view conditions.\n","authors":["Yikuang Yuluo","Yue Ma","Kuan Shen","Tongtong Jin","Wang Liao","Yangpu Ma","Fuquan Wang"],"pdf_url":"https://arxiv.org/pdf/2508.02408v1.pdf","comment":"10"},{"id":"http://arxiv.org/abs/2508.02405v1","updated":"2025-08-04T13:29:26Z","published":"2025-08-04T13:29:26Z","title":"Improving Generalization of Language-Conditioned Robot Manipulation","summary":"  The control of robots for manipulation tasks generally relies on visual\ninput. Recent advances in vision-language models (VLMs) enable the use of\nnatural language instructions to condition visual input and control robots in a\nwider range of environments. However, existing methods require a large amount\nof data to fine-tune VLMs for operating in unseen environments. In this paper,\nwe present a framework that learns object-arrangement tasks from just a few\ndemonstrations. We propose a two-stage framework that divides\nobject-arrangement tasks into a target localization stage, for picking the\nobject, and a region determination stage for placing the object. We present an\ninstance-level semantic fusion module that aligns the instance-level image\ncrops with the text embedding, enabling the model to identify the target\nobjects defined by the natural language instructions. We validate our method on\nboth simulation and real-world robotic environments. Our method, fine-tuned\nwith a few demonstrations, improves generalization capability and demonstrates\nzero-shot ability in real-robot manipulation scenarios.\n","authors":["Chenglin Cui","Chaoran Zhu","Changjae Oh","Andrea Cavallaro"],"pdf_url":"https://arxiv.org/pdf/2508.02405v1.pdf","comment":"7 pages,18 figures,2 tables"},{"id":"http://arxiv.org/abs/2507.21905v2","updated":"2025-08-04T13:19:11Z","published":"2025-07-29T15:17:00Z","title":"Evaluating Deepfake Detectors in the Wild","summary":"  Deepfakes powered by advanced machine learning models present a significant\nand evolving threat to identity verification and the authenticity of digital\nmedia. Although numerous detectors have been developed to address this problem,\ntheir effectiveness has yet to be tested when applied to real-world data. In\nthis work we evaluate modern deepfake detectors, introducing a novel testing\nprocedure designed to mimic real-world scenarios for deepfake detection. Using\nstate-of-the-art deepfake generation methods, we create a comprehensive dataset\ncontaining more than 500,000 high-quality deepfake images. Our analysis shows\nthat detecting deepfakes still remains a challenging task. The evaluation shows\nthat in fewer than half of the deepfake detectors tested achieved an AUC score\ngreater than 60%, with the lowest being 50%. We demonstrate that basic image\nmanipulations, such as JPEG compression or image enhancement, can significantly\nreduce model performance. All code and data are publicly available at\nhttps://github.com/SumSubstance/Deepfake-Detectors-in-the-Wild.\n","authors":["Viacheslav Pirogov","Maksim Artemev"],"pdf_url":"https://arxiv.org/pdf/2507.21905v2.pdf","comment":"Accepted to the ICML 2025 Workshop 'DataWorld: Unifying Data Curation\n  Frameworks Across Domains'"},{"id":"http://arxiv.org/abs/2405.17446v4","updated":"2025-08-04T13:14:00Z","published":"2024-05-20T20:13:03Z","title":"Comparing ImageNet Pre-training with Digital Pathology Foundation Models\n  for Whole Slide Image-Based Survival Analysis","summary":"  The abundance of information present in Whole Slide Images (WSIs) renders\nthem an essential tool for survival analysis. Several Multiple Instance\nLearning frameworks proposed for this task utilize a ResNet50 backbone\npre-trained on natural images. By leveraging recenetly released\nhistopathological foundation models such as UNI and Hibou, the predictive\nprowess of existing MIL networks can be enhanced. Furthermore, deploying an\nensemble of digital pathology foundation models yields higher baseline\naccuracy, although the benefits appear to diminish with more complex MIL\narchitectures. Our code will be made publicly available upon acceptance.\n","authors":["Kleanthis Marios Papadopoulos","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2405.17446v4.pdf","comment":"Accepted (Oral) at the 6th International Conference on Computer\n  Vision and Information Technology (CVIT 2025)"},{"id":"http://arxiv.org/abs/2508.02387v1","updated":"2025-08-04T13:10:48Z","published":"2025-08-04T13:10:48Z","title":"$ε$-Softmax: Approximating One-Hot Vectors for Mitigating Label\n  Noise","summary":"  Noisy labels pose a common challenge for training accurate deep neural\nnetworks. To mitigate label noise, prior studies have proposed various robust\nloss functions to achieve noise tolerance in the presence of label noise,\nparticularly symmetric losses. However, they usually suffer from the\nunderfitting issue due to the overly strict symmetric condition. In this work,\nwe propose a simple yet effective approach for relaxing the symmetric\ncondition, namely $\\epsilon$-softmax, which simply modifies the outputs of the\nsoftmax layer to approximate one-hot vectors with a controllable error\n$\\epsilon$. Essentially, $\\epsilon$-softmax not only acts as an alternative for\nthe softmax layer, but also implicitly plays the crucial role in modifying the\nloss function. We prove theoretically that $\\epsilon$-softmax can achieve\nnoise-tolerant learning with controllable excess risk bound for almost any loss\nfunction. Recognizing that $\\epsilon$-softmax-enhanced losses may slightly\nreduce fitting ability on clean datasets, we further incorporate them with one\nsymmetric loss, thereby achieving a better trade-off between robustness and\neffective learning. Extensive experiments demonstrate the superiority of our\nmethod in mitigating synthetic and real-world label noise. The code is\navailable at https://github.com/cswjl/eps-softmax.\n","authors":["Jialiang Wang","Xiong Zhou","Deming Zhai","Junjun Jiang","Xiangyang Ji","Xianming Liu"],"pdf_url":"https://arxiv.org/pdf/2508.02387v1.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2508.02386v1","updated":"2025-08-04T13:10:39Z","published":"2025-08-04T13:10:39Z","title":"Enhancing Object Discovery for Unsupervised Instance Segmentation and\n  Object Detection","summary":"  We propose Cut-Once-and-LEaRn (COLER), a simple approach for unsupervised\ninstance segmentation and object detection. COLER first uses our developed\nCutOnce to generate coarse pseudo labels, then enables the detector to learn\nfrom these masks. CutOnce applies Normalized Cut only once and does not rely on\nany clustering methods, but it can generate multiple object masks in an image.\nWe have designed several novel yet simple modules that not only allow CutOnce\nto fully leverage the object discovery capabilities of self-supervised models,\nbut also free it from reliance on mask post-processing. During training, COLER\nachieves strong performance without requiring specially designed loss functions\nfor pseudo labels, and its performance is further improved through\nself-training. COLER is a zero-shot unsupervised model that outperforms\nprevious state-of-the-art methods on multiple benchmarks.We believe our method\ncan help advance the field of unsupervised object localization.\n","authors":["Xingyu Feng","Hebei Gao","Hong Li"],"pdf_url":"https://arxiv.org/pdf/2508.02386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02384v1","updated":"2025-08-04T13:09:58Z","published":"2025-08-04T13:09:58Z","title":"SMART-Ship: A Comprehensive Synchronized Multi-modal Aligned Remote\n  Sensing Targets Dataset and Benchmark for Berthed Ships Analysis","summary":"  Given the limitations of satellite orbits and imaging conditions, multi-modal\nremote sensing (RS) data is crucial in enabling long-term earth observation.\nHowever, maritime surveillance remains challenging due to the complexity of\nmulti-scale targets and the dynamic environments. To bridge this critical gap,\nwe propose a Synchronized Multi-modal Aligned Remote sensing Targets dataset\nfor berthed ships analysis (SMART-Ship), containing spatiotemporal registered\nimages with fine-grained annotation for maritime targets from five modalities:\nvisible-light, synthetic aperture radar (SAR), panchromatic, multi-spectral,\nand near-infrared. Specifically, our dataset consists of 1092 multi-modal image\nsets, covering 38,838 ships. Each image set is acquired within one week and\nregistered to ensure spatiotemporal consistency. Ship instances in each set are\nannotated with polygonal location information, fine-grained categories,\ninstance-level identifiers, and change region masks, organized hierarchically\nto support diverse multi-modal RS tasks. Furthermore, we define standardized\nbenchmarks on five fundamental tasks and comprehensively compare representative\nmethods across the dataset. Thorough experiment evaluations validate that the\nproposed SMART-Ship dataset could support various multi-modal RS interpretation\ntasks and reveal the promising directions for further exploration.\n","authors":["Chen-Chen Fan","Peiyao Guo","Linping Zhang","Kehan Qi","Haolin Huang","Yong-Qiang Mao","Yuxi Suo","Zhizhuo Jiang","Yu Liu","You He"],"pdf_url":"https://arxiv.org/pdf/2508.02384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02374v1","updated":"2025-08-04T13:02:23Z","published":"2025-08-04T13:02:23Z","title":"Uni-Layout: Integrating Human Feedback in Unified Layout Generation and\n  Evaluation","summary":"  Layout generation plays a crucial role in enhancing both user experience and\ndesign efficiency. However, current approaches suffer from task-specific\ngeneration capabilities and perceptually misaligned evaluation metrics, leading\nto limited applicability and ineffective measurement. In this paper, we propose\n\\textit{Uni-Layout}, a novel framework that achieves unified generation,\nhuman-mimicking evaluation and alignment between the two. For universal\ngeneration, we incorporate various layout tasks into a single taxonomy and\ndevelop a unified generator that handles background or element contents\nconstrained tasks via natural language prompts. To introduce human feedback for\nthe effective evaluation of layouts, we build \\textit{Layout-HF100k}, the first\nlarge-scale human feedback dataset with 100,000 expertly annotated layouts.\nBased on \\textit{Layout-HF100k}, we introduce a human-mimicking evaluator that\nintegrates visual and geometric information, employing a Chain-of-Thought\nmechanism to conduct qualitative assessments alongside a confidence estimation\nmodule to yield quantitative measurements. For better alignment between the\ngenerator and the evaluator, we integrate them into a cohesive system by\nadopting Dynamic-Margin Preference Optimization (DMPO), which dynamically\nadjusts margins based on preference strength to better align with human\njudgments. Extensive experiments show that \\textit{Uni-Layout} significantly\noutperforms both task-specific and general-purpose methods. Our code is\npublicly available at https://github.com/JD-GenX/Uni-Layout.\n","authors":["Shuo Lu","Yanyin Chen","Wei Feng","Jiahao Fan","Fengheng Li","Zheng Zhang","Jingjing Lv","Junjie Shen","Ching Law","Jian Liang"],"pdf_url":"https://arxiv.org/pdf/2508.02374v1.pdf","comment":"Accepted to ACM MM 2025"},{"id":"http://arxiv.org/abs/2508.02372v1","updated":"2025-08-04T13:02:04Z","published":"2025-08-04T13:02:04Z","title":"TRUDI and TITUS: A Multi-Perspective Dataset and A Three-Stage\n  Recognition System for Transportation Unit Identification","summary":"  Identifying transportation units (TUs) is essential for improving the\nefficiency of port logistics. However, progress in this field has been hindered\nby the lack of publicly available benchmark datasets that capture the diversity\nand dynamics of real-world port environments. To address this gap, we present\nthe TRUDI dataset-a comprehensive collection comprising 35,034 annotated\ninstances across five categories: container, tank container, trailer, ID text,\nand logo. The images were captured at operational ports using both ground-based\nand aerial cameras, under a wide variety of lighting and weather conditions.\nFor the identification of TUs-which involves reading the 11-digit alphanumeric\nID typically painted on each unit-we introduce TITUS, a dedicated pipeline that\noperates in three stages: (1) segmenting the TU instances, (2) detecting the\nlocation of the ID text, and (3) recognising and validating the extracted ID.\nUnlike alternative systems, which often require similar scenes, specific camera\nangles or gate setups, our evaluation demonstrates that TITUS reliably\nidentifies TUs from a range of camera perspectives and in varying lighting and\nweather conditions. By making the TRUDI dataset publicly available, we provide\na robust benchmark that enables the development and comparison of new\napproaches. This contribution supports digital transformation efforts in\nmultipurpose ports and helps to increase the efficiency of entire logistics\nchains.\n","authors":["Emre Gülsoylu","André Kelm","Lennart Bengtson","Matthias Hirsch","Christian Wilms","Tim Rolff","Janick Edinger","Simone Frintrop"],"pdf_url":"https://arxiv.org/pdf/2508.02372v1.pdf","comment":"13 pages, 2 figures, 6 tables. Author version of the paper. Accepted\n  for publication in The 36th British Machine Vision Conference (BMVC) 2025"},{"id":"http://arxiv.org/abs/2507.10072v2","updated":"2025-08-04T12:53:13Z","published":"2025-07-14T08:58:38Z","title":"Frequency Regulation for Exposure Bias Mitigation in Diffusion Models","summary":"  Diffusion models exhibit impressive generative capabilities but are\nsignificantly impacted by exposure bias. In this paper, we make a key\nobservation: the energy of predicted noisy samples in the reverse process\ncontinuously declines compared to perturbed samples in the forward process.\nBuilding on this, we identify two important findings: 1) The reduction in\nenergy follows distinct patterns in the low-frequency and high-frequency\nsubbands; 2) The subband energy of reverse-process reconstructed samples is\nconsistently lower than that of forward-process ones, and both are lower than\nthe original data samples. Based on the first finding, we introduce a dynamic\nfrequency regulation mechanism utilizing wavelet transforms, which separately\nadjusts the low- and high-frequency subbands. Leveraging the second insight, we\nderive the rigorous mathematical form of exposure bias. It is worth noting\nthat, our method is training-free and plug-and-play, significantly improving\nthe generative quality of various diffusion models and frameworks with\nnegligible computational cost. The source code is available at\nhttps://github.com/kunzhan/wpp.\n","authors":["Meng Yu","Kun Zhan"],"pdf_url":"https://arxiv.org/pdf/2507.10072v2.pdf","comment":"ACM Multimedia 2025 accepted!"},{"id":"http://arxiv.org/abs/2508.02363v1","updated":"2025-08-04T12:50:58Z","published":"2025-08-04T12:50:58Z","title":"Transport-Guided Rectified Flow Inversion: Improved Image Editing Using\n  Optimal Transport Theory","summary":"  Effective image inversion in rectified flow models - mapping real images to\neditable latent representations - is crucial for practical image editing\napplications; however, achieving optimal balance between reconstruction\nfidelity and editing flexibility remains a fundamental challenge. In this work,\nwe introduce the Optimal Transport Inversion Pipeline (OTIP), a zero-shot\nframework that leverages optimal transport theory to guide the inversion\nprocess in rectified flow models. Our underlying hypothesis is that\nincorporating transport-based guidance during the reverse diffusion process can\neffectively balance reconstruction accuracy and editing controllability through\nprincipled trajectory optimization. The method computes optimal transport paths\nbetween image and noise distributions while maintaining computational\nefficiency. Our approach achieves high-fidelity reconstruction with LPIPS\nscores of 0.001 and SSIM of 0.992 on face editing benchmarks, demonstrating\nsuperior preservation of fine-grained details compared to existing methods. We\nevaluate the framework across multiple editing tasks, observing 7.8% to 12.9%\nimprovements in reconstruction loss over RF-Inversion on the LSUN-Bedroom and\nLSUN-Church datasets, respectively. For semantic face editing, our method\nachieves an 11.2% improvement in identity preservation and a 1.6% enhancement\nin perceptual quality, while maintaining computational efficiency comparable to\nbaseline approaches. Qualitatively, our method produces visually compelling\nedits with superior semantic consistency and fine-grained detail preservation\nacross diverse editing scenarios. Code is available at:\nhttps://github.com/marianlupascu/OT-Inversion\n","authors":["Marian Lupascu","Mihai-Sorin Stupariu"],"pdf_url":"https://arxiv.org/pdf/2508.02363v1.pdf","comment":"25 pages, 24 figures, WACV conference"},{"id":"http://arxiv.org/abs/2508.02362v1","updated":"2025-08-04T12:50:22Z","published":"2025-08-04T12:50:22Z","title":"Text2Lip: Progressive Lip-Synced Talking Face Generation from Text via\n  Viseme-Guided Rendering","summary":"  Generating semantically coherent and visually accurate talking faces requires\nbridging the gap between linguistic meaning and facial articulation. Although\naudio-driven methods remain prevalent, their reliance on high-quality paired\naudio visual data and the inherent ambiguity in mapping acoustics to lip motion\npose significant challenges in terms of scalability and robustness. To address\nthese issues, we propose Text2Lip, a viseme-centric framework that constructs\nan interpretable phonetic-visual bridge by embedding textual input into\nstructured viseme sequences. These mid-level units serve as a linguistically\ngrounded prior for lip motion prediction. Furthermore, we design a progressive\nviseme-audio replacement strategy based on curriculum learning, enabling the\nmodel to gradually transition from real audio to pseudo-audio reconstructed\nfrom enhanced viseme features via cross-modal attention. This allows for robust\ngeneration in both audio-present and audio-free scenarios. Finally, a\nlandmark-guided renderer synthesizes photorealistic facial videos with accurate\nlip synchronization. Extensive evaluations show that Text2Lip outperforms\nexisting approaches in semantic fidelity, visual realism, and modality\nrobustness, establishing a new paradigm for controllable and flexible talking\nface generation. Our project homepage is https://plyon1.github.io/Text2Lip/.\n","authors":["Xu Wang","Shengeng Tang","Fei Wang","Lechao Cheng","Dan Guo","Feng Xue","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2508.02362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02359v1","updated":"2025-08-04T12:48:59Z","published":"2025-08-04T12:48:59Z","title":"Toward a reliable PWM-based light-emitting diode visual stimulus for\n  improved SSVEP response with minimal visual fatigue","summary":"  Steady state visual evoked response (SSVEP) is widely used in visual-based\ndiagnosis and applications such as brain computer interfacing due to its high\ninformation transfer rate and the capability to activate commands through\nsimple gaze control. However, one major impediment in using flashing visual\nstimulus to obtain SSVEP is eye fatigue that prevents continued long term use\npreventing practical deployment. This combined with the difficulty in\nestablishing precise pulse-width modulation (PWM) that results in poorer\naccuracy warrants the development of appropriate approach to solve these\nissues. Various studies have suggested the usage of high frequencies of visual\nstimulus to reduce the visual fatigue for the user but this results in poor\nresponse performance. Here, the authors study the use of extremely high\nduty-cycles in the stimulus in the hope of solving these constraints.\nElectroencephalogram data was recorded with PWM duty-cycles of 50 to 95%\ngenerated by a precise custom-made light-emitting diode hardware and tested ten\nsubjects responded that increasing duty-cycles had less visual strain for all\nthe frequency values and the SSVEP exhibited a subject-independent peak\nresponse for duty-cycle of 85%. This could pave the way for increased usage of\nSSVEP for practical applications.\n","authors":["Surej Mouli","Ramaswamy Palaniappan"],"pdf_url":"https://arxiv.org/pdf/2508.02359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02348v1","updated":"2025-08-04T12:31:11Z","published":"2025-08-04T12:31:11Z","title":"mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at\n  T-Junctions Utilizing Road Layout Extraction via Camera","summary":"  Pedestrians Localization in Non-Line-of-Sight (NLoS) regions within urban\nenvironments poses a significant challenge for autonomous driving systems.\nWhile mmWave radar has demonstrated potential for detecting objects in such\nscenarios, the 2D radar point cloud (PCD) data is susceptible to distortions\ncaused by multipath reflections, making accurate spatial inference difficult.\nAdditionally, although camera images provide high-resolution visual\ninformation, they lack depth perception and cannot directly observe objects in\nNLoS regions. In this paper, we propose a novel framework that interprets radar\nPCD through road layout inferred from camera for localization of NLoS\npedestrians. The proposed method leverages visual information from the camera\nto interpret 2D radar PCD, enabling spatial scene reconstruction. The\neffectiveness of the proposed approach is validated through experiments\nconducted using a radar-camera system mounted on a real vehicle. The\nlocalization performance is evaluated using a dataset collected in outdoor NLoS\ndriving environments, demonstrating the practical applicability of the method.\n","authors":["Byeonggyu Park","Hee-Yeun Kim","Byonghyok Choi","Hansang Cho","Byungkwan Kim","Soomok Lee","Mingu Jeon","Seong-Woo Kim"],"pdf_url":"https://arxiv.org/pdf/2508.02348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00748v2","updated":"2025-08-04T12:27:33Z","published":"2025-08-01T16:23:27Z","title":"Is It Really You? Exploring Biometric Verification Scenarios in\n  Photorealistic Talking-Head Avatar Videos","summary":"  Photorealistic talking-head avatars are becoming increasingly common in\nvirtual meetings, gaming, and social platforms. These avatars allow for more\nimmersive communication, but they also introduce serious security risks. One\nemerging threat is impersonation: an attacker can steal a user's avatar,\npreserving his appearance and voice, making it nearly impossible to detect its\nfraudulent usage by sight or sound alone. In this paper, we explore the\nchallenge of biometric verification in such avatar-mediated scenarios. Our main\nquestion is whether an individual's facial motion patterns can serve as\nreliable behavioral biometrics to verify their identity when the avatar's\nvisual appearance is a facsimile of its owner. To answer this question, we\nintroduce a new dataset of realistic avatar videos created using a\nstate-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and\nimpostor avatar videos. We also propose a lightweight, explainable\nspatio-temporal Graph Convolutional Network architecture with temporal\nattention pooling, that uses only facial landmarks to model dynamic facial\ngestures. Experimental results demonstrate that facial motion cues enable\nmeaningful identity verification with AUC values approaching 80%. The proposed\nbenchmark and biometric system are available for the research community in\norder to bring attention to the urgent need for more advanced behavioral\nbiometric defenses in avatar-based communication systems.\n","authors":["Laura Pedrouzo-Rodriguez","Pedro Delgado-DeRobles","Luis F. Gomez","Ruben Tolosana","Ruben Vera-Rodriguez","Aythami Morales","Julian Fierrez"],"pdf_url":"https://arxiv.org/pdf/2508.00748v2.pdf","comment":"Accepted at the IEEE International Joint Conference on Biometrics\n  (IJCB 2025)"},{"id":"http://arxiv.org/abs/2508.02340v1","updated":"2025-08-04T12:21:16Z","published":"2025-08-04T12:21:16Z","title":"Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search","summary":"  Ad-hoc Video Search (AVS) involves using a textual query to search for\nmultiple relevant videos in a large collection of unlabeled short videos. The\nmain challenge of AVS is the visual diversity of relevant videos. A simple\nquery such as \"Find shots of a man and a woman dancing together indoors\" can\nspan a multitude of environments, from brightly lit halls and shadowy bars to\ndance scenes in black-and-white animations. It is therefore essential to\nretrieve relevant videos as comprehensively as possible. Current solutions for\nthe AVS task primarily fuse multiple features into one or more common spaces,\nyet overlook the need for diverse spaces. To fully exploit the expressive\ncapability of individual features, we propose LPD, short for Learning Partially\nDecorrelated common spaces. LPD incorporates two key innovations:\nfeature-specific common space construction and the de-correlation loss.\nSpecifically, LPD learns a separate common space for each video and text\nfeature, and employs de-correlation loss to diversify the ordering of negative\nsamples across different spaces. To enhance the consistency of multi-space\nconvergence, we designed an entropy-based fair multi-space triplet ranking\nloss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify\nthe effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces\nhighlight its ability to enhance result diversity.\n","authors":["Fan Hu","Zijie Xin","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2508.02340v1.pdf","comment":"Accepted by ACMMM2025"},{"id":"http://arxiv.org/abs/2508.02339v1","updated":"2025-08-04T12:21:05Z","published":"2025-08-04T12:21:05Z","title":"Correspondence-Free Fast and Robust Spherical Point Pattern Registration","summary":"  Existing methods for rotation estimation between two spherical\n($\\mathbb{S}^2$) patterns typically rely on spherical cross-correlation\nmaximization between two spherical function. However, these approaches exhibit\ncomputational complexities greater than cubic $O(n^3)$ with respect to rotation\nspace discretization and lack extensive evaluation under significant outlier\ncontamination. To this end, we propose a rotation estimation algorithm between\ntwo spherical patterns with linear time complexity $O(n)$. Unlike existing\nspherical-function-based methods, we explicitly represent spherical patterns as\ndiscrete 3D point sets on the unit sphere, reformulating rotation estimation as\na spherical point-set alignment (i.e., Wahba problem for 3D unit vectors).\nGiven the geometric nature of our formulation, our spherical pattern alignment\nalgorithm naturally aligns with the Wahba problem framework for 3D unit\nvectors. Specifically, we introduce three novel algorithms: (1) SPMC (Spherical\nPattern Matching by Correlation), (2) FRS (Fast Rotation Search), and (3) a\nhybrid approach (SPMC+FRS) that combines the advantages of the previous two\nmethods. Our experiments demonstrate that in the $\\mathbb{S}^2$ domain and in\ncorrespondence-free settings, our algorithms are over 10x faster and over 10x\nmore accurate than current state-of-the-art methods for the Wahba problem with\noutliers. We validate our approach through extensive simulations on a new\ndataset of spherical patterns, the ``Robust Vector Alignment Dataset.\n\"Furthermore, we adapt our methods to two real-world tasks: (i) Point Cloud\nRegistration (PCR) and (ii) rotation estimation for spherical images.\n","authors":["Anik Sarker","Alan T. Asbeck"],"pdf_url":"https://arxiv.org/pdf/2508.02339v1.pdf","comment":"Accepted at ICCV 2025"},{"id":"http://arxiv.org/abs/2504.12997v2","updated":"2025-08-04T12:14:27Z","published":"2025-04-17T15:06:52Z","title":"All-in-One Transferring Image Compression from Human Perception to\n  Multi-Machine Perception","summary":"  Efficiently transferring Learned Image Compression (LIC) model from human\nperception to machine perception is an emerging challenge in vision-centric\nrepresentation learning. Existing approaches typically adapt LIC to downstream\ntasks in a single-task manner, which is inefficient, lacks task interaction,\nand results in multiple task-specific bitstreams. In this paper, we propose a\nmulti-task adaptation framework that enables transferring a pre-trained base\ncodec to multiple machine vision tasks through a unified model and a single\ntraining process. To achieve this, we design an asymmetric adaptation\narchitecture consisting of a task-agnostic encoder adaptation and task-specific\ndecoder adaptation. Furthermore, we introduce two feature propagation modules\nto facilitate inter-task and inter-scale feature represenation learning.\nExperiments on PASCAL-Context and NYUD-V2 dataset demonstrate that our method\noutperforms both Fully Fine-Tuned and other Parameter Efficient Fine-Tuned\n(PEFT) baselines. Code will be released.\n","authors":["Jiancheng Zhao","Xiang Ji","Yinqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2504.12997v2.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2508.02329v1","updated":"2025-08-04T11:57:10Z","published":"2025-08-04T11:57:10Z","title":"CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via\n  Instruction Editing Data and Long Captions","summary":"  Despite the success of Vision-Language Models (VLMs) like CLIP in aligning\nvision and language, their proficiency in detailed, fine-grained visual\ncomprehension remains a key challenge. We present CLIP-IN, a novel framework\nthat bolsters CLIP's fine-grained perception through two core innovations.\nFirstly, we leverage instruction-editing datasets, originally designed for\nimage manipulation, as a unique source of hard negative image-text pairs.\nCoupled with a symmetric hard negative contrastive loss, this enables the model\nto effectively distinguish subtle visual-semantic differences. Secondly,\nCLIP-IN incorporates long descriptive captions, utilizing rotary positional\nencodings to capture rich semantic context often missed by standard CLIP. Our\nexperiments demonstrate that CLIP-IN achieves substantial gains on the MMVP\nbenchmark and various fine-grained visual recognition tasks, without\ncompromising robust zero-shot performance on broader classification and\nretrieval tasks. Critically, integrating CLIP-IN's visual representations into\nMultimodal Large Language Models significantly reduces visual hallucinations\nand enhances reasoning abilities. This work underscores the considerable\npotential of synergizing targeted, instruction-based contrastive learning with\ncomprehensive descriptive information to elevate the fine-grained understanding\nof VLMs.\n","authors":["Ziteng Wang","Siqi Yang","Limeng Qiao","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2508.02329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02324v1","updated":"2025-08-04T11:49:20Z","published":"2025-08-04T11:49:20Z","title":"Qwen-Image Technical Report","summary":"  We present Qwen-Image, an image generation foundation model in the Qwen\nseries that achieves significant advances in complex text rendering and precise\nimage editing. To address the challenges of complex text rendering, we design a\ncomprehensive data pipeline that includes large-scale data collection,\nfiltering, annotation, synthesis, and balancing. Moreover, we adopt a\nprogressive training strategy that starts with non-text-to-text rendering,\nevolves from simple to complex textual inputs, and gradually scales up to\nparagraph-level descriptions. This curriculum learning approach substantially\nenhances the model's native text rendering capabilities. As a result,\nQwen-Image not only performs exceptionally well in alphabetic languages such as\nEnglish, but also achieves remarkable progress on more challenging logographic\nlanguages like Chinese. To enhance image editing consistency, we introduce an\nimproved multi-task training paradigm that incorporates not only traditional\ntext-to-image (T2I) and text-image-to-image (TI2I) tasks but also\nimage-to-image (I2I) reconstruction, effectively aligning the latent\nrepresentations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed\nthe original image into Qwen2.5-VL and the VAE encoder to obtain semantic and\nreconstructive representations, respectively. This dual-encoding mechanism\nenables the editing module to strike a balance between preserving semantic\nconsistency and maintaining visual fidelity. Qwen-Image achieves\nstate-of-the-art performance, demonstrating its strong capabilities in both\nimage generation and editing across multiple benchmarks.\n","authors":["Chenfei Wu","Jiahao Li","Jingren Zhou","Junyang Lin","Kaiyuan Gao","Kun Yan","Sheng-ming Yin","Shuai Bai","Xiao Xu","Yilei Chen","Yuxiang Chen","Zecheng Tang","Zekai Zhang","Zhengyi Wang","An Yang","Bowen Yu","Chen Cheng","Dayiheng Liu","Deqing Li","Hang Zhang","Hao Meng","Hu Wei","Jingyuan Ni","Kai Chen","Kuan Cao","Liang Peng","Lin Qu","Minggang Wu","Peng Wang","Shuting Yu","Tingkun Wen","Wensen Feng","Xiaoxiao Xu","Yi Wang","Yichang Zhang","Yongqiang Zhu","Yujia Wu","Yuxuan Cai","Zenan Liu"],"pdf_url":"https://arxiv.org/pdf/2508.02324v1.pdf","comment":"https://github.com/QwenLM/Qwen-Image"},{"id":"http://arxiv.org/abs/2508.02323v1","updated":"2025-08-04T11:43:12Z","published":"2025-08-04T11:43:12Z","title":"Dream-to-Recon: Monocular 3D Reconstruction with Diffusion-Depth\n  Distillation from Single Images","summary":"  Volumetric scene reconstruction from a single image is crucial for a broad\nrange of applications like autonomous driving and robotics. Recent volumetric\nreconstruction methods achieve impressive results, but generally require\nexpensive 3D ground truth or multi-view supervision. We propose to leverage\npre-trained 2D diffusion models and depth prediction models to generate\nsynthetic scene geometry from a single image. This can then be used to distill\na feed-forward scene reconstruction model. Our experiments on the challenging\nKITTI-360 and Waymo datasets demonstrate that our method matches or outperforms\nstate-of-the-art baselines that use multi-view supervision, and offers unique\nadvantages, for example regarding dynamic scenes.\n","authors":["Philipp Wulff","Felix Wimbauer","Dominik Muhle","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2508.02323v1.pdf","comment":"ICCV 2025. Website: https://philippwulff.github.io/dream-to-recon"},{"id":"http://arxiv.org/abs/2505.14754v2","updated":"2025-08-04T11:42:09Z","published":"2025-05-20T14:55:26Z","title":"Model-Independent Machine Learning Approach for Nanometric Axial\n  Localization and Tracking","summary":"  Accurately tracking particles and determining their coordinate along the\noptical axis is a major challenge in optical microscopy, especially when\nextremely high precision is needed. In this study, we introduce a deep learning\napproach using convolutional neural networks (CNNs) that can determine axial\ncoordinates from dual-focal-plane images without relying on predefined models.\nOur method achieves an axial localization precision of 40 nanometers-six times\nbetter than traditional single-focal-plane techniques. The model's simple\ndesign and strong performance make it suitable for a wide range of uses,\nincluding dark matter detection, proton therapy for cancer, and radiation\nprotection in space. It also shows promise in fields like biological imaging,\nmaterials science, and environmental monitoring. This work highlights how\nmachine learning can turn complex image data into reliable, precise\ninformation, offering a flexible and powerful tool for many scientific\napplications.\n","authors":["Andrey Alexandrov","Giovanni Acampora","Giovanni De Lellis","Antonia Di Crescenzo","Chiara Errico","Daria Morozova","Valeri Tioukov","Autilia Vittiello"],"pdf_url":"https://arxiv.org/pdf/2505.14754v2.pdf","comment":"13 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2508.02320v1","updated":"2025-08-04T11:40:42Z","published":"2025-08-04T11:40:42Z","title":"Zero-shot Compositional Action Recognition with Neural Logic Constraints","summary":"  Zero-shot compositional action recognition (ZS-CAR) aims to identify unseen\nverb-object compositions in the videos by exploiting the learned knowledge of\nverb and object primitives during training. Despite compositional learning's\nprogress in ZS-CAR, two critical challenges persist: 1) Missing compositional\nstructure constraint, leading to spurious correlations between primitives; 2)\nNeglecting semantic hierarchy constraint, leading to semantic ambiguity and\nimpairing the training process. In this paper, we argue that human-like\nsymbolic reasoning offers a principled solution to these challenges by\nexplicitly modeling compositional and hierarchical structured abstraction. To\nthis end, we propose a logic-driven ZS-CAR framework LogicCAR that integrates\ndual symbolic constraints: Explicit Compositional Logic and Hierarchical\nPrimitive Logic. Specifically, the former models the restrictions within the\ncompositions, enhancing the compositional reasoning ability of our model. The\nlatter investigates the semantical dependencies among different primitives,\nempowering the models with fine-to-coarse reasoning capacity. By formalizing\nthese constraints in first-order logic and embedding them into neural network\narchitectures, LogicCAR systematically bridges the gap between symbolic\nabstraction and existing models. Extensive experiments on the Sth-com dataset\ndemonstrate that our LogicCAR outperforms existing baseline methods, proving\nthe effectiveness of our logic-driven constraints.\n","authors":["Gefan Ye","Lin Li","Kexin Li","Jun Xiao","Long chen"],"pdf_url":"https://arxiv.org/pdf/2508.02320v1.pdf","comment":"14 pages, 6 figures; Accepted by ACM MM2025"},{"id":"http://arxiv.org/abs/2508.02319v1","updated":"2025-08-04T11:37:59Z","published":"2025-08-04T11:37:59Z","title":"Is Uncertainty Quantification a Viable Alternative to Learned Deferral?","summary":"  Artificial Intelligence (AI) holds the potential to dramatically improve\npatient care. However, it is not infallible, necessitating\nhuman-AI-collaboration to ensure safe implementation. One aspect of AI safety\nis the models' ability to defer decisions to a human expert when they are\nlikely to misclassify autonomously. Recent research has focused on methods that\nlearn to defer by optimising a surrogate loss function that finds the optimal\ntrade-off between predicting a class label or deferring. However, during\nclinical translation, models often face challenges such as data shift.\nUncertainty quantification methods aim to estimate a model's confidence in its\npredictions. However, they may also be used as a deferral strategy which does\nnot rely on learning from specific training distribution. We hypothesise that\nmodels developed to quantify uncertainty are more robust to out-of-distribution\n(OOD) input than learned deferral models that have been trained in a supervised\nfashion. To investigate this hypothesis, we constructed an extensive evaluation\nstudy on a large ophthalmology dataset, examining both learned deferral models\nand established uncertainty quantification methods, assessing their performance\nin- and out-of-distribution. Specifically, we evaluate their ability to\naccurately classify glaucoma from fundus images while deferring cases with a\nhigh likelihood of error. We find that uncertainty quantification methods may\nbe a promising choice for AI deferral.\n","authors":["Anna M. Wundram","Christian F. Baumgartner"],"pdf_url":"https://arxiv.org/pdf/2508.02319v1.pdf","comment":"Accepted as an oral presentation at MICCAI UNSURE 2025"},{"id":"http://arxiv.org/abs/2508.02307v1","updated":"2025-08-04T11:20:31Z","published":"2025-08-04T11:20:31Z","title":"Whole-body Representation Learning For Competing Preclinical Disease\n  Risk Assessment","summary":"  Reliable preclinical disease risk assessment is essential to move public\nhealthcare from reactive treatment to proactive identification and prevention.\nHowever, image-based risk prediction algorithms often consider one condition at\na time and depend on hand-crafted features obtained through segmentation tools.\nWe propose a whole-body self-supervised representation learning method for the\npreclinical disease risk assessment under a competing risk modeling. This\napproach outperforms whole-body radiomics in multiple diseases, including\ncardiovascular disease (CVD), type 2 diabetes (T2D), chronic obstructive\npulmonary disease (COPD), and chronic kidney disease (CKD). Simulating a\npreclinical screening scenario and subsequently combining with cardiac MRI, it\nsharpens further the prediction for CVD subgroups: ischemic heart disease\n(IHD), hypertensive diseases (HD), and stroke. The results indicate the\ntranslational potential of whole-body representations as a standalone screening\nmodality and as part of a multi-modal framework within clinical workflows for\nearly personalized risk stratification. The code is available at\nhttps://github.com/yayapa/WBRLforCR/\n","authors":["Dmitrii Seletkov","Sophie Starck","Ayhan Can Erdur","Yundi Zhang","Daniel Rueckert","Rickmer Braren"],"pdf_url":"https://arxiv.org/pdf/2508.02307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00496v2","updated":"2025-08-04T11:18:28Z","published":"2025-08-01T10:19:53Z","title":"LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast\n  Lesion Segmentation in Longitudinal DCE-MRI","summary":"  Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced\nMRI (DCE-MRI) is critical for early cancer detection, especially in high-risk\npatients. While recent deep learning methods have advanced lesion segmentation,\nthey primarily target large lesions and neglect valuable longitudinal and\nclinical information routinely used by radiologists. In real-world screening,\ndetecting subtle or emerging lesions requires radiologists to compare across\ntimepoints and consider previous radiology assessments, such as the BI-RADS\nscore. We propose LesiOnTime, a novel 3D segmentation approach that mimics\nclinical diagnostic workflows by jointly leveraging longitudinal imaging and\nBIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)\nblock that dynamically integrates information from previous and current scans;\nand (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent\nspace alignment for scans with similar radiological assessments, thus embedding\ndomain knowledge into the training process. Evaluated on a curated in-house\nlongitudinal dataset of high-risk patients with DCE-MRI, our approach\noutperforms state-of-the-art single-timepoint and longitudinal baselines by 5%\nin terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute\ncomplementary performance gains. These results highlight the importance of\nincorporating temporal and clinical context for reliable early lesion\nsegmentation in real-world breast cancer screening. Our code is publicly\navailable at https://github.com/cirmuw/LesiOnTime\n","authors":["Mohammed Kamran","Maria Bernathova","Raoul Varga","Christian F. Singer","Zsuzsanna Bago-Horvath","Thomas Helbich","Georg Langs","Philipp Seeböck"],"pdf_url":"https://arxiv.org/pdf/2508.00496v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02293v1","updated":"2025-08-04T11:03:12Z","published":"2025-08-04T11:03:12Z","title":"Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning","summary":"  So-called unsupervised anomaly detection is better described as\nsemi-supervised, as it assumes all training data are nominal. This assumption\nsimplifies training but requires manual data curation, introducing bias and\nlimiting adaptability. We propose Confident Meta-learning (CoMet), a novel\ntraining strategy that enables deep anomaly detection models to learn from\nuncurated datasets where nominal and anomalous samples coexist, eliminating the\nneed for explicit filtering. Our approach integrates Soft Confident Learning,\nwhich assigns lower weights to low-confidence samples, and Meta-Learning, which\nstabilizes training by regularizing updates based on training validation loss\ncovariance. This prevents overfitting and enhances robustness to noisy data.\nCoMet is model-agnostic and can be applied to any anomaly detection method\ntrainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2\nwith two state-of-the-art models demonstrate the effectiveness of our approach,\nconsistently improving over the baseline methods, remaining insensitive to\nanomalies in the training set, and setting a new state-of-the-art across all\ndatasets.\n","authors":["Muhammad Aqeel","Shakiba Sharifi","Marco Cristani","Francesco Setti"],"pdf_url":"https://arxiv.org/pdf/2508.02293v1.pdf","comment":"Accepted to ieee/cvf international conference on computer vision\n  (ICCV2025)"},{"id":"http://arxiv.org/abs/2508.02288v1","updated":"2025-08-04T10:57:03Z","published":"2025-08-04T10:57:03Z","title":"Unleashing the Temporal Potential of Stereo Event Cameras for\n  Continuous-Time 3D Object Detection","summary":"  3D object detection is essential for autonomous systems, enabling precise\nlocalization and dimension estimation. While LiDAR and RGB cameras are widely\nused, their fixed frame rates create perception gaps in high-speed scenarios.\nEvent cameras, with their asynchronous nature and high temporal resolution,\noffer a solution by capturing motion continuously. The recent approach, which\nintegrates event cameras with conventional sensors for continuous-time\ndetection, struggles in fast-motion scenarios due to its dependency on\nsynchronized sensors. We propose a novel stereo 3D object detection framework\nthat relies solely on event cameras, eliminating the need for conventional 3D\nsensors. To compensate for the lack of semantic and geometric information in\nevent data, we introduce a dual filter mechanism that extracts both.\nAdditionally, we enhance regression by aligning bounding boxes with\nobject-centric information. Experiments show that our method outperforms prior\napproaches in dynamic environments, demonstrating the potential of event\ncameras for robust, continuous-time 3D perception. The code is available at\nhttps://github.com/mickeykang16/Ev-Stereo3D.\n","authors":["Jae-Young Kang","Hoonhee Cho","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2508.02288v1.pdf","comment":"Accepted to ICCV 2025"},{"id":"http://arxiv.org/abs/2508.02281v1","updated":"2025-08-04T10:52:42Z","published":"2025-08-04T10:52:42Z","title":"Do Edges Matter? Investigating Edge-Enhanced Pre-Training for Medical\n  Image Segmentation","summary":"  Medical image segmentation is crucial for disease diagnosis and treatment\nplanning, yet developing robust segmentation models often requires substantial\ncomputational resources and large datasets. Existing research shows that\npre-trained and finetuned foundation models can boost segmentation performance.\nHowever, questions remain about how particular image preprocessing steps may\ninfluence segmentation performance across different medical imaging modalities.\nIn particular, edges-abrupt transitions in pixel intensity-are widely\nacknowledged as vital cues for object boundaries but have not been\nsystematically examined in the pre-training of foundation models. We address\nthis gap by investigating to which extend pre-training with data processed\nusing computationally efficient edge kernels, such as kirsch, can improve\ncross-modality segmentation capabilities of a foundation model. Two versions of\na foundation model are first trained on either raw or edge-enhanced data across\nmultiple medical imaging modalities, then finetuned on selected raw subsets\ntailored to specific medical modalities. After systematic investigation using\nthe medical domains Dermoscopy, Fundus, Mammography, Microscopy, OCT, US, and\nXRay, we discover both increased and reduced segmentation performance across\nmodalities using edge-focused pre-training, indicating the need for a selective\napplication of this approach. To guide such selective applications, we propose\na meta-learning strategy. It uses standard deviation and image entropy of the\nraw image to choose between a model pre-trained on edge-enhanced or on raw data\nfor optimal performance. Our experiments show that integrating this\nmeta-learning layer yields an overall segmentation performance improvement\nacross diverse medical imaging tasks by 16.42% compared to models pre-trained\non edge-enhanced data only and 19.30% compared to models pre-trained on raw\ndata only.\n","authors":["Paul Zaha","Lars Böcking","Simeon Allmendinger","Leopold Müller","Niklas Kühl"],"pdf_url":"https://arxiv.org/pdf/2508.02281v1.pdf","comment":"11 pages, 5 figures, Third International Workshop on Data Engineering\n  in Medical Imaging (DEMI 2025)"},{"id":"http://arxiv.org/abs/2412.17632v3","updated":"2025-08-04T10:47:24Z","published":"2024-12-23T15:08:08Z","title":"D-Judge: How Far Are We? Evaluating the Discrepancies Between\n  AI-synthesized Images and Natural Images through Multimodal Guidance","summary":"  In the rapidly evolving field of Artificial Intelligence Generated Content\n(AIGC), a central challenge is distinguishing AI-synthesized images from\nnatural images. Despite the impressive capabilities of advanced AI generative\nmodels in producing visually compelling content, significant discrepancies\nremain when compared to natural images. To systematically investigate and\nquantify these differences, we construct a large-scale multimodal dataset named\nDANI, comprising 5,000 natural images and over 440,000 AI-generated image\n(AIGI) samples produced by nine representative models using both unimodal and\nmultimodal prompts, including Text-to-Image (T2I), Image-to-Image (I2I), and\nText and Image-to-Image (TI2I). We then introduce D-Judge, a benchmark designed\nto answer the critical question: how far are AI-generated images from truly\nrealistic images? Our fine-grained evaluation framework assesses DANI across\nfive key dimensions: naive visual quality, semantic alignment, aesthetic\nappeal, downstream task applicability, and coordinated human validation.\nExtensive experiments reveal substantial discrepancies across these dimensions,\nhighlighting the importance of aligning quantitative metrics with human\njudgment to achieve a comprehensive understanding of AI-generated image\nquality. The code and dataset are publicly available at:\nhttps://github.com/ryliu68/DJudge and\nhttps://huggingface.co/datasets/Renyang/DANI.\n","authors":["Renyang Liu","Ziyu Lyu","Wei Zhou","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2412.17632v3.pdf","comment":"Accepted by ACM MM 2025"},{"id":"http://arxiv.org/abs/2508.02278v1","updated":"2025-08-04T10:46:53Z","published":"2025-08-04T10:46:53Z","title":"SGAD: Semantic and Geometric-aware Descriptor for Local Feature Matching","summary":"  Local feature matching remains a fundamental challenge in computer vision.\nRecent Area to Point Matching (A2PM) methods have improved matching accuracy.\nHowever, existing research based on this framework relies on inefficient\npixel-level comparisons and complex graph matching that limit scalability. In\nthis work, we introduce the Semantic and Geometric-aware Descriptor Network\n(SGAD), which fundamentally rethinks area-based matching by generating highly\ndiscriminative area descriptors that enable direct matching without complex\ngraph optimization. This approach significantly improves both accuracy and\nefficiency of area matching. We further improve the performance of area\nmatching through a novel supervision strategy that decomposes the area matching\ntask into classification and ranking subtasks. Finally, we introduce the\nHierarchical Containment Redundancy Filter (HCRF) to eliminate overlapping\nareas by analyzing containment graphs. SGAD demonstrates remarkable performance\ngains, reducing runtime by 60x (0.82s vs. 60.23s) compared to MESA. Extensive\nevaluations show consistent improvements across multiple point matchers:\nSGAD+LoFTR reduces runtime compared to DKM, while achieving higher accuracy\n(0.82s vs. 1.51s, 65.98 vs. 61.11) in outdoor pose estimation, and SGAD+ROMA\ndelivers +7.39% AUC@5{\\deg} in indoor pose estimation, establishing a new\nstate-of-the-art.\n","authors":["Xiangzeng Liu","Chi Wang","Guanglu Shi","Xiaodong Zhang","Qiguang Miao","Miao Fan"],"pdf_url":"https://arxiv.org/pdf/2508.02278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02265v1","updated":"2025-08-04T10:15:53Z","published":"2025-08-04T10:15:53Z","title":"Semi-Supervised Dual-Threshold Contrastive Learning for Ultrasound Image\n  Classification and Segmentation","summary":"  Confidence-based pseudo-label selection usually generates overly confident\nyet incorrect predictions, due to the early misleadingness of model and\noverfitting inaccurate pseudo-labels in the learning process, which heavily\ndegrades the performance of semi-supervised contrastive learning. Moreover,\nsegmentation and classification tasks are treated independently and the\naffinity fails to be fully explored. To address these issues, we propose a\nnovel semi-supervised dual-threshold contrastive learning strategy for\nultrasound image classification and segmentation, named Hermes. This strategy\ncombines the strengths of contrastive learning with semi-supervised learning,\nwhere the pseudo-labels assist contrastive learning by providing additional\nguidance. Specifically, an inter-task attention and saliency module is also\ndeveloped to facilitate information sharing between the segmentation and\nclassification tasks. Furthermore, an inter-task consistency learning strategy\nis designed to align tumor features across both tasks, avoiding negative\ntransfer for reducing features discrepancy. To solve the lack of publicly\navailable ultrasound datasets, we have collected the SZ-TUS dataset, a thyroid\nultrasound image dataset. Extensive experiments on two public ultrasound\ndatasets and one private dataset demonstrate that Hermes consistently\noutperforms several state-of-the-art methods across various semi-supervised\nsettings.\n","authors":["Peng Zhang","Zhihui Lai","Heng Kong"],"pdf_url":"https://arxiv.org/pdf/2508.02265v1.pdf","comment":"Accepted in ECAI 2025"},{"id":"http://arxiv.org/abs/2508.02261v1","updated":"2025-08-04T10:09:31Z","published":"2025-08-04T10:09:31Z","title":"SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene\n  Completion","summary":"  Monocular 3D Semantic Scene Completion (SSC) is a challenging yet promising\ntask that aims to infer dense geometric and semantic descriptions of a scene\nfrom a single image. While recent object-centric paradigms significantly\nimprove efficiency by leveraging flexible 3D Gaussian primitives, they still\nrely heavily on a large number of randomly initialized primitives, which\ninevitably leads to 1) inefficient primitive initialization and 2) outlier\nprimitives that introduce erroneous artifacts. In this paper, we propose\nSplatSSC, a novel framework that resolves these limitations with a depth-guided\ninitialization strategy and a principled Gaussian aggregator. Instead of random\ninitialization, SplatSSC utilizes a dedicated depth branch composed of a\nGroup-wise Multi-scale Fusion (GMF) module, which integrates multi-scale image\nand depth features to generate a sparse yet representative set of initial\nGaussian primitives. To mitigate noise from outlier primitives, we develop the\nDecoupled Gaussian Aggregator (DGA), which enhances robustness by decomposing\ngeometric and semantic predictions during the Gaussian-to-voxel splatting\nprocess. Complemented with a specialized Probability Scale Loss, our method\nachieves state-of-the-art performance on the Occ-ScanNet dataset, outperforming\nprior approaches by over 6.3% in IoU and 4.1% in mIoU, while reducing both\nlatency and memory consumption by more than 9.3%. The code will be released\nupon acceptance.\n","authors":["Rui Qian","Haozhi Cao","Tianchen Deng","Shenghai Yuan","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2508.02261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02258v1","updated":"2025-08-04T10:03:08Z","published":"2025-08-04T10:03:08Z","title":"Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented\n  Generation for Pathology VLMs via Reinforcement Learning","summary":"  Although Vision Language Models (VLMs) have shown strong generalization in\nmedical imaging, pathology presents unique challenges due to ultra-high\nresolution, complex tissue structures, and nuanced clinical semantics. These\nfactors make pathology VLMs prone to hallucinations, i.e., generating outputs\ninconsistent with visual evidence, which undermines clinical trust. Existing\nRAG approaches in this domain largely depend on text-based knowledge bases,\nlimiting their ability to leverage diagnostic visual cues. To address this, we\npropose Patho-AgenticRAG, a multimodal RAG framework with a database built on\npage-level embeddings from authoritative pathology textbooks. Unlike\ntraditional text-only retrieval systems, it supports joint text-image search,\nenabling direct retrieval of textbook pages that contain both the queried text\nand relevant visual cues, thus avoiding the loss of critical image-based\ninformation. Patho-AgenticRAG also supports reasoning, task decomposition, and\nmulti-turn search interactions, improving accuracy in complex diagnostic\nscenarios. Experiments show that Patho-AgenticRAG significantly outperforms\nexisting multimodal models in complex pathology tasks like multiple-choice\ndiagnosis and visual question answering. Our project is available at the\nPatho-AgenticRAG repository:\nhttps://github.com/Wenchuan-Zhang/Patho-AgenticRAG.\n","authors":["Wenchuan Zhang","Jingru Guo","Hengzhe Zhang","Penghao Zhang","Jie Chen","Shuwan Zhang","Zhang Zhang","Yuhao Yi","Hong Bu"],"pdf_url":"https://arxiv.org/pdf/2508.02258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06593v3","updated":"2025-08-04T10:01:54Z","published":"2025-02-10T15:56:28Z","title":"SAGI: Semantically Aligned and Uncertainty Guided AI Image Inpainting","summary":"  Recent advancements in generative AI have made text-guided image inpainting -\nadding, removing, or altering image regions using textual prompts - widely\naccessible. However, generating semantically correct photorealistic imagery,\ntypically requires carefully-crafted prompts and iterative refinement by\nevaluating the realism of the generated content - tasks commonly performed by\nhumans. To automate the generative process, we propose Semantically Aligned and\nUncertainty Guided AI Image Inpainting (SAGI), a model-agnostic pipeline, to\nsample prompts from a distribution that closely aligns with human perception\nand to evaluate the generated content and discard instances that deviate from\nsuch a distribution, which we approximate using pretrained large language\nmodels and vision-language models. By applying this pipeline on multiple\nstate-of-the-art inpainting models, we create the SAGI Dataset (SAGI-D),\ncurrently the largest and most diverse dataset of AI-generated inpaintings,\ncomprising over 95k inpainted images and a human-evaluated subset. Our\nexperiments show that semantic alignment significantly improves image quality\nand aesthetics, while uncertainty guidance effectively identifies realistic\nmanipulations - human ability to distinguish inpainted images from real ones\ndrops from 74% to 35% in terms of accuracy, after applying our pipeline.\nMoreover, using SAGI-D for training several image forensic approaches increases\nin-domain detection performance on average by 37.4% and out-of-domain\ngeneralization by 26.1% in terms of IoU, also demonstrating its utility in\ncountering malicious exploitation of generative AI. Code and dataset are\navailable at https://mever-team.github.io/SAGI/\n","authors":["Paschalis Giakoumoglou","Dimitrios Karageorgiou","Symeon Papadopoulos","Panagiotis C. Petrantonakis"],"pdf_url":"https://arxiv.org/pdf/2502.06593v3.pdf","comment":"ICCV2025"},{"id":"http://arxiv.org/abs/2508.02254v1","updated":"2025-08-04T10:01:12Z","published":"2025-08-04T10:01:12Z","title":"Semi-Supervised Semantic Segmentation via Derivative Label Propagation","summary":"  Semi-supervised semantic segmentation, which leverages a limited set of\nlabeled images, helps to relieve the heavy annotation burden. While\npseudo-labeling strategies yield promising results, there is still room for\nenhancing the reliability of pseudo-labels. Hence, we develop a semi-supervised\nframework, namely DerProp, equipped with a novel derivative label propagation\nto rectify imperfect pseudo-labels. Our label propagation method imposes\ndiscrete derivative operations on pixel-wise feature vectors as additional\nregularization, thereby generating strictly regularized similarity metrics.\nDoing so effectively alleviates the ill-posed problem that identical\nsimilarities correspond to different features, through constraining the\nsolution space. Extensive experiments are conducted to verify the rationality\nof our design, and demonstrate our superiority over other methods. Codes are\navailable at https://github.com/ForawardStar/DerProp/.\n","authors":["Yuanbin Fu","Xiaojie Guo"],"pdf_url":"https://arxiv.org/pdf/2508.02254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02243v1","updated":"2025-08-04T09:43:54Z","published":"2025-08-04T09:43:54Z","title":"I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal\n  Entity Linking","summary":"  Multimodal entity linking plays a crucial role in a wide range of\napplications. Recent advances in large language model-based methods have become\nthe dominant paradigm for this task, effectively leveraging both textual and\nvisual modalities to enhance performance. Despite their success, these methods\nstill face two challenges, including unnecessary incorporation of image data in\ncertain scenarios and the reliance only on a one-time extraction of visual\nfeatures, which can undermine their effectiveness and accuracy. To address\nthese challenges, we propose a novel LLM-based framework for the multimodal\nentity linking task, called Intra- and Inter-modal Collaborative Reflections.\nThis framework prioritizes leveraging text information to address the task.\nWhen text alone is insufficient to link the correct entity through intra- and\ninter-modality evaluations, it employs a multi-round iterative strategy that\nintegrates key visual clues from various aspects of the image to support\nreasoning and enhance matching accuracy. Extensive experiments on three widely\nused public datasets demonstrate that our framework consistently outperforms\ncurrent state-of-the-art methods in the task, achieving improvements of 3.2%,\n5.1%, and 1.6%, respectively. Our code is available at\nhttps://github.com/ziyan-xiaoyu/I2CR/.\n","authors":["Ziyan Liu","Junwen Li","Kaiwen Li","Tong Ruan","Chao Wang","Xinyan He","Zongyu Wang","Xuezhi Cao","Jingping Liu"],"pdf_url":"https://arxiv.org/pdf/2508.02243v1.pdf","comment":"10 pages, 6 figures, accepted by ACMMM 2025"},{"id":"http://arxiv.org/abs/2508.02240v1","updated":"2025-08-04T09:39:31Z","published":"2025-08-04T09:39:31Z","title":"Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor","summary":"  Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}\n","authors":["Xiaoliu Guan","Lielin Jiang","Hanqi Chen","Xu Zhang","Jiaxing Yan","Guanzhong Wang","Yi Liu","Zetao Zhang","Yu Wu"],"pdf_url":"https://arxiv.org/pdf/2508.02240v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2508.02238v1","updated":"2025-08-04T09:37:00Z","published":"2025-08-04T09:37:00Z","title":"An Event-based Fast Intensity Reconstruction Scheme for UAV Real-time\n  Perception","summary":"  Event cameras offer significant advantages, including a wide dynamic range,\nhigh temporal resolution, and immunity to motion blur, making them highly\npromising for addressing challenging visual conditions. Extracting and\nutilizing effective information from asynchronous event streams is essential\nfor the onboard implementation of event cameras. In this paper, we propose a\nstreamlined event-based intensity reconstruction scheme, event-based single\nintegration (ESI), to address such implementation challenges. This method\nguarantees the portability of conventional frame-based vision methods to\nevent-based scenarios and maintains the intrinsic advantages of event cameras.\nThe ESI approach reconstructs intensity images by performing a single\nintegration of the event streams combined with an enhanced decay algorithm.\nSuch a method enables real-time intensity reconstruction at a high frame rate,\ntypically 100 FPS. Furthermore, the relatively low computation load of ESI fits\nonboard implementation suitably, such as in UAV-based visual tracking\nscenarios. Extensive experiments have been conducted to evaluate the\nperformance comparison of ESI and state-of-the-art algorithms. Compared to\nstate-of-the-art algorithms, ESI demonstrates remarkable runtime efficiency\nimprovements, superior reconstruction quality, and a high frame rate. As a\nresult, ESI enhances UAV onboard perception significantly under visual\nadversary surroundings. In-flight tests, ESI demonstrates effective performance\nfor UAV onboard visual tracking under extremely low illumination\nconditions(2-10lux), whereas other comparative algorithms fail due to\ninsufficient frame rate, poor image quality, or limited real-time performance.\n","authors":["Xin Dong","Yiwei Zhang","Yangjie Cui","Jinwu Xiang","Daochun Li","Zhan Tu"],"pdf_url":"https://arxiv.org/pdf/2508.02238v1.pdf","comment":"A supplementary video is available at https://youtu.be/tLzXjXVRkVg"},{"id":"http://arxiv.org/abs/2507.01607v2","updated":"2025-08-04T09:21:10Z","published":"2025-07-02T11:21:27Z","title":"Survivability of Backdoor Attacks on Unconstrained Face Recognition\n  Systems","summary":"  The widespread use of deep learning face recognition raises several security\nconcerns. Although prior works point at existing vulnerabilities, DNN backdoor\nattacks against real-life, unconstrained systems dealing with images captured\nin the wild remain a blind spot of the literature. This paper conducts the\nfirst system-level study of backdoors in deep learning-based face recognition\nsystems. This paper yields four contributions by exploring the feasibility of\nDNN backdoors on these pipelines in a holistic fashion. We demonstrate for the\nfirst time two backdoor attacks on the face detection task: face generation and\nface landmark shift attacks. We then show that face feature extractors trained\nwith large margin losses also fall victim to backdoor attacks. Combining our\nmodels, we then show using 20 possible pipeline configurations and 15 attack\ncases that a single backdoor enables an attacker to bypass the entire function\nof a system. Finally, we provide stakeholders with several best practices and\ncountermeasures.\n","authors":["Quentin Le Roux","Yannick Teglia","Teddy Furon","Philippe Loubet-Moundi","Eric Bourbao"],"pdf_url":"https://arxiv.org/pdf/2507.01607v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02220v1","updated":"2025-08-04T09:11:51Z","published":"2025-08-04T09:11:51Z","title":"Welcome New Doctor: Continual Learning with Expert Consultation and\n  Autoregressive Inference for Whole Slide Image Analysis","summary":"  Whole Slide Image (WSI) analysis, with its ability to reveal detailed tissue\nstructures in magnified views, plays a crucial role in cancer diagnosis and\nprognosis. Due to their giga-sized nature, WSIs require substantial storage and\ncomputational resources for processing and training predictive models. With the\nrapid increase in WSIs used in clinics and hospitals, there is a growing need\nfor a continual learning system that can efficiently process and adapt existing\nmodels to new tasks without retraining or fine-tuning on previous tasks. Such a\nsystem must balance resource efficiency with high performance. In this study,\nwe introduce COSFormer, a Transformer-based continual learning framework\ntailored for multi-task WSI analysis. COSFormer is designed to learn\nsequentially from new tasks wile avoiding the need to revisit full historical\ndatasets. We evaluate COSFormer on a sequence of seven WSI datasets covering\nseven organs and six WSI-related tasks under both class-incremental and\ntask-incremental settings. The results demonstrate COSFormer's superior\ngeneralizability and effectiveness compared to existing continual learning\nframeworks, establishing it as a robust solution for continual WSI analysis in\nclinical applications.\n","authors":["Doanh Cao Bui","Jin Tae Kwak"],"pdf_url":"https://arxiv.org/pdf/2508.02220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10857v2","updated":"2025-08-04T09:11:48Z","published":"2025-06-12T16:17:17Z","title":"VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos","summary":"  We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 960 long videos (with an average duration of\n1.6 hours), along with 8,243 human-labeled multi-step question-answering pairs\nand 25,106 reasoning steps with timestamps. These videos are curated via a\nmulti-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 19 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.\n","authors":["Jiashuo Yu","Yue Wu","Meng Chu","Zhifei Ren","Zizheng Huang","Pei Chu","Ruijie Zhang","Yinan He","Qirui Li","Songze Li","Zhenxiang Li","Zhongying Tu","Conghui He","Yu Qiao","Yali Wang","Yi Wang","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2506.10857v2.pdf","comment":"ICCV2025"},{"id":"http://arxiv.org/abs/2508.00733v2","updated":"2025-08-04T09:11:26Z","published":"2025-08-01T16:03:57Z","title":"AudioGen-Omni: A Unified Multimodal Diffusion Transformer for\n  Video-Synchronized Audio, Speech, and Song Generation","summary":"  We present AudioGen-Omni - a unified approach based on multimodal diffusion\ntransformers (MMDit), capable of generating high-fidelity audio, speech, and\nsongs coherently synchronized with the input video. AudioGen-Omni introduces a\nnovel joint training paradigm that seamlessly integrates large-scale\nvideo-text-audio corpora, enabling a model capable of generating semantically\nrich, acoustically diverse audio conditioned on multimodal inputs and adaptable\nto a wide range of audio generation tasks. AudioGen-Omni employs a unified\nlyrics-transcription encoder that encodes graphemes and phonemes from both sung\nand spoken inputs into dense frame-level representations. Dense frame-level\nrepresentations are fused using an AdaLN-based joint attention mechanism\nenhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein\nRoPE is selectively applied to temporally structured modalities to ensure\nprecise and robust cross-modal alignment. By unfreezing all modalities and\nmasking missing inputs, AudioGen-Omni mitigates the semantic constraints of\ntext-frozen paradigms, enabling effective cross-modal conditioning. This joint\ntraining approach enhances audio quality, semantic alignment, and lip-sync\naccuracy, while also achieving state-of-the-art results on\nText-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8\nseconds of audio, it offers substantial improvements in both efficiency and\ngenerality.\n","authors":["Le Wang","Jun Wang","Feng Deng","Chen Zhang","Di Zhang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2508.00733v2.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.14959v2","updated":"2025-08-04T09:09:13Z","published":"2025-07-20T13:39:50Z","title":"Polymorph: Energy-Efficient Multi-Label Classification for Video Streams\n  on Embedded Devices","summary":"  Real-time multi-label video classification on embedded devices is constrained\nby limited compute and energy budgets. Yet, video streams exhibit structural\nproperties such as label sparsity, temporal continuity, and label co-occurrence\nthat can be leveraged for more efficient inference. We introduce Polymorph, a\ncontext-aware framework that activates a minimal set of lightweight Low Rank\nAdapters (LoRA) per frame. Each adapter specializes in a subset of classes\nderived from co-occurrence patterns and is implemented as a LoRA weight over a\nshared backbone. At runtime, Polymorph dynamically selects and composes only\nthe adapters needed to cover the active labels, avoiding full-model switching\nand weight merging. This modular strategy improves scalability while reducing\nlatency and energy overhead. Polymorph achieves 40% lower energy consumption\nand improves mAP by 9 points over strong baselines on the TAO dataset.\nPolymorph is open source at https://github.com/inference-serving/polymorph/.\n","authors":["Saeid Ghafouri","Mohsen Fayyaz","Xiangchen Li","Deepu John","Bo Ji","Dimitrios Nikolopoulos","Hans Vandierendonck"],"pdf_url":"https://arxiv.org/pdf/2507.14959v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02192v1","updated":"2025-08-04T08:42:23Z","published":"2025-08-04T08:42:23Z","title":"CMIC: Content-Adaptive Mamba for Learned Image Compression","summary":"  Recent Learned image compression (LIC) leverages Mamba-style state-space\nmodels (SSMs) for global receptive fields with linear complexity. However,\nvanilla Mamba is content-agnostic, relying on fixed and predefined selective\nscans, which restricts its ability to dynamically and fully exploit content\ndependencies. We introduce Content-Adaptive Mamba (CAM), a dynamic SSM that\naddresses two critical limitations. First, it employs content-aware token\nreorganization, clustering and reordering tokens based on content similarity to\nprioritize proximity in feature space over Euclidean space. Second, it\nintegrates global priors into SSM via a prompt dictionary, effectively\nmitigating the strict causality and long-range decay in the token interactions\nof Mamba. These innovations enable CAM to better capture global dependencies\nwhile preserving computational efficiency. Leveraging CAM, our Content-Adaptive\nMamba-based LIC model (CMIC) achieves state-of-the-art rate-distortion\nperformance, surpassing VTM-21.0 by -15.91\\%, -21.34\\%, and -17.58\\% BD-rate on\nKodak, Tecnick, and CLIC benchmarks, respectively.\n","authors":["Yunuo Chen","Zezheng Lyu","Bing He","Hongwei Hu","Qi Wang","Yuan Tian","Li Song","Wenjun Zhang","Guo Lu"],"pdf_url":"https://arxiv.org/pdf/2508.02192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17457v2","updated":"2025-08-04T08:35:27Z","published":"2025-05-23T04:33:54Z","title":"Hypergraph Mamba for Efficient Whole Slide Image Understanding","summary":"  Whole Slide Images (WSIs) in histopathology pose a significant challenge for\nextensive medical image analysis due to their ultra-high resolution, massive\nscale, and intricate spatial relationships. Although existing Multiple Instance\nLearning (MIL) approaches like Graph Neural Networks (GNNs) and Transformers\ndemonstrate strong instance-level modeling capabilities, they encounter\nconstraints regarding scalability and computational expenses. To overcome these\nlimitations, we introduce the WSI-HGMamba, a novel framework that unifies the\nhigh-order relational modeling capabilities of the Hypergraph Neural Networks\n(HGNNs) with the linear-time sequential modeling efficiency of the State Space\nModels. At the core of our design is the HGMamba block, which integrates\nmessage passing, hypergraph scanning & flattening, and bidirectional state\nspace modeling (Bi-SSM), enabling the model to retain both relational and\ncontextual cues while remaining computationally efficient. Compared to\nTransformer and Graph Transformer counterparts, WSI-HGMamba achieves superior\nperformance with up to 7* reduction in FLOPs. Extensive experiments on multiple\npublic and private WSI benchmarks demonstrate that our method provides a\nscalable, accurate, and efficient solution for slide-level understanding,\nmaking it a promising backbone for next-generation pathology AI systems.\n","authors":["Jiaxuan Lu","Yuhui Lin","Junyan Shi","Fang Yan","Dongzhan Zhou","Yue Gao","Xiaosong Wang"],"pdf_url":"https://arxiv.org/pdf/2505.17457v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14796v3","updated":"2025-08-04T08:32:18Z","published":"2024-11-22T08:41:33Z","title":"Adaptive Hyper-Graph Convolution Network for Skeleton-based Human Action\n  Recognition with Virtual Connections","summary":"  The shared topology of human skeletons motivated the recent investigation of\ngraph convolutional network (GCN) solutions for action recognition. However,\nmost of the existing GCNs rely on the binary connection of two neighboring\nvertices (joints) formed by an edge (bone), overlooking the potential of\nconstructing multi-vertex convolution structures. Although some studies have\nattempted to utilize hyper-graphs to represent the topology, they rely on a\nfixed construction strategy, which limits their adaptivity in uncovering the\nintricate latent relationships within the action. In this paper, we address\nthis oversight and explore the merits of an adaptive hyper-graph convolutional\nnetwork (Hyper-GCN) to achieve the aggregation of rich semantic information\nconveyed by skeleton vertices. In particular, our Hyper-GCN adaptively\noptimises the hyper-graphs during training, revealing the action-driven\nmulti-vertex relations. Besides, virtual connections are often designed to\nsupport efficient feature aggregation, implicitly extending the spectrum of\ndependencies within the skeleton. By injecting virtual connections into\nhyper-graphs, the semantic clues of diverse action categories can be\nhighlighted. The results of experiments conducted on the NTU-60, NTU-120, and\nNW-UCLA datasets demonstrate the merits of our Hyper-GCN, compared to the\nstate-of-the-art methods. The code is available at\nhttps://github.com/6UOOON9/Hyper-GCN.\n","authors":["Youwei Zhou","Tianyang Xu","Cong Wu","Xiaojun Wu","Josef Kittler"],"pdf_url":"https://arxiv.org/pdf/2411.14796v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02187v1","updated":"2025-08-04T08:31:53Z","published":"2025-08-04T08:31:53Z","title":"A Moment Matching-Based Method for Sparse and Noisy Point Cloud\n  Registration","summary":"  Point cloud registration is a key step in robotic perception tasks, such as\nSimultaneous Localization and Mapping (SLAM). It is especially challenging in\nconditions with sparse points and heavy noise. Traditional registration\nmethods, such as Iterative Closest Point (ICP) and Normal Distributions\nTransform (NDT), often have difficulties in achieving a robust and accurate\nalignment under these conditions. In this paper, we propose a registration\nframework based on moment matching. In particular, the point clouds are\nregarded as i.i.d. samples drawn from the same distribution observed in the\nsource and target frames. We then match the generalized Gaussian Radial Basis\nmoments calculated from the point clouds to estimate the rigid transformation\nbetween two frames. Moreover, such method does not require explicit\npoint-to-point correspondences among the point clouds. We further show the\nconsistency of the proposed method. Experiments on synthetic and real-world\ndatasets show that our approach achieves higher accuracy and robustness than\nexisting methods. In addition, we integrate our framework into a 4D Radar SLAM\nsystem. The proposed method significantly improves the localization performance\nand achieves results comparable to LiDAR-based systems. These findings\ndemonstrate the potential of moment matching technique for robust point cloud\nregistration in sparse and noisy scenarios.\n","authors":["Xingyi Li","Han Zhang","Ziliang Wang","Yukai Yang","Weidong Chen"],"pdf_url":"https://arxiv.org/pdf/2508.02187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11616v2","updated":"2025-08-04T08:31:50Z","published":"2025-06-13T09:38:57Z","title":"Wi-CBR: Salient-aware Adaptive WiFi Sensing for Cross-domain Behavior\n  Recognition","summary":"  The challenge in WiFi-based cross-domain Behavior Recognition lies in the\nsignificant interference of domain-specific signals on gesture variation.\nHowever, previous methods alleviate this interference by mapping the phase from\nmultiple domains into a common feature space. If the Doppler Frequency Shift\n(DFS) signal is used to dynamically supplement the phase features to achieve\nbetter generalization, enabling model to not only explore a wider feature space\nbut also avoid potential degradation of gesture semantic information.\nSpecifically, we propose a novel Salient-aware Adaptive WiFi Sensing for\nCross-domain Behavior Recognition (Wi-CBR}, which constructs a dual-branch\nself-attention module that captures temporal features from phase information\nreflecting dynamic path length variations, while extracting spatial features\nfrom DFS correlated with motion velocity. Moreover, we design a Saliency\nGuidance Module that employs group attention mechanisms to mine critical\nactivity features, and utilizes gating mechanisms to optimize information\nentropy, facilitating feature fusion and enabling effective interaction between\nsalient and non-salient behavior characteristics. Extensive experiments on two\nlarge-scale public datasets (Widar3.0 and XRF55) demonstrate the superior\nperformance of our method in both in-domain and cross-domain scenarios.\n","authors":["Ruobei Zhang","Shengeng Tang","Huan Yan","Xiang Zhang","Jiabao Guo"],"pdf_url":"https://arxiv.org/pdf/2506.11616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02186v1","updated":"2025-08-04T08:31:45Z","published":"2025-08-04T08:31:45Z","title":"Failure Cases Are Better Learned But Boundary Says Sorry: Facilitating\n  Smooth Perception Change for Accuracy-Robustness Trade-Off in Adversarial\n  Training","summary":"  Adversarial Training (AT) is one of the most effective methods to train\nrobust Deep Neural Networks (DNNs). However, AT creates an inherent trade-off\nbetween clean accuracy and adversarial robustness, which is commonly attributed\nto the more complicated decision boundary caused by the insufficient learning\nof hard adversarial samples. In this work, we reveal a counterintuitive fact\nfor the first time: From the perspective of perception consistency, hard\nadversarial samples that can still attack the robust model after AT are already\nlearned better than those successfully defended. Thus, different from previous\nviews, we argue that it is rather the over-sufficient learning of hard\nadversarial samples that degrades the decision boundary and contributes to the\ntrade-off problem. Specifically, the excessive pursuit of perception\nconsistency would force the model to view the perturbations as noise and ignore\nthe information within them, which should have been utilized to induce a\nsmoother perception transition towards the decision boundary to support its\nestablishment to an appropriate location. In response, we define a new AT\nobjective named Robust Perception, encouraging the model perception to change\nsmoothly with input perturbations, based on which we propose a novel Robust\nPerception Adversarial Training (RPAT) method, effectively mitigating the\ncurrent accuracy-robustness trade-off. Experiments on CIFAR-10, CIFAR-100, and\nTiny-ImageNet with ResNet-18, PreActResNet-18, and WideResNet-34-10 demonstrate\nthe effectiveness of our method beyond four common baselines and 12\nstate-of-the-art (SOTA) works. The code is available at\nhttps://github.com/FlaAI/RPAT.\n","authors":["Yanyun Wang","Li Liu"],"pdf_url":"https://arxiv.org/pdf/2508.02186v1.pdf","comment":"2025 IEEE/CVF International Conference on Computer Vision (ICCV'25)"},{"id":"http://arxiv.org/abs/2508.02180v1","updated":"2025-08-04T08:24:19Z","published":"2025-08-04T08:24:19Z","title":"Test-Time Model Adaptation for Quantized Neural Networks","summary":"  Quantizing deep models prior to deployment is a widely adopted technique to\nspeed up inference for various real-time applications, such as autonomous\ndriving. However, quantized models often suffer from severe performance\ndegradation in dynamic environments with potential domain shifts and this\ndegradation is significantly more pronounced compared with their full-precision\ncounterparts, as shown by our theoretical and empirical illustrations. To\naddress the domain shift problem, test-time adaptation (TTA) has emerged as an\neffective solution by enabling models to learn adaptively from test data.\nUnfortunately, existing TTA methods are often impractical for quantized models\nas they typically rely on gradient backpropagation--an operation that is\nunsupported on quantized models due to vanishing gradients, as well as memory\nand latency constraints. In this paper, we focus on TTA for quantized models to\nimprove their robustness and generalization ability efficiently. We propose a\ncontinual zeroth-order adaptation (ZOA) framework that enables efficient model\nadaptation using only two forward passes, eliminating the computational burden\nof existing methods. Moreover, we propose a domain knowledge management scheme\nto store and reuse different domain knowledge with negligible memory\nconsumption, reducing the interference of different domain knowledge and\nfostering the knowledge accumulation during long-term adaptation. Experimental\nresults on three classical architectures, including quantized transformer-based\nand CNN-based models, demonstrate the superiority of our methods for quantized\nmodel adaptation. On the quantized W6A6 ViT-B model, our ZOA is able to achieve\na 5.0\\% improvement over the state-of-the-art FOA on ImageNet-C dataset. The\nsource code is available at https://github.com/DengZeshuai/ZOA.\n","authors":["Zeshuai Deng","Guohao Chen","Shuaicheng Niu","Hui Luo","Shuhai Zhang","Yifan Yang","Renjie Chen","Wei Luo","Mingkui Tan"],"pdf_url":"https://arxiv.org/pdf/2508.02180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02179v1","updated":"2025-08-04T08:22:39Z","published":"2025-08-04T08:22:39Z","title":"Weakly Supervised Multimodal Temporal Forgery Localization via Multitask\n  Learning","summary":"  The spread of Deepfake videos has caused a trust crisis and impaired social\nstability. Although numerous approaches have been proposed to address the\nchallenges of Deepfake detection and localization, there is still a lack of\nsystematic research on the weakly supervised multimodal fine-grained temporal\nforgery localization (WS-MTFL). In this paper, we propose a novel weakly\nsupervised multimodal temporal forgery localization via multitask learning\n(WMMT), which addresses the WS-MTFL under the multitask learning paradigm. WMMT\nachieves multimodal fine-grained Deepfake detection and temporal partial\nforgery localization using merely video-level annotations. Specifically, visual\nand audio modality detection are formulated as two binary classification tasks.\nThe multitask learning paradigm is introduced to integrate these tasks into a\nmultimodal task. Furthermore, WMMT utilizes a Mixture-of-Experts structure to\nadaptively select appropriate features and localization head, achieving\nexcellent flexibility and localization precision in WS-MTFL. A feature\nenhancement module with temporal property preserving attention mechanism is\nproposed to identify the intra- and inter-modality feature deviation and\nconstruct comprehensive video features. To further explore the temporal\ninformation for weakly supervised learning, an extensible deviation perceiving\nloss has been proposed, which aims to enlarge the deviation of adjacent\nsegments of the forged samples and reduce the deviation of genuine samples.\nExtensive experiments demonstrate the effectiveness of multitask learning for\nWS-MTFL, and the WMMT achieves comparable results to fully supervised\napproaches in several evaluation metrics.\n","authors":["Wenbo Xu","Wei Lu","Xiangyang Luo"],"pdf_url":"https://arxiv.org/pdf/2508.02179v1.pdf","comment":"13 pages,4 figures. arXiv admin note: text overlap with\n  arXiv:2507.16596"},{"id":"http://arxiv.org/abs/2508.02177v1","updated":"2025-08-04T08:21:18Z","published":"2025-08-04T08:21:18Z","title":"Deep classification algorithm for De-identification of DICOM medical\n  images","summary":"  Background : De-identification of DICOM (Digital Imaging and Communi-cations\nin Medicine) files is an essential component of medical image research.\nPersonal Identifiable Information (PII) and/or Personal Health Identifying\nInformation (PHI) need to be hidden or removed due to legal reasons. According\nto the Health Insurance Portability and Accountability Act (HIPAA) and privacy\nrules, also full-face photographic images and any compa-rable images are direct\nidentifiers and are considered protected health information that also need to\nbe de-identified. Objective : The study aimed to implement a method that permit\nto de-identify the PII and PHI information present in the header and burned on\nthe pixel data of DICOM. Methods : To execute the de-identification, we\nimplemented an algorithm based on the safe harbor method, defined by HIPAA. Our\nalgorithm uses input customizable parameter to classify and then possibly\nde-identify individual DICOM tags. Results : The most sensible information,\nlike names, history, personal data and institution were successfully\nrecognized. Conclusions : We developed a python algorithm that is able to\nclassify infor-mation present in a DICOM file. The flexibility provided by the\nuse of customi-zable input parameters, which allow the user to customize the\nentire process de-pending on the case (e.g., the language), makes the entire\nprogram very promis-ing for both everyday use and research purposes. Our code\nis available at https://github.com/rtdicomexplorer/deep_deidentification.\n","authors":["Bufano Michele","Kotter Elmar"],"pdf_url":"https://arxiv.org/pdf/2508.02177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02172v1","updated":"2025-08-04T08:12:44Z","published":"2025-08-04T08:12:44Z","title":"GaussianCross: Cross-modal Self-supervised 3D Representation Learning\n  via Gaussian Splatting","summary":"  The significance of informative and robust point representations has been\nwidely acknowledged for 3D scene understanding. Despite existing\nself-supervised pre-training counterparts demonstrating promising performance,\nthe model collapse and structural information deficiency remain prevalent due\nto insufficient point discrimination difficulty, yielding unreliable\nexpressions and suboptimal performance. In this paper, we present\nGaussianCross, a novel cross-modal self-supervised 3D representation learning\narchitecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques\nto address current challenges. GaussianCross seamlessly converts\nscale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian\nrepresentation without missing details, enabling stable and generalizable\npre-training. Subsequently, a tri-attribute adaptive distillation splatting\nmodule is incorporated to construct a 3D feature field, facilitating synergetic\nfeature capturing of appearance, geometry, and semantic cues to maintain\ncross-modal consistency. To validate GaussianCross, we perform extensive\nevaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In\nparticular, GaussianCross shows a prominent parameter and data efficiency,\nachieving superior performance through linear probing (<0.1% parameters) and\nlimited data training (1% of scenes) compared to state-of-the-art methods.\nFurthermore, GaussianCross demonstrates strong generalization capabilities,\nimproving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on\nScanNet200 semantic and instance segmentation tasks, respectively, supporting\nthe effectiveness of our approach. The code, weights, and visualizations are\npublicly available at\n\\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.\n","authors":["Lei Yao","Yi Wang","Yi Zhang","Moyun Liu","Lap-Pui Chau"],"pdf_url":"https://arxiv.org/pdf/2508.02172v1.pdf","comment":"14 pages, 8 figures, accepted by MM'25"},{"id":"http://arxiv.org/abs/2507.16596v2","updated":"2025-08-04T08:10:14Z","published":"2025-07-22T13:55:16Z","title":"A Multimodal Deviation Perceiving Framework for Weakly-Supervised\n  Temporal Forgery Localization","summary":"  Current researches on Deepfake forensics often treat detection as a\nclassification task or temporal forgery localization problem, which are usually\nrestrictive, time-consuming, and challenging to scale for large datasets. To\nresolve these issues, we present a multimodal deviation perceiving framework\nfor weakly-supervised temporal forgery localization (MDP), which aims to\nidentify temporal partial forged segments using only video-level annotations.\nThe MDP proposes a novel multimodal interaction mechanism (MI) and an\nextensible deviation perceiving loss to perceive multimodal deviation, which\nachieves the refined start and end timestamps localization of forged segments.\nSpecifically, MI introduces a temporal property preserving cross-modal\nattention to measure the relevance between the visual and audio modalities in\nthe probabilistic embedding space. It could identify the inter-modality\ndeviation and construct comprehensive video features for temporal forgery\nlocalization. To explore further temporal deviation for weakly-supervised\nlearning, an extensible deviation perceiving loss has been proposed, aiming at\nenlarging the deviation of adjacent segments of the forged samples and reducing\nthat of genuine samples. Extensive experiments demonstrate the effectiveness of\nthe proposed framework and achieve comparable results to fully-supervised\napproaches in several evaluation metrics.\n","authors":["Wenbo Xu","Junyan Wu","Wei Lu","Xiangyang Luo","Qian Wang"],"pdf_url":"https://arxiv.org/pdf/2507.16596v2.pdf","comment":"9 pages, 3 figures,conference"},{"id":"http://arxiv.org/abs/2508.02168v1","updated":"2025-08-04T08:07:03Z","published":"2025-08-04T08:07:03Z","title":"After the Party: Navigating the Mapping From Color to Ambient Lighting","summary":"  Illumination in practical scenarios is inherently complex, involving colored\nlight sources, occlusions, and diverse material interactions that produce\nintricate reflectance and shading effects. However, existing methods often\noversimplify this challenge by assuming a single light source or uniform,\nwhite-balanced lighting, leaving many of these complexities unaddressed.In this\npaper, we introduce CL3AN, the first large-scale, high-resolution dataset of\nits kind designed to facilitate the restoration of images captured under\nmultiple Colored Light sources to their Ambient-Normalized counterparts.\nThrough benchmarking, we find that leading approaches often produce artifacts,\nsuch as illumination inconsistencies, texture leakage, and color distortion,\nprimarily due to their limited ability to precisely disentangle illumination\nfrom reflectance. Motivated by this insight, we achieve such a desired\ndecomposition through a novel learning framework that leverages explicit\nchromaticity and luminance components guidance, drawing inspiration from the\nprinciples of the Retinex model. Extensive evaluations on existing benchmarks\nand our dataset demonstrate the effectiveness of our approach, showcasing\nenhanced robustness under non-homogeneous color lighting and material-specific\nreflectance variations, all while maintaining a highly competitive\ncomputational cost. The benchmark, codes, and models are available at\nwww.github.com/fvasluianu97/RLN2.\n","authors":["Florin-Alexandru Vasluianu","Tim Seizinger","Zongwei Wu","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2508.02168v1.pdf","comment":"an 8-pages manuscript, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2508.02165v1","updated":"2025-08-04T08:05:18Z","published":"2025-08-04T08:05:18Z","title":"Subject or Style: Adaptive and Training-Free Mixture of LoRAs","summary":"  Fine-tuning models via Low-Rank Adaptation (LoRA) demonstrates remarkable\nperformance in subject-driven or style-driven generation tasks. Studies have\nexplored combinations of different LoRAs to jointly generate learned styles and\ncontent. However, current methods struggle to balance the original subject and\nstyle, and often require additional training. Recently, K-LoRA proposed a\ntraining-free LoRA fusion method. But it involves multiple hyperparameters,\nmaking it difficult to adapt to all styles and subjects. In this paper, we\npropose EST-LoRA, a training-free adaptive LoRA fusion method. It\ncomprehensively considers three critical factors: \\underline{E}nergy of matrix,\n\\underline{S}tyle discrepancy scores and \\underline{T}ime steps. Analogous to\nthe Mixture of Experts (MoE) architecture, the model adaptively selects between\nsubject LoRA and style LoRA within each attention layer. This integrated\nselection mechanism ensures balanced contributions from both components during\nthe generation process. Experimental results show that EST-LoRA outperforms\nstate-of-the-art methods in both qualitative and quantitative evaluations and\nachieves faster generation speed compared to other efficient fusion approaches.\nOur code is publicly available at:\nhttps://anonymous.4open.science/r/EST-LoRA-F318.\n","authors":["Jia-Chen Zhang","Yu-Jie Xiong"],"pdf_url":"https://arxiv.org/pdf/2508.02165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12768v3","updated":"2025-08-04T08:04:09Z","published":"2025-03-17T03:05:21Z","title":"Dynamic-Dark SLAM: RGB-Thermal Cooperative Robot Vision Strategy for\n  Multi-Person Tracking in Both Well-Lit and Low-Light Scenes","summary":"  In robot vision, thermal cameras hold great potential for recognizing humans\neven in complete darkness. However, their application to multi-person tracking\n(MPT) has been limited due to data scarcity and the inherent difficulty of\ndistinguishing individuals. In this study, we propose a cooperative MPT system\nthat utilizes co-located RGB and thermal cameras, where pseudo-annotations\n(bounding boxes and person IDs) are used to train both RGB and thermal\ntrackers. Evaluation experiments demonstrate that the thermal tracker performs\nrobustly in both bright and dark environments. Moreover, the results suggest\nthat a tracker-switching strategy -- guided by a binary brightness classifier\n-- is more effective for information integration than a tracker-fusion\napproach. As an application example, we present an image change pattern\nrecognition (ICPR) method, the ``human-as-landmark,'' which combines two key\nproperties: the thermal recognizability of humans in dark environments and the\nrich landmark characteristics -- appearance, geometry, and semantics -- of\nstatic objects (occluders). Whereas conventional SLAM focuses on mapping static\nlandmarks in well-lit environments, the present study takes a first step toward\na new Human-Only SLAM paradigm, ``Dynamic-Dark SLAM,'' which aims to map even\ndynamic landmarks in complete darkness. Additionally, this study demonstrates\nthat knowledge transfer between thermal and depth modalities enables reliable\nperson tracking using low-resolution 3D LiDAR data without RGB input,\ncontributing an important advance toward cross-robot SLAM systems.\n","authors":["Tatsuro Sakai","Kanji Tanaka","Yuki Minase","Jonathan Tay Yu Liang","Muhammad Adil Luqman","Daiki Iwata"],"pdf_url":"https://arxiv.org/pdf/2503.12768v3.pdf","comment":"11 pages, 11 figures, technical report"},{"id":"http://arxiv.org/abs/2508.02157v1","updated":"2025-08-04T07:57:39Z","published":"2025-08-04T07:57:39Z","title":"Unified Category-Level Object Detection and Pose Estimation from RGB\n  Images using 3D Prototypes","summary":"  Recognizing objects in images is a fundamental problem in computer vision.\nAlthough detecting objects in 2D images is common, many applications require\ndetermining their pose in 3D space. Traditional category-level methods rely on\nRGB-D inputs, which may not always be available, or employ two-stage approaches\nthat use separate models and representations for detection and pose estimation.\nFor the first time, we introduce a unified model that integrates detection and\npose estimation into a single framework for RGB images by leveraging neural\nmesh models with learned features and multi-model RANSAC. Our approach achieves\nstate-of-the-art results for RGB category-level pose estimation on REAL275,\nimproving on the current state-of-the-art by 22.9% averaged across all\nscale-agnostic metrics. Finally, we demonstrate that our unified method\nexhibits greater robustness compared to single-stage baselines. Our code and\nmodels are available at\nhttps://github.com/Fischer-Tom/unified-detection-and-pose-estimation.\n","authors":["Tom Fischer","Xiaojie Zhang","Eddy Ilg"],"pdf_url":"https://arxiv.org/pdf/2508.02157v1.pdf","comment":"Published at ICCV 2025"},{"id":"http://arxiv.org/abs/2508.02155v1","updated":"2025-08-04T07:54:37Z","published":"2025-08-04T07:54:37Z","title":"DreamPainter: Image Background Inpainting for E-commerce Scenarios","summary":"  Although diffusion-based image genenation has been widely explored and\napplied, background generation tasks in e-commerce scenarios still face\nsignificant challenges. The first challenge is to ensure that the generated\nproducts are consistent with the given product inputs while maintaining a\nreasonable spatial arrangement, harmonious shadows, and reflections between\nforeground products and backgrounds. Existing inpainting methods fail to\naddress this due to the lack of domain-specific data. The second challenge\ninvolves the limitation of relying solely on text prompts for image control, as\neffective integrating visual information to achieve precise control in\ninpainting tasks remains underexplored. To address these challenges, we\nintroduce DreamEcom-400K, a high-quality e-commerce dataset containing accurate\nproduct instance masks, background reference images, text prompts, and\naesthetically pleasing product images. Based on this dataset, we propose\nDreamPainter, a novel framework that not only utilizes text prompts for control\nbut also flexibly incorporates reference image information as an additional\ncontrol signal. Extensive experiments demonstrate that our approach\nsignificantly outperforms state-of-the-art methods, maintaining high product\nconsistency while effectively integrating both text prompt and reference image\ninformation.\n","authors":["Sijie Zhao","Jing Cheng","Yaoyao Wu","Hao Xu","Shaohui Jiao"],"pdf_url":"https://arxiv.org/pdf/2508.02155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02152v1","updated":"2025-08-04T07:49:59Z","published":"2025-08-04T07:49:59Z","title":"Efficient Chambolle-Pock based algorithms for Convoltional sparse\n  representation","summary":"  Recently convolutional sparse representation (CSR), as a sparse\nrepresentation technique, has attracted increasing attention in the field of\nimage processing, due to its good characteristic of translate-invariance. The\ncontent of CSR usually consists of convolutional sparse coding (CSC) and\nconvolutional dictionary learning (CDL), and many studies focus on how to solve\nthe corresponding optimization problems. At present, the most efficient\noptimization scheme for CSC is based on the alternating direction method of\nmultipliers (ADMM). However, the ADMM-based approach involves a penalty\nparameter that needs to be carefully selected, and improper parameter selection\nmay result in either no convergence or very slow convergence. In this paper, a\nnovel fast and efficient method using Chambolle-Pock(CP) framework is proposed,\nwhich does not require extra manual selection parameters in solving processing,\nand has faster convergence speed. Furthermore, we propose an anisotropic total\nvariation penalty of the coefficient maps for CSC and apply the CP algorithm to\nsolve it. In addition, we also apply the CP framework to solve the\ncorresponding CDL problem. Experiments show that for noise-free image the\nproposed CSC algorithms can achieve rival results of the latest ADMM-based\napproach, while outperforms in removing noise from Gaussian noise pollution\nimage.\n","authors":["Yi Liu","Junjing Li","Yang Chen","Haowei Tang","Pengcheng Zhang","Tianling Lyu","Zhiguo Gui"],"pdf_url":"https://arxiv.org/pdf/2508.02152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02151v1","updated":"2025-08-04T07:49:40Z","published":"2025-08-04T07:49:40Z","title":"AttriCtrl: Fine-Grained Control of Aesthetic Attribute Intensity in\n  Diffusion Models","summary":"  Recent breakthroughs in text-to-image diffusion models have significantly\nenhanced both the visual fidelity and semantic controllability of generated\nimages. However, fine-grained control over aesthetic attributes remains\nchallenging, especially when users require continuous and intensity-specific\nadjustments. Existing approaches often rely on vague textual prompts, which are\ninherently ambiguous in expressing both the aesthetic semantics and the desired\nintensity, or depend on costly human preference data for alignment, limiting\ntheir scalability and practicality. To address these limitations, we propose\nAttriCtrl, a plug-and-play framework for precise and continuous control of\naesthetic attributes. Specifically, we quantify abstract aesthetics by\nleveraging semantic similarity from pre-trained vision-language models, and\nemploy a lightweight value encoder that maps scalar intensities in $[0,1]$ to\nlearnable embeddings within diffusion-based generation. This design enables\nintuitive and customizable aesthetic manipulation, with minimal training\noverhead and seamless integration into existing generation pipelines. Extensive\nexperiments demonstrate that AttriCtrl achieves accurate control over\nindividual attributes as well as flexible multi-attribute composition.\nMoreover, it is fully compatible with popular open-source controllable\ngeneration frameworks, showcasing strong integration capability and practical\nutility across diverse generation scenarios.\n","authors":["Die Chen","Zhongjie Duan","Zhiwen Li","Cen Chen","Daoyuan Chen","Yaliang Li","Yinda Chen"],"pdf_url":"https://arxiv.org/pdf/2508.02151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02149v1","updated":"2025-08-04T07:47:38Z","published":"2025-08-04T07:47:38Z","title":"AURORA: Augmented Understanding via Structured Reasoning and\n  Reinforcement Learning for Reference Audio-Visual Segmentation","summary":"  Reference Audio-Visual Segmentation (Ref-AVS) tasks challenge models to\nprecisely locate sounding objects by integrating visual, auditory, and textual\ncues. Existing methods often lack genuine semantic understanding, tending to\nmemorize fixed reasoning patterns. Furthermore, jointly training for reasoning\nand segmentation can compromise pixel-level precision. To address these issues,\nwe introduce AURORA, a novel framework designed to enhance genuine reasoning\nand language comprehension in reference audio-visual segmentation. We employ a\nstructured Chain-of-Thought (CoT) prompting mechanism to guide the model\nthrough a step-by-step reasoning process and introduce a novel segmentation\nfeature distillation loss to effectively integrate these reasoning abilities\nwithout sacrificing segmentation performance. To further cultivate the model's\ngenuine reasoning capabilities, we devise a further two-stage training\nstrategy: first, a ``corrective reflective-style training\" stage utilizes\nself-correction to enhance the quality of reasoning paths, followed by\nreinforcement learning via Group Reward Policy Optimization (GRPO) to bolster\nrobustness in challenging scenarios. Experiments demonstrate that AURORA\nachieves state-of-the-art performance on Ref-AVS benchmarks and generalizes\neffectively to unreferenced segmentation.\n","authors":["Ziyang Luo","Nian Liu","Fahad Shahbaz Khan","Junwei Han"],"pdf_url":"https://arxiv.org/pdf/2508.02149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02146v1","updated":"2025-08-04T07:45:31Z","published":"2025-08-04T07:45:31Z","title":"ScrewSplat: An End-to-End Method for Articulated Object Recognition","summary":"  Articulated object recognition -- the task of identifying both the geometry\nand kinematic joints of objects with movable parts -- is essential for enabling\nrobots to interact with everyday objects such as doors and laptops. However,\nexisting approaches often rely on strong assumptions, such as a known number of\narticulated parts; require additional inputs, such as depth images; or involve\ncomplex intermediate steps that can introduce potential errors -- limiting\ntheir practicality in real-world settings. In this paper, we introduce\nScrewSplat, a simple end-to-end method that operates solely on RGB\nobservations. Our approach begins by randomly initializing screw axes, which\nare then iteratively optimized to recover the object's underlying kinematic\nstructure. By integrating with Gaussian Splatting, we simultaneously\nreconstruct the 3D geometry and segment the object into rigid, movable parts.\nWe demonstrate that our method achieves state-of-the-art recognition accuracy\nacross a diverse set of articulated objects, and further enables zero-shot,\ntext-guided manipulation using the recovered kinematic model.\n","authors":["Seungyeon Kim","Junsu Ha","Young Hun Kim","Yonghyeon Lee","Frank C. Park"],"pdf_url":"https://arxiv.org/pdf/2508.02146v1.pdf","comment":"26 pages, 12 figures, Conference on Robot Learning (CoRL) 2025"},{"id":"http://arxiv.org/abs/2508.02143v1","updated":"2025-08-04T07:43:04Z","published":"2025-08-04T07:43:04Z","title":"TrackletGait: A Robust Framework for Gait Recognition in the Wild","summary":"  Gait recognition aims to identify individuals based on their body shape and\nwalking patterns. Though much progress has been achieved driven by deep\nlearning, gait recognition in real-world surveillance scenarios remains quite\nchallenging to current methods. Conventional approaches, which rely on periodic\ngait cycles and controlled environments, struggle with the non-periodic and\noccluded silhouette sequences encountered in the wild. In this paper, we\npropose a novel framework, TrackletGait, designed to address these challenges\nin the wild. We propose Random Tracklet Sampling, a generalization of existing\nsampling methods, which strikes a balance between robustness and representation\nin capturing diverse walking patterns. Next, we introduce Haar Wavelet-based\nDownsampling to preserve information during spatial downsampling. Finally, we\npresent a Hardness Exclusion Triplet Loss, designed to exclude low-quality\nsilhouettes by discarding hard triplet samples. TrackletGait achieves\nstate-of-the-art results, with 77.8 and 80.4 rank-1 accuracy on the Gait3D and\nGREW datasets, respectively, while using only 10.3M backbone parameters.\nExtensive experiments are also conducted to further investigate the factors\naffecting gait recognition in the wild.\n","authors":["Shaoxiong Zhang","Jinkai Zheng","Shangdong Zhu","Chenggang Yan"],"pdf_url":"https://arxiv.org/pdf/2508.02143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02140v1","updated":"2025-08-04T07:38:18Z","published":"2025-08-04T07:38:18Z","title":"AID4AD: Aerial Image Data for Automated Driving Perception","summary":"  This work investigates the integration of spatially aligned aerial imagery\ninto perception tasks for automated vehicles (AVs). As a central contribution,\nwe present AID4AD, a publicly available dataset that augments the nuScenes\ndataset with high-resolution aerial imagery precisely aligned to its local\ncoordinate system. The alignment is performed using SLAM-based point cloud maps\nprovided by nuScenes, establishing a direct link between aerial data and\nnuScenes local coordinate system. To ensure spatial fidelity, we propose an\nalignment workflow that corrects for localization and projection distortions. A\nmanual quality control process further refines the dataset by identifying a set\nof high-quality alignments, which we publish as ground truth to support future\nresearch on automated registration. We demonstrate the practical value of\nAID4AD in two representative tasks: in online map construction, aerial imagery\nserves as a complementary input that improves the mapping process; in motion\nprediction, it functions as a structured environmental representation that\nreplaces high-definition maps. Experiments show that aerial imagery leads to a\n15-23% improvement in map construction accuracy and a 2% gain in trajectory\nprediction performance. These results highlight the potential of aerial imagery\nas a scalable and adaptable source of environmental context in automated\nvehicle systems, particularly in scenarios where high-definition maps are\nunavailable, outdated, or costly to maintain. AID4AD, along with evaluation\ncode and pretrained models, is publicly released to foster further research in\nthis direction: https://github.com/DriverlessMobility/AID4AD.\n","authors":["Daniel Lengerer","Mathias Pechinger","Klaus Bogenberger","Carsten Markgraf"],"pdf_url":"https://arxiv.org/pdf/2508.02140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04847v5","updated":"2025-08-04T07:36:34Z","published":"2025-02-07T11:36:36Z","title":"HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion\n  Video Generation","summary":"  Human motion video generation has advanced significantly, while existing\nmethods still struggle with accurately rendering detailed body parts like hands\nand faces, especially in long sequences and intricate motions. Current\napproaches also rely on fixed resolution and struggle to maintain visual\nconsistency. To address these limitations, we propose HumanDiT, a pose-guided\nDiffusion Transformer (DiT)-based framework trained on a large and wild dataset\ncontaining 14,000 hours of high-quality video to produce high-fidelity videos\nwith fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT,\nsupports numerous video resolutions and variable sequence lengths, facilitating\nlearning for long-sequence video generation; (ii) we introduce a prefix-latent\nreference strategy to maintain personalized characteristics across extended\nsequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to\ngenerate subsequent pose sequences, facilitating video continuation from static\nimages or existing videos. It also utilizes a Pose Adapter to enable pose\ntransfer with given sequences. Extensive experiments demonstrate its superior\nperformance in generating long-form, pose-accurate videos across diverse\nscenarios.\n","authors":["Qijun Gan","Yi Ren","Chen Zhang","Zhenhui Ye","Pan Xie","Xiang Yin","Zehuan Yuan","Bingyue Peng","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.04847v5.pdf","comment":"https://agnjason.github.io/HumanDiT-page/"},{"id":"http://arxiv.org/abs/2508.02134v1","updated":"2025-08-04T07:31:10Z","published":"2025-08-04T07:31:10Z","title":"Free-MoRef: Instantly Multiplexing Context Perception Capabilities of\n  Video-MLLMs within Single Inference","summary":"  Video Multimodal Large Language Models~(Video-MLLM) have achieved remarkable\nadvancements in video understanding tasks. However, constrained by the context\nlength limitation in the underlying LLMs, existing Video-MLLMs typically\nexhibit suboptimal performance on long video scenarios. To understand extended\ninput frames, common solutions span token compression and streaming inference\ntechniques, which sacrifice feature granularity or inference efficiency.\nDifferently, to efficiently achieve comprehensive understanding of longer frame\ninputs, we draw ideas from MoE and propose a training-free approach\n\\textbf{Free-MoRef}, which instantly multiplexes the context perception\ncapabilities of Video-MLLMs within one inference pass. Specifically, Free-MoRef\nreconstructs the vision tokens into several short sequences as\nmulti-references. Subsequently, we introduce MoRef-attention, which gathers\nclues from the multi-reference chunks in parallel to summarize unified query\nactivations. After the shadow layers in LLMs, a reference fusion step is\nderived to compose a final mixed reasoning sequence with key tokens from\nparallel chunks, which compensates the cross-reference vision interactions that\nare neglected in MoRef-attention. By splitting and fusing the long vision token\nsequences, Free-MoRef achieves improved performance under much lower computing\ncosts in reasoning multiplexed context length, demonstrating strong efficiency\nand effectiveness. Experiments on VideoMME, MLVU, LongVideoBench show that\nFree-MoRef achieves full perception of 2$\\times$ to 8$\\times$ longer input\nframes without compression on a single A100 GPU while keeping instant\nresponses, thereby bringing significant performance gains, even surpassing\ndedicatedly trained long-video-MLLMs. Codes are available at\nhttps://github.com/wkfdb/Free-MoRef\n","authors":["Kuo Wang","Quanlong Zheng","Junlin Xie","Yanhao Zhang","Jinguo Luo","Haonan Lu","Liang Lin","Fan Zhou","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2508.02134v1.pdf","comment":"published in ICCV 2025"},{"id":"http://arxiv.org/abs/2508.02131v1","updated":"2025-08-04T07:24:46Z","published":"2025-08-04T07:24:46Z","title":"A Neural Quality Metric for BRDF Models","summary":"  Accurately evaluating the quality of bidirectional reflectance distribution\nfunction (BRDF) models is essential for photo-realistic rendering. Traditional\nBRDF-space metrics often employ numerical error measures that fail to capture\nperceptual differences evident in rendered images. In this paper, we introduce\nthe first perceptually informed neural quality metric for BRDF evaluation that\noperates directly in BRDF space, eliminating the need for rendering during\nquality assessment. Our metric is implemented as a compact multi-layer\nperceptron (MLP), trained on a dataset of measured BRDFs supplemented with\nsynthetically generated data and labelled using a perceptually validated\nimage-space metric. The network takes as input paired samples of reference and\napproximated BRDFs and predicts their perceptual quality in terms of\njust-objectionable-difference (JOD) scores. We show that our neural metric\nachieves significantly higher correlation with human judgments than existing\nBRDF-space metrics. While its performance as a loss function for BRDF fitting\nremains limited, the proposed metric offers a perceptually grounded alternative\nfor evaluating BRDF models.\n","authors":["Behnaz Kavoosighafi","Rafal K. Mantiuk","Saghi Hajisharif","Ehsan Miandji","Jonas Unger"],"pdf_url":"https://arxiv.org/pdf/2508.02131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02129v1","updated":"2025-08-04T07:24:05Z","published":"2025-08-04T07:24:05Z","title":"VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic\n  Urban Scenes Modeling","summary":"  Dynamic urban scene modeling is a rapidly evolving area with broad\napplications. While current approaches leveraging neural radiance fields or\nGaussian Splatting have achieved fine-grained reconstruction and high-fidelity\nnovel view synthesis, they still face significant limitations. These often stem\nfrom a dependence on pre-calibrated object tracks or difficulties in accurately\nmodeling fast-moving objects from undersampled capture, particularly due to\nchallenges in handling temporal discontinuities. To overcome these issues, we\npropose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our\nkey insight is to distill robust, temporally consistent priors from a test-time\nadapted video diffusion model. To ensure precise pose alignment and effective\nintegration of this denoised content, we introduce two core innovations: a\njoint timestamp optimization strategy that refines interpolated frame poses,\nand an uncertainty distillation method that adaptively extracts target content\nwhile preserving well-reconstructed regions. Extensive experiments demonstrate\nthat our method significantly enhances dynamic modeling, especially for\nfast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view\nsynthesis over baseline approaches.\n","authors":["Yuru Xiao","Zihan Lin","Chao Lu","Deming Zhai","Kui Jiang","Wenbo Zhao","Wei Zhang","Junjun Jiang","Huanran Wang","Xianming Liu"],"pdf_url":"https://arxiv.org/pdf/2508.02129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.23685v2","updated":"2025-08-04T07:22:07Z","published":"2025-07-31T16:02:00Z","title":"UniLDiff: Unlocking the Power of Diffusion Priors for All-in-One Image\n  Restoration","summary":"  All-in-One Image Restoration (AiOIR) has emerged as a promising yet\nchallenging research direction. To address the core challenges of diverse\ndegradation modeling and detail preservation, we propose UniLDiff, a unified\nframework enhanced with degradation- and detail-aware mechanisms, unlocking the\npower of diffusion priors for robust image restoration. Specifically, we\nintroduce a Degradation-Aware Feature Fusion (DAFF) to dynamically inject\nlow-quality features into each denoising step via decoupled fusion and adaptive\nmodulation, enabling implicit modeling of diverse and compound degradations.\nFurthermore, we design a Detail-Aware Expert Module (DAEM) in the decoder to\nenhance texture and fine-structure recovery through expert routing. Extensive\nexperiments across multi-task and mixed degradation settings demonstrate that\nour method consistently achieves state-of-the-art performance, highlighting the\npractical potential of diffusion priors for unified image restoration. Our code\nwill be released.\n","authors":["Zihan Cheng","Liangtai Zhou","Dian Chen","Ni Tang","Xiaotong Luo","Yanyun Qu"],"pdf_url":"https://arxiv.org/pdf/2507.23685v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12869v2","updated":"2025-08-04T07:21:26Z","published":"2025-07-17T07:40:50Z","title":"WhoFi: Deep Person Re-Identification via Wi-Fi Channel Signal Encoding","summary":"  Person Re-Identification is a key and challenging task in video surveillance.\nWhile traditional methods rely on visual data, issues like poor lighting,\nocclusion, and suboptimal angles often hinder performance. To address these\nchallenges, we introduce WhoFi, a novel pipeline that utilizes Wi-Fi signals\nfor person re-identification. Biometric features are extracted from Channel\nState Information (CSI) and processed through a modular Deep Neural Network\n(DNN) featuring a Transformer-based encoder. The network is trained using an\nin-batch negative loss function to learn robust and generalizable biometric\nsignatures. Experiments on the NTU-Fi dataset show that our approach achieves\ncompetitive results compared to state-of-the-art methods, confirming its\neffectiveness in identifying individuals via Wi-Fi signals.\n","authors":["Danilo Avola","Emad Emam","Dario Montagnini","Daniele Pannone","Amedeo Ranaldi"],"pdf_url":"https://arxiv.org/pdf/2507.12869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02127v1","updated":"2025-08-04T07:19:20Z","published":"2025-08-04T07:19:20Z","title":"Beyond RGB and Events: Enhancing Object Detection under Adverse Lighting\n  with Monocular Normal Maps","summary":"  Accurate object detection under adverse lighting conditions is critical for\nreal-world applications such as autonomous driving. Although neuromorphic event\ncameras have been introduced to handle these scenarios, adverse lighting often\ninduces distracting reflections from tunnel walls or road surfaces, which\nfrequently lead to false obstacle detections. However, neither RGB nor event\ndata alone is robust enough to address these complexities, and mitigating these\nissues without additional sensors remains underexplored. To overcome these\nchallenges, we propose leveraging normal maps, directly predicted from\nmonocular RGB images, as robust geometric cues to suppress false positives and\nenhance detection accuracy. We introduce NRE-Net, a novel multi-modal detection\nframework that effectively fuses three complementary modalities: monocularly\npredicted surface normal maps, RGB images, and event streams. To optimize the\nfusion process, our framework incorporates two key modules: the Adaptive\nDual-stream Fusion Module (ADFM), which integrates RGB and normal map features,\nand the Event-modality Aware Fusion Module (EAFM), which adapts to the high\ndynamic range characteristics of event data. Extensive evaluations on the\nDSEC-Det-sub and PKU-DAVIS-SOD datasets demonstrate that NRE-Net significantly\noutperforms state-of-the-art methods. Our approach achieves mAP50 improvements\nof 7.9% and 6.1% over frame-based approaches (e.g., YOLOX), while surpassing\nthe fusion-based SFNet by 2.7% on the DSEC-Det-sub dataset and SODFormer by\n7.1% on the PKU-DAVIS-SOD dataset.\n","authors":["Mingjie Liu","Hanqing Liu","Chuang Zhu"],"pdf_url":"https://arxiv.org/pdf/2508.02127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08524v2","updated":"2025-08-04T06:50:43Z","published":"2024-08-16T04:38:31Z","title":"GS-ID: Illumination Decomposition on Gaussian Splatting via Adaptive\n  Light Aggregation and Diffusion-Guided Material Priors","summary":"  Gaussian Splatting (GS) has emerged as an effective representation for\nphotorealistic rendering, but the underlying geometry, material, and lighting\nremain entangled, hindering scene editing. Existing GS-based methods struggle\nto disentangle these components under non-Lambertian conditions, especially in\nthe presence of specularities and shadows. We propose \\textbf{GS-ID}, an\nend-to-end framework for illumination decomposition that integrates adaptive\nlight aggregation with diffusion-based material priors. In addition to a\nlearnable environment map for ambient illumination, we model spatially-varying\nlocal lighting using anisotropic spherical Gaussian mixtures (SGMs) that are\njointly optimized with scene content. To better capture cast shadows, we\nassociate each splat with a learnable unit vector that encodes shadow\ndirections from multiple light sources, further improving material and lighting\nestimation. By combining SGMs with intrinsic priors from diffusion models,\nGS-ID significantly reduces ambiguity in light-material-geometry interactions\nand achieves state-of-the-art performance on inverse rendering and relighting\nbenchmarks. Experiments also demonstrate the effectiveness of GS-ID for\ndownstream applications such as relighting and scene composition.\n","authors":["Kang Du","Zhihao Liang","Yulin Shen","Zeyu Wang"],"pdf_url":"https://arxiv.org/pdf/2408.08524v2.pdf","comment":"17 pages, 14 figures"},{"id":"http://arxiv.org/abs/2508.02113v1","updated":"2025-08-04T06:49:48Z","published":"2025-08-04T06:49:48Z","title":"DeflareMamba: Hierarchical Vision Mamba for Contextually Consistent Lens\n  Flare Removal","summary":"  Lens flare removal remains an information confusion challenge in the\nunderlying image background and the optical flares, due to the complex optical\ninteractions between light sources and camera lens. While recent solutions have\nshown promise in decoupling the flare corruption from image, they often fail to\nmaintain contextual consistency, leading to incomplete and inconsistent flare\nremoval. To eliminate this limitation, we propose DeflareMamba, which leverages\nthe efficient sequence modeling capabilities of state space models while\nmaintains the ability to capture local-global dependencies. Particularly, we\ndesign a hierarchical framework that establishes long-range pixel correlations\nthrough varied stride sampling patterns, and utilize local-enhanced state space\nmodels that simultaneously preserves local details. To the best of our\nknowledge, this is the first work that introduces state space models to the\nflare removal task. Extensive experiments demonstrate that our method\neffectively removes various types of flare artifacts, including scattering and\nreflective flares, while maintaining the natural appearance of non-flare\nregions. Further downstream applications demonstrate the capacity of our method\nto improve visual object recognition and cross-modal semantic understanding.\nCode is available at https://github.com/BNU-ERC-ITEA/DeflareMamba.\n","authors":["Yihang Huang","Yuanfei Huang","Junhui Lin","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2508.02113v1.pdf","comment":"Accepted by ACMMM 2025"},{"id":"http://arxiv.org/abs/2508.02111v1","updated":"2025-08-04T06:40:01Z","published":"2025-08-04T06:40:01Z","title":"Tackling Ill-posedness of Reversible Image Conversion with Well-posed\n  Invertible Network","summary":"  Reversible image conversion (RIC) suffers from ill-posedness issues due to\nits forward conversion process being considered an underdetermined system.\nDespite employing invertible neural networks (INN), existing RIC methods\nintrinsically remain ill-posed as inevitably introducing uncertainty by\nincorporating randomly sampled variables. To tackle the ill-posedness dilemma,\nwe focus on developing a reliable approximate left inverse for the\nunderdetermined system by constructing an overdetermined system with a non-zero\nGram determinant, thus ensuring a well-posed solution. Based on this principle,\nwe propose a well-posed invertible $1\\times1$ convolution (WIC), which\neliminates the reliance on random variable sampling and enables the development\nof well-posed invertible networks. Furthermore, we design two innovative\nnetworks, WIN-Na\\\"ive and WIN, with the latter incorporating advanced\nskip-connections to enhance long-term memory. Our methods are evaluated across\ndiverse RIC tasks, including reversible image hiding, image rescaling, and\nimage decolorization, consistently achieving state-of-the-art performance.\nExtensive experiments validate the effectiveness of our approach, demonstrating\nits ability to overcome the bottlenecks of existing RIC solutions and setting a\nnew benchmark in the field. Codes are available in\nhttps://github.com/BNU-ERC-ITEA/WIN.\n","authors":["Yuanfei Huang","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2508.02111v1.pdf","comment":"Submitted to IEEE Transactions"},{"id":"http://arxiv.org/abs/2407.13541v2","updated":"2025-08-04T06:37:59Z","published":"2024-07-18T14:18:03Z","title":"On the Discriminability of Self-Supervised Representation Learning","summary":"  Self-supervised learning (SSL) has recently shown notable success in various\nvisual tasks. However, in terms of discriminability, SSL is still not on par\nwith supervised learning (SL). This paper identifies a key issue, the\n``crowding problem,\" where features from different classes are not\nwell-separated, and there is high intra-class variance. In contrast, SL ensures\nclear class separation. Our analysis reveals that SSL objectives do not\nadequately constrain the relationships between samples and their augmentations,\nleading to poorer performance in complex tasks. We further establish a\ntheoretical framework that connects SSL objectives to cross-entropy risk\nbounds, explaining how reducing intra-class variance and increasing inter-class\nseparation can improve generalization. To address this, we propose the Dynamic\nSemantic Adjuster (DSA), a learnable regulator that enhances feature\naggregation and separation while being robust to outliers. Comprehensive\nexperiments conducted on diverse benchmark datasets validate that DSA leads to\nsubstantial gains in SSL performance, narrowing the performance gap with SL.\n","authors":["Zeen Song","Wenwen Qiang","Changwen Zheng","Fuchun Sun","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2407.13541v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02107v1","updated":"2025-08-04T06:36:00Z","published":"2025-08-04T06:36:00Z","title":"AutoLoRA: Automatic LoRA Retrieval and Fine-Grained Gated Fusion for\n  Text-to-Image Generation","summary":"  Despite recent advances in photorealistic image generation through\nlarge-scale models like FLUX and Stable Diffusion v3, the practical deployment\nof these architectures remains constrained by their inherent intractability to\nparameter fine-tuning. While low-rank adaptation (LoRA) have demonstrated\nefficacy in enabling model customization with minimal parameter overhead, the\neffective utilization of distributed open-source LoRA modules faces three\ncritical challenges: sparse metadata annotation, the requirement for zero-shot\nadaptation capabilities, and suboptimal fusion strategies for multi-LoRA fusion\nstrategies. To address these limitations, we introduce a novel framework that\nenables semantic-driven LoRA retrieval and dynamic aggregation through two key\ncomponents: (1) weight encoding-base LoRA retriever that establishes a shared\nsemantic space between LoRA parameter matrices and text prompts, eliminating\ndependence on original training data, and (2) fine-grained gated fusion\nmechanism that computes context-specific fusion weights across network layers\nand diffusion timesteps to optimally integrate multiple LoRA modules during\ngeneration. Our approach achieves significant improvement in image generation\nperfermance, thereby facilitating scalable and data-efficient enhancement of\nfoundational models. This work establishes a critical bridge between the\nfragmented landscape of community-developed LoRAs and practical deployment\nrequirements, enabling collaborative model evolution through standardized\nadapter integration.\n","authors":["Zhiwen Li","Zhongjie Duan","Die Chen","Cen Chen","Daoyuan Chen","Yaliang Li","Yingda Chen"],"pdf_url":"https://arxiv.org/pdf/2508.02107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02106v1","updated":"2025-08-04T06:35:48Z","published":"2025-08-04T06:35:48Z","title":"Towards Immersive Human-X Interaction: A Real-Time Framework for\n  Physically Plausible Motion Synthesis","summary":"  Real-time synthesis of physically plausible human interactions remains a\ncritical challenge for immersive VR/AR systems and humanoid robotics. While\nexisting methods demonstrate progress in kinematic motion generation, they\noften fail to address the fundamental tension between real-time responsiveness,\nphysical feasibility, and safety requirements in dynamic human-machine\ninteractions. We introduce Human-X, a novel framework designed to enable\nimmersive and physically plausible human interactions across diverse entities,\nincluding human-avatar, human-humanoid, and human-robot systems. Unlike\nexisting approaches that focus on post-hoc alignment or simplified physics, our\nmethod jointly predicts actions and reactions in real-time using an\nauto-regressive reaction diffusion planner, ensuring seamless synchronization\nand context-aware responses. To enhance physical realism and safety, we\nintegrate an actor-aware motion tracking policy trained with reinforcement\nlearning, which dynamically adapts to interaction partners' movements while\navoiding artifacts like foot sliding and penetration. Extensive experiments on\nthe Inter-X and InterHuman datasets demonstrate significant improvements in\nmotion quality, interaction continuity, and physical plausibility over\nstate-of-the-art methods. Our framework is validated in real-world\napplications, including virtual reality interface for human-robot interaction,\nshowcasing its potential for advancing human-robot collaboration.\n","authors":["Kaiyang Ji","Ye Shi","Zichen Jin","Kangyi Chen","Lan Xu","Yuexin Ma","Jingyi Yu","Jingya Wang"],"pdf_url":"https://arxiv.org/pdf/2508.02106v1.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2502.07239v2","updated":"2025-08-04T06:35:08Z","published":"2025-02-11T04:09:12Z","title":"Contextual Gesture: Co-Speech Gesture Video Generation through\n  Context-aware Gesture Representation","summary":"  Co-speech gesture generation is crucial for creating lifelike avatars and\nenhancing human-computer interactions by synchronizing gestures with speech.\nDespite recent advancements, existing methods struggle with accurately\nidentifying the rhythmic or semantic triggers from audio for generating\ncontextualized gesture patterns and achieving pixel-level realism. To address\nthese challenges, we introduce Contextual Gesture, a framework that improves\nco-speech gesture video generation through three innovative components: (1) a\nchronological speech-gesture alignment that temporally connects two modalities,\n(2) a contextualized gesture tokenization that incorporate speech context into\nmotion pattern representation through distillation, and (3) a structure-aware\nrefinement module that employs edge connection to link gesture keypoints to\nimprove video generation. Our extensive experiments demonstrate that Contextual\nGesture not only produces realistic and speech-aligned gesture videos but also\nsupports long-sequence generation and video gesture editing applications, shown\nin Fig.1.\n","authors":["Pinxin Liu","Pengfei Zhang","Hyeongwoo Kim","Pablo Garrido","Ari Sharpio","Kyle Olszewski"],"pdf_url":"https://arxiv.org/pdf/2502.07239v2.pdf","comment":"Accepted to ACMMM 2025. Project Page:\n  https://andypinxinliu.github.io/Contextual-Gesture/"},{"id":"http://arxiv.org/abs/2508.02104v1","updated":"2025-08-04T06:29:34Z","published":"2025-08-04T06:29:34Z","title":"REACT-KD: Region-Aware Cross-modal Topological Knowledge Distillation\n  for Interpretable Medical Image Classification","summary":"  Reliable and interpretable tumor classification from clinical imaging remains\na core challenge due to heterogeneous modality quality, limited annotations,\nand the lack of structured anatomical guidance. We introduce REACT-KD, a\nRegion-Aware Cross-modal Topological Knowledge Distillation framework that\ntransfers rich supervision from high-fidelity multi-modal sources into a\nlightweight CT-based student model. The framework uses a dual teacher design:\none branch captures structure-function relationships using dual-tracer PET/CT,\nand the other models dose-aware features through synthetically degraded\nlow-dose CT data. These branches jointly guide the student model through two\ncomplementary objectives. The first focuses on semantic alignment via logits\ndistillation, while the second models anatomical topology using region graph\ndistillation. A shared CBAM-3D module is employed to maintain consistent\nattention across modalities. To improve reliability for deployment, REACT-KD\nintroduces modality dropout during training, allowing inference under partial\nor noisy inputs. The staging task for hepatocellular carcinoma (HCC) is\nconducted as a case study. REACT-KD achieves an average AUC of 93.4% on an\ninternal PET/CT cohort and maintains 76.6% to 81.5% AUC across varying dose\nlevels in external CT testing. Decision curve analysis shows that REACT-KD\nconsistently provides the highest clinical benefit across decision thresholds,\nsupporting its potential in real-world diagnostics. Code is available at\nhttps://github.com/Kinetics-JOJO/REACT-KD.\n","authors":["Hongzhao Chen","Hexiao Ding","Yufeng Jiang","Jing Lan","Ka Chun Li","Gerald W. Y. Cheng","Sam Ng","Chi Lai Ho","Jing Cai","Liang-ting Lin","Jung Sun Yoo"],"pdf_url":"https://arxiv.org/pdf/2508.02104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19634v6","updated":"2025-08-04T06:07:13Z","published":"2025-04-28T09:49:35Z","title":"NSegment : Label-specific Deformations for Remote Sensing Image\n  Segmentation","summary":"  Labeling errors in remote sensing (RS) image segmentation datasets often\nremain implicit and subtle due to ambiguous class boundaries, mixed pixels,\nshadows, complex terrain features, and subjective annotator bias. Furthermore,\nthe scarcity of annotated RS data due to the high cost of labeling complicates\ntraining noise-robust models. While sophisticated mechanisms such as label\nselection or noise correction might address the issue mentioned above, they\ntend to increase training time and add implementation complexity. In this\npaper, we propose NSegment-a simple yet effective data augmentation solution to\nmitigate this issue. Unlike traditional methods, it applies elastic\ntransformations only to segmentation labels, varying deformation intensity per\nsample in each training epoch to address annotation inconsistencies.\nExperimental results demonstrate that our approach improves the performance of\nRS image segmentation over various state-of-the-art models.\n","authors":["Yechan Kim","DongHo Yoon","SooYeon Kim","Moongu Jeon"],"pdf_url":"https://arxiv.org/pdf/2504.19634v6.pdf","comment":"Accepted in IEEE Geoscience and Remote Sensing Letters (GRSL)"},{"id":"http://arxiv.org/abs/2508.02095v1","updated":"2025-08-04T06:06:06Z","published":"2025-08-04T06:06:06Z","title":"VLM4D: Towards Spatiotemporal Awareness in Vision Language Models","summary":"  Vision language models (VLMs) have shown remarkable capabilities in\nintegrating linguistic and visual reasoning but remain fundamentally limited in\nunderstanding dynamic spatiotemporal interactions. Humans effortlessly track\nand reason about object movements, rotations, and perspective shifts-abilities\nessential for robust dynamic real-world understanding yet notably lacking in\ncurrent VLMs. In this paper, we introduce VLM4D, the first benchmark\nspecifically designed to evaluate the spatiotemporal reasoning capabilities of\nVLMs. Our benchmark comprises diverse real-world and synthetic videos\naccompanied by carefully curated question-answer pairs emphasizing\ntranslational and rotational motions, perspective awareness, and motion\ncontinuity. Through comprehensive evaluations of state-of-the-art open and\nclosed-source VLMs, we identify significant performance gaps compared to human\nbaselines, highlighting fundamental deficiencies in existing models. Extensive\nanalysis reveals that VLMs struggle particularly with integrating multiple\nvisual cues and maintaining temporal coherence. We further explore promising\ndirections, such as leveraging 4D feature field reconstruction and targeted\nspatiotemporal supervised fine-tuning, demonstrating their effectiveness in\nenhancing spatiotemporal comprehension. Our work aims to encourage deeper\nexploration into improving VLMs' spatial and temporal grounding, paving the way\ntowards more capable and reliable visual intelligence for dynamic environments.\n","authors":["Shijie Zhou","Alexander Vilesov","Xuehai He","Ziyu Wan","Shuwang Zhang","Aditya Nagachandra","Di Chang","Dongdong Chen","Xin Eric Wang","Achuta Kadambi"],"pdf_url":"https://arxiv.org/pdf/2508.02095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02082v1","updated":"2025-08-04T05:49:41Z","published":"2025-08-04T05:49:41Z","title":"S-RRG-Bench: Structured Radiology Report Generation with Fine-Grained\n  Evaluation Framework","summary":"  Radiology report generation (RRG) for diagnostic images, such as chest\nX-rays, plays a pivotal role in both clinical practice and AI. Traditional\nfree-text reports suffer from redundancy and inconsistent language,\ncomplicating the extraction of critical clinical details. Structured radiology\nreport generation (S-RRG) offers a promising solution by organizing information\ninto standardized, concise formats. However, existing approaches often rely on\nclassification or visual question answering (VQA) pipelines that require\npredefined label sets and produce only fragmented outputs. Template-based\napproaches, which generate reports by replacing keywords within fixed sentence\npatterns, further compromise expressiveness and often omit clinically important\ndetails. In this work, we present a novel approach to S-RRG that includes\ndataset construction, model training, and the introduction of a new evaluation\nframework. We first create a robust chest X-ray dataset (MIMIC-STRUC) that\nincludes disease names, severity levels, probabilities, and anatomical\nlocations, ensuring that the dataset is both clinically relevant and\nwell-structured. We train an LLM-based model to generate standardized,\nhigh-quality reports. To assess the generated reports, we propose a specialized\nevaluation metric (S-Score) that not only measures disease prediction accuracy\nbut also evaluates the precision of disease-specific details, thus offering a\nclinically meaningful metric for report quality that focuses on elements\ncritical to clinical decision-making and demonstrates a stronger alignment with\nhuman assessments. Our approach highlights the effectiveness of structured\nreports and the importance of a tailored evaluation metric for S-RRG, providing\na more clinically relevant measure of report quality.\n","authors":["Yingshu Li","Yunyi Liu","Zhanyu Wang","Xinyu Liang","Lingqiao Liu","Lei Wang","Luping Zhou"],"pdf_url":"https://arxiv.org/pdf/2508.02082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.23309v2","updated":"2025-08-04T05:19:02Z","published":"2025-07-31T07:43:19Z","title":"PriorFusion: Unified Integration of Priors for Robust Road Perception in\n  Autonomous Driving","summary":"  With the growing interest in autonomous driving, there is an increasing\ndemand for accurate and reliable road perception technologies. In complex\nenvironments without high-definition map support, autonomous vehicles must\nindependently interpret their surroundings to ensure safe and robust\ndecision-making. However, these scenarios pose significant challenges due to\nthe large number, complex geometries, and frequent occlusions of road elements.\nA key limitation of existing approaches lies in their insufficient exploitation\nof the structured priors inherently present in road elements, resulting in\nirregular, inaccurate predictions. To address this, we propose PriorFusion, a\nunified framework that effectively integrates semantic, geometric, and\ngenerative priors to enhance road element perception. We introduce an\ninstance-aware attention mechanism guided by shape-prior features, then\nconstruct a data-driven shape template space that encodes low-dimensional\nrepresentations of road elements, enabling clustering to generate anchor points\nas reference priors. We design a diffusion-based framework that leverages these\nprior anchors to generate accurate and complete predictions. Experiments on\nlarge-scale autonomous driving datasets demonstrate that our method\nsignificantly improves perception accuracy, particularly under challenging\nconditions. Visualization results further confirm that our approach produces\nmore accurate, regular, and coherent predictions of road elements.\n","authors":["Xuewei Tang","Mengmeng Yang","Tuopu Wen","Peijin Jia","Le Cui","Mingshang Luo","Kehua Sheng","Bo Zhang","Diange Yang","Kun Jiang"],"pdf_url":"https://arxiv.org/pdf/2507.23309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02067v1","updated":"2025-08-04T05:13:51Z","published":"2025-08-04T05:13:51Z","title":"YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection\n  Innovations and Challenges","summary":"  Over the past decade, object detection has advanced significantly, with the\nYOLO (You Only Look Once) family of models transforming the landscape of\nreal-time vision applications through unified, end-to-end detection frameworks.\nFrom YOLOv1's pioneering regression-based detection to the latest YOLOv9, each\nversion has systematically enhanced the balance between speed, accuracy, and\ndeployment efficiency through continuous architectural and algorithmic\nadvancements.. Beyond core object detection, modern YOLO architectures have\nexpanded to support tasks such as instance segmentation, pose estimation,\nobject tracking, and domain-specific applications including medical imaging and\nindustrial automation. This paper offers a comprehensive review of the YOLO\nfamily, highlighting architectural innovations, performance benchmarks,\nextended capabilities, and real-world use cases. We critically analyze the\nevolution of YOLO models and discuss emerging research directions that extend\ntheir impact across diverse computer vision domains.\n","authors":["Manikanta Kotthapalli","Deepika Ravipati","Reshma Bhatia"],"pdf_url":"https://arxiv.org/pdf/2508.02067v1.pdf","comment":"13 pages, 2 figures"},{"id":"http://arxiv.org/abs/2503.06669v4","updated":"2025-08-04T04:50:21Z","published":"2025-03-09T15:40:29Z","title":"AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable\n  and Intelligent Embodied Systems","summary":"  We explore how scalable robot data can address real-world challenges for\ngeneralized robotic manipulation. Introducing AgiBot World, a large-scale\nplatform comprising over 1 million trajectories across 217 tasks in five\ndeployment scenarios, we achieve an order-of-magnitude increase in data scale\ncompared to existing datasets. Accelerated by a standardized collection\npipeline with human-in-the-loop verification, AgiBot World guarantees\nhigh-quality and diverse data distribution. It is extensible from grippers to\ndexterous hands and visuo-tactile sensors for fine-grained skill acquisition.\nBuilding on top of data, we introduce Genie Operator-1 (GO-1), a novel\ngeneralist policy that leverages latent action representations to maximize data\nutilization, demonstrating predictable performance scaling with increased data\nvolume. Policies pre-trained on our dataset achieve an average performance\nimprovement of 30% over those trained on Open X-Embodiment, both in in-domain\nand out-of-distribution scenarios. GO-1 exhibits exceptional capability in\nreal-world dexterous and long-horizon tasks, achieving over 60% success rate on\ncomplex tasks and outperforming prior RDT approach by 32%. By open-sourcing the\ndataset, tools, and models, we aim to democratize access to large-scale,\nhigh-quality robot data, advancing the pursuit of scalable and general-purpose\nintelligence.\n","authors":[" AgiBot-World-Contributors","Qingwen Bu","Jisong Cai","Li Chen","Xiuqi Cui","Yan Ding","Siyuan Feng","Shenyuan Gao","Xindong He","Xuan Hu","Xu Huang","Shu Jiang","Yuxin Jiang","Cheng Jing","Hongyang Li","Jialu Li","Chiming Liu","Yi Liu","Yuxiang Lu","Jianlan Luo","Ping Luo","Yao Mu","Yuehan Niu","Yixuan Pan","Jiangmiao Pang","Yu Qiao","Guanghui Ren","Cheng Ruan","Jiaqi Shan","Yongjian Shen","Chengshi Shi","Mingkang Shi","Modi Shi","Chonghao Sima","Jianheng Song","Huijie Wang","Wenhao Wang","Dafeng Wei","Chengen Xie","Guo Xu","Junchi Yan","Cunbiao Yang","Lei Yang","Shukai Yang","Maoqing Yao","Jia Zeng","Chi Zhang","Qinglin Zhang","Bin Zhao","Chengyue Zhao","Jiaqi Zhao","Jianchao Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.06669v4.pdf","comment":"Project website: https://agibot-world.com/. Github repo:\n  https://github.com/OpenDriveLab/AgiBot-World. The author list is ordered\n  alphabetically by surname, with detailed contributions provided in the\n  appendix"},{"id":"http://arxiv.org/abs/2508.02056v1","updated":"2025-08-04T04:50:05Z","published":"2025-08-04T04:50:05Z","title":"StarPose: 3D Human Pose Estimation via Spatial-Temporal Autoregressive\n  Diffusion","summary":"  Monocular 3D human pose estimation remains a challenging task due to inherent\ndepth ambiguities and occlusions. Compared to traditional methods based on\nTransformers or Convolutional Neural Networks (CNNs), recent diffusion-based\napproaches have shown superior performance, leveraging their probabilistic\nnature and high-fidelity generation capabilities. However, these methods often\nfail to account for the spatial and temporal correlations across predicted\nframes, resulting in limited temporal consistency and inferior accuracy in\npredicted 3D pose sequences. To address these shortcomings, this paper proposes\nStarPose, an autoregressive diffusion framework that effectively incorporates\nhistorical 3D pose predictions and spatial-temporal physical guidance to\nsignificantly enhance both the accuracy and temporal coherence of pose\npredictions. Unlike existing approaches, StarPose models the 2D-to-3D pose\nmapping as an autoregressive diffusion process. By synergically integrating\npreviously predicted 3D poses with 2D pose inputs via a Historical Pose\nIntegration Module (HPIM), the framework generates rich and informative\nhistorical pose embeddings that guide subsequent denoising steps, ensuring\ntemporally consistent predictions. In addition, a fully plug-and-play\nSpatial-Temporal Physical Guidance (STPG) mechanism is tailored to refine the\ndenoising process in an iterative manner, which further enforces spatial\nanatomical plausibility and temporal motion dynamics, rendering robust and\nrealistic pose estimates. Extensive experiments on benchmark datasets\ndemonstrate that StarPose outperforms state-of-the-art methods, achieving\nsuperior accuracy and temporal consistency in 3D human pose estimation. Code is\navailable at https://github.com/wileychan/StarPose.\n","authors":["Haoxin Yang","Weihong Chen","Xuemiao Xu","Cheng Xu","Peng Xiao","Cuifeng Sun","Shaoyu Huang","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2508.02056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00599v2","updated":"2025-08-04T04:44:14Z","published":"2025-08-01T12:56:39Z","title":"DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior","summary":"  We present DPoser-X, a diffusion-based prior model for 3D whole-body human\nposes. Building a versatile and robust full-body human pose prior remains\nchallenging due to the inherent complexity of articulated human poses and the\nscarcity of high-quality whole-body pose datasets. To address these\nlimitations, we introduce a Diffusion model as body Pose prior (DPoser) and\nextend it to DPoser-X for expressive whole-body human pose modeling. Our\napproach unifies various pose-centric tasks as inverse problems, solving them\nthrough variational diffusion sampling. To enhance performance on downstream\napplications, we introduce a novel truncated timestep scheduling method\nspecifically designed for pose data characteristics. We also propose a masked\ntraining mechanism that effectively combines whole-body and part-specific\ndatasets, enabling our model to capture interdependencies between body parts\nwhile avoiding overfitting to specific actions. Extensive experiments\ndemonstrate DPoser-X's robustness and versatility across multiple benchmarks\nfor body, hand, face, and full-body pose modeling. Our model consistently\noutperforms state-of-the-art alternatives, establishing a new benchmark for\nwhole-body human pose prior modeling.\n","authors":["Junzhe Lu","Jing Lin","Hongkun Dou","Ailing Zeng","Yue Deng","Xian Liu","Zhongang Cai","Lei Yang","Yulun Zhang","Haoqian Wang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2508.00599v2.pdf","comment":"ICCV 2025 (oral); Code released: https://github.com/moonbow721/DPoser"},{"id":"http://arxiv.org/abs/2508.02051v1","updated":"2025-08-04T04:37:56Z","published":"2025-08-04T04:37:56Z","title":"HCF: Hierarchical Cascade Framework for Distributed Multi-Stage Image\n  Compression","summary":"  Distributed multi-stage image compression -- where visual content traverses\nmultiple processing nodes under varying quality requirements -- poses\nchallenges. Progressive methods enable bitstream truncation but underutilize\navailable compute resources; successive compression repeats costly pixel-domain\noperations and suffers cumulative quality loss and inefficiency;\nfixed-parameter models lack post-encoding flexibility. In this work, we\ndeveloped the Hierarchical Cascade Framework (HCF) that achieves high\nrate-distortion performance and better computational efficiency through direct\nlatent-space transformations across network nodes in distributed multi-stage\nimage compression system. Under HCF, we introduced policy-driven quantization\ncontrol to optimize rate-distortion trade-offs, and established the edge\nquantization principle through differential entropy analysis. The configuration\nbased on this principle demonstrates up to 0.6dB PSNR gains over other\nconfigurations. When comprehensively evaluated on the Kodak, CLIC, and\nCLIC2020-mobile datasets, HCF outperforms successive-compression methods by up\nto 5.56% BD-Rate in PSNR on CLIC, while saving up to 97.8% FLOPs, 96.5% GPU\nmemory, and 90.0% execution time. It also outperforms state-of-the-art\nprogressive compression methods by up to 12.64% BD-Rate on Kodak and enables\nretraining-free cross-quality adaptation with 7.13-10.87% BD-Rate reductions on\nCLIC2020-mobile.\n","authors":["Junhao Cai","Taegun An","Chengjun Jin","Sung Il Choi","JuHyun Park","Changhee Joo"],"pdf_url":"https://arxiv.org/pdf/2508.02051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02047v1","updated":"2025-08-04T04:29:06Z","published":"2025-08-04T04:29:06Z","title":"Mapillary Vistas Validation for Fine-Grained Traffic Signs: A Benchmark\n  Revealing Vision-Language Model Limitations","summary":"  Obtaining high-quality fine-grained annotations for traffic signs is critical\nfor accurate and safe decision-making in autonomous driving. Widely used\ndatasets, such as Mapillary, often provide only coarse-grained labels - without\ndistinguishing semantically important types such as stop signs or speed limit\nsigns. To this end, we present a new validation set for traffic signs derived\nfrom the Mapillary dataset called Mapillary Vistas Validation for Traffic Signs\n(MVV), where we decompose composite traffic signs into granular, semantically\nmeaningful categories. The dataset includes pixel-level instance masks and has\nbeen manually annotated by expert annotators to ensure label fidelity. Further,\nwe benchmark several state-of-the-art VLMs against the self-supervised DINOv2\nmodel on this dataset and show that DINOv2 consistently outperforms all VLM\nbaselines-not only on traffic sign recognition, but also on heavily represented\ncategories like vehicles and humans. Our analysis reveals significant\nlimitations in current vision-language models for fine-grained visual\nunderstanding and establishes DINOv2 as a strong baseline for dense semantic\nmatching in autonomous driving scenarios. This dataset and evaluation framework\npave the way for more reliable, interpretable, and scalable perception systems.\n  Code and data are available at: https://github.com/nec-labs-ma/relabeling\n","authors":["Sparsh Garg","Abhishek Aich"],"pdf_url":"https://arxiv.org/pdf/2508.02047v1.pdf","comment":"Accepted to ICCV 2025 Workshop (4th DataCV Workshop and Challenge)"},{"id":"http://arxiv.org/abs/2508.02043v1","updated":"2025-08-04T04:25:47Z","published":"2025-08-04T04:25:47Z","title":"Conditional Diffusion Model with Anatomical-Dose Dual Constraints for\n  End-to-End Multi-Tumor Dose Prediction","summary":"  Radiotherapy treatment planning often relies on time-consuming,\ntrial-and-error adjustments that heavily depend on the expertise of\nspecialists, while existing deep learning methods face limitations in\ngeneralization, prediction accuracy, and clinical applicability. To tackle\nthese challenges, we propose ADDiff-Dose, an Anatomical-Dose Dual Constraints\nConditional Diffusion Model for end-to-end multi-tumor dose prediction. The\nmodel employs LightweightVAE3D to compress high-dimensional CT data and\nintegrates multimodal inputs, including target and organ-at-risk (OAR) masks\nand beam parameters, within a progressive noise addition and denoising\nframework. It incorporates conditional features via a multi-head attention\nmechanism and utilizes a composite loss function combining MSE, conditional\nterms, and KL divergence to ensure both dosimetric accuracy and compliance with\nclinical constraints. Evaluation on a large-scale public dataset (2,877 cases)\nand three external institutional cohorts (450 cases in total) demonstrates that\nADDiff-Dose significantly outperforms traditional baselines, achieving an MAE\nof 0.101-0.154 (compared to 0.316 for UNet and 0.169 for GAN models), a DICE\ncoefficient of 0.927 (a 6.8% improvement), and limiting spinal cord maximum\ndose error to within 0.1 Gy. The average plan generation time per case is\nreduced to 22 seconds. Ablation studies confirm that the structural encoder\nenhances compliance with clinical dose constraints by 28.5%. To our knowledge,\nthis is the first study to introduce a conditional diffusion model framework\nfor radiotherapy dose prediction, offering a generalizable and efficient\nsolution for automated treatment planning across diverse tumor sites, with the\npotential to substantially reduce planning time and improve clinical workflow\nefficiency.\n","authors":["Hui Xie","Haiqin Hu","Lijuan Ding","Qing Li","Yue Sun","Tao Tan"],"pdf_url":"https://arxiv.org/pdf/2508.02043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12937v2","updated":"2025-08-04T04:22:09Z","published":"2025-03-17T08:51:44Z","title":"R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization","summary":"  Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods.\n","authors":["Jingyi Zhang","Jiaxing Huang","Huanjin Yao","Shunyu Liu","Xikun Zhang","Shijian Lu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2503.12937v2.pdf","comment":"ICCV 2025 Camera Ready"},{"id":"http://arxiv.org/abs/2507.19121v2","updated":"2025-08-04T04:21:06Z","published":"2025-07-25T09:58:41Z","title":"Preserving Topological and Geometric Embeddings for Point Cloud Recovery","summary":"  Recovering point clouds involves the sequential process of sampling and\nrestoration, yet existing methods struggle to effectively leverage both\ntopological and geometric attributes. To address this, we propose an end-to-end\narchitecture named \\textbf{TopGeoFormer}, which maintains these critical\nproperties throughout the sampling and restoration phases. First, we revisit\ntraditional feature extraction techniques to yield topological embedding using\na continuous mapping of relative relationships between neighboring points, and\nintegrate it in both phases for preserving the structure of the original space.\nSecond, we propose the \\textbf{InterTwining Attention} to fully merge\ntopological and geometric embeddings, which queries shape with local awareness\nin both phases to form a learnable 3D shape context facilitated with\npoint-wise, point-shape-wise, and intra-shape features. Third, we introduce a\nfull geometry loss and a topological constraint loss to optimize the embeddings\nin both Euclidean and topological spaces. The geometry loss uses inconsistent\nmatching between coarse-to-fine generations and targets for reconstructing\nbetter geometric details, and the constraint loss limits embedding variances\nfor better approximation of the topological space. In experiments, we\ncomprehensively analyze the circumstances using the conventional and\nlearning-based sampling/upsampling/recovery algorithms. The quantitative and\nqualitative results demonstrate that our method significantly outperforms\nexisting sampling and recovery methods.\n","authors":["Kaiyue Zhou","Zelong Tan","Hongxiao Wang","Ya-li Li","Shengjin Wang"],"pdf_url":"https://arxiv.org/pdf/2507.19121v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13389v4","updated":"2025-08-04T04:20:16Z","published":"2025-05-19T17:30:13Z","title":"VSA: Faster Video Diffusion with Trainable Sparse Attention","summary":"  Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D\nattention, even though most of the attention mass concentrates on a small\nsubset of positions. We turn this observation into VSA, a trainable,\nhardware-efficient sparse attention that replaces full attention at \\emph{both}\ntraining and inference. In VSA, a lightweight coarse stage pools tokens into\ntiles and identifies high-weight \\emph{critical tokens}; a fine stage computes\ntoken-level attention only inside those tiles subjecting to block computing\nlayout to ensure hard efficiency. This leads to a single differentiable kernel\nthat trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of\nFlashAttention3 MFU. We perform a large sweep of ablation studies and\nscaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA\nreaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in\ndiffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention\ntime by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with\ncomparable quality. These results establish trainable sparse attention as a\npractical alternative to full attention and a key enabler for further scaling\nof video diffusion models. Code will be available at\nhttps://github.com/hao-ai-lab/FastVideo.\n","authors":["Peiyuan Zhang","Yongqi Chen","Haofeng Huang","Will Lin","Zhengzhong Liu","Ion Stoica","Eric Xing","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.13389v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20619v2","updated":"2025-08-04T04:15:15Z","published":"2025-02-28T00:56:46Z","title":"Style Content Decomposition-based Data Augmentation for Domain\n  Generalizable Medical Image Segmentation","summary":"  Due to domain shifts across diverse medical imaging modalities, learned\nsegmentation models often suffer significant performance degradation during\ndeployment. These domain shifts, typically caused by variations in imaging\nsystems, generally comprise two principal components: 1) \\textbf{\"style\"\nshifts}, referring to global disparities in image properties such as\nillumination, contrast, and color; and 2) \\textbf{\"content\" shifts}, which\ninvolve local discrepancies in anatomical structures. To address domain shifts\nin medical image segmentation, a core challenge arises: how can we decouple the\nfactors within images that determine their \"style\" and \"content\" components? To\nthis end, we first propose a linear style-content decomposition method that\nfactorizes an image into style codes and content maps, explicitly modeling the\n\"style\" and \"content\" components. Building on this, we introduce a\n\\textbf{Sty}le-\\textbf{Con}tent decomposition-based data \\textbf{a}ugmentation\nalgorithm (StyCona), which leverages this decomposition strategy to guide\naugmentation of both the global style and local content of source-domain\nimages, enabling the training of a well-generalized model for\ndomain-generalizable medical image segmentation. StyCona is a simple yet\neffective plug-and-play module that substantially improves model generalization\nwithout requiring additional training parameters or modifications to\nsegmentation model architectures. Experiments on cardiac magnetic resonance\nimaging and fundus photography segmentation tasks, with single and multiple\ntarget domains respectively, demonstrate the effectiveness of StyCona and its\nsuperiority over state-of-the-art domain generalization methods. The code will\nbe released at https://github.com/Senyh/StyCona.\n","authors":["Zhiqiang Shen","Peng Cao","Jinzhu Yang","Osmar R. Zaiane","Zhaolin Chen"],"pdf_url":"https://arxiv.org/pdf/2502.20619v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10340v3","updated":"2025-08-04T04:07:02Z","published":"2025-07-14T14:44:59Z","title":"Text Embedding Knows How to Quantize Text-Guided Diffusion Models","summary":"  Despite the success of diffusion models in image generation tasks such as\ntext-to-image, the enormous computational complexity of diffusion models limits\ntheir use in resource-constrained environments. To address this, network\nquantization has emerged as a promising solution for designing efficient\ndiffusion models. However, existing diffusion model quantization methods do not\nconsider input conditions, such as text prompts, as an essential source of\ninformation for quantization. In this paper, we propose a novel quantization\nmethod dubbed Quantization of Language-to-Image diffusion models using text\nPrompts (QLIP). QLIP leverages text prompts to guide the selection of bit\nprecision for every layer at each time step. In addition, QLIP can be\nseamlessly integrated into existing quantization methods to enhance\nquantization efficiency. Our extensive experiments demonstrate the\neffectiveness of QLIP in reducing computational complexity and improving the\nquality of the generated images across various datasets.\n","authors":["Hongjae Lee","Myungjun Son","Dongjea Kang","Seung-Won Jung"],"pdf_url":"https://arxiv.org/pdf/2507.10340v3.pdf","comment":"ICCV 2025"},{"id":"http://arxiv.org/abs/2503.22881v2","updated":"2025-08-04T04:04:04Z","published":"2025-03-28T21:13:43Z","title":"Pairwise Matching of Intermediate Representations for Fine-grained\n  Explainability","summary":"  The differences between images belonging to fine-grained categories are often\nsubtle and highly localized, and existing explainability techniques for deep\nlearning models are often too diffuse to provide useful and interpretable\nexplanations. We propose a new explainability method (PAIR-X) that leverages\nboth intermediate model activations and backpropagated relevance scores to\ngenerate fine-grained, highly-localized pairwise visual explanations. We use\nanimal and building re-identification (re-ID) as a primary case study of our\nmethod, and we demonstrate qualitatively improved results over a diverse set of\nexplainability baselines on 35 public re-ID datasets. In interviews, animal\nre-ID experts found PAIR-X to be a meaningful improvement over existing\nbaselines for deep model explainability, and suggested that its visualizations\nwould be directly applicable to their work. We also propose a novel\nquantitative evaluation metric for our method, and demonstrate that PAIR-X\nvisualizations appear more plausible for correct image matches than incorrect\nones even when the model similarity score for the pairs is the same. By\nimproving interpretability, PAIR-X enables humans to better distinguish correct\nand incorrect matches. Our code is available at:\nhttps://github.com/pairx-explains/pairx\n","authors":["Lauren Shrack","Timm Haucke","Antoine Salaün","Arjun Subramonian","Sara Beery"],"pdf_url":"https://arxiv.org/pdf/2503.22881v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2504.17349v2","updated":"2025-08-04T16:14:00Z","published":"2025-04-24T08:10:10Z","title":"DRC: Enhancing Personalized Image Generation via Disentangled\n  Representation Composition","summary":"  Personalized image generation has emerged as a promising direction in\nmultimodal content creation. It aims to synthesize images tailored to\nindividual style preferences (e.g., color schemes, character appearances,\nlayout) and semantic intentions (e.g., emotion, action, scene contexts) by\nleveraging user-interacted history images and multimodal instructions. Despite\nnotable progress, existing methods -- whether based on diffusion models, large\nlanguage models, or Large Multimodal Models (LMMs) -- struggle to accurately\ncapture and fuse user style preferences and semantic intentions. In particular,\nthe state-of-the-art LMM-based method suffers from the entanglement of visual\nfeatures, leading to Guidance Collapse, where the generated images fail to\npreserve user-preferred styles or reflect the specified semantics.\n  To address these limitations, we introduce DRC, a novel personalized image\ngeneration framework that enhances LMMs through Disentangled Representation\nComposition. DRC explicitly extracts user style preferences and semantic\nintentions from history images and the reference image, respectively, to form\nuser-specific latent instructions that guide image generation within LMMs.\nSpecifically, it involves two critical learning stages: 1) Disentanglement\nlearning, which employs a dual-tower disentangler to explicitly separate style\nand semantic features, optimized via a reconstruction-driven paradigm with\ndifficulty-aware importance sampling; and 2) Personalized modeling, which\napplies semantic-preserving augmentations to effectively adapt the disentangled\nrepresentations for robust personalized generation. Extensive experiments on\ntwo benchmarks demonstrate that DRC shows competitive performance while\neffectively mitigating the guidance collapse issue, underscoring the importance\nof disentangled representation learning for controllable and effective\npersonalized image generation.\n","authors":["Yiyan Xu","Wuqiang Zheng","Wenjie Wang","Fengbin Zhu","Xinting Hu","Yang Zhang","Fuli Feng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2504.17349v2.pdf","comment":"Accepted for publication in ACM MM'25"},{"id":"http://arxiv.org/abs/2508.02538v1","updated":"2025-08-04T15:45:48Z","published":"2025-08-04T15:45:48Z","title":"Hubness Reduction with Dual Bank Sinkhorn Normalization for Cross-Modal\n  Retrieval","summary":"  The past decade has witnessed rapid advancements in cross-modal retrieval,\nwith significant progress made in accurately measuring the similarity between\ncross-modal pairs. However, the persistent hubness problem, a phenomenon where\na small number of targets frequently appear as nearest neighbors to numerous\nqueries, continues to hinder the precision of similarity measurements. Despite\nseveral proposed methods to reduce hubness, their underlying mechanisms remain\npoorly understood. To bridge this gap, we analyze the widely-adopted Inverted\nSoftmax approach and demonstrate its effectiveness in balancing target\nprobabilities during retrieval. Building on these insights, we propose a\nprobability-balancing framework for more effective hubness reduction. We\ncontend that balancing target probabilities alone is inadequate and, therefore,\nextend the framework to balance both query and target probabilities by\nintroducing Sinkhorn Normalization (SN). Notably, we extend SN to scenarios\nwhere the true query distribution is unknown, showing that current methods,\nwhich rely solely on a query bank to estimate target hubness, produce\nsuboptimal results due to a significant distributional gap between the query\nbank and targets. To mitigate this issue, we introduce Dual Bank Sinkhorn\nNormalization (DBSN), incorporating a corresponding target bank alongside the\nquery bank to narrow this distributional gap. Our comprehensive evaluation\nacross various cross-modal retrieval tasks, including image-text retrieval,\nvideo-text retrieval, and audio-text retrieval, demonstrates consistent\nperformance improvements, validating the effectiveness of both SN and DBSN. All\ncodes are publicly available at https://github.com/ppanzx/DBSN.\n","authors":["Zhengxin Pan","Haishuai Wang","Fangyu Wu","Peng Zhang","Jiajun Bu"],"pdf_url":"https://arxiv.org/pdf/2508.02538v1.pdf","comment":"ACMMM 2025"},{"id":"http://arxiv.org/abs/2508.02506v1","updated":"2025-08-04T15:14:09Z","published":"2025-08-04T15:14:09Z","title":"Decomposed Reasoning with Reinforcement Learning for Relevance\n  Assessment in UGC Platforms","summary":"  Retrieval-augmented generation (RAG) plays a critical role in user-generated\ncontent (UGC) platforms, but its effectiveness depends heavily on accurate\nrelevance assessment of query-document pairs. Despite recent advances in\napplying large language models (LLMs) to relevance modeling, UGC platforms\npresent unique challenges: 1) ambiguous user intent due to sparse user feedback\nin RAG scenarios, and 2) substantial noise introduced by informal and\nunstructured language. To address these issues, we propose the Reinforced\nReasoning Model for Relevance Assessment (R3A), which introduces a decomposed\nreasoning framework over queries and candidate documents before scoring. R3A\nfirst leverages auxiliary high-ranked documents within the platform to infer\nlatent query intent. It then performs verbatim fragment extraction to justify\nrelevance decisions, thereby reducing errors caused by noisy UGC. Based on a\nreinforcement learning framework, R3A is optimized to mitigate distortions\narising from ambiguous queries and unstructured content. Experimental results\nshow that R3A significantly outperforms existing baseline methods in terms of\nrelevance accuracy, across both offline benchmarks and online experiments.\n","authors":["Xiaowei Yuan","Lei Jin","Haoxin Zhang","Yan Gao","Yi Wu","Yao Hu","Ziyang Huang","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2508.02506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20243v3","updated":"2025-08-04T14:52:16Z","published":"2025-05-26T17:21:26Z","title":"It's High Time: A Survey of Temporal Question Answering","summary":"  Time plays a critical role in how information is generated, retrieved, and\ninterpreted. In this survey, we provide a comprehensive overview of Temporal\nQuestion Answering (TQA), a research area that focuses on answering questions\ninvolving temporal constraints or context. As the amount of time-stamped\ncontent from sources like news articles, web archives, and knowledge bases\nincreases, systems must address challenges such as detecting temporal intent,\nnormalizing time expressions, ordering events, and reasoning over evolving or\nambiguous facts. We focus on recent advances in TQA enabled by neural\narchitectures, especially transformer-based models and Large Language Models\n(LLMs), highlighting progress in temporal language modeling,\nretrieval-augmented generation (RAG), and temporal reasoning. We also discuss\nbenchmark datasets and evaluation strategies designed to test temporal\nrobustness, recency awareness, and generalization.\n","authors":["Bhawna Piryani","Abdelrahman Abdallah","Jamshid Mozafari","Avishek Anand","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2505.20243v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17356v2","updated":"2025-08-04T14:20:55Z","published":"2025-07-23T09:37:23Z","title":"\"Beyond the past\": Leveraging Audio and Human Memory for Sequential\n  Music Recommendation","summary":"  On music streaming services, listening sessions are often composed of a\nbalance of familiar and new tracks. Recently, sequential recommender systems\nhave adopted cognitive-informed approaches, such as Adaptive Control of\nThought-Rational (ACT-R), to successfully improve the prediction of the most\nrelevant tracks for the next user session. However, one limitation of using a\nmodel inspired by human memory (or the past), is that it struggles to recommend\nnew tracks that users have not previously listened to. To bridge this gap, here\nwe propose a model that leverages audio information to predict in advance the\nACT-R-like activation of new tracks and incorporates them into the\nrecommendation scoring process. We demonstrate the empirical effectiveness of\nthe proposed model using proprietary data, which we publicly release along with\nthe model's source code to foster future research in this field.\n","authors":["Viet-Anh Tran","Bruno Sguerra","Gabriel Meseguer-Brocal","Lea Briand","Manuel Moussallam"],"pdf_url":"https://arxiv.org/pdf/2507.17356v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02455v1","updated":"2025-08-04T14:20:39Z","published":"2025-08-04T14:20:39Z","title":"TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions\n  in IDEs","summary":"  Token-level code completion is one of the most critical features in modern\nIntegrated Development Environments (IDEs). It assists developers by suggesting\nrelevant identifiers and APIs during coding. While completions are typically\nderived from static analysis, their usefulness depends heavily on how they are\nranked, as correct predictions buried deep in the list are rarely seen by\nusers. Most current systems rely on hand-crafted heuristics or lightweight\nmachine learning models trained on user logs, which can be further improved to\ncapture context information and generalize across projects and coding styles.\nIn this work, we propose a new scoring approach to ranking static completions\nusing language models in a lightweight and model-agnostic way. Our method\norganizes all valid completions into a prefix tree and performs a single greedy\ndecoding pass to collect token-level scores across the tree. This enables a\nprecise token-aware ranking without needing beam search, prompt engineering, or\nmodel adaptations. The approach is fast, architecture-agnostic, and compatible\nwith already deployed models for code completion. These findings highlight a\npractical and effective pathway for integrating language models into already\nexisting tools within IDEs, and ultimately providing smarter and more\nresponsive developer assistance.\n","authors":["Daniele Cipollone","Egor Bogomolov","Arie van Deursen","Maliheh Izadi"],"pdf_url":"https://arxiv.org/pdf/2508.02455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02451v1","updated":"2025-08-04T14:16:49Z","published":"2025-08-04T14:16:49Z","title":"Dynamic Forgetting and Spatio-Temporal Periodic Interest Modeling for\n  Local-Life Service Recommendation","summary":"  In the context of the booming digital economy, recommendation systems, as a\nkey link connecting users and numerous services, face challenges in modeling\nuser behavior sequences on local-life service platforms, including the sparsity\nof long sequences and strong spatio-temporal dependence. Such challenges can be\naddressed by drawing an analogy to the forgetting process in human memory. This\nis because users' responses to recommended content follow the recency effect\nand the cyclicality of memory. By exploring this, this paper introduces the\nforgetting curve and proposes Spatio-Temporal periodic Interest Modeling (STIM)\nwith long sequences for local-life service recommendation. STIM integrates\nthree key components: a dynamic masking module based on the forgetting curve,\nwhich is used to extract both recent spatiotemporal features and periodic\nspatiotemporal features; a query-based mixture of experts (MoE) approach that\ncan adaptively activate expert networks under different dynamic masks, enabling\nthe collaborative modeling of time, location, and items; and a hierarchical\nmulti-interest network unit, which captures multi-interest representations by\nmodeling the hierarchical interactions between the shallow and deep semantics\nof users' recent behaviors. By introducing the STIM method, we conducted online\nA/B tests and achieved a 1.54\\% improvement in gross transaction volume (GTV).\nIn addition, extended offline experiments also showed improvements. STIM has\nbeen deployed in a large-scale local-life service recommendation system,\nserving hundreds of millions of daily active users in core application\nscenarios.\n","authors":["Zhaoyu Hu","Hao Guo","Yuan Tian","Erpeng Xue","Jianyang Wang","Xianyang Qi","Hongxiang Lin","Lei Wang","Sheng Chen"],"pdf_url":"https://arxiv.org/pdf/2508.02451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02435v1","updated":"2025-08-04T13:50:44Z","published":"2025-08-04T13:50:44Z","title":"Beyond Chunks and Graphs: Retrieval-Augmented Generation through\n  Triplet-Driven Thinking","summary":"  Retrieval-augmented generation (RAG) is critical for reducing hallucinations\nand incorporating external knowledge into Large Language Models (LLMs).\nHowever, advanced RAG systems face a trade-off between performance and\nefficiency. Multi-round RAG approaches achieve strong reasoning but incur\nexcessive LLM calls and token costs, while Graph RAG methods suffer from\ncomputationally expensive, error-prone graph construction and retrieval\nredundancy. To address these challenges, we propose T$^2$RAG, a novel framework\nthat operates on a simple, graph-free knowledge base of atomic triplets.\nT$^2$RAG leverages an LLM to decompose questions into searchable triplets with\nplaceholders, which it then iteratively resolves by retrieving evidence from\nthe triplet database. Empirical results show that T$^2$RAG significantly\noutperforms state-of-the-art multi-round and Graph RAG methods, achieving an\naverage performance gain of up to 11\\% across six datasets while reducing\nretrieval costs by up to 45\\%. Our code is available at\nhttps://github.com/rockcor/T2RAG\n","authors":["Shengbo Gong","Xianfeng Tang","Carl Yang","Wei jin"],"pdf_url":"https://arxiv.org/pdf/2508.02435v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2508.02383v1","updated":"2025-08-04T13:09:47Z","published":"2025-08-04T13:09:47Z","title":"Graph Embedding in the Graph Fractional Fourier Transform Domain","summary":"  Spectral graph embedding plays a critical role in graph representation\nlearning by generating low-dimensional vector representations from graph\nspectral information. However, the embedding space of traditional spectral\nembedding methods often exhibit limited expressiveness, failing to exhaustively\ncapture latent structural features across alternative transform domains. To\naddress this issue, we use the graph fractional Fourier transform to extend the\nexisting state-of-the-art generalized frequency filtering embedding (GEFFE)\ninto fractional domains, giving birth to the generalized fractional filtering\nembedding (GEFRFE), which enhances embedding informativeness via the graph\nfractional domain. The GEFRFE leverages graph fractional domain filtering and a\nnonlinear composition of eigenvector components derived from a fractionalized\ngraph Laplacian. To dynamically determine the fractional order, two parallel\nstrategies are introduced: search-based optimization and a ResNet18-based\nadaptive learning. Extensive experiments on six benchmark datasets demonstrate\nthat the GEFRFE captures richer structural features and significantly enhance\nclassification performance. Notably, the proposed method retains computational\ncomplexity comparable to GEFFE approaches.\n","authors":["Changjie Sheng","Zhichao Zhang","Wei Yao"],"pdf_url":"https://arxiv.org/pdf/2508.02383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02374v1","updated":"2025-08-04T13:02:23Z","published":"2025-08-04T13:02:23Z","title":"Uni-Layout: Integrating Human Feedback in Unified Layout Generation and\n  Evaluation","summary":"  Layout generation plays a crucial role in enhancing both user experience and\ndesign efficiency. However, current approaches suffer from task-specific\ngeneration capabilities and perceptually misaligned evaluation metrics, leading\nto limited applicability and ineffective measurement. In this paper, we propose\n\\textit{Uni-Layout}, a novel framework that achieves unified generation,\nhuman-mimicking evaluation and alignment between the two. For universal\ngeneration, we incorporate various layout tasks into a single taxonomy and\ndevelop a unified generator that handles background or element contents\nconstrained tasks via natural language prompts. To introduce human feedback for\nthe effective evaluation of layouts, we build \\textit{Layout-HF100k}, the first\nlarge-scale human feedback dataset with 100,000 expertly annotated layouts.\nBased on \\textit{Layout-HF100k}, we introduce a human-mimicking evaluator that\nintegrates visual and geometric information, employing a Chain-of-Thought\nmechanism to conduct qualitative assessments alongside a confidence estimation\nmodule to yield quantitative measurements. For better alignment between the\ngenerator and the evaluator, we integrate them into a cohesive system by\nadopting Dynamic-Margin Preference Optimization (DMPO), which dynamically\nadjusts margins based on preference strength to better align with human\njudgments. Extensive experiments show that \\textit{Uni-Layout} significantly\noutperforms both task-specific and general-purpose methods. Our code is\npublicly available at https://github.com/JD-GenX/Uni-Layout.\n","authors":["Shuo Lu","Yanyin Chen","Wei Feng","Jiahao Fan","Fengheng Li","Zheng Zhang","Jingjing Lv","Junjie Shen","Ching Law","Jian Liang"],"pdf_url":"https://arxiv.org/pdf/2508.02374v1.pdf","comment":"Accepted to ACM MM 2025"},{"id":"http://arxiv.org/abs/2508.02342v1","updated":"2025-08-04T12:22:25Z","published":"2025-08-04T12:22:25Z","title":"Agentic Personalized Fashion Recommendation in the Age of Generative AI:\n  Challenges, Opportunities, and Evaluation","summary":"  Fashion recommender systems (FaRS) face distinct challenges due to rapid\ntrend shifts, nuanced user preferences, intricate item-item compatibility, and\nthe complex interplay among consumers, brands, and influencers. Traditional\nrecommendation approaches, largely static and retrieval-focused, struggle to\neffectively capture these dynamic elements, leading to decreased user\nsatisfaction and elevated return rates. This paper synthesizes both academic\nand industrial viewpoints to map the distinctive output space and stakeholder\necosystem of modern FaRS, identifying the complex interplay among users,\nbrands, platforms, and influencers, and highlighting the unique data and\nmodeling challenges that arise.\n  We outline a research agenda for industrial FaRS, centered on five\nrepresentative scenarios spanning static queries, outfit composition, and\nmulti-turn dialogue, and argue that mixed-modality refinement-the ability to\ncombine image-based references (anchors) with nuanced textual constraints-is a\nparticularly critical task for real-world deployment. To this end, we propose\nan Agentic Mixed-Modality Refinement (AMMR) pipeline, which fuses multimodal\nencoders with agentic LLM planners and dynamic retrieval, bridging the gap\nbetween expressive user intent and fast-changing fashion inventories. Our work\nshows that moving beyond static retrieval toward adaptive, generative, and\nstakeholder-aware systems is essential to satisfy the evolving expectations of\nfashion consumers and brands.\n","authors":["Yashar Deldjoo","Nima Rafiee","Mahdyar Ravanbakhsh"],"pdf_url":"https://arxiv.org/pdf/2508.02342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02340v1","updated":"2025-08-04T12:21:16Z","published":"2025-08-04T12:21:16Z","title":"Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search","summary":"  Ad-hoc Video Search (AVS) involves using a textual query to search for\nmultiple relevant videos in a large collection of unlabeled short videos. The\nmain challenge of AVS is the visual diversity of relevant videos. A simple\nquery such as \"Find shots of a man and a woman dancing together indoors\" can\nspan a multitude of environments, from brightly lit halls and shadowy bars to\ndance scenes in black-and-white animations. It is therefore essential to\nretrieve relevant videos as comprehensively as possible. Current solutions for\nthe AVS task primarily fuse multiple features into one or more common spaces,\nyet overlook the need for diverse spaces. To fully exploit the expressive\ncapability of individual features, we propose LPD, short for Learning Partially\nDecorrelated common spaces. LPD incorporates two key innovations:\nfeature-specific common space construction and the de-correlation loss.\nSpecifically, LPD learns a separate common space for each video and text\nfeature, and employs de-correlation loss to diversify the ordering of negative\nsamples across different spaces. To enhance the consistency of multi-space\nconvergence, we designed an entropy-based fair multi-space triplet ranking\nloss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify\nthe effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces\nhighlight its ability to enhance result diversity.\n","authors":["Fan Hu","Zijie Xin","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2508.02340v1.pdf","comment":"Accepted by ACMMM2025"},{"id":"http://arxiv.org/abs/2508.02328v1","updated":"2025-08-04T11:56:47Z","published":"2025-08-04T11:56:47Z","title":"Understanding User Preferences for Interaction Styles in Conversational\n  Recommender Systems: The Predictive Role of System Qualities, User\n  Experience, and Traits","summary":"  Conversational Recommender Systems (CRSs) deliver personalised\nrecommendations through multi-turn natural language dialogue and increasingly\nsupport both task-oriented and exploratory interactions. Yet, the factors\nshaping user interaction preferences remain underexplored. In this\nwithin-subjects study (\\(N = 139\\)), participants experienced two scripted CRS\ndialogues, rated their experiences, and indicated the importance of eight\nsystem qualities. Logistic regression revealed that preference for the\nexploratory interaction was predicted by enjoyment, usefulness, novelty, and\nconversational quality. Unexpectedly, perceived effectiveness was also\nassociated with exploratory preference. Clustering uncovered five latent user\nprofiles with distinct dialogue style preferences. Moderation analyses\nindicated that age, gender, and control preference significantly influenced\nthese choices. These findings integrate affective, cognitive, and trait-level\npredictors into CRS user modelling and inform autonomy-sensitive,\nvalue-adaptive dialogue design. The proposed predictive and adaptive framework\napplies broadly to conversational AI systems seeking to align dynamically with\nevolving user needs.\n","authors":["Raj Mahmud","Shlomo Berkovsky","Mukesh Prasad","A. Baki Kocaballi"],"pdf_url":"https://arxiv.org/pdf/2508.02328v1.pdf","comment":"Accepted at OZCHI 2025. 21 pages, 9 figures, 8 tables"},{"id":"http://arxiv.org/abs/2508.02300v1","updated":"2025-08-04T11:11:51Z","published":"2025-08-04T11:11:51Z","title":"Research Knowledge Graphs in NFDI4DataScience: Key Activities,\n  Achievements, and Future Directions","summary":"  As research in Artificial Intelligence and Data Science continues to grow in\nvolume and complexity, it becomes increasingly difficult to ensure\ntransparency, reproducibility, and discoverability. To address these\nchallenges, as research artifacts should be understandable and usable by\nmachines, the NFDI4DataScience consortium is developing and providing Research\nKnowledge Graphs (RKGs). Building upon earlier works, this paper presents\nrecent progress in creating semantically rich RKGs using standardized\nontologies, shared vocabularies, and automated Information Extraction\ntechniques. Key achievements include the development of the NFDI4DS ontology,\nmetadata standards, tools, and services designed to support the FAIR\nprinciples, as well as community-led projects and various implementations of\nRKGs. Together, these efforts aim to capture and connect the complex\nrelationships between datasets, models, software, and scientific publications.\n","authors":["Kanishka Silva","Marcel R. Ackermann","Heike Fliegl","Genet-Asefa Gesese","Fidan Limani","Philipp Mayr","Peter Mutschke","Allard Oelen","Muhammad Asif Suryani","Sharmila Upadhyaya","Benjamin Zapilko","Harald Sack","Stefan Dietze"],"pdf_url":"https://arxiv.org/pdf/2508.02300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02266v1","updated":"2025-08-04T10:16:48Z","published":"2025-08-04T10:16:48Z","title":"Voronoi Diagram Encoded Hashing","summary":"  The goal of learning to hash (L2H) is to derive data-dependent hash functions\nfrom a given data distribution in order to map data from the input space to a\nbinary coding space. Despite the success of L2H, two observations have cast\ndoubt on the source of the power of L2H, i.e., learning. First, a recent study\nshows that even using a version of locality sensitive hashing functions without\nlearning achieves binary representations that have comparable accuracy as those\nof L2H, but with less time cost. Second, existing L2H methods are constrained\nto three types of hash functions: thresholding, hyperspheres, and hyperplanes\nonly. In this paper, we unveil the potential of Voronoi diagrams in hashing.\nVoronoi diagram is a suitable candidate because of its three properties. This\ndiscovery has led us to propose a simple and efficient no-learning binary\nhashing method, called Voronoi Diagram Encoded Hashing (VDeH), which constructs\na set of hash functions through a data-dependent similarity measure and\nproduces independent binary bits through encoded hashing. We demonstrate\nthrough experiments on several benchmark datasets that VDeH achieves superior\nperformance and lower computational cost compared to existing state-of-the-art\nmethods under the same bit length.\n","authors":["Yang Xu","Kai Ming Ting"],"pdf_url":"https://arxiv.org/pdf/2508.02266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02243v1","updated":"2025-08-04T09:43:54Z","published":"2025-08-04T09:43:54Z","title":"I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal\n  Entity Linking","summary":"  Multimodal entity linking plays a crucial role in a wide range of\napplications. Recent advances in large language model-based methods have become\nthe dominant paradigm for this task, effectively leveraging both textual and\nvisual modalities to enhance performance. Despite their success, these methods\nstill face two challenges, including unnecessary incorporation of image data in\ncertain scenarios and the reliance only on a one-time extraction of visual\nfeatures, which can undermine their effectiveness and accuracy. To address\nthese challenges, we propose a novel LLM-based framework for the multimodal\nentity linking task, called Intra- and Inter-modal Collaborative Reflections.\nThis framework prioritizes leveraging text information to address the task.\nWhen text alone is insufficient to link the correct entity through intra- and\ninter-modality evaluations, it employs a multi-round iterative strategy that\nintegrates key visual clues from various aspects of the image to support\nreasoning and enhance matching accuracy. Extensive experiments on three widely\nused public datasets demonstrate that our framework consistently outperforms\ncurrent state-of-the-art methods in the task, achieving improvements of 3.2%,\n5.1%, and 1.6%, respectively. Our code is available at\nhttps://github.com/ziyan-xiaoyu/I2CR/.\n","authors":["Ziyan Liu","Junwen Li","Kaiwen Li","Tong Ruan","Chao Wang","Xinyan He","Zongyu Wang","Xuezhi Cao","Jingping Liu"],"pdf_url":"https://arxiv.org/pdf/2508.02243v1.pdf","comment":"10 pages, 6 figures, accepted by ACMMM 2025"},{"id":"http://arxiv.org/abs/2508.02242v1","updated":"2025-08-04T09:43:21Z","published":"2025-08-04T09:43:21Z","title":"From Generation to Consumption: Personalized List Value Estimation for\n  Re-ranking","summary":"  Re-ranking is critical in recommender systems for optimizing the order of\nrecommendation lists, thus improving user satisfaction and platform revenue.\nMost existing methods follow a generator-evaluator paradigm, where the\nevaluator estimates the overall value of each candidate list. However, they\noften ignore the fact that users may exit before consuming the full list,\nleading to a mismatch between estimated generation value and actual consumption\nvalue. To bridge this gap, we propose CAVE, a personalized Consumption-Aware\nlist Value Estimation framework. CAVE formulates the list value as the\nexpectation over sub-list values, weighted by user-specific exit probabilities\nat each position. The exit probability is decomposed into an interest-driven\ncomponent and a stochastic component, the latter modeled via a Weibull\ndistribution to capture random external factors such as fatigue. By jointly\nmodeling sub-list values and user exit behavior, CAVE yields a more faithful\nestimate of actual list consumption value. We further contribute three\nlarge-scale real-world list-wise benchmarks from the Kuaishou platform, varying\nin size and user activity patterns. Extensive experiments on these benchmarks,\ntwo Amazon datasets, and online A/B testing on Kuaishou show that CAVE\nconsistently outperforms strong baselines, highlighting the benefit of\nexplicitly modeling user exits in re-ranking.\n","authors":["Kaike Zhang","Xiaobei Wang","Xiaoyu Liu","Shuchang Liu","Hailan Yang","Xiang Li","Fei Sun","Qi Cao"],"pdf_url":"https://arxiv.org/pdf/2508.02242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02222v1","updated":"2025-08-04T09:12:45Z","published":"2025-08-04T09:12:45Z","title":"FinCPRG: A Bidirectional Generation Pipeline for Hierarchical Queries\n  and Rich Relevance in Financial Chinese Passage Retrieval","summary":"  In recent years, large language models (LLMs) have demonstrated significant\npotential in constructing passage retrieval datasets. However, existing methods\nstill face limitations in expressing cross-doc query needs and controlling\nannotation quality. To address these issues, this paper proposes a\nbidirectional generation pipeline, which aims to generate 3-level hierarchical\nqueries for both intra-doc and cross-doc scenarios and mine additional\nrelevance labels on top of direct mapping annotation. The pipeline introduces\ntwo query generation methods: bottom-up from single-doc text and top-down from\nmulti-doc titles. The bottom-up method uses LLMs to disassemble and generate\nstructured queries at both sentence-level and passage-level simultaneously from\nintra-doc passages. The top-down approach incorporates three key financial\nelements--industry, topic, and time--to divide report titles into clusters and\nprompts LLMs to generate topic-level queries from each cluster. For relevance\nannotation, our pipeline not only relies on direct mapping annotation from the\ngeneration relationship but also implements an indirect positives mining method\nto enrich the relevant query-passage pairs. Using this pipeline, we constructed\na Financial Passage Retrieval Generated dataset (FinCPRG) from almost 1.3k\nChinese financial research reports, which includes hierarchical queries and\nrich relevance labels. Through evaluations of mined relevance labels,\nbenchmarking and training experiments, we assessed the quality of FinCPRG and\nvalidated its effectiveness as a passage retrieval dataset for both training\nand benchmarking.\n","authors":["Xuan Xu","Beilin Chu","Qinhong Lin","Yixiao Zhong","Fufang Wen","Jiaqi Liu","Binjie Fei","Yu Li","Zhongliang Yang","Linna Zhou"],"pdf_url":"https://arxiv.org/pdf/2508.02222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02096v1","updated":"2025-08-04T06:07:33Z","published":"2025-08-04T06:07:33Z","title":"Evaluating User Experience in Conversational Recommender Systems: A\n  Systematic Review Across Classical and LLM-Powered Approaches","summary":"  Conversational Recommender Systems (CRSs) are receiving growing research\nattention across domains, yet their user experience (UX) evaluation remains\nlimited. Existing reviews largely overlook empirical UX studies, particularly\nin adaptive and large language model (LLM)-based CRSs. To address this gap, we\nconducted a systematic review following PRISMA guidelines, synthesising 23\nempirical studies published between 2017 and 2025. We analysed how UX has been\nconceptualised, measured, and shaped by domain, adaptivity, and LLM.\n  Our findings reveal persistent limitations: post hoc surveys dominate,\nturn-level affective UX constructs are rarely assessed, and adaptive behaviours\nare seldom linked to UX outcomes. LLM-based CRSs introduce further challenges,\nincluding epistemic opacity and verbosity, yet evaluations infrequently address\nthese issues. We contribute a structured synthesis of UX metrics, a comparative\nanalysis of adaptive and nonadaptive systems, and a forward-looking agenda for\nLLM-aware UX evaluation. These findings support the development of more\ntransparent, engaging, and user-centred CRS evaluation practices.\n","authors":["Raj Mahmud","Yufeng Wu","Abdullah Bin Sawad","Shlomo Berkovsky","Mukesh Prasad","A. Baki Kocaballi"],"pdf_url":"https://arxiv.org/pdf/2508.02096v1.pdf","comment":"Accepted at OZCHI 2025. 23 pages, 1 figure, 5 tables"},{"id":"http://arxiv.org/abs/2508.02050v1","updated":"2025-08-04T04:33:26Z","published":"2025-08-04T04:33:26Z","title":"Why Generate When You Can Transform? Unleashing Generative Attention for\n  Dynamic Recommendation","summary":"  Sequential Recommendation (SR) focuses on personalizing user experiences by\npredicting future preferences based on historical interactions. Transformer\nmodels, with their attention mechanisms, have become the dominant architecture\nin SR tasks due to their ability to capture dependencies in user behavior\nsequences. However, traditional attention mechanisms, where attention weights\nare computed through query-key transformations, are inherently linear and\ndeterministic. This fixed approach limits their ability to account for the\ndynamic and non-linear nature of user preferences, leading to challenges in\ncapturing evolving interests and subtle behavioral patterns. Given that\ngenerative models excel at capturing non-linearity and probabilistic\nvariability, we argue that generating attention distributions offers a more\nflexible and expressive alternative compared to traditional attention\nmechanisms. To support this claim, we present a theoretical proof demonstrating\nthat generative attention mechanisms offer greater expressiveness and\nstochasticity than traditional deterministic approaches. Building upon this\ntheoretical foundation, we introduce two generative attention models for SR,\neach grounded in the principles of Variational Autoencoders (VAE) and Diffusion\nModels (DMs), respectively. These models are designed specifically to generate\nadaptive attention distributions that better align with variable user\npreferences. Extensive experiments on real-world datasets show our models\nsignificantly outperform state-of-the-art in both accuracy and diversity.\n","authors":["Yuli Liu","Wenjun Kong","Cheng Luo","Weizhi Ma"],"pdf_url":"https://arxiv.org/pdf/2508.02050v1.pdf","comment":"Accepted at ACMMM 2025"},{"id":"http://arxiv.org/abs/2508.02020v1","updated":"2025-08-04T03:30:26Z","published":"2025-08-04T03:30:26Z","title":"Evaluating Position Bias in Large Language Model Recommendations","summary":"  Large Language Models (LLMs) are being increasingly explored as\ngeneral-purpose tools for recommendation tasks, enabling zero-shot and\ninstruction-following capabilities without the need for task-specific training.\nWhile the research community is enthusiastically embracing LLMs, there are\nimportant caveats to directly adapting them for recommendation tasks. In this\npaper, we show that LLM-based recommendation models suffer from position bias,\nwhere the order of candidate items in a prompt can disproportionately influence\nthe recommendations produced by LLMs. First, we analyse the position bias of\nLLM-based recommendations on real-world datasets, where results uncover\nsystemic biases of LLMs with high sensitivity to input orders. Furthermore, we\nintroduce a new prompting strategy to mitigate the position bias of LLM\nrecommendation models called Ranking via Iterative SElection (RISE). We compare\nour proposed method against various baselines on key benchmark datasets.\nExperiment results show that our method reduces sensitivity to input ordering\nand improves stability without requiring model fine-tuning or post-processing.\n","authors":["Ethan Bito","Yongli Ren","Estrid He"],"pdf_url":"https://arxiv.org/pdf/2508.02020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01987v1","updated":"2025-08-04T01:54:32Z","published":"2025-08-04T01:54:32Z","title":"Controllable and Stealthy Shilling Attacks via Dispersive Latent\n  Diffusion","summary":"  Recommender systems (RSs) are now fundamental to various online platforms,\nbut their dependence on user-contributed data leaves them vulnerable to\nshilling attacks that can manipulate item rankings by injecting fake users.\nAlthough widely studied, most existing attack models fail to meet two critical\nobjectives simultaneously: achieving strong adversarial promotion of target\nitems while maintaining realistic behavior to evade detection. As a result, the\ntrue severity of shilling threats that manage to reconcile the two objectives\nremains underappreciated. To expose this overlooked vulnerability, we present\nDLDA, a diffusion-based attack framework that can generate highly effective yet\nindistinguishable fake users by enabling fine-grained control over target\npromotion. Specifically, DLDA operates in a pre-aligned collaborative embedding\nspace, where it employs a conditional latent diffusion process to iteratively\nsynthesize fake user profiles with precise target item control. To evade\ndetection, DLDA introduces a dispersive regularization mechanism that promotes\nvariability and realism in generated behavioral patterns. Extensive experiments\non three real-world datasets and five popular RS models demonstrate that,\ncompared to prior attacks, DLDA consistently achieves stronger item promotion\nwhile remaining harder to detect. These results highlight that modern RSs are\nmore vulnerable than previously recognized, underscoring the urgent need for\nmore robust defenses.\n","authors":["Shutong Qiao","Wei Yuan","Junliang Yu","Tong Chen","Quoc Viet Hung Nguyen","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2508.01987v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2508.02668v1","updated":"2025-08-04T17:58:22Z","published":"2025-08-04T17:58:22Z","title":"LOST: Low-rank and Sparse Pre-training for Large Language Models","summary":"  While large language models (LLMs) have achieved remarkable performance\nacross a wide range of tasks, their massive scale incurs prohibitive\ncomputational and memory costs for pre-training from scratch. Recent studies\nhave investigated the use of low-rank parameterization as a means of reducing\nmodel size and training cost. In this context, sparsity is often employed as a\ncomplementary technique to recover important information lost in low-rank\ncompression by capturing salient features in the residual space. However,\nexisting approaches typically combine low-rank and sparse components in a\nsimplistic or ad hoc manner, often resulting in undesirable performance\ndegradation compared to full-rank training. In this paper, we propose\n\\textbf{LO}w-rank and \\textbf{S}parse pre-\\textbf{T}raining (\\textbf{LOST}) for\nLLMs, a novel method that ingeniously integrates low-rank and sparse structures\nto enable effective training of LLMs from scratch under strict efficiency\nconstraints. LOST applies singular value decomposition to weight matrices,\npreserving the dominant low-rank components, while allocating the remaining\nsingular values to construct channel-wise sparse components to complement the\nexpressiveness of low-rank training. We evaluate LOST on LLM pretraining\nranging from 60M to 7B parameters. Our experiments show that LOST achieves\ncompetitive or superior performance compared to full-rank models, while\nsignificantly reducing both memory and compute overhead. Moreover, Code is\navailable at\n\\href{https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models}{LOST\nRepo}\n","authors":["Jiaxi Li","Lu Yin","Li Shen","Jinjin Xu","Liwu Xu","Tianjin Huang","Wenwu Wang","Shiwei Liu","Xilu Wang"],"pdf_url":"https://arxiv.org/pdf/2508.02668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02643v1","updated":"2025-08-04T17:33:36Z","published":"2025-08-04T17:33:36Z","title":"CAK: Emergent Audio Effects from Minimal Deep Learning","summary":"  We demonstrate that a single 3x3 convolutional kernel can produce emergent\naudio effects when trained on 200 samples from a personalized corpus. We\nachieve this through two key techniques: (1) Conditioning Aware Kernels (CAK),\nwhere output = input + (learned_pattern x control), with a soft-gate mechanism\nsupporting identity preservation at zero control; and (2) AuGAN (Audit GAN),\nwhich reframes adversarial training from \"is this real?\" to \"did you apply the\nrequested value?\" Rather than learning to generate or detect forgeries, our\nnetworks cooperate to verify control application, discovering unique\ntransformations. The learned kernel exhibits a diagonal structure creating\nfrequency-dependent temporal shifts that are capable of producing musical\neffects based on input characteristics. Our results show the potential of\nadversarial training to discover audio transformations from minimal data,\nenabling new approaches to effect design.\n","authors":["Austin Rockman"],"pdf_url":"https://arxiv.org/pdf/2508.02643v1.pdf","comment":"8 pages, 3 figures, code and other resources at\n  https://github.com/gloame-ai/cak-audio/tree/main/cak-audio"},{"id":"http://arxiv.org/abs/2508.02641v1","updated":"2025-08-04T17:25:55Z","published":"2025-08-04T17:25:55Z","title":"FastCSP: Accelerated Molecular Crystal Structure Prediction with\n  Universal Model for Atoms","summary":"  Crystal Structure Prediction (CSP) of molecular crystals plays a central role\nin applications, such as pharmaceuticals and organic electronics. CSP is\nchallenging and computationally expensive due to the need to explore a large\nsearch space with sufficient accuracy to capture energy differences of a few\nkJ/mol between polymorphs. Dispersion-inclusive density functional theory (DFT)\nprovides the required accuracy but its computational cost is impractical for a\nlarge number of putative structures. We introduce FastCSP, an open-source,\nhigh-throughput CSP workflow based on machine learning interatomic potentials\n(MLIPs). FastCSP combines random structure generation using Genarris 3.0 with\ngeometry relaxation and free energy calculations powered entirely by the\nUniversal Model for Atoms (UMA) MLIP. We benchmark FastCSP on a curated set of\n28 mostly rigid molecules, demonstrating that our workflow consistently\ngenerates known experimental structures and ranks them within 5 kJ/mol per\nmolecule of the global minimum. Our results demonstrate that universal MLIPs\ncan be used across diverse compounds without requiring system-specific tuning.\nMoreover, the speed and accuracy afforded by UMA eliminate the need for\nclassical force fields in the early stages of CSP and for final re-ranking with\nDFT. The open-source release of the entire FastCSP workflow significantly\nlowers the barrier to accessing CSP. CSP results for a single system can be\nobtained within hours on tens of modern GPUs, making high-throughput crystal\nstructure prediction feasible for a broad range of scientific applications.\n","authors":["Vahe Gharakhanyan","Yi Yang","Luis Barroso-Luque","Muhammed Shuaibi","Daniel S. Levine","Kyle Michel","Viachaslau Bernat","Misko Dzamba","Xiang Fu","Meng Gao","Xingyu Liu","Keian Noori","Lafe J. Purvis","Tingling Rao","Brandon M. Wood","Ammar Rizvi","Matt Uyttendaele","Andrew J. Ouderkirk","Chiara Daraio","C. Lawrence Zitnick","Arman Boromand","Noa Marom","Zachary W. Ulissi","Anuroop Sriram"],"pdf_url":"https://arxiv.org/pdf/2508.02641v1.pdf","comment":"52 pages, 19 figures, 6 tables"},{"id":"http://arxiv.org/abs/2508.02637v1","updated":"2025-08-04T17:23:00Z","published":"2025-08-04T17:23:00Z","title":"Instance-Optimal Uniformity Testing and Tracking","summary":"  In the uniformity testing task, an algorithm is provided with samples from an\nunknown probability distribution over a (known) finite domain, and must decide\nwhether it is the uniform distribution, or, alternatively, if its total\nvariation distance from uniform exceeds some input distance parameter. This\nquestion has received a significant amount of interest and its complexity is,\nby now, fully settled. Yet, we argue that it fails to capture many scenarios of\ninterest, and that its very definition as a gap problem in terms of a\nprespecified distance may lead to suboptimal performance.\n  To address these shortcomings, we introduce the problem of uniformity\ntracking, whereby an algorithm is required to detect deviations from uniformity\n(however they may manifest themselves) using as few samples as possible, and be\ncompetitive against an optimal algorithm knowing the distribution profile in\nhindsight. Our main contribution is a\n$\\operatorname{polylog}(\\operatorname{opt})$-competitive uniformity tracking\nalgorithm. We obtain this result by leveraging new structural results on\nPoisson mixtures, which we believe to be of independent interest.\n","authors":["Guy Blanc","Clément L. Canonne","Erik Waingarten"],"pdf_url":"https://arxiv.org/pdf/2508.02637v1.pdf","comment":"FOCS 2025, to appear"},{"id":"http://arxiv.org/abs/2508.02634v1","updated":"2025-08-04T17:20:50Z","published":"2025-08-04T17:20:50Z","title":"Actionable Counterfactual Explanations Using Bayesian Networks and Path\n  Planning with Applications to Environmental Quality Improvement","summary":"  Counterfactual explanations study what should have changed in order to get an\nalternative result, enabling end-users to understand machine learning\nmechanisms with counterexamples. Actionability is defined as the ability to\ntransform the original case to be explained into a counterfactual one. We\ndevelop a method for actionable counterfactual explanations that, unlike\npredecessors, does not directly leverage training data. Rather, data is only\nused to learn a density estimator, creating a search landscape in which to\napply path planning algorithms to solve the problem and masking the endogenous\ndata, which can be sensitive or private. We put special focus on estimating the\ndata density using Bayesian networks, demonstrating how their enhanced\ninterpretability is useful in high-stakes scenarios in which fairness is\nraising concern. Using a synthetic benchmark comprised of 15 datasets, our\nproposal finds more actionable and simpler counterfactuals than the current\nstate-of-the-art algorithms. We also test our algorithm with a real-world\nEnvironmental Protection Agency dataset, facilitating a more efficient and\nequitable study of policies to improve the quality of life in United States of\nAmerica counties. Our proposal captures the interaction of variables, ensuring\nequity in decisions, as policies to improve certain domains of study (air,\nwater quality, etc.) can be detrimental in others. In particular, the\nsociodemographic domain is often involved, where we find important variables\nrelated to the ongoing housing crisis that can potentially have a severe\nnegative impact on communities.\n","authors":["Enrique Valero-Leal","Pedro Larrañaga","Concha Bielza"],"pdf_url":"https://arxiv.org/pdf/2508.02634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02627v1","updated":"2025-08-04T17:15:57Z","published":"2025-08-04T17:15:57Z","title":"Tensor Dynamic Mode Decomposition","summary":"  Dynamic mode decomposition (DMD) has become a powerful data-driven method for\nanalyzing the spatiotemporal dynamics of complex, high-dimensional systems.\nHowever, conventional DMD methods are limited to matrix-based formulations,\nwhich might be inefficient or inadequate for modeling inherently\nmultidimensional data including images, videos, and higher-order networks. In\nthis letter, we propose tensor dynamic mode decomposition (TDMD), a novel\nextension of DMD to third-order tensors based on the recently developed\nT-product framework. By incorporating tensor factorization techniques, TDMD\nachieves more efficient computation and better preservation of spatial and\ntemporal structures in multiway data for tasks such as state reconstruction and\ndynamic component separation, compared to standard DMD with data flattening. We\ndemonstrate the effectiveness of TDMD on both synthetic and real-world\ndatasets.\n","authors":["Ziqin He","Mengqi Hu","Yifei Lou","Can Chen"],"pdf_url":"https://arxiv.org/pdf/2508.02627v1.pdf","comment":"6 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2508.02625v1","updated":"2025-08-04T17:13:45Z","published":"2025-08-04T17:13:45Z","title":"AutoML-Med: A Framework for Automated Machine Learning in Medical\n  Tabular Data","summary":"  Medical datasets are typically affected by issues such as missing values,\nclass imbalance, a heterogeneous feature types, and a high number of features\nversus a relatively small number of samples, preventing machine learning models\nfrom obtaining proper results in classification and regression tasks. This\npaper introduces AutoML-Med, an Automated Machine Learning tool specifically\ndesigned to address these challenges, minimizing user intervention and\nidentifying the optimal combination of preprocessing techniques and predictive\nmodels. AutoML-Med's architecture incorporates Latin Hypercube Sampling (LHS)\nfor exploring preprocessing methods, trains models using selected metrics, and\nutilizes Partial Rank Correlation Coefficient (PRCC) for fine-tuned\noptimization of the most influential preprocessing steps. Experimental results\ndemonstrate AutoML-Med's effectiveness in two different clinical settings,\nachieving higher balanced accuracy and sensitivity, which are crucial for\nidentifying at-risk patients, compared to other state-of-the-art tools.\nAutoML-Med's ability to improve prediction results, especially in medical\ndatasets with sparse data and class imbalance, highlights its potential to\nstreamline Machine Learning applications in healthcare.\n","authors":["Riccardo Francia","Maurizio Leone","Giorgio Leonardi","Stefania Montani","Marzio Pennisi","Manuel Striani","Sandra D'Alfonso"],"pdf_url":"https://arxiv.org/pdf/2508.02625v1.pdf","comment":"8 pages, preprint for conference"},{"id":"http://arxiv.org/abs/2508.02621v1","updated":"2025-08-04T17:08:47Z","published":"2025-08-04T17:08:47Z","title":"HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous\n  Healthcare Research","summary":"  The efficacy of AI agents in healthcare research is hindered by their\nreliance on static, predefined strategies. This creates a critical limitation:\nagents can become better tool-users but cannot learn to become better strategic\nplanners, a crucial skill for complex domains like healthcare. We introduce\nHealthFlow, a self-evolving AI agent that overcomes this limitation through a\nnovel meta-level evolution mechanism. HealthFlow autonomously refines its own\nhigh-level problem-solving policies by distilling procedural successes and\nfailures into a durable, strategic knowledge base. To anchor our research and\nfacilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark\nfeaturing complex, realistic health data analysis tasks derived from\npeer-reviewed clinical research. Our comprehensive experiments demonstrate that\nHealthFlow's self-evolving approach significantly outperforms state-of-the-art\nagent frameworks. This work marks a necessary shift from building better\ntool-users to designing smarter, self-evolving task-managers, paving the way\nfor more autonomous and effective AI for scientific discovery.\n","authors":["Yinghao Zhu","Yifan Qi","Zixiang Wang","Lei Gu","Dehao Sui","Haoran Hu","Xichen Zhang","Ziyi He","Liantao Ma","Lequan Yu"],"pdf_url":"https://arxiv.org/pdf/2508.02621v1.pdf","comment":"Code: https://github.com/yhzhu99/HealthFlow"},{"id":"http://arxiv.org/abs/2508.02616v1","updated":"2025-08-04T17:05:55Z","published":"2025-08-04T17:05:55Z","title":"DeepKoopFormer: A Koopman Enhanced Transformer Based Architecture for\n  Time Series Forecasting","summary":"  Time series forecasting plays a vital role across scientific, industrial, and\nenvironmental domains, especially when dealing with high-dimensional and\nnonlinear systems. While Transformer-based models have recently achieved\nstate-of-the-art performance in long-range forecasting, they often suffer from\ninterpretability issues and instability in the presence of noise or dynamical\nuncertainty. In this work, we propose DeepKoopFormer, a principled forecasting\nframework that combines the representational power of Transformers with the\ntheoretical rigor of Koopman operator theory. Our model features a modular\nencoder-propagator-decoder structure, where temporal dynamics are learned via a\nspectrally constrained, linear Koopman operator in a latent space. We impose\nstructural guarantees-such as bounded spectral radius, Lyapunov based energy\nregularization, and orthogonal parameterization to ensure stability and\ninterpretability. Comprehensive evaluations are conducted on both synthetic\ndynamical systems, real-world climate dataset (wind speed and surface\npressure), financial time series (cryptocurrency), and electricity generation\ndataset using the Python package that is prepared for this purpose. Across all\nexperiments, DeepKoopFormer consistently outperforms standard LSTM and baseline\nTransformer models in terms of accuracy, robustness to noise, and long-term\nforecasting stability. These results establish DeepKoopFormer as a flexible,\ninterpretable, and robust framework for forecasting in high dimensional and\ndynamical settings.\n","authors":["Ali Forootani","Mohammad Khosravi","Masoud Barati"],"pdf_url":"https://arxiv.org/pdf/2508.02616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02609v1","updated":"2025-08-04T17:00:53Z","published":"2025-08-04T17:00:53Z","title":"Entity Representation Learning Through Onsite-Offsite Graph for\n  Pinterset Ads","summary":"  Graph Neural Networks (GNN) have been extensively applied to industry\nrecommendation systems, as seen in models like GraphSage\\cite{GraphSage},\nTwHIM\\cite{TwHIM}, LiGNN\\cite{LiGNN} etc. In these works, graphs were\nconstructed based on users' activities on the platforms, and various graph\nmodels were developed to effectively learn node embeddings. In addition to\nusers' onsite activities, their offsite conversions are crucial for Ads models\nto capture their shopping interest. To better leverage offsite conversion data\nand explore the connection between onsite and offsite activities, we\nconstructed a large-scale heterogeneous graph based on users' onsite ad\ninteractions and opt-in offsite conversion activities. Furthermore, we\nintroduced TransRA (TransR\\cite{TransR} with Anchors), a novel Knowledge Graph\nEmbedding (KGE) model, to more efficiently integrate graph embeddings into Ads\nranking models. However, our Ads ranking models initially struggled to directly\nincorporate Knowledge Graph Embeddings (KGE), and only modest gains were\nobserved during offline experiments. To address this challenge, we employed the\nLarge ID Embedding Table technique and innovated an attention based KGE\nfinetuning approach within the Ads ranking models. As a result, we observed a\nsignificant AUC lift in Click-Through Rate (CTR) and Conversion Rate (CVR)\nprediction models. Moreover, this framework has been deployed in Pinterest's\nAds Engagement Model and contributed to $2.69\\%$ CTR lift and $1.34\\%$ CPC\nreduction. We believe the techniques presented in this paper can be leveraged\nby other large-scale industrial models.\n","authors":["Jiayin Jin","Zhimeng Pan","Yang Tang","Jiarui Feng","Kungang Li","Chongyuan Xiang","Jiacheng Li","Runze Su","Siping Ji","Han Sun","Ling Leng","Prathibha Deshikachar"],"pdf_url":"https://arxiv.org/pdf/2508.02609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05430v2","updated":"2025-08-04T16:56:32Z","published":"2024-12-06T21:23:35Z","title":"DART-Eval: A Comprehensive DNA Language Model Evaluation Benchmark on\n  Regulatory DNA","summary":"  Recent advances in self-supervised models for natural language, vision, and\nprotein sequences have inspired the development of large genomic DNA language\nmodels (DNALMs). These models aim to learn generalizable representations of\ndiverse DNA elements, potentially enabling various genomic prediction,\ninterpretation and design tasks. Despite their potential, existing benchmarks\ndo not adequately assess the capabilities of DNALMs on key downstream\napplications involving an important class of non-coding DNA elements critical\nfor regulating gene activity. In this study, we introduce DART-Eval, a suite of\nrepresentative benchmarks specifically focused on regulatory DNA to evaluate\nmodel performance across zero-shot, probed, and fine-tuned scenarios against\ncontemporary ab initio models as baselines. Our benchmarks target biologically\nmeaningful downstream tasks such as functional sequence feature discovery,\npredicting cell-type specific regulatory activity, and counterfactual\nprediction of the impacts of genetic variants. We find that current DNALMs\nexhibit inconsistent performance and do not offer compelling gains over\nalternative baseline models for most tasks, while requiring significantly more\ncomputational resources. We discuss potentially promising modeling, data\ncuration, and evaluation strategies for the next generation of DNALMs. Our code\nis available at https://github.com/kundajelab/DART-Eval.\n","authors":["Aman Patel","Arpita Singhal","Austin Wang","Anusri Pampari","Maya Kasowski","Anshul Kundaje"],"pdf_url":"https://arxiv.org/pdf/2412.05430v2.pdf","comment":"NeurIPS Datasets and Benchmarks 2024"},{"id":"http://arxiv.org/abs/2508.02602v1","updated":"2025-08-04T16:56:11Z","published":"2025-08-04T16:56:11Z","title":"Trustworthy scientific inference for inverse problems with generative\n  models","summary":"  Generative artificial intelligence (AI) excels at producing complex data\nstructures (text, images, videos) by learning patterns from training examples.\nAcross scientific disciplines, researchers are now applying generative models\nto ``inverse problems'' to infer hidden parameters from observed data. While\nthese methods can handle intractable models and large-scale studies, they can\nalso produce biased or overconfident conclusions. We present a solution with\nFrequentist-Bayes (FreB), a mathematically rigorous protocol that reshapes\nAI-generated probability distributions into confidence regions that\nconsistently include true parameters with the expected probability, while\nachieving minimum size when training and target data align. We demonstrate\nFreB's effectiveness by tackling diverse case studies in the physical sciences:\nidentifying unknown sources under dataset shift, reconciling competing\ntheoretical models, and mitigating selection bias and systematics in\nobservational studies. By providing validity guarantees with interpretable\ndiagnostics, FreB enables trustworthy scientific inference across fields where\ndirect likelihood evaluation remains impossible or prohibitively expensive.\n","authors":["James Carzon","Luca Masserano","Joshua D. Ingram","Alex Shen","Antonio Carlos Herling Ribeiro Junior","Tommaso Dorigo","Michele Doro","Joshua S. Speagle","Rafael Izbicki","Ann B. Lee"],"pdf_url":"https://arxiv.org/pdf/2508.02602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02600v1","updated":"2025-08-04T16:55:02Z","published":"2025-08-04T16:55:02Z","title":"Adaptive Riemannian Graph Neural Networks","summary":"  Graph data often exhibits complex geometric heterogeneity, where structures\nwith varying local curvature, such as tree-like hierarchies and dense\ncommunities, coexist within a single network. Existing geometric GNNs, which\nembed graphs into single fixed-curvature manifolds or discrete product spaces,\nstruggle to capture this diversity. We introduce Adaptive Riemannian Graph\nNeural Networks (ARGNN), a novel framework that learns a continuous and\nanisotropic Riemannian metric tensor field over the graph. It allows each node\nto determine its optimal local geometry, enabling the model to fluidly adapt to\nthe graph's structural landscape. Our core innovation is an efficient\nparameterization of the node-wise metric tensor, specializing to a learnable\ndiagonal form that captures directional geometric information while maintaining\ncomputational tractability. To ensure geometric regularity and stable training,\nwe integrate a Ricci flow-inspired regularization that smooths the learned\nmanifold. Theoretically, we establish the rigorous geometric evolution\nconvergence guarantee for ARGNN and provide a continuous generalization that\nunifies prior fixed or mixed-curvature GNNs. Empirically, our method\ndemonstrates superior performance on both homophilic and heterophilic benchmark\ndatasets with the ability to capture diverse structures adaptively. Moreover,\nthe learned geometries both offer interpretable insights into the underlying\ngraph structure and empirically corroborate our theoretical analysis.\n","authors":["Xudong Wang","Tongxin Li","Chris Ding","Jicong Fan"],"pdf_url":"https://arxiv.org/pdf/2508.02600v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2508.02601v1","updated":"2025-08-04T16:55:02Z","published":"2025-08-04T16:55:02Z","title":"StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis\n  in Low-Data Regimes","summary":"  The application of machine learning on tabular data in specialized domains is\nseverely limited by data scarcity. While generative models offer a solution,\ntraditional methods falter in low-data regimes, and recent Large Language\nModels (LLMs) often ignore the explicit dependency structure of tabular data,\nleading to low-fidelity synthetics. To address these limitations, we introduce\nStructSynth, a novel framework that integrates the generative power of LLMs\nwith robust structural control. StructSynth employs a two-stage architecture.\nFirst, it performs explicit structure discovery to learn a Directed Acyclic\nGraph (DAG) from the available data. Second, this learned structure serves as a\nhigh-fidelity blueprint to steer the LLM's generation process, forcing it to\nadhere to the learned feature dependencies and thereby ensuring the generated\ndata respects the underlying structure by design. Our extensive experiments\ndemonstrate that StructSynth produces synthetic data with significantly higher\nstructural integrity and downstream utility than state-of-the-art methods. It\nproves especially effective in challenging low-data scenarios, successfully\nnavigating the trade-off between privacy preservation and statistical fidelity.\n","authors":["Siyi Liu","Yujia Zheng","Yongqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.02601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.22967v2","updated":"2025-08-04T16:54:44Z","published":"2025-06-28T17:57:58Z","title":"ActAlign: Zero-Shot Fine-Grained Video Classification via\n  Language-Guided Sequence Alignment","summary":"  We address the task of zero-shot video classification for extremely\nfine-grained actions (e.g., Windmill Dunk in basketball), where no video\nexamples or temporal annotations are available for unseen classes. While\nimage-language models (e.g., CLIP, SigLIP) show strong open-set recognition,\nthey lack temporal modeling needed for video understanding. We propose\nActAlign, a truly zero-shot, training-free method that formulates video\nclassification as a sequence alignment problem, preserving the generalization\nstrength of pretrained image-language models. For each class, a large language\nmodel (LLM) generates an ordered sequence of sub-actions, which we align with\nvideo frames using Dynamic Time Warping (DTW) in a shared embedding space.\nWithout any video-text supervision or fine-tuning, ActAlign achieves 30.5%\naccuracy on ActionAtlas--the most diverse benchmark of fine-grained actions\nacross multiple sports--where human performance is only 61.6%. ActAlign\noutperforms billion-parameter video-language models while using 8x fewer\nparameters. Our approach is model-agnostic and domain-general, demonstrating\nthat structured language priors combined with classical alignment methods can\nunlock the open-set recognition potential of image-language models for\nfine-grained video understanding.\n","authors":["Amir Aghdam","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2506.22967v2.pdf","comment":"Preprint manuscript - Project page:\n  https://amir-aghdam.github.io/act-align/"},{"id":"http://arxiv.org/abs/2508.02587v1","updated":"2025-08-04T16:43:09Z","published":"2025-08-04T16:43:09Z","title":"Parameter-Efficient Routed Fine-Tuning: Mixture-of-Experts Demands\n  Mixture of Adaptation Modules","summary":"  Mixture-of-Experts (MoE) benefits from a dynamic routing mechanism among\ntheir specialized experts, which existing Parameter- Efficient Fine-Tuning\n(PEFT) strategies fail to leverage. This motivates us to investigate whether\nadaptation modules themselves should incorporate routing mechanisms to align\nwith MoE's multi-expert architecture. We analyze dynamics of core components\nwhen applying PEFT to MoE language models and examine how different routing\nstrategies affect adaptation effectiveness. Extensive experiments adapting\nOLMoE-1B-7B and Mixtral-8x7B on various commonsense and math reasoning tasks\nvalidate the performance and efficiency of our routed approach. We identify the\noptimal configurations for different scenarios and provide empirical analyses\nwith practical insights to facilitate better PEFT and MoE applications.\n","authors":["Yilun Liu","Yunpu Ma","Yuetian Lu","Shuo Chen","Zifeng Ding","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2508.02587v1.pdf","comment":"This paper is a preprint under review. arXiv admin note: text overlap\n  with arXiv:2411.08212"},{"id":"http://arxiv.org/abs/2508.02583v1","updated":"2025-08-04T16:39:24Z","published":"2025-08-04T16:39:24Z","title":"CAMA: Enhancing Mathematical Reasoning in Large Language Models with\n  Causal Knowledge","summary":"  Large Language Models (LLMs) have demonstrated strong performance across a\nwide range of tasks, yet they still struggle with complex mathematical\nreasoning, a challenge fundamentally rooted in deep structural dependencies. To\naddress this challenge, we propose \\textbf{CA}usal \\textbf{MA}thematician\n(\\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit,\nreusable mathematical structure. In the learning stage, CAMA first constructs\nthe \\textbf{M}athematical \\textbf{C}ausal \\textbf{G}raph (\\textbf{MCG}), a\nhigh-level representation of solution strategies, by combining LLM priors with\ncausal discovery algorithms applied to a corpus of question-solution pairs. The\nresulting MCG encodes essential knowledge points and their causal dependencies.\nTo better align the graph with downstream reasoning tasks, CAMA further refines\nthe MCG through iterative feedback derived from a selected subset of the\nquestion-solution pairs. In the reasoning stage, given a new question, CAMA\ndynamically extracts a task-relevant subgraph from the MCG, conditioned on both\nthe question content and the LLM's intermediate reasoning trace. This subgraph,\nwhich encodes the most pertinent knowledge points and their causal\ndependencies, is then injected back into the LLM to guide its reasoning\nprocess. Empirical results on real-world datasets show that CAMA significantly\nimproves LLM performance on challenging mathematical problems. Furthermore, our\nexperiments demonstrate that structured guidance consistently outperforms\nunstructured alternatives, and that incorporating asymmetric causal\nrelationships yields greater improvements than using symmetric associations\nalone.\n","authors":["Lei Zan","Keli Zhang","Ruichu Cai","Lujia Pan"],"pdf_url":"https://arxiv.org/pdf/2508.02583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07927v3","updated":"2025-08-04T16:37:00Z","published":"2025-01-14T08:30:49Z","title":"Gandalf the Red: Adaptive Security for LLMs","summary":"  Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and expresses the security-utility in an\noptimizable form. We further address the shortcomings in existing evaluations\nby introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed\nto generate realistic, adaptive attack. Using Gandalf, we collect and release a\ndataset of 279k prompt attacks. Complemented by benign user data, our analysis\nreveals the interplay between security and utility, showing that defenses\nintegrated in the LLM (e.g., system prompts) can degrade usability even without\nblocking requests. We demonstrate that restricted application domains,\ndefense-in-depth, and adaptive defenses are effective strategies for building\nsecure and useful LLM applications.\n","authors":["Niklas Pfister","Václav Volhejn","Manuel Knott","Santiago Arias","Julia Bazińska","Mykhailo Bichurin","Alan Commike","Janet Darling","Peter Dienes","Matthew Fiedler","David Haber","Matthias Kraft","Marco Lancini","Max Mathys","Damián Pascual-Ortiz","Jakub Podolak","Adrià Romero-López","Kyriacos Shiarlis","Andreas Signer","Zsolt Terek","Athanasios Theocharis","Daniel Timbrell","Samuel Trautwein","Samuel Watts","Yun-Han Wu","Mateo Rojas-Carulla"],"pdf_url":"https://arxiv.org/pdf/2501.07927v3.pdf","comment":"Niklas Pfister, V\\'aclav Volhejn and Manuel Knott contributed equally"},{"id":"http://arxiv.org/abs/2508.02574v1","updated":"2025-08-04T16:28:58Z","published":"2025-08-04T16:28:58Z","title":"EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based\n  Sentiment Analysis in Healthcare","summary":"  Arabic-language patient feedback remains under-analysed because dialect\ndiversity and scarce aspect-level sentiment labels hinder automated assessment.\nTo address this gap, we introduce EHSAN, a data-centric hybrid pipeline that\nmerges ChatGPT pseudo-labelling with targeted human review to build the first\nexplainable Arabic aspect-based sentiment dataset for healthcare. Each sentence\nis annotated with an aspect and sentiment label (positive, negative, or\nneutral), forming a pioneering Arabic dataset aligned with healthcare themes,\nwith ChatGPT-generated rationales provided for each label to enhance\ntransparency. To evaluate the impact of annotation quality on model\nperformance, we created three versions of the training data: a fully supervised\nset with all labels reviewed by humans, a semi-supervised set with 50% human\nreview, and an unsupervised set with only machine-generated labels. We\nfine-tuned two transformer models on these datasets for both aspect and\nsentiment classification. Experimental results show that our Arabic-specific\nmodel achieved high accuracy even with minimal human supervision, reflecting\nonly a minor performance drop when using ChatGPT-only labels. Reducing the\nnumber of aspect classes notably improved classification metrics across the\nboard. These findings demonstrate an effective, scalable approach to Arabic\naspect-based sentiment analysis (SA) in healthcare, combining large language\nmodel annotation with human expertise to produce a robust and explainable\ndataset. Future directions include generalisation across hospitals, prompt\nrefinement, and interpretable data-driven modelling.\n","authors":["Eman Alamoudi","Ellis Solaiman"],"pdf_url":"https://arxiv.org/pdf/2508.02574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05376v3","updated":"2025-08-04T16:28:18Z","published":"2024-06-08T07:05:26Z","title":"Adversarial flows: A gradient flow characterization of adversarial\n  attacks","summary":"  A popular method to perform adversarial attacks on neuronal networks is the\nso-called fast gradient sign method and its iterative variant. In this paper,\nwe interpret this method as an explicit Euler discretization of a differential\ninclusion, where we also show convergence of the discretization to the\nassociated gradient flow. To do so, we consider the concept of p-curves of\nmaximal slope in the case $p=\\infty$. We prove existence of $\\infty$-curves of\nmaximum slope and derive an alternative characterization via differential\ninclusions. Furthermore, we also consider Wasserstein gradient flows for\npotential energies, where we show that curves in the Wasserstein space can be\ncharacterized by a representing measure on the space of curves in the\nunderlying Banach space, which fulfill the differential inclusion. The\napplication of our theory to the finite-dimensional setting is twofold: On the\none hand, we show that a whole class of normalized gradient descent methods (in\nparticular signed gradient descent) converge, up to subsequences, to the flow,\nwhen sending the step size to zero. On the other hand, in the distributional\nsetting, we show that the inner optimization task of adversarial training\nobjective can be characterized via $\\infty$-curves of maximum slope on an\nappropriate optimal transport space.\n","authors":["Lukas Weigand","Tim Roith","Martin Burger"],"pdf_url":"https://arxiv.org/pdf/2406.05376v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.05833v2","updated":"2025-08-04T16:25:44Z","published":"2025-03-06T12:52:11Z","title":"Refined Policy Distillation: From VLA Generalists to RL Experts","summary":"  Vision-Language-Action Models (VLAs) have demonstrated remarkable\ngeneralization capabilities in real-world experiments. However, their success\nrates are often not on par with expert policies, and they require fine-tuning\nwhen the setup changes. In this work, we introduce Refined Policy Distillation\n(RPD), a novel Reinforcement Learning (RL)-based policy refinement method that\nbridges this performance gap through a combination of on-policy RL with\nbehavioral cloning. The core idea of RPD is to distill and refine VLAs into\ncompact, high-performing expert policies by guiding the student policy during\nRL exploration using the actions of a teacher VLA, resulting in increased\nsample efficiency and faster convergence. We complement our method by\nfine-tuned versions of Octo and OpenVLA for ManiSkill3 to evaluate RPD in\nsimulation. While this is a key requirement for applying RL, it also yields new\ninsights beyond existing studies on VLA performance in real-world settings. Our\nexperimental results across various manipulation tasks show that RPD enables\nthe RL student to learn expert policies that outperform the VLA teacher in both\ndense and sparse reward settings, while also achieving faster convergence than\nthe RL baseline. Our approach is even robust to changes in camera perspective\nand can generalize to task variations that the underlying VLA cannot solve. Our\ncode, dataset, VLA checkpoints, and videos are available at\nhttps://refined-policy-distillation.github.io\n","authors":["Tobias Jülg","Wolfram Burgard","Florian Walter"],"pdf_url":"https://arxiv.org/pdf/2503.05833v2.pdf","comment":"accepted for publication at IROS 2026"},{"id":"http://arxiv.org/abs/2508.02566v1","updated":"2025-08-04T16:21:43Z","published":"2025-08-04T16:21:43Z","title":"Dynamic Feature Selection based on Rule-based Learning for Explainable\n  Classification with Uncertainty Quantification","summary":"  Dynamic feature selection (DFS) offers a compelling alternative to\ntraditional, static feature selection by adapting the selected features to each\nindividual sample. Unlike classical methods that apply a uniform feature set,\nDFS customizes feature selection per sample, providing insight into the\ndecision-making process for each case. DFS is especially significant in\nsettings where decision transparency is key, i.e., clinical decisions; however,\nexisting methods use opaque models, which hinder their applicability in\nreal-life scenarios. This paper introduces a novel approach leveraging a\nrule-based system as a base classifier for the DFS process, which enhances\ndecision interpretability compared to neural estimators. We also show how this\nmethod provides a quantitative measure of uncertainty for each feature query\nand can make the feature selection process computationally lighter by\nconstraining the feature search space. We also discuss when greedy selection of\nconditional mutual information is equivalent to selecting features that\nminimize the difference with respect to the global model predictions. Finally,\nwe demonstrate the competitive performance of our rule-based DFS approach\nagainst established and state-of-the-art greedy and RL methods, which are\nmostly considered opaque, compared to our explainable rule-based system.\n","authors":["Javier Fumanal-Idocin","Raquel Fernandez-Peralta","Javier Andreu-Perez"],"pdf_url":"https://arxiv.org/pdf/2508.02566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.21448v2","updated":"2025-08-04T16:20:43Z","published":"2025-07-29T02:38:56Z","title":"Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual\n  Representations","summary":"  Speech enhancement in audio-only settings remains challenging, particularly\nin the presence of interfering speakers. This paper presents a simple yet\neffective real-time audio-visual speech enhancement (AVSE) system, RAVEN, which\nisolates and enhances the on-screen target speaker while suppressing\ninterfering speakers and background noise. We investigate how visual embeddings\nlearned from audio-visual speech recognition (AVSR) and active speaker\ndetection (ASD) contribute to AVSE across different SNR conditions and numbers\nof interfering speakers. Our results show concatenating embeddings from AVSR\nand ASD models provides the greatest improvement in low-SNR, multi-speaker\nenvironments, while AVSR embeddings alone perform best in noise-only scenarios.\nIn addition, we develop a real-time streaming system that operates on a\ncomputer CPU and we provide a video demonstration and code repository. To our\nknowledge, this is the first open-source implementation of a real-time AVSE\nsystem.\n","authors":["T. Aleksandra Ma","Sile Yin","Li-Chia Yang","Shuo Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.21448v2.pdf","comment":"Accepted into Interspeech 2025; corrected author name typo"},{"id":"http://arxiv.org/abs/2508.02560v1","updated":"2025-08-04T16:14:15Z","published":"2025-08-04T16:14:15Z","title":"Explainable AI Methods for Neuroimaging: Systematic Failures of Common\n  Tools, the Need for Domain-Specific Validation, and a Proposal for Safe\n  Application","summary":"  Trustworthy interpretation of deep learning models is critical for\nneuroimaging applications, yet commonly used Explainable AI (XAI) methods lack\nrigorous validation, risking misinterpretation. We performed the first\nlarge-scale, systematic comparison of XAI methods on ~45,000 structural brain\nMRIs using a novel XAI validation framework. This framework establishes\nverifiable ground truth by constructing prediction tasks with known signal\nsources - from localized anatomical features to subject-specific clinical\nlesions - without artificially altering input images. Our analysis reveals\nsystematic failures in two of the most widely used methods: GradCAM\nconsistently failed to localize predictive features, while Layer-wise Relevance\nPropagation generated extensive, artifactual explanations that suggest\nincompatibility with neuroimaging data characteristics. Our results indicate\nthat these failures stem from a domain mismatch, where methods with design\nprinciples tailored to natural images require substantial adaptation for\nneuroimaging data. In contrast, the simpler, gradient-based method SmoothGrad,\nwhich makes fewer assumptions about data structure, proved consistently\naccurate, suggesting its conceptual simplicity makes it more robust to this\ndomain shift. These findings highlight the need for domain-specific adaptation\nand validation of XAI methods, suggest that interpretations from prior\nneuroimaging studies using standard XAI methodology warrant re-evaluation, and\nprovide urgent guidance for practical application of XAI in neuroimaging.\n","authors":["Nys Tjade Siegel","James H. Cole","Mohamad Habes","Stefan Haufe","Kerstin Ritter","Marc-André Schulz"],"pdf_url":"https://arxiv.org/pdf/2508.02560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02556v1","updated":"2025-08-04T16:08:49Z","published":"2025-08-04T16:08:49Z","title":"Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU\n  Neural Networks","summary":"  Automated annotation of clinical text with standardized medical concepts is\ncritical for enabling structured data extraction and decision support. SNOMED\nCT provides a rich ontology for labeling clinical entities, but manual\nannotation is labor-intensive and impractical at scale. This study introduces a\nneural sequence labeling approach for SNOMED CT concept recognition using a\nBidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text\nwith domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences\ninto overlapping 19-token chunks enriched with contextual, syntactic, and\nmorphological features. The Bi-GRU model assigns IOB tags to identify concept\nspans and achieves strong performance with a 90 percent F1-score on the\nvalidation set. These results surpass traditional rule-based systems and match\nor exceed existing neural models. Qualitative analysis shows effective handling\nof ambiguous terms and misspellings. Our findings highlight that lightweight\nRNN-based architectures can deliver high-quality clinical concept annotation\nwith significantly lower computational cost than transformer-based models,\nmaking them well-suited for real-world deployment.\n","authors":["Ali Noori","Pratik Devkota","Somya Mohanty","Prashanti Manda"],"pdf_url":"https://arxiv.org/pdf/2508.02556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02553v1","updated":"2025-08-04T16:04:20Z","published":"2025-08-04T16:04:20Z","title":"CSI Obfuscation: Single-Antenna Transmitters Can Not Hide from\n  Adversarial Multi-Antenna Radio Localization Systems","summary":"  The ability of modern telecommunication systems to locate users and objects\nin the radio environment raises justified privacy concerns. To prevent\nunauthorized localization, single-antenna transmitters can obfuscate the signal\nby convolving it with a randomized sequence prior to transmission, which alters\nthe channel state information (CSI) estimated at the receiver. However, this\nstrategy is only effective against CSI-based localization systems deploying\nsingle-antenna receivers. Inspired by the concept of blind multichannel\nidentification, we propose a simple CSI recovery method for multi-antenna\nreceivers to extract channel features that ensure reliable user localization\nregardless of the transmitted signal. We comparatively evaluate the impact of\nsignal obfuscation and the proposed recovery method on the localization\nperformance of CSI fingerprinting, channel charting, and classical\ntriangulation using real-world channel measurements. This work aims to\ndemonstrate the necessity for further efforts to protect the location privacy\nof users from adversarial radio-based localization systems.\n","authors":["Phillip Stephan","Florian Euchner","Stephan ten Brink"],"pdf_url":"https://arxiv.org/pdf/2508.02553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02546v1","updated":"2025-08-04T15:59:15Z","published":"2025-08-04T15:59:15Z","title":"What are you sinking? A geometric approach on attention sink","summary":"  Attention sink (AS) is a consistent pattern in transformer attention maps\nwhere certain tokens (often special tokens or positional anchors)\ndisproportionately attract attention from other tokens. We show that in\ntransformers, AS is not an architectural artifact, but it is the manifestation\nof a fundamental geometric principle: the establishment of reference frames\nthat anchor representational spaces. We analyze several architectures and\nidentify three distinct reference frame types, centralized, distributed, and\nbidirectional, that correlate with the attention sink phenomenon. We show that\nthey emerge during the earliest stages of training as optimal solutions to the\nproblem of establishing stable coordinate systems in high-dimensional spaces.\nWe show the influence of architecture components, particularly position\nencoding implementations, on the specific type of reference frame. This\nperspective transforms our understanding of transformer attention mechanisms\nand provides insights for both architecture design and the relationship with\nAS.\n","authors":["Valeria Ruscio","Umberto Nanni","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2508.02546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.03336v2","updated":"2025-08-04T15:48:55Z","published":"2025-07-04T06:49:02Z","title":"Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky","summary":"  Large language models (LLMs) are increasingly tasked with invoking enterprise\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\nintent or when required arguments are left underspecified. We introduce\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\npersona-driven, multi-turn dialogues in which the assistant must distinguish\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\nreal-world readiness via a dynamic suite that redeploys each model in a live\nagentic loop and reports end-to-end goal completion alongside conventional\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\nrelease an open corpus of 5000 production-grade enterprise API specifications\npaired with rigorously validated, disambiguation-focused dialogues, offering a\npractical blueprint for building reliable, enterprise-ready tool-calling\nagents.\n","authors":["Ashutosh Hathidara","Julien Yu","Sebastian Schreiber"],"pdf_url":"https://arxiv.org/pdf/2507.03336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00982v2","updated":"2025-08-04T15:48:47Z","published":"2025-05-02T04:02:36Z","title":"DHO$_2$: Accelerating Distributed Hybrid Order Optimization via Model\n  Parallelism and ADMM","summary":"  Scaling deep neural network (DNN) training to more devices can reduce\ntime-to-solution. However, it is impractical for users with limited computing\nresources. FOSI, as a hybrid order optimizer, converges faster than\nconventional optimizers by taking advantage of both gradient information and\ncurvature information when updating the DNN model. Therefore, it provides a new\nchance for accelerating DNN training in the resource-constrained setting. In\nthis paper, we explore its distributed design, namely DHO$_2$, including\ndistributed calculation of curvature information and model update with partial\ncurvature information to accelerate DNN training with a low memory burden. To\nfurther reduce the training time, we design a novel strategy to parallelize the\ncalculation of curvature information and the model update on different devices.\nExperimentally, our distributed design can achieve an approximate linear\nreduction of memory burden on each device with the increase of the device\nnumber. Meanwhile, it achieves $1.4\\times\\sim2.1\\times$ speedup in the total\ntraining time compared with other distributed designs based on conventional\nfirst- and second-order optimizers.\n","authors":["Shunxian Gu","Chaoqun You","Bangbang Ren","Lailong Luo","Junxu Xia","Deke Guo"],"pdf_url":"https://arxiv.org/pdf/2505.00982v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02537v1","updated":"2025-08-04T15:45:03Z","published":"2025-08-04T15:45:03Z","title":"Solved in Unit Domain: JacobiNet for Differentiable Coordinate\n  Transformations","summary":"  Physics-Informed Neural Networks (PINNs) are effective for solving PDEs by\nincorporating physical laws into the learning process. However, they face\nchallenges with irregular boundaries, leading to instability and slow\nconvergence due to inconsistent normalization, inaccurate boundary enforcement,\nand imbalanced loss terms. A common solution is to map the domain to a regular\nspace, but traditional methods rely on case-specific meshes and simple\ngeometries, limiting their compatibility with modern frameworks. To overcome\nthese limitations, we introduce JacobiNet, a neural network-based coordinate\ntransformation method that learns continuous, differentiable mappings from\nsupervised point pairs. Utilizing lightweight MLPs, JacobiNet allows for direct\nJacobian computation via autograd and integrates seamlessly with downstream\nPINNs, enabling end-to-end differentiable PDE solving without the need for\nmeshing or explicit Jacobian computation. JacobiNet effectively addresses\nnormalization challenges, facilitates hard constraints of boundary conditions,\nand mitigates the long-standing imbalance among loss terms. It demonstrates\nsignificant improvements, reducing the relative L2 error from 0.287-0.637 to\n0.013-0.039, achieving an average accuracy improvement of 18.3*. In vessel-like\ndomains, it enables rapid mapping for unseen geometries, improving prediction\naccuracy by 3.65* and achieving over 10* speedup, showcasing its\ngeneralization, accuracy, and efficiency.\n","authors":["Xi Chen","Jianchuan Yang","Junjie Zhang","Runnan Yang","Xu Liu","Hong Wang","Ziyu Ren","Wenqi Hu"],"pdf_url":"https://arxiv.org/pdf/2508.02537v1.pdf","comment":"Submitted to CMAME, revision in progress"},{"id":"http://arxiv.org/abs/2508.02534v1","updated":"2025-08-04T15:42:53Z","published":"2025-08-04T15:42:53Z","title":"Communication and Computation Efficient Split Federated Learning in\n  O-RAN","summary":"  The hierarchical architecture of Open Radio Access Network (O-RAN) has\nenabled a new Federated Learning (FL) paradigm that trains models using data\nfrom non- and near-real-time (near-RT) Radio Intelligent Controllers (RICs).\nHowever, the ever-increasing model size leads to longer training time,\njeopardizing the deadline requirements for both non-RT and near-RT RICs. To\naddress this issue, split federated learning (SFL) offers an approach by\noffloading partial model layers from near-RT-RIC to high-performance\nnon-RT-RIC. Nonetheless, its deployment presents two challenges: (i) Frequent\ndata/gradient transfers between near-RT-RIC and non-RT-RIC in SFL incur\nsignificant communication cost in O-RAN. (ii) Proper allocation of\ncomputational and communication resources in O-RAN is vital to satisfying the\ndeadline and affects SFL convergence. Therefore, we propose SplitMe, an SFL\nframework that exploits mutual learning to alternately and independently train\nthe near-RT-RIC's model and the non-RT-RIC's inverse model, eliminating\nfrequent transfers. The ''inverse'' of the inverse model is derived via a\nzeroth-order technique to integrate the final model. Then, we solve a joint\noptimization problem for SplitMe to minimize overall resource costs with\ndeadline-aware selection of near-RT-RICs and adaptive local updates. Our\nnumerical results demonstrate that SplitMe remarkably outperforms FL frameworks\nlike SFL, FedAvg and O-RANFed regarding costs and convergence.\n","authors":["Shunxian Gu","Chaoqun You","Bangbang Ren","Deke Guo"],"pdf_url":"https://arxiv.org/pdf/2508.02534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02532v1","updated":"2025-08-04T15:41:35Z","published":"2025-08-04T15:41:35Z","title":"Contextual Graph Transformer: A Small Language Model for Enhanced\n  Engineering Document Information Extraction","summary":"  Standard transformer-based language models, while powerful for general text,\noften struggle with the fine-grained syntax and entity relationships in complex\ntechnical, engineering documents. To address this, we propose the Contextual\nGraph Transformer (CGT), a hybrid neural architecture that combines Graph\nNeural Networks (GNNs) and Transformers for domain-specific question answering.\nCGT constructs a dynamic graph over input tokens using sequential, skip-gram,\nand semantic similarity edges, which is processed by GATv2Conv layers for local\nstructure learning. These enriched embeddings are then passed to a Transformer\nencoder to capture global dependencies. Unlike generic large models, technical\ndomains often require specialized language models with stronger\ncontextualization and structure awareness. CGT offers a parameter-efficient\nsolution for such use cases. Integrated into a Retrieval-Augmented Generation\n(RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7%\nhigher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from\nCGTs ability to jointly model structural token interactions and long-range\nsemantic coherence. The model is trained from scratch using a two-phase\napproach: pretraining on general text followed by fine-tuning on\ndomain-specific manuals. This highlights CGTs adaptability to technical\nlanguage, enabling better grounding, entity tracking, and retrieval-augmented\nresponses in real-world applications.\n","authors":["Karan Reddy","Mayukha Pal"],"pdf_url":"https://arxiv.org/pdf/2508.02532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02527v1","updated":"2025-08-04T15:36:51Z","published":"2025-08-04T15:36:51Z","title":"I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic\n  Representations in LLaMA 3.2","summary":"  Large language models demonstrate proficiency on phonetic tasks, such as\nrhyming, without explicit phonetic or auditory grounding. In this work, we\ninvestigate how \\verb|Llama-3.2-1B-Instruct| represents token-level phonetic\ninformation. Our results suggest that Llama uses a rich internal model of\nphonemes to complete phonetic tasks. We provide evidence for high-level\norganization of phoneme representations in its latent space. In doing so, we\nalso identify a ``phoneme mover head\" which promotes phonetic information\nduring rhyming tasks. We visualize the output space of this head and find that,\nwhile notable differences exist, Llama learns a model of vowels similar to the\nstandard IPA vowel chart for humans, despite receiving no direct supervision to\ndo so.\n","authors":["Jack Merullo","Arjun Khurana","Oliver McLaughlin"],"pdf_url":"https://arxiv.org/pdf/2508.02527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02524v1","updated":"2025-08-04T15:35:08Z","published":"2025-08-04T15:35:08Z","title":"Causality and Interpretability for Electrical Distribution System faults","summary":"  Causal analysis helps us understand variables that are responsible for system\nfailures. This improves fault detection and makes system more reliable. In this\nwork, we present a new method that combines causal inference with machine\nlearning to classify faults in electrical distribution systems (EDS) using\ngraph-based models. We first build causal graphs using transfer entropy (TE).\nEach fault case is represented as a graph, where the nodes are features such as\nvoltage and current, and the edges demonstrate how these features influence\neach other. Then, the graphs are classified using machine learning and\nGraphSAGE where the model learns from both the node values and the structure of\nthe graph to predict the type of fault. To make the predictions understandable,\nwe further developed an integrated approach using GNNExplainer and Captums\nIntegrated Gradients to highlight the nodes (features) that influences the most\non the final prediction. This gives us clear insights into the possible causes\nof the fault. Our experiments show high accuracy: 99.44% on the EDS fault\ndataset, which is better than state of art models. By combining causal graphs\nwith machine learning, our method not only predicts faults accurately but also\nhelps understand their root causes. This makes it a strong and practical tool\nfor improving system reliability.\n","authors":["Karthik Peddi","Sai Ram Aditya Parisineni","Hemanth Macharla","Mayukha Pal"],"pdf_url":"https://arxiv.org/pdf/2508.02524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02518v1","updated":"2025-08-04T15:25:48Z","published":"2025-08-04T15:25:48Z","title":"AnalogCoder-Pro: Unifying Analog Circuit Generation and Optimization via\n  Multi-modal LLMs","summary":"  Despite advances in analog design automation, analog front-end design still\nheavily depends on expert intuition and iterative simulations, underscoring\ncritical gaps in fully automated optimization for performance-critical\napplications. Recently, the rapid development of Large Language Models (LLMs)\nhas brought new promise to analog design automation. However, existing work\nremains in its early stages, and holistic joint optimization for practical\nend-to-end solutions remains largely unexplored. We propose AnalogCoder-Pro, a\nunified multimodal LLM-based framework that integrates generative capabilities\nand optimization techniques to jointly explore circuit topologies and optimize\ndevice sizing, automatically generating performance-specific, fully sized\nschematic netlists. AnalogCoder-Pro employs rejection sampling for fine-tuning\nLLMs on high-quality synthesized circuit data and introduces a multimodal\ndiagnosis and repair workflow based on functional specifications and waveform\nimages. By leveraging LLMs to interpret generated circuit netlists,\nAnalogCoder-Pro automates the extraction of critical design parameters and the\nformulation of parameter spaces, establishing an end-to-end workflow for\nsimultaneous topology generation and device sizing optimization. Extensive\nexperiments demonstrate that these orthogonal approaches significantly improve\nthe success rate of analog circuit design and enhance circuit performance.\n","authors":["Yao Lai","Souradip Poddar","Sungyoung Lee","Guojin Chen","Mengkang Hu","Bei Yu","Ping Luo","David Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2508.02518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22089v2","updated":"2025-08-04T15:23:32Z","published":"2024-10-29T14:46:49Z","title":"Hierarchical Structure Sharing Empowers Multi-task Heterogeneous GNNs\n  for Customer Expansion","summary":"  Customer expansion, i.e., growing a business existing customer base by\nacquiring new customers, is critical for scaling operations and sustaining the\nlong-term profitability of logistics companies. Although state-of-the-art works\nmodel this task as a single-node classification problem under a heterogeneous\ngraph learning framework and achieve good performance, they struggle with\nextremely positive label sparsity issues in our scenario. Multi-task learning\n(MTL) offers a promising solution by introducing a correlated, label-rich task\nto enhance the label-sparse task prediction through knowledge sharing. However,\nexisting MTL methods result in performance degradation because they fail to\ndiscriminate task-shared and task-specific structural patterns across tasks.\nThis issue arises from their limited consideration of the inherently complex\nstructure learning process of heterogeneous graph neural networks, which\ninvolves the multi-layer aggregation of multi-type relations. To address the\nchallenge, we propose a Structure-Aware Hierarchical Information Sharing\nFramework (SrucHIS), which explicitly regulates structural information sharing\nacross tasks in logistics customer expansion. SrucHIS breaks down the structure\nlearning phase into multiple stages and introduces sharing mechanisms at each\nstage, effectively mitigating the influence of task-specific structural\npatterns during each stage. We evaluate StrucHIS on both private and public\ndatasets, achieving a 51.41% average precision improvement on the private\ndataset and a 10.52% macro F1 gain on the public dataset. StrucHIS is further\ndeployed at one of the largest logistics companies in China and demonstrates a\n41.67% improvement in the success contract-signing rate over existing\nstrategies, generating over 453K new orders within just two months.\n","authors":["Xinyue Feng","Shuxin Zhong","Jinquan Hang","Wenjun Lyu","Yuequn Zhang","Guang Yang","Haotian Wang","Desheng Zhang","Guang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.22089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02515v1","updated":"2025-08-04T15:19:22Z","published":"2025-08-04T15:19:22Z","title":"PoeTone: A Framework for Constrained Generation of Structured Chinese\n  Songci with LLMs","summary":"  This paper presents a systematic investigation into the constrained\ngeneration capabilities of large language models (LLMs) in producing Songci, a\nclassical Chinese poetry form characterized by strict structural, tonal, and\nrhyme constraints defined by Cipai templates. We first develop a comprehensive,\nmulti-faceted evaluation framework that includes: (i) a formal conformity\nscore, (ii) automated quality assessment using LLMs, (iii) human evaluation,\nand (iv) classification-based probing tasks. Using this framework, we evaluate\nthe generative performance of 18 LLMs, including 3 proprietary models and 15\nopen-source models across four families, under five prompting strategies:\nzero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.\nFinally, we propose a Generate-Critic architecture in which the evaluation\nframework functions as an automated critic. Leveraging the critic's feedback as\na reward signal, we fine-tune three lightweight open-source LLMs via supervised\nfine-tuning (SFT), resulting in improvements of up to 5.88% in formal\nconformity. Our findings offer new insights into the generative strengths and\nlimitations of LLMs in producing culturally significant and formally\nconstrained literary texts.\n","authors":["Zhan Qu","Shuzhou Yuan","Michael Färber"],"pdf_url":"https://arxiv.org/pdf/2508.02515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02510v1","updated":"2025-08-04T15:17:08Z","published":"2025-08-04T15:17:08Z","title":"On Distributional Dependent Performance of Classical and Neural Routing\n  Solvers","summary":"  Neural Combinatorial Optimization aims to learn to solve a class of\ncombinatorial problems through data-driven methods and notably through\nemploying neural networks by learning the underlying distribution of problem\ninstances. While, so far neural methods struggle to outperform highly\nengineered problem specific meta-heuristics, this work explores a novel\napproach to formulate the distribution of problem instances to learn from and,\nmore importantly, plant a structure in the sampled problem instances. In\napplication to routing problems, we generate large problem instances that\nrepresent custom base problem instance distributions from which training\ninstances are sampled. The test instances to evaluate the methods on the\nrouting task consist of unseen problems sampled from the underlying large\nproblem instance. We evaluate representative NCO methods and specialized\nOperation Research meta heuristics on this novel task and demonstrate that the\nperformance gap between neural routing solvers and highly specialized\nmeta-heuristics decreases when learning from sub-samples drawn from a fixed\nbase node distribution.\n","authors":["Daniela Thyssens","Tim Dernedde","Wilson Sentanoe","Lars Schmidt-Thieme"],"pdf_url":"https://arxiv.org/pdf/2508.02510v1.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2503.12172v3","updated":"2025-08-04T15:15:16Z","published":"2025-03-15T15:29:05Z","title":"SEAL: Semantic Aware Image Watermarking","summary":"  Generative models have rapidly evolved to generate realistic outputs.\nHowever, their synthetic outputs increasingly challenge the clear distinction\nbetween natural and AI-generated content, necessitating robust watermarking\ntechniques. Watermarks are typically expected to preserve the integrity of the\ntarget image, withstand removal attempts, and prevent unauthorized replication\nonto unrelated images. To address this need, recent methods embed persistent\nwatermarks into images produced by diffusion models using the initial noise.\nYet, to do so, they either distort the distribution of generated images or rely\non searching through a long dictionary of used keys for detection.\n  In this paper, we propose a novel watermarking method that embeds semantic\ninformation about the generated image directly into the watermark, enabling a\ndistortion-free watermark that can be verified without requiring a database of\nkey patterns. Instead, the key pattern can be inferred from the semantic\nembedding of the image using locality-sensitive hashing. Furthermore,\nconditioning the watermark detection on the original image content improves\nrobustness against forgery attacks. To demonstrate that, we consider two\nlargely overlooked attack strategies: (i) an attacker extracting the initial\nnoise and generating a novel image with the same pattern; (ii) an attacker\ninserting an unrelated (potentially harmful) object into a watermarked image,\npossibly while preserving the watermark. We empirically validate our method's\nincreased robustness to these attacks. Taken together, our results suggest that\ncontent-aware watermarks can mitigate risks arising from image-generative\nmodels.\n","authors":["Kasra Arabi","R. Teal Witter","Chinmay Hegde","Niv Cohen"],"pdf_url":"https://arxiv.org/pdf/2503.12172v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02495v1","updated":"2025-08-04T15:05:27Z","published":"2025-08-04T15:05:27Z","title":"Clinical Expert Uncertainty Guided Generalized Label Smoothing for\n  Medical Noisy Label Learning","summary":"  Many previous studies have proposed extracting image labels from clinical\nnotes to create large-scale medical image datasets at a low cost. However,\nthese approaches inherently suffer from label noise due to uncertainty from the\nclinical experts. When radiologists and physicians analyze medical images to\nmake diagnoses, they often include uncertainty-aware notes such as ``maybe'' or\n``not excluded''. Unfortunately, current text-mining methods overlook these\nnuances, resulting in the creation of noisy labels. Existing methods for\nhandling noisy labels in medical image analysis, which typically address the\nproblem through post-processing techniques, have largely ignored the important\nissue of expert-driven uncertainty contributing to label noise. To better\nincorporate the expert-written uncertainty in clinical notes into medical image\nanalysis and address the label noise issue, we first examine the impact of\nclinical expert uncertainty on label noise. We then propose a clinical expert\nuncertainty-aware benchmark, along with a label smoothing method, which\nsignificantly improves performance compared to current state-of-the-art\napproaches.\n","authors":["Kunyu Zhang","Lin Gu","Liangchen Liu","Yingke Chen","Bingyang Wang","Jin Yan","Yingying Zhu"],"pdf_url":"https://arxiv.org/pdf/2508.02495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02485v1","updated":"2025-08-04T14:57:03Z","published":"2025-08-04T14:57:03Z","title":"Federated Graph Unlearning","summary":"  The demand for data privacy has led to the development of frameworks like\nFederated Graph Learning (FGL), which facilitate decentralized model training.\nHowever, a significant operational challenge in such systems is adhering to the\nright to be forgotten. This principle necessitates robust mechanisms for two\ndistinct types of data removal: the selective erasure of specific entities and\ntheir associated knowledge from local subgraphs and the wholesale removal of a\nuser's entire dataset and influence. Existing methods often struggle to fully\naddress both unlearning requirements, frequently resulting in incomplete data\nremoval or the persistence of residual knowledge within the system. This work\nintroduces a unified framework, conceived to provide a comprehensive solution\nto these challenges. The proposed framework employs a bifurcated strategy\ntailored to the specific unlearning request. For fine-grained Meta Unlearning,\nit uses prototype gradients to direct the initial local forgetting process,\nwhich is then refined by generating adversarial graphs to eliminate any\nremaining data traces among affected clients. In the case of complete client\nunlearning, the framework utilizes adversarial graph generation exclusively to\npurge the departed client's contributions from the remaining network. Extensive\nexperiments on multiple benchmark datasets validate the proposed approach. The\nframework achieves substantial improvements in model prediction accuracy across\nboth client and meta-unlearning scenarios when compared to existing methods.\nFurthermore, additional studies confirm its utility as a plug-in module, where\nit materially enhances the predictive capabilities and unlearning effectiveness\nof other established methods.\n","authors":["Yuming Ai","Xunkai Li","Jiaqi Chao","Bowen Fan","Zhengyu Wu","Yinlin Zhu","Rong-Hua Li","Guoren Wang"],"pdf_url":"https://arxiv.org/pdf/2508.02485v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2504.09006v2","updated":"2025-08-04T14:51:59Z","published":"2025-04-11T23:14:32Z","title":"Learning in Structured Stackelberg Games","summary":"  We study structured Stackelberg games, in which both players (the leader and\nthe follower) observe contextual information about the state of the world at\ntime of play. The leader plays against one of a finite number of followers, but\nthe follower's type is not known until after the game has ended. Importantly,\nwe assume a fixed relationship between the contextual information and the\nfollower's type, thereby allowing the leader to leverage this additional\nstructure when deciding her strategy. Under this setting, we find that standard\nlearning theoretic measures of complexity do not characterize the difficulty of\nthe leader's learning task. Instead, we introduce a new notion of dimension,\nthe Stackelberg-Littlestone dimension, which we show characterizes the\ninstance-optimal regret of the leader in the online setting. Based on this, we\nalso provide a provably optimal learning algorithm. We extend our results to\nthe distributional setting, where we use two new notions of dimension, the\n$\\gamma$-Stackelberg-Natarajan dimension and $\\gamma$-Stackelberg-Graph\ndimension. We prove that these control the sample complexity lower and upper\nbounds respectively, and we design a simple, improper algorithm that achieves\nthe upper bound.\n","authors":["Maria-Florina Balcan","Kiriaki Fragkia","Keegan Harris"],"pdf_url":"https://arxiv.org/pdf/2504.09006v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02482v1","updated":"2025-08-04T14:50:44Z","published":"2025-08-04T14:50:44Z","title":"Toward Using Machine Learning as a Shape Quality Metric for Liver Point\n  Cloud Generation","summary":"  While 3D medical shape generative models such as diffusion models have shown\npromise in synthesizing diverse and anatomically plausible structures, the\nabsence of ground truth makes quality evaluation challenging. Existing\nevaluation metrics commonly measure distributional distances between training\nand generated sets, while the medical field requires assessing quality at the\nindividual level for each generated shape, which demands labor-intensive expert\nreview.\n  In this paper, we investigate the use of classical machine learning (ML)\nmethods and PointNet as an alternative, interpretable approach for assessing\nthe quality of generated liver shapes. We sample point clouds from the surfaces\nof the generated liver shapes, extract handcrafted geometric features, and\ntrain a group of supervised ML and PointNet models to classify liver shapes as\ngood or bad. These trained models are then used as proxy discriminators to\nassess the quality of synthetic liver shapes produced by generative models.\n  Our results show that ML-based shape classifiers provide not only\ninterpretable feedback but also complementary insights compared to expert\nevaluation. This suggests that ML classifiers can serve as lightweight,\ntask-relevant quality metrics in 3D organ shape generation, supporting more\ntransparent and clinically aligned evaluation protocols in medical shape\nmodeling.\n","authors":["Khoa Tuan Nguyen","Gaeun Oh","Ho-min Park","Francesca Tozzi","Wouter Willaert","Joris Vankerschaver","Niki Rashidian","Wesley De Neve"],"pdf_url":"https://arxiv.org/pdf/2508.02482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02473v1","updated":"2025-08-04T14:37:32Z","published":"2025-08-04T14:37:32Z","title":"An Efficient and Adaptive Next Edit Suggestion Framework with Zero Human\n  Instructions in IDEs","summary":"  Code editing, including modifying, refactoring, and maintaining existing\ncode, is the most frequent task in software development and has garnered\nsignificant attention from AI-powered tools. However, existing solutions that\ntranslate explicit natural language instructions into code edits face critical\nlimitations, such as heavy reliance on human instruction input and high\nlatency, which hinder their effective integration into a developer's workflow.\nWe observe that developers' habitual behaviors and coding objectives are often\nreflected in their historical editing patterns, making this data key to\naddressing existing limitations. To leverage these insights, we propose NES\n(Next Edit Suggestion), an LLM-driven code editing framework that delivers an\ninstruction-free and low-latency experience. Built on a dual-model architecture\nand trained with our high-quality SFT and DAPO datasets, NES enhances\nproductivity by understanding developer intent while optimizing inference to\nminimize latency. NES is a scalable, industry-ready solution with a continuous\nTab key interaction workflow, seamlessly adopted by a FinTech company with over\n20,000 developers. Evaluations on real-world datasets show NES achieves 75.6%\nand 81.6% accuracy in two tasks of predicting next edit locations, alongside\n91.36% ES and 27.7% EMR for intent-aligned edits, outperforming SOTA models.\nOur open-sourced SFT and DAPO datasets have been demonstrated to enhance the\nperformance of open-source CodeLLMs. The demonstration of NES is available at\nhttps://youtu.be/yGoyYOe6fbY.\n","authors":["Xinfang Chen","Siyang Xiao","Xianying Zhu","Junhong Xie","Ming Liang","Dajun Chen","Wei Jiang","Yong Li","Peng Di"],"pdf_url":"https://arxiv.org/pdf/2508.02473v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2409.00029v2","updated":"2025-08-04T14:34:44Z","published":"2024-08-17T12:46:53Z","title":"Attack Anything: Blind DNNs via Universal Background Adversarial Attack","summary":"  It has been widely substantiated that deep neural networks (DNNs) are\nsusceptible and vulnerable to adversarial perturbations. Existing studies\nmainly focus on performing attacks by corrupting targeted objects (physical\nattack) or images (digital attack), which is intuitively acceptable and\nunderstandable in terms of the attack's effectiveness. In contrast, our focus\nlies in conducting background adversarial attacks in both digital and physical\ndomains, without causing any disruptions to the targeted objects themselves.\nSpecifically, an effective background adversarial attack framework is proposed\nto attack anything, by which the attack efficacy generalizes well between\ndiverse objects, models, and tasks. Technically, we approach the background\nadversarial attack as an iterative optimization problem, analogous to the\nprocess of DNN learning. Besides, we offer a theoretical demonstration of its\nconvergence under a set of mild but sufficient conditions. To strengthen the\nattack efficacy and transferability, we propose a new ensemble strategy\ntailored for adversarial perturbations and introduce an improved smooth\nconstraint for the seamless connection of integrated perturbations. We conduct\ncomprehensive and rigorous experiments in both digital and physical domains\nacross various objects, models, and tasks, demonstrating the effectiveness of\nattacking anything of the proposed method. The findings of this research\nsubstantiate the significant discrepancy between human and machine vision on\nthe value of background variations, which play a far more critical role than\npreviously recognized, necessitating a reevaluation of the robustness and\nreliability of DNNs. The code will be publicly available at\nhttps://github.com/JiaweiLian/Attack_Anything\n","authors":["Jiawei Lian","Shaohui Mei","Xiaofei Wang","Yi Wang","Lefan Wang","Yingjie Lu","Mingyang Ma","Lap-Pui Chau"],"pdf_url":"https://arxiv.org/pdf/2409.00029v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16525v4","updated":"2025-08-04T14:28:05Z","published":"2024-06-24T11:01:43Z","title":"Enhancing OOD Detection Using Latent Diffusion","summary":"  Out-of-distribution (OOD) detection is crucial for the reliable deployment of\nmachine learning models in real-world scenarios, enabling the identification of\nunknown samples or objects. A prominent approach to enhance OOD detection\nperformance involves leveraging auxiliary datasets for training. Recent efforts\nhave explored using generative models, such as Stable Diffusion (SD), to\nsynthesize outlier data in the pixel space. However, synthesizing OOD data in\nthe pixel space can lead to reduced robustness due to over-generation. To\naddress this challenge, we propose Outlier-Aware Learning (OAL), a novel\nframework that generates synthetic OOD training data within the latent space,\ntaking a further step to study how to utilize Stable Diffusion for developing a\nlatent-based outlier synthesis approach. This improvement facilitates network\ntraining with fewer outliers and less computational cost. Besides, to\nregularize the model's decision boundary, we develop a mutual information-based\ncontrastive learning module (MICL) that amplifies the distinction between\nIn-Distribution (ID) and collected OOD data. Moreover, we develop a knowledge\ndistillation module to prevent the degradation of ID classification accuracy\nwhen training with OOD data. The superior performance of our method on several\nbenchmark datasets demonstrates its efficiency and effectiveness. Source code\nis available in https://github.com/HengGao12/OAL.\n","authors":["Heng Gao","Jun Li"],"pdf_url":"https://arxiv.org/pdf/2406.16525v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18376v3","updated":"2025-08-04T14:25:00Z","published":"2025-07-24T12:52:32Z","title":"A Comprehensive Review of Diffusion Models in Smart Agriculture:\n  Progress, Applications, and Challenges","summary":"  With the global population increasing and arable land resources becoming\nincreasingly limited, smart and precision agriculture have emerged as essential\ndirections for sustainable agricultural development. Artificial intelligence\n(AI), particularly deep learning models, has been widely adopted in\napplications such as crop monitoring, pest detection, and yield prediction.\nAmong recent generative models, diffusion models have demonstrated considerable\npotential in agricultural image processing, data augmentation, and remote\nsensing analysis. Compared to traditional generative adversarial networks\n(GANs), diffusion models exhibit greater training stability and superior image\ngeneration quality, effectively addressing challenges such as limited annotated\ndatasets and imbalanced sample distributions in agricultural scenarios. This\npaper reviews recent advancements in the application of diffusion models within\nagriculture, focusing on their roles in crop disease and pest detection, remote\nsensing image enhancement, crop growth prediction, and agricultural resource\nmanagement. Empirical studies show that diffusion models significantly enhance\nthe performance of downstream models by improving accuracy, robustness, and\ngeneralization in tasks involving image synthesis, augmentation, and denoising\nunder complex environmental conditions. Despite ongoing challenges in\ncomputational efficiency and domain generalization, diffusion models are\nexpected to play an increasingly important role in the future of intelligent\nagriculture. As the technology continues to evolve, it holds substantial\npromise for addressing pressing global issues in food security and\nenvironmental sustainability.\n","authors":["Xing Hu","Haodong Chen","Choon Ki Ahn","Danfeng Hong","Qianqian Duan","Huiliang Shang","Guoxiang Li","Linhua Jiang","Dawei Zhang Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.18376v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00560v2","updated":"2025-08-04T14:13:34Z","published":"2024-11-30T19:07:16Z","title":"Friend or Foe? Harnessing Controllable Overfitting for Anomaly Detection","summary":"  Overfitting has traditionally been viewed as detrimental to anomaly\ndetection, where excessive generalization often limits models' sensitivity to\nsubtle anomalies. Our work challenges this conventional view by introducing\nControllable Overfitting-based Anomaly Detection (COAD), a novel framework that\nstrategically leverages overfitting to enhance anomaly discrimination\ncapabilities. We propose the Aberrance Retention Quotient (ARQ), a novel metric\nthat systematically quantifies the extent of overfitting, enabling the\nidentification of an optimal golden overfitting interval wherein model\nsensitivity to anomalies is maximized without sacrificing generalization. To\ncomprehensively capture how overfitting affects detection performance, we\nfurther propose the Relative Anomaly Distribution Index (RADI), a metric\nsuperior to traditional AUROC by explicitly modeling the separation between\nnormal and anomalous score distributions. Theoretically, RADI leverages ARQ to\ntrack and evaluate how overfitting impacts anomaly detection, offering an\nintegrated approach to understanding the relationship between overfitting\ndynamics and model efficacy. We also rigorously validate the statistical\nefficacy of Gaussian noise as pseudo-anomaly generators, reinforcing the\nmethod's broad applicability. Empirical evaluations demonstrate that our\ncontrollable overfitting method achieves State-Of-The-Art(SOTA) performance in\nboth one-class and multi-class anomaly detection tasks, thus redefining\noverfitting as a powerful strategy rather than a limitation.\n","authors":["Long Qian","Bingke Zhu","Yingying Chen","Ming Tang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2412.00560v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14334v3","updated":"2025-08-04T14:10:44Z","published":"2024-10-18T09:44:35Z","title":"Evaluating the evaluators: Towards human-aligned metrics for missing\n  markers reconstruction","summary":"  Animation data is often obtained through optical motion capture systems,\nwhich utilize a multitude of cameras to establish the position of optical\nmarkers. However, system errors or occlusions can result in missing markers,\nthe manual cleaning of which can be time-consuming. This has sparked interest\nin machine learning-based solutions for missing marker reconstruction in the\nacademic community. Most academic papers utilize a simplistic mean square error\nas the main metric. In this paper, we show that this metric does not correlate\nwith subjective perception of the fill quality. Additionally, we introduce and\nevaluate a set of better-correlated metrics that can drive progress in the\nfield.\n","authors":["Taras Kucherenko","Derek Peristy","Judith Bütepage"],"pdf_url":"https://arxiv.org/pdf/2410.14334v3.pdf","comment":"Accepted at the ACM International Conference on Multimedia 2025 (ACM\n  MM'25)"},{"id":"http://arxiv.org/abs/2411.14017v2","updated":"2025-08-04T14:05:44Z","published":"2024-11-21T11:01:03Z","title":"Automatic brain tumor segmentation in 2D intra-operative ultrasound\n  images using magnetic resonance imaging tumor annotations","summary":"  Automatic segmentation of brain tumors in intra-operative ultrasound (iUS)\nimages could facilitate localization of tumor tissue during resection surgery.\nThe lack of large annotated datasets limits the current models performances. In\nthis paper, we investigated the use of tumor annotations in magnetic resonance\nimaging (MRI) scans, which are more accessible than annotations in iUS images,\nfor training of deep learning models for iUS brain tumor segmentation. We used\n180 annotated MRI scans with corresponding unannotated iUS images, and 29\nannotated iUS images. Image registration was performed to transfer the MRI\nannotations to the corresponding iUS images before training the nnU-Net model\nwith different configurations of the data and label origins. The results showed\nno significant difference in Dice score for a model trained with only MRI\nannotated tumors compared to models trained with only iUS annotations and both,\nand to expert annotations, indicating that MRI tumor annotations can be used as\na substitute for iUS tumor annotations to train a deep learning model for\nautomatic brain tumor segmentation in iUS images. The best model obtained an\naverage Dice score of $0.62\\pm0.31$, compared to $0.67\\pm0.25$ for an expert\nneurosurgeon, where the performance on larger tumors were similar, but lower\nfor the models on smaller tumors. In addition, the results showed that removing\nsmaller tumors from the training sets improved the results. The main models are\navailable here:\nhttps://github.com/mathildefaanes/us_brain_tumor_segmentation/tree/main\n","authors":["Mathilde Faanes","Ragnhild Holden Helland","Ole Solheim","Sébastien Muller","Ingerid Reinertsen"],"pdf_url":"https://arxiv.org/pdf/2411.14017v2.pdf","comment":"14, 5figures. This work has been submitted to the IEEE for possible\n  publication"},{"id":"http://arxiv.org/abs/2508.02441v1","updated":"2025-08-04T14:00:40Z","published":"2025-08-04T14:00:40Z","title":"Computationally efficient Gauss-Newton reinforcement learning for model\n  predictive control","summary":"  Model predictive control (MPC) is widely used in process control due to its\ninterpretability and ability to handle constraints. As a parametric policy in\nreinforcement learning (RL), MPC offers strong initial performance and low data\nrequirements compared to black-box policies like neural networks. However, most\nRL methods rely on first-order updates, which scale well to large parameter\nspaces but converge at most linearly, making them inefficient when each policy\nupdate requires solving an optimal control problem, as is the case with MPC.\nWhile MPC policies are typically sparsely parameterized and thus amenable to\nsecond-order approaches, existing second-order methods demand second-order\npolicy derivatives, which can be computationally and memory-wise intractable.\n  This work introduces a Gauss-Newton approximation of the deterministic policy\nHessian that eliminates the need for second-order policy derivatives, enabling\nsuperlinear convergence with minimal computational overhead. To further improve\nrobustness, we propose a momentum-based Hessian averaging scheme for stable\ntraining under noisy estimates. We demonstrate the effectiveness of the\napproach on a nonlinear continuously stirred tank reactor (CSTR), showing\nfaster convergence and improved data efficiency over state-of-the-art\nfirst-order methods.\n","authors":["Dean Brandner","Sebastien Gros","Sergio Lucia"],"pdf_url":"https://arxiv.org/pdf/2508.02441v1.pdf","comment":"14 pages, 8 figures, submitted to Elsevier"},{"id":"http://arxiv.org/abs/2508.02429v1","updated":"2025-08-04T13:49:03Z","published":"2025-08-04T13:49:03Z","title":"Multimodal Large Language Models for End-to-End Affective Computing:\n  Benchmarking and Boosting with Generative Knowledge Prompting","summary":"  Multimodal Affective Computing (MAC) aims to recognize and interpret human\nemotions by integrating information from diverse modalities such as text,\nvideo, and audio. Recent advancements in Multimodal Large Language Models\n(MLLMs) have significantly reshaped the landscape of MAC by offering a unified\nframework for processing and aligning cross-modal information. However,\npractical challenges remain, including performance variability across complex\nMAC tasks and insufficient understanding of how architectural designs and data\ncharacteristics impact affective analysis. To address these gaps, we conduct a\nsystematic benchmark evaluation of state-of-the-art open-source MLLMs capable\nof concurrently processing audio, visual, and textual modalities across\nmultiple established MAC datasets. Our evaluation not only compares the\nperformance of these MLLMs but also provides actionable insights into model\noptimization by analyzing the influence of model architectures and dataset\nproperties. Furthermore, we propose a novel hybrid strategy that combines\ngenerative knowledge prompting with supervised fine-tuning to enhance MLLMs'\naffective computing capabilities. Experimental results demonstrate that this\nintegrated approach significantly improves performance across various MAC\ntasks, offering a promising avenue for future research and development in this\nfield. Our code is released on https://github.com/LuoMSen/MLLM-MAC.\n","authors":["Miaosen Luo","Jiesen Long","Zequn Li","Yunying Yang","Yuncheng Jiang","Sijie Mai"],"pdf_url":"https://arxiv.org/pdf/2508.02429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02426v1","updated":"2025-08-04T13:46:33Z","published":"2025-08-04T13:46:33Z","title":"Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding","summary":"  Since knowledge graphs (KG) will continue to evolve in real scenarios,\ntraditional KGE models are only suitable for static knowledge graphs.\nTherefore, continual knowledge graph embedding (CKGE) has attracted the\nattention of researchers. Currently, a key challenge facing CKGE is that the\nmodel is prone to \"catastrophic forgetting\", resulting in the loss of\npreviously learned knowledge. In order to effectively alleviate this problem,\nwe propose a new CKGE model BAKE. First, we note that the Bayesian posterior\nupdate principle provides a natural continual learning strategy that is\ninsensitive to data order and can theoretically effectively resist the\nforgetting of previous knowledge during data evolution. Different from the\nexisting CKGE method, BAKE regards each batch of new data as a Bayesian update\nof the model prior. Under this framework, as long as the posterior distribution\nof the model is maintained, the model can better preserve the knowledge of\nearly snapshots even after evolving through multiple time snapshots. Secondly,\nwe propose a continual clustering method for CKGE, which further directly\ncombats knowledge forgetting by constraining the evolution difference (or\nchange amplitude) between new and old knowledge between different snapshots. We\nconduct extensive experiments on BAKE on multiple datasets, and the results\nshow that BAKE significantly outperforms existing baseline models.\n","authors":["Linyu Li","Zhi Jin","Yuanpeng He","Dongming Jin","Yichi Zhang","Haoran Duan","Nyima Tash"],"pdf_url":"https://arxiv.org/pdf/2508.02426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02410v3","updated":"2025-08-04T13:44:01Z","published":"2025-02-04T15:29:00Z","title":"Privacy Amplification by Structured Subsampling for Deep Differentially\n  Private Time Series Forecasting","summary":"  Many forms of sensitive data, such as web traffic, mobility data, or hospital\noccupancy, are inherently sequential. The standard method for training machine\nlearning models while ensuring privacy for units of sensitive information, such\nas individual hospital visits, is differentially private stochastic gradient\ndescent (DP-SGD). However, we observe in this work that the formal guarantees\nof DP-SGD are incompatible with time series specific tasks like forecasting,\nsince they rely on the privacy amplification attained by training on small,\nunstructured batches sampled from an unstructured dataset. In contrast, batches\nfor forecasting are generated by (1) sampling sequentially structured time\nseries from a dataset, (2) sampling contiguous subsequences from these series,\nand (3) partitioning them into context and ground-truth forecast windows. We\ntheoretically analyze the privacy amplification attained by this structured\nsubsampling to enable the training of forecasting models with sound and tight\nevent- and user-level privacy guarantees. Towards more private models, we\nadditionally prove how data augmentation amplifies privacy in self-supervised\ntraining of sequence models. Our empirical evaluation demonstrates that\namplification by structured subsampling enables the training of forecasting\nmodels with strong formal privacy guarantees.\n","authors":["Jan Schuchardt","Mina Dalirrooyfard","Jed Guzelkabaagac","Anderson Schneider","Yuriy Nevmyvaka","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2502.02410v3.pdf","comment":"Accepted as ICML 2025 Spotlight"},{"id":"http://arxiv.org/abs/2508.02422v1","updated":"2025-08-04T13:43:47Z","published":"2025-08-04T13:43:47Z","title":"Superior resilience to poisoning and amenability to unlearning in\n  quantum machine learning","summary":"  The reliability of artificial intelligence hinges on the integrity of its\ntraining data, a foundation often compromised by noise and corruption. Here,\nthrough a comparative study of classical and quantum neural networks on both\nclassical and quantum data, we reveal a fundamental difference in their\nresponse to data corruption. We find that classical models exhibit brittle\nmemorization, leading to a failure in generalization. In contrast, quantum\nmodels demonstrate remarkable resilience, which is underscored by a phase\ntransition-like response to increasing label noise, revealing a critical point\nbeyond which the model's performance changes qualitatively. We further\nestablish and investigate the field of quantum machine unlearning, the process\nof efficiently forcing a trained model to forget corrupting influences. We show\nthat the brittle nature of the classical model forms rigid, stubborn memories\nof erroneous data, making efficient unlearning challenging, while the quantum\nmodel is significantly more amenable to efficient forgetting with approximate\nunlearning methods. Our findings establish that quantum machine learning can\npossess a dual advantage of intrinsic resilience and efficient adaptability,\nproviding a promising paradigm for the trustworthy and robust artificial\nintelligence of the future.\n","authors":["Yu-Qin Chen","Shi-Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.02422v1.pdf","comment":"9 pages, 4 figures with references and supplemental materials"},{"id":"http://arxiv.org/abs/2508.02421v1","updated":"2025-08-04T13:42:45Z","published":"2025-08-04T13:42:45Z","title":"Emergence of Fair Leaders via Mediators in Multi-Agent Reinforcement\n  Learning","summary":"  Stackelberg games and their resulting equilibria have received increasing\nattention in the multi-agent reinforcement learning literature. Each stage of a\ntraditional Stackelberg game involves a leader(s) acting first, followed by the\nfollowers. In situations where the roles of leader(s) and followers can be\ninterchanged, the designated role can have considerable advantages, for\nexample, in first-mover advantage settings. Then the question arises: Who\nshould be the leader and when? A bias in the leader selection process can lead\nto unfair outcomes. This problem is aggravated if the agents are\nself-interested and care only about their goals and rewards. We formally define\nthis leader selection problem and show its relation to fairness in agents'\nreturns. Furthermore, we propose a multi-agent reinforcement learning framework\nthat maximizes fairness by integrating mediators. Mediators have previously\nbeen used in the simultaneous action setting with varying levels of control,\nsuch as directly performing agents' actions or just recommending them. Our\nframework integrates mediators in the Stackelberg setting with minimal control\n(leader selection). We show that the presence of mediators leads to\nself-interested agents taking fair actions, resulting in higher overall\nfairness in agents' returns.\n","authors":["Akshay Dodwadmath","Setareh Maghsudi"],"pdf_url":"https://arxiv.org/pdf/2508.02421v1.pdf","comment":"Accepted to ECAI 2025"},{"id":"http://arxiv.org/abs/2505.19785v2","updated":"2025-08-04T13:42:18Z","published":"2025-05-26T10:16:39Z","title":"medDreamer: Model-Based Reinforcement Learning with Latent Imagination\n  on Complex EHRs for Clinical Decision Support","summary":"  Timely and personalized treatment decisions are essential across a wide range\nof healthcare settings where patient responses can vary significantly and\nevolve over time. Clinical data used to support these treatment decisions are\noften irregularly sampled, where missing data frequencies may implicitly convey\ninformation about the patient's condition. Existing Reinforcement Learning (RL)\nbased clinical decision support systems often ignore the missing patterns and\ndistort them with coarse discretization and simple imputation. They are also\npredominantly model-free and largely depend on retrospective data, which could\nlead to insufficient exploration and bias by historical behaviors. To address\nthese limitations, we propose medDreamer, a novel model-based reinforcement\nlearning framework for personalized treatment recommendation. medDreamer\ncontains a world model with an Adaptive Feature Integration module that\nsimulates latent patient states from irregular data and a two-phase policy\ntrained on a hybrid of real and imagined trajectories. This enables learning\noptimal policies that go beyond the sub-optimality of historical clinical\ndecisions, while remaining close to real clinical data. We evaluate medDreamer\non both sepsis and mechanical ventilation treatment tasks using two large-scale\nElectronic Health Records (EHRs) datasets. Comprehensive evaluations show that\nmedDreamer significantly outperforms model-free and model-based baselines in\nboth clinical outcomes and off-policy metrics.\n","authors":["Qianyi Xu","Gousia Habib","Dilruk Perera","Mengling Feng"],"pdf_url":"https://arxiv.org/pdf/2505.19785v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05108v4","updated":"2025-08-04T13:41:35Z","published":"2025-04-07T14:14:15Z","title":"Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement\n  Learning","summary":"  Discovering efficient algorithms for solving complex problems has been an\noutstanding challenge in mathematics and computer science, requiring\nsubstantial human expertise over the years. Recent advancements in evolutionary\nsearch with large language models (LLMs) have shown promise in accelerating the\ndiscovery of algorithms across various domains, particularly in mathematics and\noptimization. However, existing approaches treat the LLM as a static generator,\nmissing the opportunity to update the model with the signal obtained from\nevolutionary exploration. In this work, we propose to augment LLM-based\nevolutionary search by continuously refining the search operator - the LLM -\nthrough reinforcement learning (RL) fine-tuning. Our method leverages\nevolutionary search as an exploration strategy to discover improved algorithms,\nwhile RL optimizes the LLM policy based on these discoveries. Our experiments\non combinatorial optimization tasks demonstrate that integrating RL with\nevolutionary search accelerates the discovery of superior algorithms,\nshowcasing the potential of RL-enhanced evolutionary strategies for algorithm\ndesign.\n","authors":["Anja Surina","Amin Mansouri","Lars Quaedvlieg","Amal Seddas","Maryna Viazovska","Emmanuel Abbe","Caglar Gulcehre"],"pdf_url":"https://arxiv.org/pdf/2504.05108v4.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2508.02417v1","updated":"2025-08-04T13:40:25Z","published":"2025-08-04T13:40:25Z","title":"The Role of Review Process Failures in Affective State Estimation: An\n  Empirical Investigation of DEAP Dataset","summary":"  The reliability of affective state estimation using EEG data is in question,\ngiven the variability in reported performance and the lack of standardized\nevaluation protocols. To investigate this, we reviewed 101 studies, focusing on\nthe widely used DEAP dataset for emotion recognition. Our analysis revealed\nwidespread methodological issues that include data leakage from improper\nsegmentation, biased feature selection, flawed hyperparameter optimization,\nneglect of class imbalance, and insufficient methodological reporting. Notably,\nwe found that nearly 87% of the reviewed papers contained one or more of these\nerrors. Moreover, through experimental analysis, we observed that such\nmethodological flaws can inflate the classification accuracy by up to 46%.\nThese findings reveal fundamental gaps in standardized evaluation practices and\nhighlight critical deficiencies in the peer review process for machine learning\napplications in neuroscience, emphasizing the urgent need for stricter\nmethodological standards and evaluation protocols.\n","authors":["Nazmun N Khan","Taylor Sweet","Chase A Harvey","Calder Knapp","Dean J. Krusienski","David E Thompson"],"pdf_url":"https://arxiv.org/pdf/2508.02417v1.pdf","comment":"25 pages, 4 figures, This is a preprint version of the manuscript. It\n  is intended for submission to a peer-reviewed journal"},{"id":"http://arxiv.org/abs/2505.05026v3","updated":"2025-08-04T13:38:49Z","published":"2025-05-08T08:00:32Z","title":"Do MLLMs Capture How Interfaces Guide User Behavior? A Benchmark for\n  Multimodal UI/UX Design Understanding","summary":"  User interface (UI) design goes beyond visuals, guiding user behavior and\noverall user experience (UX). Strategically crafted interfaces, for example,\ncan boost sign-ups and drive business sales, underscoring the shift toward\nUI/UX as a unified design concept. While recent studies have explored UI\nquality evaluation using Multimodal Large Language Models (MLLMs), they largely\nfocus on surface-level features, overlooking behavior-oriented aspects. To fill\nthis gap, we introduce WiserUI-Bench, a novel benchmark for assessing models'\nmultimodal understanding of UI/UX design. It includes 300 diverse real-world UI\nimage pairs, each consisting of two design variants A/B-tested at scale by\nactual companies, where one was empirically validated to steer more user\nactions than the other. Each pair is accompanied one or more of 684\nexpert-curated rationales that capture key factors behind each winning design's\neffectiveness, spanning diverse cognitive dimensions of UX. Our benchmark\nsupports two core tasks: (1) selecting the more effective UI/UX design by\npredicting the A/B test verified winner and (2) assessing how well a model,\ngiven the winner, can explain its effectiveness in alignment with expert\nreasoning. Experiments across several MLLMs show that current models exhibit\nlimited nuanced reasoning about UI/UX design and its behavioral impact. We\nbelieve our work will foster research in UI/UX understanding and enable broader\napplications such as behavior-aware interface optimization.\n","authors":["Jaehyun Jeon","Min Soo Kim","Jang Han Yoon","Sumin Shim","Yejin Choi","Hanbin Kim","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2505.05026v3.pdf","comment":"26 pages, 25 figures, Our code and dataset:\n  https://github.com/jeochris/wiserui-bench"},{"id":"http://arxiv.org/abs/2508.02414v1","updated":"2025-08-04T13:36:41Z","published":"2025-08-04T13:36:41Z","title":"ASMR: Angular Support for Malfunctioning Client Resilience in Federated\n  Learning","summary":"  Federated Learning (FL) allows the training of deep neural networks in a\ndistributed and privacy-preserving manner. However, this concept suffers from\nmalfunctioning updates sent by the attending clients that cause global model\nperformance degradation. Reasons for this malfunctioning might be technical\nissues, disadvantageous training data, or malicious attacks. Most of the\ncurrent defense mechanisms are meant to require impractical prerequisites like\nknowledge about the number of malfunctioning updates, which makes them\nunsuitable for real-world applications. To counteract these problems, we\nintroduce a novel method called Angular Support for Malfunctioning Client\nResilience (ASMR), that dynamically excludes malfunctioning clients based on\ntheir angular distance. Our novel method does not require any hyperparameters\nor knowledge about the number of malfunctioning clients. Our experiments\nshowcase the detection capabilities of ASMR in an image classification task on\na histopathological dataset, while also presenting findings on the significance\nof dynamically adapting decision boundaries.\n","authors":["Mirko Konstantin","Moritz Fuchs","Anirban Mukhopadhyay"],"pdf_url":"https://arxiv.org/pdf/2508.02414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02411v1","updated":"2025-08-04T13:33:28Z","published":"2025-08-04T13:33:28Z","title":"HGTS-Former: Hierarchical HyperGraph Transformer for Multivariate Time\n  Series Analysis","summary":"  Multivariate time series analysis has long been one of the key research\ntopics in the field of artificial intelligence. However, analyzing complex time\nseries data remains a challenging and unresolved problem due to its high\ndimensionality, dynamic nature, and complex interactions among variables.\nInspired by the strong structural modeling capability of hypergraphs, this\npaper proposes a novel hypergraph-based time series transformer backbone\nnetwork, termed HGTS-Former, to address the multivariate coupling in time\nseries data. Specifically, given the multivariate time series signal, we first\nnormalize and embed each patch into tokens. Then, we adopt the multi-head\nself-attention to enhance the temporal representation of each patch. The\nhierarchical hypergraphs are constructed to aggregate the temporal patterns\nwithin each channel and fine-grained relations between different variables.\nAfter that, we convert the hyperedge into node features through the EdgeToNode\nmodule and adopt the feed-forward network to further enhance the output\nfeatures. Extensive experiments conducted on two multivariate time series tasks\nand eight datasets fully validated the effectiveness of our proposed\nHGTS-Former. The source code will be released on\nhttps://github.com/Event-AHU/Time_Series_Analysis.\n","authors":["Xiao Wang","Hao Si","Fan Zhang","Xiaoya Zhou","Dengdi Sun","Wanli Lyu","Qingquan Yang","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2508.02411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08611v2","updated":"2025-08-04T13:33:25Z","published":"2025-02-12T17:59:21Z","title":"Robustly Learning Monotone Generalized Linear Models via Data\n  Augmentation","summary":"  We study the task of learning Generalized Linear models (GLMs) in the\nagnostic model under the Gaussian distribution. We give the first\npolynomial-time algorithm that achieves a constant-factor approximation for\n\\textit{any} monotone Lipschitz activation. Prior constant-factor GLM learners\nsucceed for a substantially smaller class of activations. Our work resolves a\nwell-known open problem, by developing a robust counterpart to the classical\nGLMtron algorithm (Kakade et al., 2011). Our robust learner applies more\ngenerally, encompassing all monotone activations with bounded\n$(2+\\zeta)$-moments, for any fixed $\\zeta>0$ -- a condition that is essentially\nnecessary. To obtain our results, we leverage a novel data augmentation\ntechnique with decreasing Gaussian noise injection and prove a number of\nstructural results that may be useful in other settings.\n","authors":["Nikos Zarifis","Puqian Wang","Ilias Diakonikolas","Jelena Diakonikolas"],"pdf_url":"https://arxiv.org/pdf/2502.08611v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08544v2","updated":"2025-08-04T13:29:01Z","published":"2025-04-11T13:57:09Z","title":"Slicing the Gaussian Mixture Wasserstein Distance","summary":"  Gaussian mixture models (GMMs) are widely used in machine learning for tasks\nsuch as clustering, classification, image reconstruction, and generative\nmodeling. A key challenge in working with GMMs is defining a computationally\nefficient and geometrically meaningful metric. The mixture Wasserstein (MW)\ndistance adapts the Wasserstein metric to GMMs and has been applied in various\ndomains, including domain adaptation, dataset comparison, and reinforcement\nlearning. However, its high computational cost -- arising from repeated\nWasserstein distance computations involving matrix square root estimations and\nan expensive linear program -- limits its scalability to high-dimensional and\nlarge-scale problems. To address this, we propose multiple novel slicing-based\napproximations to the MW distance that significantly reduce computational\ncomplexity while preserving key optimal transport properties. From a\ntheoretical viewpoint, we establish several weak and strong equivalences\nbetween the introduced metrics, and show the relations to the original MW\ndistance and the well-established sliced Wasserstein distance. Furthermore, we\nvalidate the effectiveness of our approach through numerical experiments,\ndemonstrating computational efficiency and applications in clustering,\nperceptual image comparison, and GMM minimization\n","authors":["Moritz Piening","Robert Beinert"],"pdf_url":"https://arxiv.org/pdf/2504.08544v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.21905v2","updated":"2025-08-04T13:19:11Z","published":"2025-07-29T15:17:00Z","title":"Evaluating Deepfake Detectors in the Wild","summary":"  Deepfakes powered by advanced machine learning models present a significant\nand evolving threat to identity verification and the authenticity of digital\nmedia. Although numerous detectors have been developed to address this problem,\ntheir effectiveness has yet to be tested when applied to real-world data. In\nthis work we evaluate modern deepfake detectors, introducing a novel testing\nprocedure designed to mimic real-world scenarios for deepfake detection. Using\nstate-of-the-art deepfake generation methods, we create a comprehensive dataset\ncontaining more than 500,000 high-quality deepfake images. Our analysis shows\nthat detecting deepfakes still remains a challenging task. The evaluation shows\nthat in fewer than half of the deepfake detectors tested achieved an AUC score\ngreater than 60%, with the lowest being 50%. We demonstrate that basic image\nmanipulations, such as JPEG compression or image enhancement, can significantly\nreduce model performance. All code and data are publicly available at\nhttps://github.com/SumSubstance/Deepfake-Detectors-in-the-Wild.\n","authors":["Viacheslav Pirogov","Maksim Artemev"],"pdf_url":"https://arxiv.org/pdf/2507.21905v2.pdf","comment":"Accepted to the ICML 2025 Workshop 'DataWorld: Unifying Data Curation\n  Frameworks Across Domains'"},{"id":"http://arxiv.org/abs/2508.02387v1","updated":"2025-08-04T13:10:48Z","published":"2025-08-04T13:10:48Z","title":"$ε$-Softmax: Approximating One-Hot Vectors for Mitigating Label\n  Noise","summary":"  Noisy labels pose a common challenge for training accurate deep neural\nnetworks. To mitigate label noise, prior studies have proposed various robust\nloss functions to achieve noise tolerance in the presence of label noise,\nparticularly symmetric losses. However, they usually suffer from the\nunderfitting issue due to the overly strict symmetric condition. In this work,\nwe propose a simple yet effective approach for relaxing the symmetric\ncondition, namely $\\epsilon$-softmax, which simply modifies the outputs of the\nsoftmax layer to approximate one-hot vectors with a controllable error\n$\\epsilon$. Essentially, $\\epsilon$-softmax not only acts as an alternative for\nthe softmax layer, but also implicitly plays the crucial role in modifying the\nloss function. We prove theoretically that $\\epsilon$-softmax can achieve\nnoise-tolerant learning with controllable excess risk bound for almost any loss\nfunction. Recognizing that $\\epsilon$-softmax-enhanced losses may slightly\nreduce fitting ability on clean datasets, we further incorporate them with one\nsymmetric loss, thereby achieving a better trade-off between robustness and\neffective learning. Extensive experiments demonstrate the superiority of our\nmethod in mitigating synthetic and real-world label noise. The code is\navailable at https://github.com/cswjl/eps-softmax.\n","authors":["Jialiang Wang","Xiong Zhou","Deming Zhai","Junjun Jiang","Xiangyang Ji","Xianming Liu"],"pdf_url":"https://arxiv.org/pdf/2508.02387v1.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2508.02383v1","updated":"2025-08-04T13:09:47Z","published":"2025-08-04T13:09:47Z","title":"Graph Embedding in the Graph Fractional Fourier Transform Domain","summary":"  Spectral graph embedding plays a critical role in graph representation\nlearning by generating low-dimensional vector representations from graph\nspectral information. However, the embedding space of traditional spectral\nembedding methods often exhibit limited expressiveness, failing to exhaustively\ncapture latent structural features across alternative transform domains. To\naddress this issue, we use the graph fractional Fourier transform to extend the\nexisting state-of-the-art generalized frequency filtering embedding (GEFFE)\ninto fractional domains, giving birth to the generalized fractional filtering\nembedding (GEFRFE), which enhances embedding informativeness via the graph\nfractional domain. The GEFRFE leverages graph fractional domain filtering and a\nnonlinear composition of eigenvector components derived from a fractionalized\ngraph Laplacian. To dynamically determine the fractional order, two parallel\nstrategies are introduced: search-based optimization and a ResNet18-based\nadaptive learning. Extensive experiments on six benchmark datasets demonstrate\nthat the GEFRFE captures richer structural features and significantly enhance\nclassification performance. Notably, the proposed method retains computational\ncomplexity comparable to GEFFE approaches.\n","authors":["Changjie Sheng","Zhichao Zhang","Wei Yao"],"pdf_url":"https://arxiv.org/pdf/2508.02383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02381v1","updated":"2025-08-04T13:08:35Z","published":"2025-08-04T13:08:35Z","title":"Beyond Manually Designed Pruning Policies with Second-Level Performance\n  Prediction: A Pruning Framework for LLMs","summary":"  Non-uniform structured network pruning methods can effectively reduce Large\nLanguage Model (LLM) size by eliminating redundant channels or layers, offering\nlower performance degradation than uniform strategies. However, existing\nnon-uniform methods rely heavily on manually designed pruning policies (e.g.,\nlayer importance and scaling factors), and therefore cannot efficiently adapt\nto scenarios with dynamic pruning ratio requirements. Additionly, a critical\nbottleneck -- the time-consuming evaluation of pruning policies -- further\nlimits the feasibility of iteratively and dynamically finding optimal pruning\npolicies. To address these limitations, we propose PPF (Predictive Pruning\nFramework), a novel pruning framework for LLMs that eliminates manual design\ndependencies via second-level performance prediction. PPF not only supports\nreal-time pruning decisions under dynamic pruning ratios but is also applicable\nto static pruning scenarios. It employs an agent for producing adaptive and\nreal-time pruning actions, while a lightweight performance predictor that can\nevaluate a pruning policy in seconds, significantly speeding up the iterative\noptimization process. Experiments on Llama2-7B and Llama3-8B show that PPF can\ngenerate dynamic/static pruning policies and it reduces perplexity by up to\n33.4% (dynamic pruning) and 84.78% (static pruning) over existing methods,\noutperforming manually designed pruning policies. The performance predictor\nachieves second-level performance prediction with high accuracy (prediction\nerror < 0.0011). It reduces the mean evaluation latency from minute-level (1\nminute and 38.02 seconds of test-set evaluation methods) to second-level (1.52\nsecond), achieving over 64 times speedup. Our code will be available at\nhttps://github.com/Ma-zx/PPF .\n","authors":["Zuxin Ma","Yunhe Cui","Yongbin Qin"],"pdf_url":"https://arxiv.org/pdf/2508.02381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02374v1","updated":"2025-08-04T13:02:23Z","published":"2025-08-04T13:02:23Z","title":"Uni-Layout: Integrating Human Feedback in Unified Layout Generation and\n  Evaluation","summary":"  Layout generation plays a crucial role in enhancing both user experience and\ndesign efficiency. However, current approaches suffer from task-specific\ngeneration capabilities and perceptually misaligned evaluation metrics, leading\nto limited applicability and ineffective measurement. In this paper, we propose\n\\textit{Uni-Layout}, a novel framework that achieves unified generation,\nhuman-mimicking evaluation and alignment between the two. For universal\ngeneration, we incorporate various layout tasks into a single taxonomy and\ndevelop a unified generator that handles background or element contents\nconstrained tasks via natural language prompts. To introduce human feedback for\nthe effective evaluation of layouts, we build \\textit{Layout-HF100k}, the first\nlarge-scale human feedback dataset with 100,000 expertly annotated layouts.\nBased on \\textit{Layout-HF100k}, we introduce a human-mimicking evaluator that\nintegrates visual and geometric information, employing a Chain-of-Thought\nmechanism to conduct qualitative assessments alongside a confidence estimation\nmodule to yield quantitative measurements. For better alignment between the\ngenerator and the evaluator, we integrate them into a cohesive system by\nadopting Dynamic-Margin Preference Optimization (DMPO), which dynamically\nadjusts margins based on preference strength to better align with human\njudgments. Extensive experiments show that \\textit{Uni-Layout} significantly\noutperforms both task-specific and general-purpose methods. Our code is\npublicly available at https://github.com/JD-GenX/Uni-Layout.\n","authors":["Shuo Lu","Yanyin Chen","Wei Feng","Jiahao Fan","Fengheng Li","Zheng Zhang","Jingjing Lv","Junjie Shen","Ching Law","Jian Liang"],"pdf_url":"https://arxiv.org/pdf/2508.02374v1.pdf","comment":"Accepted to ACM MM 2025"},{"id":"http://arxiv.org/abs/2302.12024v3","updated":"2025-08-04T13:01:41Z","published":"2023-02-23T13:34:01Z","title":"Comparison of Affine and Rational Quadratic Spline Coupling and\n  Autoregressive Flows through Robust Statistical Tests","summary":"  Normalizing flows have emerged as a powerful brand of generative models, as\nthey not only allow for efficient sampling of complicated target distributions\nbut also deliver density estimation by construction. We propose here an\nin-depth comparison of coupling and autoregressive flows, both based on\nsymmetric (affine) and non-symmetric (rational quadratic spline) bijectors,\nconsidering four different architectures: real-valued non-Volume preserving\n(RealNVP), masked autoregressive flow (MAF), coupling rational quadratic spline\n(C-RQS), and autoregressive rational quadratic spline (A-RQS). We focus on a\nset of multimodal target distributions of increasing dimensionality ranging\nfrom 4 to 400. The performances were compared by means of different test\nstatistics for two-sample tests, built from known distance measures: the sliced\nWasserstein distance, the dimension-averaged one-dimensional\nKolmogorov--Smirnov test, and the Frobenius norm of the difference between\ncorrelation matrices. Furthermore, we included estimations of the variance of\nboth the metrics and the trained models. Our results indicate that the A-RQS\nalgorithm stands out both in terms of accuracy and training speed. Nonetheless,\nall the algorithms are generally able, without too much fine-tuning, to learn\ncomplicated distributions with limited training data and in a reasonable time\nof the order of hours on a Tesla A40 GPU. The only exception is the C-RQS,\nwhich takes significantly longer to train, does not always provide good\naccuracy, and becomes unstable for large dimensionalities. All algorithms were\nimplemented using \\textsc{TensorFlow2} and \\textsc{TensorFlow Probability} and\nhave been made available on\n\\href{https://github.com/NF4HEP/NormalizingFlowsHD}{GitHub}.\n","authors":["Andrea Coccaro","Marco Letizia","Humberto Reyes-Gonzalez","Riccardo Torre"],"pdf_url":"https://arxiv.org/pdf/2302.12024v3.pdf","comment":"v3: published version; 25 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2508.02366v1","updated":"2025-08-04T12:52:11Z","published":"2025-08-04T12:52:11Z","title":"Language Model Guided Reinforcement Learning in Quantitative Trading","summary":"  Algorithmic trading requires short-term decisions aligned with long-term\nfinancial goals. While reinforcement learning (RL) has been explored for such\ntactical decisions, its adoption remains limited by myopic behavior and opaque\npolicy rationale. In contrast, large language models (LLMs) have recently\ndemonstrated strategic reasoning and multi-modal financial signal\ninterpretation when guided by well-designed prompts.\n  We propose a hybrid system where LLMs generate high-level trading strategies\nto guide RL agents in their actions. We evaluate (i) the rationale of\nLLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and\nMaximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results\nshow improved return and risk metrics over standard RL.\n","authors":["Adam Darmanin","Vince Vella"],"pdf_url":"https://arxiv.org/pdf/2508.02366v1.pdf","comment":"12 pages (4 pages appendix and references), 6 figures, preprint under\n  review for FLLM 2025 conference"},{"id":"http://arxiv.org/abs/2508.02364v1","updated":"2025-08-04T12:51:10Z","published":"2025-08-04T12:51:10Z","title":"A Novel Sliced Fused Gromov-Wasserstein Distance","summary":"  The Gromov--Wasserstein (GW) distance and its fused extension (FGW) are\npowerful tools for comparing heterogeneous data. Their computation is, however,\nchallenging since both distances are based on non-convex, quadratic optimal\ntransport (OT) problems. Leveraging 1D OT, a sliced version of GW has been\nproposed to lower the computational burden. Unfortunately, this sliced version\nis restricted to Euclidean geometry and loses invariance to isometries,\nstrongly limiting its application in practice. To overcome these issues, we\npropose a novel slicing technique for GW as well as for FGW that is based on an\nappropriate lower bound, hierarchical OT, and suitable quadrature rules for the\nunderlying 1D OT problems. Our novel sliced FGW significantly reduces the\nnumerical effort while remaining invariant to isometric transformations and\nallowing the comparison of arbitrary geometries. We show that our new distance\nactually defines a pseudo-metric for structured spaces that bounds FGW from\nbelow and study its interpolation properties between sliced Wasserstein and GW.\nSince we avoid the underlying quadratic program, our sliced distance is\nnumerically more robust and reliable than the original GW and FGW distance;\nespecially in the context of shape retrieval and graph isomorphism testing.\n","authors":["Moritz Piening","Robert Beinert"],"pdf_url":"https://arxiv.org/pdf/2508.02364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02354v1","updated":"2025-08-04T12:44:07Z","published":"2025-08-04T12:44:07Z","title":"Detecting COPD Through Speech Analysis: A Dataset of Danish Speech and\n  Machine Learning Approach","summary":"  Chronic Obstructive Pulmonary Disease (COPD) is a serious and debilitating\ndisease affecting millions around the world. Its early detection using\nnon-invasive means could enable preventive interventions that improve quality\nof life and patient outcomes, with speech recently shown to be a valuable\nbiomarker. Yet, its validity across different linguistic groups remains to be\nseen. To that end, audio data were collected from 96 Danish participants\nconducting three speech tasks (reading, coughing, sustained vowels). Half of\nthe participants were diagnosed with different levels of COPD and the other\nhalf formed a healthy control group. Subsequently, we investigated different\nbaseline models using openSMILE features and learnt x-vector embeddings. We\nobtained a best accuracy of 67% using openSMILE features and logistic\nregression. Our findings support the potential of speech-based analysis as a\nnon-invasive, remote, and scalable screening tool as part of future COPD\nhealthcare solutions.\n","authors":["Cuno Sankey-Olsen","Rasmus Hvass Olesen","Tobias Oliver Eberhard","Andreas Triantafyllopoulos","Björn Schuller","Ilhan Aslan"],"pdf_url":"https://arxiv.org/pdf/2508.02354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02349v1","updated":"2025-08-04T12:34:34Z","published":"2025-08-04T12:34:34Z","title":"Detecting and measuring respiratory events in horses during exercise\n  with a microphone: deep learning vs. standard signal processing","summary":"  Monitoring respiration parameters such as respiratory rate could be\nbeneficial to understand the impact of training on equine health and\nperformance and ultimately improve equine welfare. In this work, we compare\ndeep learning-based methods to an adapted signal processing method to\nautomatically detect cyclic respiratory events and extract the dynamic\nrespiratory rate from microphone recordings during high intensity exercise in\nStandardbred trotters. Our deep learning models are able to detect exhalation\nsounds (median F1 score of 0.94) in noisy microphone signals and show promising\nresults on unlabelled signals at lower exercising intensity, where the\nexhalation sounds are less recognisable. Temporal convolutional networks were\nbetter at detecting exhalation events and estimating dynamic respiratory rates\n(median F1: 0.94, Mean Absolute Error (MAE) $\\pm$ Confidence Intervals (CI):\n1.44$\\pm$1.04 bpm, Limits Of Agreements (LOA): 0.63$\\pm$7.06 bpm) than long\nshort-term memory networks (median F1: 0.90, MAE$\\pm$CI: 3.11$\\pm$1.58 bpm) and\nsignal processing methods (MAE$\\pm$CI: 2.36$\\pm$1.11 bpm). This work is the\nfirst to automatically detect equine respiratory sounds and automatically\ncompute dynamic respiratory rates in exercising horses. In the future, our\nmodels will be validated on lower exercising intensity sounds and different\nmicrophone placements will be evaluated in order to find the best combination\nfor regular monitoring.\n","authors":["Jeanne I. M. Parmentier","Rhana M. Aarts","Elin Hernlund","Marie Rhodin","Berend Jan van der Zwaag"],"pdf_url":"https://arxiv.org/pdf/2508.02349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00687v3","updated":"2025-08-04T12:28:30Z","published":"2025-03-02T01:56:35Z","title":"Transformer Meets Twicing: Harnessing Unattended Residual Information","summary":"  Transformer-based deep learning models have achieved state-of-the-art\nperformance across numerous language and vision tasks. While the self-attention\nmechanism, a core component of transformers, has proven capable of handling\ncomplex data patterns, it has been observed that the representational capacity\nof the attention matrix degrades significantly across transformer layers,\nthereby hurting its overall performance. In this work, we leverage the\nconnection between self-attention computations and low-pass non-local means\n(NLM) smoothing filters and propose the Twicing Attention, a novel attention\nmechanism that uses kernel twicing procedure in nonparametric regression to\nalleviate the low-pass behavior of associated NLM smoothing with compelling\ntheoretical guarantees and enhanced adversarial robustness. This approach\nenables the extraction and reuse of meaningful information retained in the\nresiduals following the imperfect smoothing operation at each layer. Our\nproposed method offers two key advantages over standard self-attention: 1) a\nprovably slower decay of representational capacity and 2) improved robustness\nand accuracy across various data modalities and tasks. We empirically\ndemonstrate the performance gains of our model over baseline transformers on\nmultiple tasks and benchmarks, including image classification and language\nmodeling, on both clean and corrupted data.\n","authors":["Laziz Abdullaev","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2503.00687v3.pdf","comment":"10 pages in the main text. Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2508.02343v1","updated":"2025-08-04T12:22:39Z","published":"2025-08-04T12:22:39Z","title":"MicroMix: Efficient Mixed-Precision Quantization with Microscaling\n  Formats for Large Language Models","summary":"  Quantization significantly accelerates inference in large language models\n(LLMs) by replacing original high-precision matrices with low-precision\ncounterparts. Recent advances in weight-activation quantization have primarily\nfocused on mapping both weights and activations to the INT4 format. Although\nthe new FP4 Tensor Cores in NVIDIA's Blackwell architecture offer up to 4x\nspeedup over FP16, existing INT4-based kernels fail to fully exploit this\ncapability due to mismatched data formats. To bridge this gap, we propose\nMicroMix, a co-designed mixed-precision quantization algorithm and matrix\nmultiplication kernel based on Microscaling (MX) data formats. Tailored for the\nBlackwell architecture, the MicroMix kernel supports arbitrary combinations of\nMXFP4, MXFP6, and MXFP8 channels, and produces BFloat16 outputs. To achieve a\nfavorable trade-off between accuracy and efficiency for each linear layer, we\nintroduce quantization thresholds that identify activation elements where\nlower-precision formats (MXFP4 or MXFP6) incur excessive quantization error.\nOur algorithm selectively allocates higher-precision channels to preserve\naccuracy while maintaining compute efficiency. MicroMix achieves competitive or\nsuperior performance across diverse downstream tasks, including zero-shot and\nfew-shot learning, language modeling, code generation, and mathematical\nreasoning. On both consumer-grade (RTX 5070Ti laptop) and server-grade (RTX\n5090) GPUs, our kernel delivers at least 20% faster execution than\nTensorRT-FP8. Furthermore, when applied to various Llama and Qwen models,\nMicroMix consistently improves prefill latency and memory efficiency across a\nrange of batch sizes compared to TensorRT baselines. Our code is available at\nhttps://github.com/lwy2020/MicroMix.\n","authors":["Wenyuan Liu","Haoqian Meng","Yilun Luo","Peng Zhang","Xindian Ma"],"pdf_url":"https://arxiv.org/pdf/2508.02343v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2508.02337v1","updated":"2025-08-04T12:20:17Z","published":"2025-08-04T12:20:17Z","title":"Posterior Sampling of Probabilistic Word Embeddings","summary":"  Quantifying uncertainty in word embeddings is crucial for reliable inference\nfrom textual data. However, existing Bayesian methods such as Hamiltonian Monte\nCarlo (HMC) and mean-field variational inference (MFVI) are either\ncomputationally infeasible for large data or rely on restrictive assumptions.\n  We propose a scalable Gibbs sampler using Polya-Gamma augmentation as well as\nLaplace approximation and compare them with MFVI and HMC for word embeddings.\nIn addition, we address non-identifiability in word embeddings. Our Gibbs\nsampler and HMC correctly estimate uncertainties, while MFVI does not, and\nLaplace approximation only does so on large sample sizes, as expected. Applying\nthe Gibbs sampler to the US Congress and the Movielens datasets, we demonstrate\nthe feasibility on larger real data. Finally, as a result of having draws from\nthe full posterior, we show that the posterior mean of word embeddings improves\nover maximum a posteriori (MAP) estimates in terms of hold-out likelihood,\nespecially for smaller sampling sizes, further strengthening the need for\nposterior sampling of word embeddings.\n","authors":["Väinö Yrjänäinen","Isac Boström","Måns Magnusson","Johan Jonasson"],"pdf_url":"https://arxiv.org/pdf/2508.02337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02332v1","updated":"2025-08-04T12:08:12Z","published":"2025-08-04T12:08:12Z","title":"BOOST: Bayesian Optimization with Optimal Kernel and Acquisition\n  Function Selection Technique","summary":"  The performance of Bayesian optimization (BO), a highly sample-efficient\nmethod for expensive black-box problems, is critically governed by the\nselection of its hyperparameters, including the kernel and acquisition\nfunctions. This presents a challenge: an inappropriate combination of these can\nlead to poor performance and wasted evaluations. While individual improvements\nto kernel functions (e.g., tree-based kernels, deep kernel learning) and\nacquisition functions (e.g., multi-step lookahead, tree-based planning) have\nbeen explored, the joint and autonomous selection of the best pair of these\nfundamental hyperparameters has been overlooked. This forces practitioners to\nrely on heuristics or costly manual training. We propose a simple yet effective\nframework, BOOST (Bayesian Optimization with Optimal Kernel and Acquisition\nFunction Selection Technique), that automates this selection. BOOST utilizes a\nlightweight, offline evaluation stage to predict the performance of various\nkernel-acquisition function pairs and identify the most suitable configuration\nbefore expensive evaluations. BOOST partitions data-in-hand into two subsets: a\nreference subset and a query subset, and it prepares all possible\nkernel-acquisition pairs from the user's chosen candidates. For each\nconfiguration, BOOST conducts internal BO runs using the reference subset,\nevaluating how effectively each pair guides the search toward the optimum in\nthe unknown query subset, thereby identifying the configuration with the best\nretrospective performance for future optimization. Experiments on both\nsynthetic benchmark functions and real-world hyperparameter optimization tasks\ndemonstrate that BOOST consistently outperforms standard BO approaches with\nfixed hyperparameters, highlighting its effectiveness and robustness in diverse\nproblem landscapes.\n","authors":["Joon-Hyun Park","Mujin Cheon","Dong-Yeun Koh"],"pdf_url":"https://arxiv.org/pdf/2508.02332v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2508.02330v1","updated":"2025-08-04T12:00:06Z","published":"2025-08-04T12:00:06Z","title":"A Compression Based Classification Framework Using Symbolic Dynamics of\n  Chaotic Maps","summary":"  We propose a novel classification framework grounded in symbolic dynamics and\ndata compression using chaotic maps. The core idea is to model each class by\ngenerating symbolic sequences from thresholded real-valued training data, which\nare then evolved through a one-dimensional chaotic map. For each class, we\ncompute the transition probabilities of symbolic patterns (e.g., `00', `01',\n`10', and `11' for the second return map) and aggregate these statistics to\nform a class-specific probabilistic model. During testing phase, the test data\nare thresholded and symbolized, and then encoded using the class-wise symbolic\nstatistics via back iteration, a dynamical reconstruction technique. The\npredicted label corresponds to the class yielding the shortest compressed\nrepresentation, signifying the most efficient symbolic encoding under its\nrespective chaotic model. This approach fuses concepts from dynamical systems,\nsymbolic representations, and compression-based learning. We evaluate the\nproposed method: \\emph{ChaosComp} on both synthetic and real-world datasets,\ndemonstrating competitive performance compared to traditional machine learning\nalgorithms (e.g., macro F1-scores for the proposed method on Breast Cancer\nWisconsin = 0.9531, Seeds = 0.9475, Iris = 0.8317 etc.). Rather than aiming for\nstate-of-the-art performance, the goal of this research is to reinterpret the\nclassification problem through the lens of dynamical systems and compression,\nwhich are foundational perspectives in learning theory and information\nprocessing.\n","authors":["Parth Naik","Harikrishnan N B"],"pdf_url":"https://arxiv.org/pdf/2508.02330v1.pdf","comment":"4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.15322v3","updated":"2025-08-04T11:48:41Z","published":"2024-10-20T07:32:16Z","title":"UoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion\n  Model","summary":"  Mobile traffic forecasting allows operators to anticipate network dynamics\nand performance in advance, offering substantial potential for enhancing\nservice quality and improving user experience. However, existing models are\noften task-oriented and are trained with tailored data, which limits their\neffectiveness in diverse mobile network tasks of Base Station (BS) deployment,\nresource allocation, energy optimization, etc. and hinders generalization\nacross different urban environments. Foundation models have made remarkable\nstrides across various domains of NLP and CV due to their multi-tasking\nadaption and zero/few-shot learning capabilities. In this paper, we propose an\ninnovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to\nhandle diverse forecasting tasks of short/long-term predictions and\ndistribution generation across multiple cities to support network planning and\noptimization. FoMo combines diffusion models and transformers, where various\nspatio-temporal masks are proposed to enable FoMo to learn intrinsic features\nof different tasks, and a contrastive learning strategy is developed to capture\nthe correlations between mobile traffic and urban contexts, thereby improving\nits transfer learning capability. Extensive experiments on 9 real-world\ndatasets demonstrate that FoMo outperforms current models concerning diverse\nforecasting tasks and zero/few-shot learning, showcasing a strong universality.\n","authors":["Haoye Chai","Shiyuan Zhang","Xiaoqian Qi","Baohua Qiu","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2410.15322v3.pdf","comment":"2025 ACM SIGKDD International Conference on Knowledge Discovery and\n  Data Mining, KDD 2025"},{"id":"http://arxiv.org/abs/2409.11884v4","updated":"2025-08-04T11:43:24Z","published":"2024-09-18T11:30:30Z","title":"Out-of-Distribution Detection: A Task-Oriented Survey of Recent Advances","summary":"  Out-of-distribution (OOD) detection aims to detect test samples outside the\ntraining category space, which is an essential component in building reliable\nmachine learning systems. Existing reviews on OOD detection primarily focus on\nmethod taxonomy, surveying the field by categorizing various approaches.\nHowever, many recent works concentrate on non-traditional OOD detection\nscenarios, such as test-time adaptation, multi-modal data sources and other\nnovel contexts. In this survey, we uniquely review recent advances in OOD\ndetection from the task-oriented perspective for the first time. According to\nthe user's access to the model, that is, whether the OOD detection method is\nallowed to modify or retrain the model, we classify the methods as\ntraining-driven or training-agnostic. Besides, considering the rapid\ndevelopment of pre-trained models, large pre-trained model-based OOD detection\nis also regarded as an important category and discussed separately.\nFurthermore, we provide a discussion of the evaluation scenarios, a variety of\napplications, and several future research directions. We believe this survey\nwith new taxonomy will benefit the proposal of new methods and the expansion of\nmore practical scenarios. A curated list of related papers is provided in the\nGithub repository:\nhttps://github.com/shuolucs/Awesome-Out-Of-Distribution-Detection.\n","authors":["Shuo Lu","Yingsheng Wang","Lijun Sheng","Lingxiao He","Aihua Zheng","Jian Liang"],"pdf_url":"https://arxiv.org/pdf/2409.11884v4.pdf","comment":"Accepted to ACM Computing Surveys (CSUR) 2025"},{"id":"http://arxiv.org/abs/2508.02322v1","updated":"2025-08-04T11:42:48Z","published":"2025-08-04T11:42:48Z","title":"CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert\n  Redundancy Analysis","summary":"  Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.\n","authors":["Yuzhuang Xu","Xu Han","Yuanchi Zhang","Yixuan Wang","Yijun Liu","Shiyu Ji","Qingfu Zhu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2508.02322v1.pdf","comment":"16 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2505.14754v2","updated":"2025-08-04T11:42:09Z","published":"2025-05-20T14:55:26Z","title":"Model-Independent Machine Learning Approach for Nanometric Axial\n  Localization and Tracking","summary":"  Accurately tracking particles and determining their coordinate along the\noptical axis is a major challenge in optical microscopy, especially when\nextremely high precision is needed. In this study, we introduce a deep learning\napproach using convolutional neural networks (CNNs) that can determine axial\ncoordinates from dual-focal-plane images without relying on predefined models.\nOur method achieves an axial localization precision of 40 nanometers-six times\nbetter than traditional single-focal-plane techniques. The model's simple\ndesign and strong performance make it suitable for a wide range of uses,\nincluding dark matter detection, proton therapy for cancer, and radiation\nprotection in space. It also shows promise in fields like biological imaging,\nmaterials science, and environmental monitoring. This work highlights how\nmachine learning can turn complex image data into reliable, precise\ninformation, offering a flexible and powerful tool for many scientific\napplications.\n","authors":["Andrey Alexandrov","Giovanni Acampora","Giovanni De Lellis","Antonia Di Crescenzo","Chiara Errico","Daria Morozova","Valeri Tioukov","Autilia Vittiello"],"pdf_url":"https://arxiv.org/pdf/2505.14754v2.pdf","comment":"13 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2507.02724v3","updated":"2025-08-04T11:32:04Z","published":"2025-07-03T15:41:04Z","title":"Hierarchical Multi-Label Contrastive Learning for Protein-Protein\n  Interaction Prediction Across Organisms","summary":"  Recent advances in AI for science have highlighted the power of contrastive\nlearning in bridging heterogeneous biological data modalities. Building on this\nparadigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction\nacross Organisms), a hierarchical contrastive framework for protein-protein\ninteraction(PPI) prediction, where protein sequences and their hierarchical\nattributes are aligned through multi-tiered biological representation matching.\nThe proposed approach incorporates hierarchical contrastive loss functions that\nemulate the structured relationship among functional classes of proteins. The\nframework adaptively incorporates domain and family knowledge through a\ndata-driven penalty mechanism, enforcing consistency between the learned\nembedding space and the intrinsic hierarchy of protein functions. Experiments\non benchmark datasets demonstrate that HIPPO achieves state-of-the-art\nperformance, outperforming existing methods and showing robustness in low-data\nregimes. Notably, the model demonstrates strong zero-shot transferability to\nother species without retraining, enabling reliable PPI prediction and\nfunctional inference even in less characterized or rare organisms where\nexperimental data are limited. Further analysis reveals that hierarchical\nfeature fusion is critical for capturing conserved interaction determinants,\nsuch as binding motifs and functional annotations. This work advances\ncross-species PPI prediction and provides a unified framework for interaction\nprediction in scenarios with sparse or imbalanced multi-species data.\n","authors":["Shiyi Liu","Buwen Liang","Yuetong Fang","Zixuan Jiang","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2507.02724v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02313v1","updated":"2025-08-04T11:29:47Z","published":"2025-08-04T11:29:47Z","title":"NMS: Efficient Edge DNN Training via Near-Memory Sampling on Manifolds","summary":"  Training deep neural networks (DNNs) on edge devices has attracted increasing\nattention due to its potential to address challenges related to domain\nadaptation and privacy preservation. However, DNNs typically rely on large\ndatasets for training, which results in substantial energy consumption, making\nthe training in edge devices impractical. Some dataset compression methods have\nbeen proposed to solve this challenge. For instance, the coreset selection and\ndataset distillation reduce the training cost by selecting and generating\nrepresentative samples respectively. Nevertheless, these methods have two\nsignificant defects: (1) The necessary of leveraging a DNN model to evaluate\nthe quality of representative samples, which inevitably introduces inductive\nbias of DNN, resulting in a severe generalization issue; (2) All training\nimages require multiple accesses to the DDR via long-distance PCB connections,\nleading to substantial energy overhead. To address these issues, inspired by\nthe nonlinear manifold stationary of the human brain, we firstly propose a\nDNN-free sample-selecting algorithm, called DE-SNE, to improve the\ngeneralization issue. Secondly, we innovatively utilize the near-memory\ncomputing technique to implement DE-SNE, thus only a small fraction of images\nneed to access the DDR via long-distance PCB. It significantly reduces DDR\nenergy consumption. As a result, we build a novel expedited DNN training system\nwith a more efficient in-place Near-Memory Sampling characteristic for edge\ndevices, dubbed NMS. As far as we know, our NMS is the first DNN-free\nnear-memory sampling technique that can effectively alleviate generalization\nissues and significantly reduce DDR energy caused by dataset access. The\nexperimental results show that our NMS outperforms the current state-of-the-art\n(SOTA) approaches, namely DQ, DQAS, and NeSSA, in model accuracy.\n","authors":["Boran Zhao","Haiduo Huang","Qiwei Dang","Wenzhe Zhao","Tian Xia","Pengju Ren"],"pdf_url":"https://arxiv.org/pdf/2508.02313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02307v1","updated":"2025-08-04T11:20:31Z","published":"2025-08-04T11:20:31Z","title":"Whole-body Representation Learning For Competing Preclinical Disease\n  Risk Assessment","summary":"  Reliable preclinical disease risk assessment is essential to move public\nhealthcare from reactive treatment to proactive identification and prevention.\nHowever, image-based risk prediction algorithms often consider one condition at\na time and depend on hand-crafted features obtained through segmentation tools.\nWe propose a whole-body self-supervised representation learning method for the\npreclinical disease risk assessment under a competing risk modeling. This\napproach outperforms whole-body radiomics in multiple diseases, including\ncardiovascular disease (CVD), type 2 diabetes (T2D), chronic obstructive\npulmonary disease (COPD), and chronic kidney disease (CKD). Simulating a\npreclinical screening scenario and subsequently combining with cardiac MRI, it\nsharpens further the prediction for CVD subgroups: ischemic heart disease\n(IHD), hypertensive diseases (HD), and stroke. The results indicate the\ntranslational potential of whole-body representations as a standalone screening\nmodality and as part of a multi-modal framework within clinical workflows for\nearly personalized risk stratification. The code is available at\nhttps://github.com/yayapa/WBRLforCR/\n","authors":["Dmitrii Seletkov","Sophie Starck","Ayhan Can Erdur","Yundi Zhang","Daniel Rueckert","Rickmer Braren"],"pdf_url":"https://arxiv.org/pdf/2508.02307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02298v1","updated":"2025-08-04T11:06:08Z","published":"2025-08-04T11:06:08Z","title":"CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative\n  Credit Assignment","summary":"  Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback, helping to mitigate reward hacking. However, current RLVR methods\ntypically treat whole responses as single actions, assigning the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies and inefficient learning.\nMethods like PPO provide credit assignment through value estimation, but often\nyield inaccurate and unverifiable signals due to limited sampling. On the other\nhand, methods using Process Reward Models can provide step-by-step judgments\nfor each reasoning step, but they require high-quality process supervision\nlabels and are time-consuming when applied in online reinforcement learning\n(RL). To overcome these limitations, we introduce a simple but efficient method\nCredit Assignment Policy Optimization (CAPO). Given a reasoning response\nrollout from the policy model, CAPO directly leverages an off-the-shelf,\ngeneral-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to\ngenerate all step-wise critique by one pass, thereby providing verifiable\ntoken-level rewards to refine the tokens that were originally assigned\nidentical rule-based rewards. This enables more fine-grained credit assignment\nin an effective way. Furthermore, to enhance the accuracy and robustness of\nCAPO, we employ voting mechanisms that scale with the number of generated\ncritiques. Extensive experiments using different backbones like Llama and Qwen\nmodels and in different sizes show that CAPO consistently outperforms\nsupervised learning-based and RL-based fine-tuning methods across six\nchallenging mathematical benchmarks and three out-of-domain benchmarks.\n","authors":["Guofu Xie","Yunsheng Shi","Hongtao Tian","Ting Yao","Xiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.02298v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2508.02294v1","updated":"2025-08-04T11:04:15Z","published":"2025-08-04T11:04:15Z","title":"Pre-Tactical Flight-Delay and Turnaround Forecasting with Synthetic\n  Aviation Data","summary":"  Access to comprehensive flight operations data remains severely restricted in\naviation due to commercial sensitivity and competitive considerations,\nhindering the development of predictive models for operational planning. This\npaper investigates whether synthetic data can effectively replace real\noperational data for training machine learning models in pre-tactical aviation\nscenarios-predictions made hours to days before operations using only scheduled\nflight information. We evaluate four state-of-the-art synthetic data generators\non three prediction tasks: aircraft turnaround time, departure delays, and\narrival delays. Using a Train on Synthetic, Test on Real (TSTR) methodology on\nover 1.7 million European flight records, we first validate synthetic data\nquality through fidelity assessments, then assess both predictive performance\nand the preservation of operational relationships. Our results show that\nadvanced neural network architectures, specifically transformer-based\ngenerators, can retain 94-97% of real-data predictive performance while\nmaintaining feature importance patterns informative for operational\ndecision-making. Our analysis reveals that even with real data, prediction\naccuracy is inherently limited when only scheduled information is\navailable-establishing realistic baselines for pre-tactical forecasting. These\nfindings suggest that high-quality synthetic data can enable broader access to\naviation analytics capabilities while preserving commercial confidentiality,\nthough stakeholders must maintain realistic expectations about pre-tactical\nprediction accuracy given the stochastic nature of flight operations.\n","authors":["Abdulmajid Murad","Massimiliano Ruocco"],"pdf_url":"https://arxiv.org/pdf/2508.02294v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2508.02293v1","updated":"2025-08-04T11:03:12Z","published":"2025-08-04T11:03:12Z","title":"Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning","summary":"  So-called unsupervised anomaly detection is better described as\nsemi-supervised, as it assumes all training data are nominal. This assumption\nsimplifies training but requires manual data curation, introducing bias and\nlimiting adaptability. We propose Confident Meta-learning (CoMet), a novel\ntraining strategy that enables deep anomaly detection models to learn from\nuncurated datasets where nominal and anomalous samples coexist, eliminating the\nneed for explicit filtering. Our approach integrates Soft Confident Learning,\nwhich assigns lower weights to low-confidence samples, and Meta-Learning, which\nstabilizes training by regularizing updates based on training validation loss\ncovariance. This prevents overfitting and enhances robustness to noisy data.\nCoMet is model-agnostic and can be applied to any anomaly detection method\ntrainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2\nwith two state-of-the-art models demonstrate the effectiveness of our approach,\nconsistently improving over the baseline methods, remaining insensitive to\nanomalies in the training set, and setting a new state-of-the-art across all\ndatasets.\n","authors":["Muhammad Aqeel","Shakiba Sharifi","Marco Cristani","Francesco Setti"],"pdf_url":"https://arxiv.org/pdf/2508.02293v1.pdf","comment":"Accepted to ieee/cvf international conference on computer vision\n  (ICCV2025)"},{"id":"http://arxiv.org/abs/2508.02292v1","updated":"2025-08-04T11:02:34Z","published":"2025-08-04T11:02:34Z","title":"FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI\n  Research and Deployment","summary":"  Financial AI holds great promise for transforming modern finance, with the\npotential to support a wide range of tasks such as market forecasting,\nportfolio management, quantitative trading, and automated analysis. However,\nexisting platforms remain limited in task coverage, lack robust multimodal data\nintegration, and offer insufficient support for the training and deployment of\nlarge language models (LLMs). In response to these limitations, we present\nFinWorld, an all-in-one open-source platform that provides end-to-end support\nfor the entire financial AI workflow, from data acquisition to experimentation\nand deployment. FinWorld distinguishes itself through native integration of\nheterogeneous financial data, unified support for diverse AI paradigms, and\nadvanced agent automation, enabling seamless development and deployment.\nLeveraging data from 2 representative markets, 4 stock pools, and over 800\nmillion financial data points, we conduct comprehensive experiments on 4 key\nfinancial AI tasks. These experiments systematically evaluate deep learning and\nreinforcement learning algorithms, with particular emphasis on RL-based\nfinetuning for LLMs and LLM Agents. The empirical results demonstrate that\nFinWorld significantly enhances reproducibility, supports transparent\nbenchmarking, and streamlines deployment, thereby providing a strong foundation\nfor future research and real-world applications. Code is available at\nGithub~\\footnote{https://github.com/DVampire/FinWorld}.\n","authors":["Wentao Zhang","Yilei Zhao","Chuqiao Zong","Xinrun Wang","Bo An"],"pdf_url":"https://arxiv.org/pdf/2508.02292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02291v1","updated":"2025-08-04T10:59:07Z","published":"2025-08-04T10:59:07Z","title":"Flexible Automatic Identification and Removal (FAIR)-Pruner: An\n  Efficient Neural Network Pruning Method","summary":"  Neural network pruning is a critical compression technique that facilitates\nthe deployment of large-scale neural networks on resource-constrained edge\ndevices, typically by identifying and eliminating redundant or insignificant\nparameters to reduce computational and memory overhead. This paper proposes the\nFlexible Automatic Identification and Removal (FAIR)-Pruner, a novel method for\nneural network structured pruning. Specifically, FAIR-Pruner first evaluates\nthe importance of each unit (e.g., neuron or channel) through the Utilization\nScore quantified by the Wasserstein distance. To reflect the performance\ndegradation after unit removal, it then introduces the Reconstruction Error,\nwhich is computed via the Taylor expansion of the loss function. Finally,\nFAIR-Pruner identifies superfluous units with negligible impact on model\nperformance by controlling the proposed Tolerance of Difference, which measures\ndifferences between unimportant units and those that cause performance\ndegradation. A major advantage of FAIR-Pruner lies in its capacity to\nautomatically determine the layer-wise pruning rates, which yields a more\nefficient subnetwork structure compared to applying a uniform pruning rate.\nAnother advantage of the FAIR-Pruner is its great one-shot performance without\npost-pruning fine-tuning. Furthermore, with utilization scores and\nreconstruction errors, users can flexibly obtain pruned models under different\npruning ratios. Comprehensive experimental validation on diverse benchmark\ndatasets (e.g., ImageNet) and various neural network architectures (e.g., VGG)\ndemonstrates that FAIR-Pruner achieves significant model compression while\nmaintaining high accuracy.\n","authors":["Chenqing Lin","Mostafa Hussien","Chengyao Yu","Mohamed Cheriet","Osama Abdelrahman","Ruixing Ming"],"pdf_url":"https://arxiv.org/pdf/2508.02291v1.pdf","comment":"Submitted to AAAI 2026"},{"id":"http://arxiv.org/abs/2508.02283v1","updated":"2025-08-04T10:53:10Z","published":"2025-08-04T10:53:10Z","title":"An Enhanced Focal Loss Function to Mitigate Class Imbalance in Auto\n  Insurance Fraud Detection with Explainable AI","summary":"  In insurance fraud prediction, handling class imbalance remains a critical\nchallenge. This paper presents a novel multistage focal loss function designed\nto enhance the performance of machine learning models in such imbalanced\nsettings by helping to escape local minima and converge to a good solution.\nBuilding upon the foundation of the standard focal loss, our proposed approach\nintroduces a dynamic, multi-stage convex and nonconvex mechanism that\nprogressively adjusts the focus on hard-to-classify samples across training\nepochs. This strategic refinement facilitates more stable learning and improved\ndiscrimination between fraudulent and legitimate cases. Through extensive\nexperimentation on a real-world insurance dataset, our method achieved better\nperformance than the traditional focal loss, as measured by accuracy,\nprecision, F1-score, recall and Area Under the Curve (AUC) metrics on the auto\ninsurance dataset. These results demonstrate the efficacy of the multistage\nfocal loss in boosting model robustness and predictive accuracy in highly\nskewed classification tasks, offering significant implications for fraud\ndetection systems in the insurance industry. An explainable model is included\nto interpret the results.\n","authors":["Francis Boabang","Samuel Asante Gyamerah"],"pdf_url":"https://arxiv.org/pdf/2508.02283v1.pdf","comment":"28 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2508.02281v1","updated":"2025-08-04T10:52:42Z","published":"2025-08-04T10:52:42Z","title":"Do Edges Matter? Investigating Edge-Enhanced Pre-Training for Medical\n  Image Segmentation","summary":"  Medical image segmentation is crucial for disease diagnosis and treatment\nplanning, yet developing robust segmentation models often requires substantial\ncomputational resources and large datasets. Existing research shows that\npre-trained and finetuned foundation models can boost segmentation performance.\nHowever, questions remain about how particular image preprocessing steps may\ninfluence segmentation performance across different medical imaging modalities.\nIn particular, edges-abrupt transitions in pixel intensity-are widely\nacknowledged as vital cues for object boundaries but have not been\nsystematically examined in the pre-training of foundation models. We address\nthis gap by investigating to which extend pre-training with data processed\nusing computationally efficient edge kernels, such as kirsch, can improve\ncross-modality segmentation capabilities of a foundation model. Two versions of\na foundation model are first trained on either raw or edge-enhanced data across\nmultiple medical imaging modalities, then finetuned on selected raw subsets\ntailored to specific medical modalities. After systematic investigation using\nthe medical domains Dermoscopy, Fundus, Mammography, Microscopy, OCT, US, and\nXRay, we discover both increased and reduced segmentation performance across\nmodalities using edge-focused pre-training, indicating the need for a selective\napplication of this approach. To guide such selective applications, we propose\na meta-learning strategy. It uses standard deviation and image entropy of the\nraw image to choose between a model pre-trained on edge-enhanced or on raw data\nfor optimal performance. Our experiments show that integrating this\nmeta-learning layer yields an overall segmentation performance improvement\nacross diverse medical imaging tasks by 16.42% compared to models pre-trained\non edge-enhanced data only and 19.30% compared to models pre-trained on raw\ndata only.\n","authors":["Paul Zaha","Lars Böcking","Simeon Allmendinger","Leopold Müller","Niklas Kühl"],"pdf_url":"https://arxiv.org/pdf/2508.02281v1.pdf","comment":"11 pages, 5 figures, Third International Workshop on Data Engineering\n  in Medical Imaging (DEMI 2025)"},{"id":"http://arxiv.org/abs/2412.13795v2","updated":"2025-08-04T10:49:54Z","published":"2024-12-18T12:39:53Z","title":"Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and\n  Post-LN","summary":"  Large Language Models (LLMs) have achieved remarkable success, yet recent\nfindings reveal that their deeper layers often contribute minimally and can be\npruned without affecting overall performance. While some view this as an\nopportunity for model compression, we identify it as a training shortfall\nrooted in the widespread use of Pre-Layer Normalization (Pre-LN). We\ndemonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads\nto diminished gradient norms in its deeper layers, reducing their\neffectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger\ngradient norms in deeper layers but suffers from vanishing gradients in earlier\nlayers. To address this, we introduce Mix-LN, a novel normalization technique\nthat combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN\napplies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring\nmore uniform gradients across layers. This allows all parts of the\nnetwork--both shallow and deep layers--to contribute effectively to training.\nExtensive experiments with various model sizes from 70M to 7B demonstrate that\nMix-LN consistently outperforms both Pre-LN and Post-LN, promoting more\nbalanced, healthier gradient norms throughout the network, and enhancing the\noverall quality of LLM pre-training. Furthermore, we demonstrate that models\npre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN\nduring supervised fine-tuning (SFT) and reinforcement learning from human\nfeedback (RLHF), highlighting the critical importance of high-quality deep\nlayers. By effectively addressing the inefficiencies of deep layers in current\nLLMs, Mix-LN unlocks their potential, enhancing model capacity without\nincreasing model size. Our code is available at\nhttps://github.com/pixeli99/MixLN.\n","authors":["Pengxiang Li","Lu Yin","Shiwei Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13795v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09509v3","updated":"2025-08-04T10:44:54Z","published":"2025-02-13T17:21:51Z","title":"EQ-VAE: Equivariance Regularized Latent Space for Improved Generative\n  Image Modeling","summary":"  Latent generative models have emerged as a leading approach for high-quality\nimage synthesis. These models rely on an autoencoder to compress images into a\nlatent space, followed by a generative model to learn the latent distribution.\nWe identify that existing autoencoders lack equivariance to semantic-preserving\ntransformations like scaling and rotation, resulting in complex latent spaces\nthat hinder generative performance. To address this, we propose EQ-VAE, a\nsimple regularization approach that enforces equivariance in the latent space,\nreducing its complexity without degrading reconstruction quality. By finetuning\npre-trained autoencoders with EQ-VAE, we enhance the performance of several\nstate-of-the-art generative models, including DiT, SiT, REPA and MaskGIT,\nachieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning.\nEQ-VAE is compatible with both continuous and discrete autoencoders, thus\noffering a versatile enhancement for a wide range of latent generative models.\nProject page and code: https://eq-vae.github.io/.\n","authors":["Theodoros Kouzelis","Ioannis Kakogeorgiou","Spyros Gidaris","Nikos Komodakis"],"pdf_url":"https://arxiv.org/pdf/2502.09509v3.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2508.02276v1","updated":"2025-08-04T10:43:31Z","published":"2025-08-04T10:43:31Z","title":"CellForge: Agentic Design of Virtual Cell Models","summary":"  Virtual cell modeling represents an emerging frontier at the intersection of\nartificial intelligence and biology, aiming to predict quantities such as\nresponses to diverse perturbations quantitatively. However, autonomously\nbuilding computational models for virtual cells is challenging due to the\ncomplexity of biological systems, the heterogeneity of data modalities, and the\nneed for domain-specific expertise across multiple disciplines. Here, we\nintroduce CellForge, an agentic system that leverages a multi-agent framework\nthat transforms presented biological datasets and research objectives directly\ninto optimized computational models for virtual cells. More specifically, given\nonly raw single-cell multi-omics data and task descriptions as input, CellForge\noutputs both an optimized model architecture and executable code for training\nvirtual cell models and inference. The framework integrates three core modules:\nTask Analysis for presented dataset characterization and relevant literature\nretrieval, Method Design, where specialized agents collaboratively develop\noptimized modeling strategies, and Experiment Execution for automated\ngeneration of code. The agents in the Design module are separated into experts\nwith differing perspectives and a central moderator, and have to\ncollaboratively exchange solutions until they achieve a reasonable consensus.\nWe demonstrate CellForge's capabilities in single-cell perturbation prediction,\nusing six diverse datasets that encompass gene knockouts, drug treatments, and\ncytokine stimulations across multiple modalities. CellForge consistently\noutperforms task-specific state-of-the-art methods. Overall, CellForge\ndemonstrates how iterative interaction between LLM agents with differing\nperspectives provides better solutions than directly addressing a modeling\nchallenge. Our code is publicly available at\nhttps://github.com/gersteinlab/CellForge.\n","authors":["Xiangru Tang","Zhuoyun Yu","Jiapeng Chen","Yan Cui","Daniel Shao","Weixu Wang","Fang Wu","Yuchen Zhuang","Wenqi Shi","Zhi Huang","Arman Cohan","Xihong Lin","Fabian Theis","Smita Krishnaswamy","Mark Gerstein"],"pdf_url":"https://arxiv.org/pdf/2508.02276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02275v1","updated":"2025-08-04T10:42:52Z","published":"2025-08-04T10:42:52Z","title":"Comparing Generative Models with the New Physics Learning Machine","summary":"  The rise of generative models for scientific research calls for the\ndevelopment of new methods to evaluate their fidelity. A natural framework for\naddressing this problem is two-sample hypothesis testing, namely the task of\ndetermining whether two data sets are drawn from the same distribution. In\nlarge-scale and high-dimensional regimes, machine learning offers a set of\ntools to push beyond the limitations of standard statistical techniques. In\nthis work, we put this claim to the test by comparing a recent proposal from\nthe high-energy physics literature, the New Physics Learning Machine, to\nperform a classification-based two-sample test against a number of alternative\napproaches, following the framework presented in Grossi et al. (2025). We\nhighlight the efficiency tradeoffs of the method and the computational costs\nthat come from adopting learning-based approaches. Finally, we discuss the\nadvantages of the different methods for different use cases.\n","authors":["Samuele Grossi","Marco Letizia","Riccardo Torre"],"pdf_url":"https://arxiv.org/pdf/2508.02275v1.pdf","comment":"v1: 14 pages, 7 figures, 8 tables, additional material on GitHub\n  referenced in the paper"},{"id":"http://arxiv.org/abs/2508.02274v1","updated":"2025-08-04T10:40:56Z","published":"2025-08-04T10:40:56Z","title":"mCardiacDx: Radar-Driven Contactless Monitoring and Diagnosis of\n  Arrhythmia","summary":"  Arrhythmia is a common cardiac condition that can precipitate severe\ncomplications without timely intervention. While continuous monitoring is\nessential for timely diagnosis, conventional approaches such as\nelectrocardiogram and wearable devices are constrained by their reliance on\nspecialized medical expertise and patient discomfort from their contact nature.\nExisting contactless monitoring, primarily designed for healthy subjects, face\nsignificant challenges when analyzing reflected signals from arrhythmia\npatients due to disrupted spatial stability and temporal consistency.\n  In this paper, we introduce mCardiacDx, a radar-driven contactless system\nthat accurately analyzes reflected signals and reconstructs heart pulse\nwaveforms for arrhythmia monitoring and diagnosis. The key contributions of our\nwork include a novel precise target localization (PTL) technique that locates\nreflected signals despite spatial disruptions, and an encoder-decoder model\nthat transforms these signals into HPWs, addressing temporal inconsistencies.\nOur evaluation on a large dataset of healthy subjects and arrhythmia patients\nshows that both mCardiacDx and PTL outperform state-of-the-art approach in\narrhythmia monitoring and diagnosis, also demonstrating improved performance in\nhealthy subjects.\n","authors":["Arjun Kumar","Noppanat Wadlom","Jaeheon Kwak","Si-Hyuck Kang","Insik Shin"],"pdf_url":"https://arxiv.org/pdf/2508.02274v1.pdf","comment":"15 pages, 27 images"},{"id":"http://arxiv.org/abs/2507.14999v2","updated":"2025-08-04T10:31:26Z","published":"2025-07-20T15:10:43Z","title":"Clustered Federated Learning for Generalizable FDIA Detection in Smart\n  Grids with Heterogeneous Data","summary":"  False Data Injection Attacks (FDIAs) pose severe security risks to smart\ngrids by manipulating measurement data collected from spatially distributed\ndevices such as SCADA systems and PMUs. These measurements typically exhibit\nNon-Independent and Identically Distributed (Non-IID) characteristics across\ndifferent regions, which significantly challenges the generalization ability of\ndetection models. Traditional centralized training approaches not only face\nprivacy risks and data sharing constraints but also incur high transmission\ncosts, limiting their scalability and deployment feasibility. To address these\nissues, this paper proposes a privacy-preserving federated learning framework,\ntermed Federated Cluster Average (FedClusAvg), designed to improve FDIA\ndetection in Non-IID and resource-constrained environments. FedClusAvg\nincorporates cluster-based stratified sampling and hierarchical communication\n(client-subserver-server) to enhance model generalization and reduce\ncommunication overhead. By enabling localized training and weighted parameter\naggregation, the algorithm achieves accurate model convergence without\ncentralizing sensitive data. Experimental results on benchmark smart grid\ndatasets demonstrate that FedClusAvg not only improves detection accuracy under\nheterogeneous data distributions but also significantly reduces communication\nrounds and bandwidth consumption. This work provides an effective solution for\nsecure and efficient FDIA detection in large-scale distributed power systems.\n","authors":["Yunfeng Li","Junhong Liu","Zhaohui Yang","Guofu Liao","Chuyun Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.14999v2.pdf","comment":"10 pages,6 figures"},{"id":"http://arxiv.org/abs/2410.04133v4","updated":"2025-08-04T10:31:10Z","published":"2024-10-05T12:12:02Z","title":"An Electrocardiogram Foundation Model Built on over 10 Million\n  Recordings with External Evaluation across Multiple Domains","summary":"  Artificial intelligence (AI) has demonstrated significant potential in ECG\nanalysis and cardiovascular disease assessment. Recently, foundation models\nhave played a remarkable role in advancing medical AI. The development of an\nECG foundation model holds the promise of elevating AI-ECG research to new\nheights. However, building such a model faces several challenges, including\ninsufficient database sample sizes and inadequate generalization across\nmultiple domains. Additionally, there is a notable performance gap between\nsingle-lead and multi-lead ECG analyses. We introduced an ECG Foundation Model\n(ECGFounder), a general-purpose model that leverages real-world ECG annotations\nfrom cardiology experts to broaden the diagnostic capabilities of ECG analysis.\nECGFounder was trained on over 10 million ECGs with 150 label categories from\nthe Harvard-Emory ECG Database, enabling comprehensive cardiovascular disease\ndiagnosis through ECG analysis. The model is designed to be both an effective\nout-of-the-box solution, and a to be fine-tunable for downstream tasks,\nmaximizing usability. Importantly, we extended its application to lower rank\nECGs, and arbitrary single-lead ECGs in particular. ECGFounder is applicable to\nsupporting various downstream tasks in mobile monitoring scenarios.\nExperimental results demonstrate that ECGFounder achieves expert-level\nperformance on internal validation sets, with AUROC exceeding 0.95 for eighty\ndiagnoses. It also shows strong classification performance and generalization\nacross various diagnoses on external validation sets. When fine-tuned,\nECGFounder outperforms baseline models in demographic analysis, clinical event\ndetection, and cross-modality cardiac rhythm diagnosis. The trained model and\ndata will be publicly released upon publication through the bdsp.io. Our code\nis available at https://github.com/PKUDigitalHealth/ECGFounder\n","authors":["Jun Li","Aaron Aguirre","Junior Moura","Che Liu","Lanhai Zhong","Chenxi Sun","Gari Clifford","Brandon Westover","Shenda Hong"],"pdf_url":"https://arxiv.org/pdf/2410.04133v4.pdf","comment":"Code: https://github.com/PKUDigitalHealth/ECGFounder"},{"id":"http://arxiv.org/abs/2508.02270v1","updated":"2025-08-04T10:21:52Z","published":"2025-08-04T10:21:52Z","title":"Skeleton-Guided Learning for Shortest Path Search","summary":"  Shortest path search is a core operation in graph-based applications, yet\nexisting methods face important limitations. Classical algorithms such as\nDijkstra's and A* become inefficient as graphs grow more complex, while\nindex-based techniques often require substantial preprocessing and storage.\nRecent learning-based approaches typically focus on spatial graphs and rely on\ncontext-specific features like geographic coordinates, limiting their general\napplicability. We propose a versatile learning-based framework for shortest\npath search on generic graphs, without requiring domain-specific features. At\nthe core of our approach is the construction of a skeleton graph that captures\nmulti-level distance and hop information in a compact form. A Skeleton Graph\nNeural Network (SGNN) operates on this structure to learn node embeddings and\npredict distances and hop lengths between node pairs. These predictions support\nLSearch, a guided search algorithm that uses model-driven pruning to reduce the\nsearch space while preserving accuracy. To handle larger graphs, we introduce a\nhierarchical training strategy that partitions the graph into subgraphs with\nindividually trained SGNNs. This structure enables HLSearch, an extension of\nour method for efficient path search across graph partitions. Experiments on\nfive diverse real-world graphs demonstrate that our framework achieves strong\nperformance across graph types, offering a flexible and effective solution for\nlearning-based shortest path search.\n","authors":["Tiantian Liu","Xiao Li","Huan Li","Hua Lu","Christian S. Jensen","Jianliang Xu"],"pdf_url":"https://arxiv.org/pdf/2508.02270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.17921v3","updated":"2025-08-04T10:13:56Z","published":"2025-04-24T20:24:31Z","title":"Avoiding Leakage Poisoning: Concept Interventions Under Distribution\n  Shifts","summary":"  In this paper, we investigate how concept-based models (CMs) respond to\nout-of-distribution (OOD) inputs. CMs are interpretable neural architectures\nthat first predict a set of high-level concepts (e.g., stripes, black) and then\npredict a task label from those concepts. In particular, we study the impact of\nconcept interventions (i.e., operations where a human expert corrects a CM's\nmispredicted concepts at test time) on CMs' task predictions when inputs are\nOOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we\nterm leakage poisoning, that prevents them from properly improving their\naccuracy when intervened on for OOD inputs. To address this, we introduce\nMixCEM, a new CM that learns to dynamically exploit leaked information missing\nfrom its concepts only when this information is in-distribution. Our results\nacross tasks with and without complete sets of concept annotations demonstrate\nthat MixCEMs outperform strong baselines by significantly improving their\naccuracy for both in-distribution and OOD samples in the presence and absence\nof concept interventions.\n","authors":["Mateo Espinosa Zarlenga","Gabriele Dominici","Pietro Barbiero","Zohreh Shams","Mateja Jamnik"],"pdf_url":"https://arxiv.org/pdf/2504.17921v3.pdf","comment":"Presented at the Forty-Second International Conference on Machine\n  Learning (ICML 2025). Post-conference manuscript"},{"id":"http://arxiv.org/abs/2507.14668v2","updated":"2025-08-04T10:12:00Z","published":"2025-07-19T15:38:56Z","title":"Rec-AD: An Efficient Computation Framework for FDIA Detection Based on\n  Tensor Train Decomposition and Deep Learning Recommendation Model","summary":"  Deep learning models have been widely adopted for False Data Injection Attack\n(FDIA) detection in smart grids due to their ability to capture unstructured\nand sparse features. However, the increasing system scale and data\ndimensionality introduce significant computational and memory burdens,\nparticularly in large-scale industrial datasets, limiting detection efficiency.\nTo address these issues, this paper proposes Rec-AD, a computationally\nefficient framework that integrates Tensor Train decomposition with the Deep\nLearning Recommendation Model (DLRM). Rec-AD enhances training and inference\nefficiency through embedding compression, optimized data access via index\nreordering, and a pipeline training mechanism that reduces memory communication\noverhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing\nFDIA detection systems without code modifications. Experimental results show\nthat Rec-AD significantly improves computational throughput and real-time\ndetection performance, narrowing the attack window and increasing attacker\ncost. These advancements strengthen edge computing capabilities and\nscalability, providing robust technical support for smart grid security.\n","authors":["Yunfeng Li","Junhong Liu","Zhaohui Yang","Guofu Liao","Chuyun Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.14668v2.pdf","comment":"15 pages, 14 figures"},{"id":"http://arxiv.org/abs/2412.16385v2","updated":"2025-08-04T10:07:01Z","published":"2024-12-20T22:41:16Z","title":"Collision-based Dynamics for Multi-Marginal Optimal Transport","summary":"  Inspired by the Boltzmann kinetics, we propose a collision-based dynamics\nwith a Monte Carlo solution algorithm that approximates the solution of the\nmulti-marginal optimal transport problem via randomized pairwise swapping of\nsample indices. The computational complexity and memory usage of the proposed\nmethod scale linearly with the number of samples, making it highly attractive\nfor high-dimensional settings. In several examples, we demonstrate the\nefficiency of the proposed method compared to the state-of-the-art methods.\n","authors":["Mohsen Sadr","Hossein Gorji"],"pdf_url":"https://arxiv.org/pdf/2412.16385v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10178v2","updated":"2025-08-04T10:03:27Z","published":"2025-07-14T11:40:17Z","title":"Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large\n  Language Model Serving","summary":"  Transformers are the driving force behind today's Large Language Models\n(LLMs), serving as the foundation for their performance and versatility. Yet,\ntheir compute and memory costs grow with sequence length, posing scalability\nchallenges for long-context inferencing. In response, the algorithm community\nis exploring alternative architectures, such as state space models (SSMs),\nlinear attention, and recurrent neural networks (RNNs), which we refer to as\npost-transformers. This shift presents a key challenge: building a serving\nsystem that efficiently supports both transformer and post-transformer LLMs\nwithin a unified framework. To address this challenge, we analyze the\nperformance characteristics of transformer and post-transformer LLMs. Despite\ntheir algorithmic differences, both are fundamentally limited by memory\nbandwidth under batched inference due to attention in transformers and state\nupdates in post-transformers. Further analyses suggest two additional insights:\n(1) state update operations, unlike attention, incur high hardware cost, making\nper-bank PIM acceleration inefficient, and (2) different low-precision\narithmetic methods offer varying accuracy-area tradeoffs, while we identify\nMicrosoft's MX as the Pareto-optimal choice. Building on these insights, we\ndesign Pimba as an array of State-update Processing Units (SPUs), each shared\nbetween two banks to enable interleaved access to PIM. Each SPU includes a\nState-update Processing Engine (SPE) that comprises element-wise multipliers\nand adders using MX-based quantized arithmetic, enabling efficient execution of\nstate update and attention operations. Our evaluation shows that, compared to\nLLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 4.1x and 2.1x\nhigher token generation throughput, respectively.\n","authors":["Wonung Kim","Yubin Lee","Yoonsung Kim","Jinwoo Hwang","Seongryong Oh","Jiyong Jung","Aziz Huseynov","Woong Gyu Park","Chang Hyun Park","Divya Mahajan","Jongse Park"],"pdf_url":"https://arxiv.org/pdf/2507.10178v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02247v1","updated":"2025-08-04T09:48:42Z","published":"2025-08-04T09:48:42Z","title":"ByteGen: A Tokenizer-Free Generative Model for Orderbook Events in Byte\n  Space","summary":"  Generative modeling of high-frequency limit order book (LOB) dynamics is a\ncritical yet unsolved challenge in quantitative finance, essential for robust\nmarket simulation and strategy backtesting. Existing approaches are often\nconstrained by simplifying stochastic assumptions or, in the case of modern\ndeep learning models like Transformers, rely on tokenization schemes that\naffect the high-precision, numerical nature of financial data through\ndiscretization and binning. To address these limitations, we introduce ByteGen,\na novel generative model that operates directly on the raw byte streams of LOB\nevents. Our approach treats the problem as an autoregressive next-byte\nprediction task, for which we design a compact and efficient 32-byte packed\nbinary format to represent market messages without information loss. The core\nnovelty of our work is the complete elimination of feature engineering and\ntokenization, enabling the model to learn market dynamics from its most\nfundamental representation. We achieve this by adapting the H-Net architecture,\na hybrid Mamba-Transformer model that uses a dynamic chunking mechanism to\ndiscover the inherent structure of market messages without predefined rules.\nOur primary contributions are: 1) the first end-to-end, byte-level framework\nfor LOB modeling; 2) an efficient packed data representation; and 3) a\ncomprehensive evaluation on high-frequency data. Trained on over 34 million\nevents from CME Bitcoin futures, ByteGen successfully reproduces key stylized\nfacts of financial markets, generating realistic price distributions,\nheavy-tailed returns, and bursty event timing. Our findings demonstrate that\nlearning directly from byte space is a promising and highly flexible paradigm\nfor modeling complex financial systems, achieving competitive performance on\nstandard market quality metrics without the biases of tokenization.\n","authors":["Yang Li","Zhi Chen"],"pdf_url":"https://arxiv.org/pdf/2508.02247v1.pdf","comment":"21 pages, 3 tables, 5 figures"},{"id":"http://arxiv.org/abs/2508.02235v1","updated":"2025-08-04T09:34:50Z","published":"2025-08-04T09:34:50Z","title":"Pigeon-SL: Robust Split Learning Framework for Edge Intelligence under\n  Malicious Clients","summary":"  Recent advances in split learning (SL) have established it as a promising\nframework for privacy-preserving, communication-efficient distributed learning\nat the network edge. However, SL's sequential update process is vulnerable to\neven a single malicious client, which can significantly degrade model accuracy.\nTo address this, we introduce Pigeon-SL, a novel scheme grounded in the\npigeonhole principle that guarantees at least one entirely honest cluster among\nM clients, even when up to N of them are adversarial. In each global round, the\naccess point partitions the clients into N+1 clusters, trains each cluster\nindependently via vanilla SL, and evaluates their validation losses on a shared\ndataset. Only the cluster with the lowest loss advances, thereby isolating and\ndiscarding malicious updates. We further enhance training and communication\nefficiency with Pigeon-SL+, which repeats training on the selected cluster to\nmatch the update throughput of standard SL. We validate the robustness and\neffectiveness of our approach under three representative attack models -- label\nflipping, activation and gradient manipulation -- demonstrating significant\nimprovements in accuracy and resilience over baseline SL methods in future\nintelligent wireless networks.\n","authors":["Sangjun Park","Tony Q. S. Quek","Hyowoon Seo"],"pdf_url":"https://arxiv.org/pdf/2508.02235v1.pdf","comment":"13 pages, 14 figures"},{"id":"http://arxiv.org/abs/2507.01607v2","updated":"2025-08-04T09:21:10Z","published":"2025-07-02T11:21:27Z","title":"Survivability of Backdoor Attacks on Unconstrained Face Recognition\n  Systems","summary":"  The widespread use of deep learning face recognition raises several security\nconcerns. Although prior works point at existing vulnerabilities, DNN backdoor\nattacks against real-life, unconstrained systems dealing with images captured\nin the wild remain a blind spot of the literature. This paper conducts the\nfirst system-level study of backdoors in deep learning-based face recognition\nsystems. This paper yields four contributions by exploring the feasibility of\nDNN backdoors on these pipelines in a holistic fashion. We demonstrate for the\nfirst time two backdoor attacks on the face detection task: face generation and\nface landmark shift attacks. We then show that face feature extractors trained\nwith large margin losses also fall victim to backdoor attacks. Combining our\nmodels, we then show using 20 possible pipeline configurations and 15 attack\ncases that a single backdoor enables an attacker to bypass the entire function\nof a system. Finally, we provide stakeholders with several best practices and\ncountermeasures.\n","authors":["Quentin Le Roux","Yannick Teglia","Teddy Furon","Philippe Loubet-Moundi","Eric Bourbao"],"pdf_url":"https://arxiv.org/pdf/2507.01607v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02219v1","updated":"2025-08-04T09:11:48Z","published":"2025-08-04T09:11:48Z","title":"CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through\n  Chunked Offline Reinforcement Learning","summary":"  Vision-Language-Action (VLA) models demonstrate significant potential for\ndeveloping generalized policies in real-world robotic control. This progress\ninspires researchers to explore fine-tuning these models with Reinforcement\nLearning (RL). However, fine-tuning VLA models with RL still faces challenges\nrelated to sample efficiency, compatibility with action chunking, and training\nstability. To address these challenges, we explore the fine-tuning of VLA\nmodels through offline reinforcement learning incorporating action chunking. In\nthis work, we propose Chunked RL, a novel reinforcement learning framework\nspecifically designed for VLA models. Within this framework, we extend temporal\ndifference (TD) learning to incorporate action chunking, a prominent\ncharacteristic of VLA models. Building upon this framework, we propose CO-RFT,\nan algorithm aimed at fine-tuning VLA models using a limited set of\ndemonstrations (30 to 60 samples). Specifically, we first conduct imitation\nlearning (IL) with full parameter fine-tuning to initialize both the backbone\nand the policy. Subsequently, we implement offline RL with action chunking to\noptimize the pretrained policy. Our empirical results in real-world\nenvironments demonstrate that CO-RFT outperforms previous supervised methods,\nachieving a 57% improvement in success rate and a 22.3% reduction in cycle\ntime. Moreover, our method exhibits robust positional generalization\ncapabilities, attaining a success rate of 44.3% in previously unseen positions.\n","authors":["Dongchi Huang","Zhirui Fang","Tianle Zhang","Yihang Li","Lin Zhao","Chunhe Xia"],"pdf_url":"https://arxiv.org/pdf/2508.02219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02217v1","updated":"2025-08-04T09:09:04Z","published":"2025-08-04T09:09:04Z","title":"Multi-Policy Pareto Front Tracking Based Online and Offline\n  Multi-Objective Reinforcement Learning","summary":"  Multi-objective reinforcement learning (MORL) plays a pivotal role in\naddressing multi-criteria decision-making problems in the real world. The\nmulti-policy (MP) based methods are widely used to obtain high-quality Pareto\nfront approximation for the MORL problems. However, traditional MP methods only\nrely on the online reinforcement learning (RL) and adopt the evolutionary\nframework with a large policy population. This may lead to sample inefficiency\nand/or overwhelmed agent-environment interactions in practice. By forsaking the\nevolutionary framework, we propose the novel Multi-policy Pareto Front Tracking\n(MPFT) framework without maintaining any policy population, where both online\nand offline MORL algorithms can be applied. The proposed MPFT framework\nincludes four stages: Stage 1 approximates all the Pareto-vertex policies,\nwhose mapping to the objective space fall on the vertices of the Pareto front.\nStage 2 designs the new Pareto tracking mechanism to track the Pareto front,\nstarting from each of the Pareto-vertex policies. Stage 3 identifies the sparse\nregions in the tracked Pareto front, and introduces a new objective weight\nadjustment method to fill the sparse regions. Finally, by combining all the\npolicies tracked in Stages 2 and 3, Stage 4 approximates the Pareto front.\nExperiments are conducted on seven different continuous-action robotic control\ntasks with both online and offline MORL algorithms, and demonstrate the\nsuperior hypervolume performance of our proposed MPFT approach over the\nstate-of-the-art benchmarks, with significantly reduced agent-environment\ninteractions and hardware requirements.\n","authors":["Zeyu Zhao","Yueling Che","Kaichen Liu","Jian Li","Junmei Yao"],"pdf_url":"https://arxiv.org/pdf/2508.02217v1.pdf","comment":"24 pages, 10 figures, conference paper"},{"id":"http://arxiv.org/abs/2508.02215v1","updated":"2025-08-04T09:08:43Z","published":"2025-08-04T09:08:43Z","title":"LeanK: Learnable K Cache Channel Pruning for Efficient Decoding","summary":"  Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.\n","authors":["Yike Zhang","Zhiyuan He","Huiqiang Jiang","Chengruidong Zhang","Yuqing Yang","Jianyong Wang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2508.02215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00117v2","updated":"2025-08-04T09:05:52Z","published":"2025-07-31T19:13:30Z","title":"StackLiverNet: A Novel Stacked Ensemble Model for Accurate and\n  Interpretable Liver Disease Detection","summary":"  Liver diseases are a serious health concern in the world, which requires\nprecise and timely diagnosis to enhance the survival chances of patients. The\ncurrent literature implemented numerous machine learning and deep learning\nmodels to classify liver diseases, but most of them had some issues like high\nmisclassification error, poor interpretability, prohibitive computational\nexpense, and lack of good preprocessing strategies. In order to address these\ndrawbacks, we introduced StackLiverNet in this study; an interpretable stacked\nensemble model tailored to the liver disease detection task. The framework uses\nadvanced data preprocessing and feature selection technique to increase model\nrobustness and predictive ability. Random undersampling is performed to deal\nwith class imbalance and make the training balanced. StackLiverNet is an\nensemble of several hyperparameter-optimized base classifiers, whose\ncomplementary advantages are used through a LightGBM meta-model. The provided\nmodel demonstrates excellent performance, with the testing accuracy of 99.89%,\nCohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and\nefficient training and inference speeds that are amenable to clinical practice\n(training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local\nInterpretable Model-Agnostic Explanations (LIME) are applied to generate\ntransparent explanations of individual predictions, revealing high\nconcentrations of Alkaline Phosphatase and moderate SGOT as important\nobservations of liver disease. Also, SHAP was used to rank features by their\nglobal contribution to predictions, while the Morris method confirmed the most\ninfluential features through sensitivity analysis.\n","authors":["Md. Ehsanul Haque","S. M. Jahidul Islam","Shakil Mia","Rumana Sharmin"," Ashikuzzaman","Md Samir Morshed","Md. Tahmidul Huque"],"pdf_url":"https://arxiv.org/pdf/2508.00117v2.pdf","comment":"Accepted and presented paper of THE 16th INTERNATIONAL IEEE\n  CONFERENCE ON COMPUTING, COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT)\n  INDIA"},{"id":"http://arxiv.org/abs/2508.02210v1","updated":"2025-08-04T09:01:07Z","published":"2025-08-04T09:01:07Z","title":"WhiSQA: Non-Intrusive Speech Quality Prediction Using Whisper Encoder\n  Features","summary":"  There has been significant research effort developing neural-network-based\npredictors of SQ in recent years. While a primary objective has been to develop\nnon-intrusive, i.e.~reference-free, metrics to assess the performance of SE\nsystems, recent work has also investigated the direct inference of neural SQ\npredictors within the loss function of downstream speech tasks. To aid in the\ntraining of SQ predictors, several large datasets of audio with corresponding\nhuman labels of quality have been created. Recent work in this area has shown\nthat speech representations derived from large unsupervised or semi-supervised\nfoundational speech models are useful input feature representations for neural\nSQ prediction. In this work, a novel and robust SQ predictor is proposed based\non feature representations extracted from an ASR model, found to be a powerful\ninput feature for the SQ prediction task. The proposed system achieves higher\ncorrelation with human MOS ratings than recent approaches on all NISQA test\nsets and shows significantly better domain adaption compared to the commonly\nused DNSMOS metric.\n","authors":["George Close","Kris Hong","Thomas Hain","Stefan Goetze"],"pdf_url":"https://arxiv.org/pdf/2508.02210v1.pdf","comment":"Accepted at SPECOM 2025"},{"id":"http://arxiv.org/abs/2507.23449v2","updated":"2025-08-04T09:01:03Z","published":"2025-07-31T11:27:01Z","title":"Manifold-regularised Large-Margin $\\ell_p$-SVDD for Multidimensional\n  Time Series Anomaly Detection","summary":"  We generalise the recently introduced large-margin $\\ell_p$-SVDD approach to\nexploit the geometry of data distribution via manifold regularising for time\nseries anomaly detection. Specifically, we formulate a manifold-regularised\nvariant of the $\\ell_p$-SVDD method to encourage label smoothness on the\nunderlying manifold to capture structural information for improved detection\nperformance. Drawing on an existing Representer theorem, we then provide an\neffective optimisation technique for the proposed method.\n  We theoretically study the proposed approach using Rademacher complexities to\nanalyse its generalisation performance and also provide an experimental\nassessment of the proposed method across various data sets to compare its\nperformance against other methods.\n","authors":["Shervin Rahimzadeh Arashloo"],"pdf_url":"https://arxiv.org/pdf/2507.23449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02209v1","updated":"2025-08-04T09:00:01Z","published":"2025-08-04T09:00:01Z","title":"Balancing Information Accuracy and Response Timeliness in Networked LLMs","summary":"  Recent advancements in Large Language Models (LLMs) have transformed many\nfields including scientific discovery, content generation, biomedical text\nmining, and educational technology. However, the substantial requirements for\ntraining data, computational resources, and energy consumption pose significant\nchallenges for their practical deployment. A promising alternative is to\nleverage smaller, specialized language models and aggregate their outputs to\nimprove overall response quality. In this work, we investigate a networked LLM\nsystem composed of multiple users, a central task processor, and clusters of\ntopic-specialized LLMs. Each user submits categorical binary (true/false)\nqueries, which are routed by the task processor to a selected cluster of $m$\nLLMs. After gathering individual responses, the processor returns a final\naggregated answer to the user. We characterize both the information accuracy\nand response timeliness in this setting, and formulate a joint optimization\nproblem to balance these two competing objectives. Our extensive simulations\ndemonstrate that the aggregated responses consistently achieve higher accuracy\nthan those of individual LLMs. Notably, this improvement is more significant\nwhen the participating LLMs exhibit similar standalone performance.\n","authors":["Yigit Turkmen","Baturalp Buyukates","Melih Bastopcu"],"pdf_url":"https://arxiv.org/pdf/2508.02209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15088v2","updated":"2025-08-04T08:52:45Z","published":"2024-09-23T15:01:47Z","title":"AdapFair: Ensuring Adaptive Fairness for Machine Learning Operations","summary":"  The biases and discrimination of machine learning algorithms have attracted\nsignificant attention, leading to the development of various algorithms\ntailored to specific contexts. However, these solutions often fall short of\naddressing fairness issues inherent in machine learning operations. In this\npaper, we present an adaptive debiasing framework designed to find an optimal\nfair transformation of input data that maximally preserves data predictability\nunder dynamic conditions. A distinctive feature of our approach is its\nflexibility and efficiency. It can be integrated with pretrained black-box\nclassifiers, providing fairness guarantees with minimal retraining efforts,\neven in the face of frequent data drifts, evolving fairness requirements, and\nbatches of similar tasks. To achieve this, we leverage the normalizing flows to\nenable efficient, information-preserving data transformation, ensuring that no\ncritical information is lost during the debiasing process. Additionally, we\nincorporate the Wasserstein distance as the fairness measure to guide the\noptimization of data transformations. Finally, we introduce an efficient\noptimization algorithm with closed-formed gradient computations, making our\nframework scalable and suitable for dynamic, real-world environments.\n","authors":["Yinghui Huang","Zihao Tang","Xiangyu Chang"],"pdf_url":"https://arxiv.org/pdf/2409.15088v2.pdf","comment":"18 pages,15 figures"},{"id":"http://arxiv.org/abs/2506.21734v3","updated":"2025-08-04T08:45:08Z","published":"2025-06-26T19:39:54Z","title":"Hierarchical Reasoning Model","summary":"  Reasoning, the process of devising and executing complex goal-oriented action\nsequences, remains a critical challenge in AI. Current large language models\n(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from\nbrittle task decomposition, extensive data requirements, and high latency.\nInspired by the hierarchical and multi-timescale processing in the human brain,\nwe propose the Hierarchical Reasoning Model (HRM), a novel recurrent\narchitecture that attains significant computational depth while maintaining\nboth training stability and efficiency. HRM executes sequential reasoning tasks\nin a single forward pass without explicit supervision of the intermediate\nprocess, through two interdependent recurrent modules: a high-level module\nresponsible for slow, abstract planning, and a low-level module handling rapid,\ndetailed computations. With only 27 million parameters, HRM achieves\nexceptional performance on complex reasoning tasks using only 1000 training\nsamples. The model operates without pre-training or CoT data, yet achieves\nnearly perfect performance on challenging tasks including complex Sudoku\npuzzles and optimal path finding in large mazes. Furthermore, HRM outperforms\nmuch larger models with significantly longer context windows on the Abstraction\nand Reasoning Corpus (ARC), a key benchmark for measuring artificial general\nintelligence capabilities. These results underscore HRM's potential as a\ntransformative advancement toward universal computation and general-purpose\nreasoning systems.\n","authors":["Guan Wang","Jin Li","Yuhao Sun","Xing Chen","Changling Liu","Yue Wu","Meng Lu","Sen Song","Yasin Abbasi Yadkori"],"pdf_url":"https://arxiv.org/pdf/2506.21734v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02193v1","updated":"2025-08-04T08:43:01Z","published":"2025-08-04T08:43:01Z","title":"Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed\n  Inference","summary":"  We present Seed Diffusion Preview, a large-scale language model based on\ndiscrete-state diffusion, offering remarkably fast inference speed. Thanks to\nnon-sequential, parallel generation, discrete diffusion models provide a\nnotable speedup to mitigate the inherent latency of token-by-token decoding, as\ndemonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion\nPreview achieves an inference speed of 2,146 token/s over H20 GPUs while\nmaintaining competitive performance across a sweep of standard code evaluation\nbenchmarks, significantly faster than contemporary Mercury and Gemini\nDiffusion, establishing new state of the art on the speed-quality Pareto\nfrontier for code models.\n","authors":["Yuxuan Song","Zheng Zhang","Cheng Luo","Pengyang Gao","Fan Xia","Hao Luo","Zheng Li","Yuehang Yang","Hongli Yu","Xingwei Qu","Yuwei Fu","Jing Su","Ge Zhang","Wenhao Huang","Mingxuan Wang","Lin Yan","Xiaoying Jia","Jingjing Liu","Wei-Ying Ma","Ya-Qin Zhang","Yonghui Wu","Hao Zhou"],"pdf_url":"https://arxiv.org/pdf/2508.02193v1.pdf","comment":"Demo is available at https://studio.seed.ai/exp/seed_diffusion/;\n  Project page is https://seed.bytedance.com/seed_diffusion"},{"id":"http://arxiv.org/abs/2504.04739v2","updated":"2025-08-04T08:37:23Z","published":"2025-04-07T05:35:16Z","title":"MedGNN: Capturing the Links Between Urban Characteristics and Medical\n  Prescriptions","summary":"  Understanding how urban socio-demographic and environmental factors relate\nwith health is essential for public health and urban planning. However,\ntraditional statistical methods struggle with nonlinear effects, while machine\nlearning models often fail to capture geographical (nearby areas being more\nsimilar) and topological (unequal connectivity between places) effects in an\ninterpretable way. To address this, we propose MedGNN, a spatio-topologically\nexplicit framework that constructs a 2-hop spatial graph, integrating\npositional and locational node embeddings with urban characteristics in a graph\nneural network. Applied to MEDSAT, a comprehensive dataset covering over 150\nenvironmental and socio-demographic factors and six prescription outcomes\n(depression, anxiety, diabetes, hypertension, asthma, and opioids) across 4,835\nGreater London neighborhoods, MedGNN improved predictions by over 25% on\naverage compared to baseline methods. Using depression prescriptions as a case\nstudy, we analyzed graph embeddings via geographical principal component\nanalysis, identifying findings that: align with prior research (e.g., higher\nantidepressant prescriptions among older and White populations), contribute to\nongoing debates (e.g., greenery linked to higher and NO2 to lower\nprescriptions), and warrant further study (e.g., canopy evaporation correlated\nwith fewer prescriptions). These results demonstrate MedGNN's potential, and\nmore broadly, of carefully applied machine learning, to advance\ntransdisciplinary public health research.\n","authors":["Minwei Zhao","Sanja Scepanovic","Stephen Law","Ivica Obadic","Cai Wu","Daniele Quercia"],"pdf_url":"https://arxiv.org/pdf/2504.04739v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14796v3","updated":"2025-08-04T08:32:18Z","published":"2024-11-22T08:41:33Z","title":"Adaptive Hyper-Graph Convolution Network for Skeleton-based Human Action\n  Recognition with Virtual Connections","summary":"  The shared topology of human skeletons motivated the recent investigation of\ngraph convolutional network (GCN) solutions for action recognition. However,\nmost of the existing GCNs rely on the binary connection of two neighboring\nvertices (joints) formed by an edge (bone), overlooking the potential of\nconstructing multi-vertex convolution structures. Although some studies have\nattempted to utilize hyper-graphs to represent the topology, they rely on a\nfixed construction strategy, which limits their adaptivity in uncovering the\nintricate latent relationships within the action. In this paper, we address\nthis oversight and explore the merits of an adaptive hyper-graph convolutional\nnetwork (Hyper-GCN) to achieve the aggregation of rich semantic information\nconveyed by skeleton vertices. In particular, our Hyper-GCN adaptively\noptimises the hyper-graphs during training, revealing the action-driven\nmulti-vertex relations. Besides, virtual connections are often designed to\nsupport efficient feature aggregation, implicitly extending the spectrum of\ndependencies within the skeleton. By injecting virtual connections into\nhyper-graphs, the semantic clues of diverse action categories can be\nhighlighted. The results of experiments conducted on the NTU-60, NTU-120, and\nNW-UCLA datasets demonstrate the merits of our Hyper-GCN, compared to the\nstate-of-the-art methods. The code is available at\nhttps://github.com/6UOOON9/Hyper-GCN.\n","authors":["Youwei Zhou","Tianyang Xu","Cong Wu","Xiaojun Wu","Josef Kittler"],"pdf_url":"https://arxiv.org/pdf/2411.14796v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02184v1","updated":"2025-08-04T08:28:25Z","published":"2025-08-04T08:28:25Z","title":"CAAD: Context-Aware Adaptive Decoding for Truthful Text Generation","summary":"  Ensuring truthfulness in large language models remains a critical challenge\nfor reliable text generation. While supervised fine-tuning and reinforcement\nlearning with human feedback have shown promise, they require substantial\namount of annotated data and computational resources, limiting scalability. In\ncontrast, decoding-time interventions offer lightweight alternatives without\nmodel retraining. However, existing decoding strategies often face issues like\nprompt sensitivity, limited generalization, or dependence on internal model\nstates. We propose a context-aware adaptive decoding method that leverages a\ncompact reference grounding space, built from as few as 10 annotated examples\nand comprising pairs of context embeddings and next token logits from truthful\nresponses, to enable retrieval-based logit shaping during inference. At each\ndecoding step, our method retrieves top-N semantically similar contexts and\naggregates their associated next token logits to modify the LLM's logits.\nAcross three open-ended question-answering benchmarks, our approach achieves a\n2.8 percent average improvement on TruthfulQA and further outperforms existing\nbaselines on both Biographies and WikiQA. Experimental results also demonstrate\ncross-task generalization, with TruthfulQA-derived grounding enhancing\nbiography generation. Our model-agnostic, scalable, and efficient method\nrequires only a single generation pass, highlighting the potential of\ncontext-aware decoding for factual reliability in LLMs.\n","authors":["Manh Nguyen","Sunil Gupta","Hung Le"],"pdf_url":"https://arxiv.org/pdf/2508.02184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02183v1","updated":"2025-08-04T08:27:50Z","published":"2025-08-04T08:27:50Z","title":"Multi-Treatment-DML: Causal Estimation for Multi-Dimensional Continuous\n  Treatments with Monotonicity Constraints in Personal Loan Risk Optimization","summary":"  Optimizing credit limits, interest rates, and loan terms is crucial for\nmanaging borrower risk and lifetime value (LTV) in personal loan platform.\nHowever, counterfactual estimation of these continuous, multi-dimensional\ntreatments faces significant challenges: randomized trials are often prohibited\nby risk controls and long repayment cycles, forcing reliance on biased\nobservational data. Existing causal methods primarily handle binary/discrete\ntreatments and struggle with continuous, multi-dimensional settings.\nFurthermore, financial domain knowledge mandates provably monotonic\ntreatment-outcome relationships (e.g., risk increases with credit limit).To\naddress these gaps, we propose Multi-Treatment-DML, a novel framework\nleveraging Double Machine Learning (DML) to: (i) debias observational data for\ncausal effect estimation; (ii) handle arbitrary-dimensional continuous\ntreatments; and (iii) enforce monotonic constraints between treatments and\noutcomes, guaranteeing adherence to domain requirements.Extensive experiments\non public benchmarks and real-world industrial datasets demonstrate the\neffectiveness of our approach. Furthermore, online A/B testing conducted on a\nrealworld personal loan platform, confirms the practical superiority of\nMulti-Treatment-DML in real-world loan operations.\n","authors":["Kexin Zhao","Bo Wang","Cuiying Zhao","Tongyao Wan"],"pdf_url":"https://arxiv.org/pdf/2508.02183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02161v1","updated":"2025-08-04T08:03:01Z","published":"2025-08-04T08:03:01Z","title":"User Trajectory Prediction Unifying Global and Local Temporal\n  Information","summary":"  Trajectory prediction is essential for formulating proactive strategies that\nanticipate user mobility and support advance preparation. Therefore, how to\nreduce the forecasting error in user trajectory prediction within an acceptable\ninference time arises as an interesting issue. However, trajectory data\ncontains both global and local temporal information, complicating the\nextraction of the complete temporal pattern. Moreover, user behavior occurs\nover different time scales, increasing the difficulty of capturing behavioral\npatterns. To address these challenges, a trajectory prediction model based on\nmultilayer perceptron (MLP), multi-scale convolutional neural network (MSCNN),\nand cross-attention (CA) is proposed. Specifically, MLP is used to extract the\nglobal temporal information of each feature. In parallel, MSCNN is employed to\nextract the local temporal information by modeling interactions among features\nwithin a local temporal range. Convolutional kernels with different sizes are\nused in MSCNN to capture temporal information at multiple resolutions,\nenhancing the model's adaptability to different behavioral patterns. Finally,\nCA is applied to fuse the global and local temporal information. Experimental\nresults show that our model reduces mean squared error (MSE) by 5.04% and mean\nabsolute error (MAE) by 4.35% compared with ModernTCN in 12-step prediction,\nwhile maintaining similar inference time.\n","authors":["Wei Hao","Bin Chong","Ronghua Ji","Chen Hou"],"pdf_url":"https://arxiv.org/pdf/2508.02161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02159v1","updated":"2025-08-04T08:01:19Z","published":"2025-08-04T08:01:19Z","title":"PIGDreamer: Privileged Information Guided World Models for Safe\n  Partially Observable Reinforcement Learning","summary":"  Partial observability presents a significant challenge for safe reinforcement\nlearning, as it impedes the identification of potential risks and rewards.\nLeveraging specific types of privileged information during training to mitigate\nthe effects of partial observability has yielded notable empirical successes.\nIn this paper, we propose Asymmetric Constrained Partially Observable Markov\nDecision Processes (ACPOMDPs) to theoretically examine the advantages of\nincorporating privileged information. Building upon ACPOMDPs, we propose the\nPrivileged Information Guided Dreamer, a model-based safe reinforcement\nlearning approach that leverages privileged information to enhance the agent's\nsafety and performance through privileged representation alignment and an\nasymmetric actor-critic structure. Our empirical results demonstrate that our\napproach significantly outperforms existing methods in terms of safety and\ntask-centric performance. Meanwhile, compared to alternative privileged\nmodel-based reinforcement learning methods, our approach exhibits superior\nperformance and ease of training.\n","authors":["Dongchi Huang","Jiaqi Wang","Yang Li","Chunhe Xia","Tianle Zhang","Kaige Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.02159v1.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2508.02158v1","updated":"2025-08-04T07:59:42Z","published":"2025-08-04T07:59:42Z","title":"Robust Detection of Planted Subgraphs in Semi-Random Models","summary":"  Detection of planted subgraphs in Erd\\\"os-R\\'enyi random graphs has been\nextensively studied, leading to a rich body of results characterizing both\nstatistical and computational thresholds. However, most prior work assumes a\npurely random generative model, making the resulting algorithms potentially\nfragile in the face of real-world perturbations. In this work, we initiate the\nstudy of semi-random models for the planted subgraph detection problem, wherein\nan adversary is allowed to remove edges outside the planted subgraph before the\ngraph is revealed to the statistician. Crucially, the statistician remains\nunaware of which edges have been removed, introducing fundamental challenges to\nthe inference task. We establish fundamental statistical limits for detection\nunder this semi-random model, revealing a sharp dichotomy. Specifically, for\nplanted subgraphs with strongly sub-logarithmic maximum density detection\nbecomes information-theoretically impossible in the presence of an adversary,\ndespite being possible in the classical random model. In stark contrast, for\nsubgraphs with super-logarithmic density, the statistical limits remain\nessentially unchanged; we prove that the optimal (albeit computationally\nintractable) likelihood ratio test remains robust. Beyond these statistical\nboundaries, we design a new computationally efficient and robust detection\nalgorithm, and provide rigorous statistical guarantees for its performance. Our\nresults establish the first robust framework for planted subgraph detection and\nopen new directions in the study of semi-random models,\ncomputational-statistical trade-offs, and robustness in graph inference\nproblems.\n","authors":["Dor Elimelech","Wasim Huleihel"],"pdf_url":"https://arxiv.org/pdf/2508.02158v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2508.02148v1","updated":"2025-08-04T07:47:18Z","published":"2025-08-04T07:47:18Z","title":"Large-Scale Model Enabled Semantic Communication Based on Robust\n  Knowledge Distillation","summary":"  Large-scale models (LSMs) can be an effective framework for semantic\nrepresentation and understanding, thereby providing a suitable tool for\ndesigning semantic communication (SC) systems. However, their direct deployment\nis often hindered by high computational complexity and resource requirements.\nIn this paper, a novel robust knowledge distillation based semantic\ncommunication (RKD-SC) framework is proposed to enable efficient and\n\\textcolor{black}{channel-noise-robust} LSM-powered SC. The framework addresses\ntwo key challenges: determining optimal compact model architectures and\neffectively transferring knowledge while maintaining robustness against channel\nnoise. First, a knowledge distillation-based lightweight differentiable\narchitecture search (KDL-DARTS) algorithm is proposed. This algorithm\nintegrates knowledge distillation loss and a complexity penalty into the neural\narchitecture search process to identify high-performance, lightweight semantic\nencoder architectures. Second, a novel two-stage robust knowledge distillation\n(RKD) algorithm is developed to transfer semantic capabilities from an LSM\n(teacher) to a compact encoder (student) and subsequently enhance system\nrobustness. To further improve resilience to channel impairments, a\nchannel-aware transformer (CAT) block is introduced as the channel codec,\ntrained under diverse channel conditions with variable-length outputs.\nExtensive simulations on image classification tasks demonstrate that the RKD-SC\nframework significantly reduces model parameters while preserving a high degree\nof the teacher model's performance and exhibiting superior robustness compared\nto existing methods.\n","authors":["Kuiyuan DIng","Caili Guo","Yang Yang","Zhongtian Du","Walid Saad"],"pdf_url":"https://arxiv.org/pdf/2508.02148v1.pdf","comment":"13 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.19166v3","updated":"2025-08-04T07:46:55Z","published":"2025-02-26T14:19:49Z","title":"CodeIF: Benchmarking the Instruction-Following Capabilities of Large\n  Language Models for Code Generation","summary":"  With the rapid advancement of Large Language Models (LLMs), the demand for\nrobust instruction-following capabilities in code generation tasks has grown\nsignificantly. Code generation not only facilitates faster prototyping and\nautomated testing, but also augments developer efficiency through improved\nmaintainability and reusability of code. In this paper, we introduce CodeIF,\nthe first benchmark specifically designed to assess the abilities of LLMs to\nadhere to task-oriented instructions within diverse code generation scenarios.\nCodeIF encompasses a broad range of tasks, including function synthesis, error\ndebugging, algorithmic refactoring, and code explanation, thereby providing a\ncomprehensive suite to evaluate model performance across varying complexity\nlevels and programming domains. We conduct extensive experiments with LLMs,\nanalyzing their strengths and limitations in meeting the demands of these\ntasks. The experimental results offer valuable insights into how well current\nmodels align with human instructions, as well as the extent to which they can\ngenerate consistent, maintainable, and contextually relevant code. Our findings\nnot only underscore the critical role that instruction-following LLMs can play\nin modern software development, but also illuminate pathways for future\nresearch aimed at enhancing their adaptability, reliability, and overall\neffectiveness in automated code generation. CodeIF data and code are publicly\navailable: https://github.com/lin-rany/codeIF\n","authors":["Kaiwen Yan","Hongcheng Guo","Xuanqing Shi","Shaosheng Cao","Donglin Di","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2502.19166v3.pdf","comment":"Accepted as an ACL 2025 Industry Track paper (15 pages)"},{"id":"http://arxiv.org/abs/2501.19128v2","updated":"2025-08-04T07:39:48Z","published":"2025-01-31T13:35:19Z","title":"Shaping Sparse Rewards in Reinforcement Learning: A Semi-supervised\n  Approach","summary":"  In many real-world scenarios, reward signal for agents are exceedingly\nsparse, making it challenging to learn an effective reward function for reward\nshaping. To address this issue, the proposed approach in this paper performs\nreward shaping not only by utilizing non-zero-reward transitions but also by\nemploying the \\emph{Semi-Supervised Learning} (SSL) technique combined with a\nnovel data augmentation to learn trajectory space representations from the\nmajority of transitions, {i.e}., zero-reward transitions, thereby improving the\nefficacy of reward shaping. Experimental results in Atari and robotic\nmanipulation demonstrate that our method outperforms supervised-based\napproaches in reward inference, leading to higher agent scores. Notably, in\nmore sparse-reward environments, our method achieves up to twice the peak\nscores compared to supervised baselines. The proposed double entropy data\naugmentation enhances performance, showcasing a 15.8\\% increase in best score\nover other augmentation methods\n","authors":["Wenyun Li","Wenjie Huang"],"pdf_url":"https://arxiv.org/pdf/2501.19128v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02137v1","updated":"2025-08-04T07:34:48Z","published":"2025-08-04T07:34:48Z","title":"Fitness aligned structural modeling enables scalable virtual screening\n  with AuroBind","summary":"  Most human proteins remain undrugged, over 96% of human proteins remain\nunexploited by approved therapeutics. While structure-based virtual screening\npromises to expand the druggable proteome, existing methods lack atomic-level\nprecision and fail to predict binding fitness, limiting translational impact.\nWe present AuroBind, a scalable virtual screening framework that fine-tunes a\ncustom atomic-level structural model on million-scale chemogenomic data.\nAuroBind integrates direct preference optimization, self-distillation from\nhigh-confidence complexes, and a teacher-student acceleration strategy to\njointly predict ligand-bound structures and binding fitness. The proposed\nmodels outperform state-of-the-art models on structural and functional\nbenchmarks while enabling 100,000-fold faster screening across ultra-large\ncompound libraries. In a prospective screen across ten disease-relevant\ntargets, AuroBind achieved experimental hit rates of 7-69%, with top compounds\nreaching sub-nanomolar to picomolar potency. For the orphan GPCRs GPR151 and\nGPR160, AuroBind identified both agonists and antagonists with success rates of\n16-30%, and functional assays confirmed GPR160 modulation in liver and prostate\ncancer models. AuroBind offers a generalizable framework for structure-function\nlearning and high-throughput molecular screening, bridging the gap between\nstructure prediction and therapeutic discovery.\n","authors":["Zhongyue Zhang","Jiahua Rao","Jie Zhong","Weiqiang Bai","Dongxue Wang","Shaobo Ning","Lifeng Qiao","Sheng Xu","Runze Ma","Will Hua","Jack Xiaoyu Chen","Odin Zhang","Wei Lu","Hanyi Feng","He Yang","Xinchao Shi","Rui Li","Wanli Ouyang","Xinzhu Ma","Jiahao Wang","Jixian Zhang","Jia Duan","Siqi Sun","Jian Zhang","Shuangjia Zheng"],"pdf_url":"https://arxiv.org/pdf/2508.02137v1.pdf","comment":"54 pages, 13 figures, code available at\n  https://github.com/GENTEL-lab/AuroBind"},{"id":"http://arxiv.org/abs/2508.02136v1","updated":"2025-08-04T07:34:00Z","published":"2025-08-04T07:34:00Z","title":"FedLAD: A Linear Algebra Based Data Poisoning Defence for Federated\n  Learning","summary":"  Sybil attacks pose a significant threat to federated learning, as malicious\nnodes can collaborate and gain a majority, thereby overwhelming the system.\nTherefore, it is essential to develop countermeasures that ensure the security\nof federated learning environments. We present a novel defence method against\ntargeted data poisoning, which is one of the types of Sybil attacks, called\nLinear Algebra-based Detection (FedLAD). Unlike existing approaches, such as\nclustering and robust training, which struggle in situations where malicious\nnodes dominate, FedLAD models the federated learning aggregation process as a\nlinear problem, transforming it into a linear algebra optimisation challenge.\nThis method identifies potential attacks by extracting the independent linear\ncombinations from the original linear combinations, effectively filtering out\nredundant and malicious elements. Extensive experimental evaluations\ndemonstrate the effectiveness of FedLAD compared to five well-established\ndefence methods: Sherpa, CONTRA, Median, Trimmed Mean, and Krum. Using tasks\nfrom both image classification and natural language processing, our experiments\nconfirm that FedLAD is robust and not dependent on specific application\nsettings. The results indicate that FedLAD effectively protects federated\nlearning systems across a broad spectrum of malicious node ratios. Compared to\nbaseline defence methods, FedLAD maintains a low attack success rate for\nmalicious nodes when their ratio ranges from 0.2 to 0.8. Additionally, it\npreserves high model accuracy when the malicious node ratio is between 0.2 and\n0.5. These findings underscore FedLAD's potential to enhance both the\nreliability and performance of federated learning systems in the face of data\npoisoning attacks.\n","authors":["Qi Xiong","Hai Dong","Nasrin Sohrabi","Zahir Tari"],"pdf_url":"https://arxiv.org/pdf/2508.02136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00768v2","updated":"2025-08-04T07:32:28Z","published":"2025-08-01T16:43:45Z","title":"Evaluating Angle and Amplitude Encoding Strategies for Variational\n  Quantum Machine Learning: their impact on model's accuracy","summary":"  Recent advancements in Quantum Computing and Machine Learning have increased\nattention to Quantum Machine Learning (QML), which aims to develop machine\nlearning models by exploiting the quantum computing paradigm. One of the widely\nused models in this area is the Variational Quantum Circuit (VQC), a hybrid\nmodel where the quantum circuit handles data inference while classical\noptimization adjusts the parameters of the circuit. The quantum circuit\nconsists of an encoding layer, which loads data into the circuit, and a\ntemplate circuit, known as the ansatz, responsible for processing the data.\nThis work involves performing an analysis by considering both Amplitude- and\nAngle-encoding models, and examining how the type of rotational gate applied\naffects the classification performance of the model. This comparison is carried\nout by training the different models on two datasets, Wine and Diabetes, and\nevaluating their performance. The study demonstrates that, under identical\nmodel topologies, the difference in accuracy between the best and worst models\nranges from 10% to 30%, with differences reaching up to 41%. Moreover, the\nresults highlight how the choice of rotational gates used in encoding can\nsignificantly impact the model's classification performance. The findings\nconfirm that the embedding represents a hyperparameter for VQC models.\n","authors":["Antonio Tudisco","Andrea Marchesin","Maurizio Zamboni","Mariagrazia Graziano","Giovanna Turvani"],"pdf_url":"https://arxiv.org/pdf/2508.00768v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13026v3","updated":"2025-08-04T07:29:42Z","published":"2025-05-19T12:10:17Z","title":"Step-wise Adaptive Integration of Supervised Fine-tuning and\n  Reinforcement Learning for Task-Specific LLMs","summary":"  Large language models (LLMs) excel at mathematical reasoning and logical\nproblem-solving. The current popular training paradigms primarily use\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the\nmodels' reasoning abilities. However, when using SFT or RL alone, there are\nrespective challenges: SFT may suffer from overfitting, while RL is prone to\nmode collapse. The state-of-the-art methods have proposed hybrid training\nschemes. However, static switching faces challenges such as poor generalization\nacross different tasks and high dependence on data quality. In response to\nthese challenges, inspired by the curriculum learning-quiz mechanism in human\nreasoning cultivation, We propose SASR, a step-wise adaptive hybrid training\nframework that theoretically unifies SFT and RL and dynamically balances the\ntwo throughout optimization. SASR uses SFT for initial warm-up to establish\nbasic reasoning skills, and then uses an adaptive dynamic adjustment algorithm\nbased on gradient norm and divergence relative to the original distribution to\nseamlessly integrate SFT with the online RL method GRPO. By monitoring the\ntraining status of LLMs and adjusting the training process in sequence, SASR\nensures a smooth transition between training schemes, maintaining core\nreasoning abilities while exploring different paths. Experimental results\ndemonstrate that SASR outperforms SFT, RL, and static hybrid training methods.\n","authors":["Jack Chen","Fazhong Liu","Naruto Liu","Yuhan Luo","Erqu Qin","Harry Zheng","Tian Dong","Haojin Zhu","Yan Meng","Xiao Wang"],"pdf_url":"https://arxiv.org/pdf/2505.13026v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02130v1","updated":"2025-08-04T07:24:30Z","published":"2025-08-04T07:24:30Z","title":"The Complexity of Extreme Climate Events on the New Zealand's Kiwifruit\n  Industry","summary":"  Climate change has intensified the frequency and severity of extreme weather\nevents, presenting unprecedented challenges to the agricultural industry\nworldwide. In this investigation, we focus on kiwifruit farming in New Zealand.\nWe propose to examine the impacts of climate-induced extreme events,\nspecifically frost, drought, extreme rainfall, and heatwave, on kiwifruit\nharvest yields. These four events were selected due to their significant\nimpacts on crop productivity and their prevalence as recorded by climate\nmonitoring institutions in the country. We employed Isolation Forest, an\nunsupervised anomaly detection method, to analyse climate history and recorded\nextreme events, alongside with kiwifruit yields. Our analysis reveals\nconsiderable variability in how different types of extreme event affect\nkiwifruit yields underscoring notable discrepancies between climatic extremes\nand individual farm's yield outcomes. Additionally, our study highlights\ncritical limitations of current anomaly detection approaches, particularly in\naccurately identifying events such as frost. These findings emphasise the need\nfor integrating supplementary features like farm management strategies with\nclimate adaptation practices. Our further investigation will employ ensemble\nmethods that consolidate nearby farms' yield data and regional climate station\nfeatures to reduce variance, thereby enhancing the accuracy and reliability of\nextreme event detection and the formulation of response strategies.\n","authors":["Boyuan Zheng","Victor W. Chu","Zhidong Li","Evan Webster","Ashley Rootsey"],"pdf_url":"https://arxiv.org/pdf/2508.02130v1.pdf","comment":"Pre-print v0.8 2025-08-04"},{"id":"http://arxiv.org/abs/2508.02128v1","updated":"2025-08-04T07:22:36Z","published":"2025-08-04T07:22:36Z","title":"Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill\n  in Large Language Models","summary":"  In the era of large language models (LLMs), N:M sparsity has emerged as a\nstructured compression technique critical for accelerating inference. While\nprior work has primarily focused on weight sparsity, it often suffers from\nsignificant accuracy degradation. Activation sparsity, though promising, is\ntypically training-dependent and faces challenges in generalization. To address\nthese limitations, we introduce Amber Pruner, a training-free N:M activation\nsparsity method designed specifically for the prefill stage, targeting the\nacceleration of linear projection layers in LLMs. Extensive experiments across\nmultiple models and sparsity ratios (2:4, 4:8, and 8:16) demonstrate that Amber\nPruner can effectively sparsify and accelerate more than 55% of linear\ncomputations without requiring model retraining. To further enhance generality\nand efficiency, we propose Outstanding-sparse, a unified framework that\nintegrates Amber Pruner with post-training W8A8 quantization. Our approach\npreserves strong performance across a range of downstream tasks, with notable\nadvantages in generative tasks. This work pioneers a new frontier in activation\nsparsity, providing foundational insights that are poised to guide the\nco-evolution of algorithms and architectures in the design of next-generation\nAI systems.\n","authors":["Tai An","Ruwu Cai","Yanzhe Zhang","Yang Liu","Hao Chen","Pengcheng Xie","Sheng Chang","Yiwu Yao","Gongyi Wang"],"pdf_url":"https://arxiv.org/pdf/2508.02128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12869v2","updated":"2025-08-04T07:21:26Z","published":"2025-07-17T07:40:50Z","title":"WhoFi: Deep Person Re-Identification via Wi-Fi Channel Signal Encoding","summary":"  Person Re-Identification is a key and challenging task in video surveillance.\nWhile traditional methods rely on visual data, issues like poor lighting,\nocclusion, and suboptimal angles often hinder performance. To address these\nchallenges, we introduce WhoFi, a novel pipeline that utilizes Wi-Fi signals\nfor person re-identification. Biometric features are extracted from Channel\nState Information (CSI) and processed through a modular Deep Neural Network\n(DNN) featuring a Transformer-based encoder. The network is trained using an\nin-batch negative loss function to learn robust and generalizable biometric\nsignatures. Experiments on the NTU-Fi dataset show that our approach achieves\ncompetitive results compared to state-of-the-art methods, confirming its\neffectiveness in identifying individuals via Wi-Fi signals.\n","authors":["Danilo Avola","Emad Emam","Dario Montagnini","Daniele Pannone","Amedeo Ranaldi"],"pdf_url":"https://arxiv.org/pdf/2507.12869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20797v2","updated":"2025-08-04T07:18:49Z","published":"2025-05-27T07:00:33Z","title":"Multi-VQC: A Novel QML Approach for Enhancing Healthcare Classification","summary":"  Accurate and reliable diagnosis of diseases is crucial in enabling timely\nmedical treatment and enhancing patient survival rates. In recent years,\nMachine Learning has revolutionized diagnostic practices by creating\nclassification models capable of identifying diseases. However, these\nclassification problems often suffer from significant class imbalances, which\ncan inhibit the effectiveness of traditional models. Therefore, the interest in\nQuantum models has arisen, driven by the captivating promise of overcoming the\nlimitations of the classical counterpart thanks to their ability to express\ncomplex patterns by mapping data in a higher-dimensional computational space.\n","authors":["Antonio Tudisco","Deborah Volpe","Giovanna Turvani"],"pdf_url":"https://arxiv.org/pdf/2505.20797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.19093v2","updated":"2025-08-04T07:16:52Z","published":"2025-07-25T09:23:04Z","title":"Graph Neural Network-Based Predictor for Optimal Quantum Hardware\n  Selection","summary":"  The growing variety of quantum hardware technologies, each with unique\npeculiarities such as connectivity and native gate sets, creates challenges\nwhen selecting the best platform for executing a specific quantum circuit. This\nselection process usually involves a brute-force approach: compiling the\ncircuit on various devices and evaluating performance based on factors such as\ncircuit depth and gate fidelity. However, this method is computationally\nexpensive and does not scale well as the number of available quantum processors\nincreases. In this work, we propose a Graph Neural Network (GNN)-based\npredictor that automates hardware selection by analyzing the Directed Acyclic\nGraph (DAG) representation of a quantum circuit. Our study evaluates 498\nquantum circuits (up to 27 qubits) from the MQT Bench dataset, compiled using\nQiskit on four devices: three superconducting quantum processors (IBM-Kyiv,\nIBM-Brisbane, IBM-Sherbrooke) and one trapped-ion processor (IONQ-Forte).\nPerformance is estimated using a metric that integrates circuit depth and gate\nfidelity, resulting in a dataset where 93 circuits are optimally compiled on\nthe trapped-ion device, while the remaining circuits prefer superconducting\nplatforms. By exploiting graph-based machine learning, our approach avoids\nextracting the circuit features for the model evaluation but directly embeds it\nas a graph, significantly accelerating the optimal target decision-making\nprocess and maintaining all the information. Experimental results prove 94.4%\naccuracy and an 85.5% F1 score for the minority class, effectively predicting\nthe best compilation target. The developed code is publicly available on GitHub\n(https://github.com/antotu/GNN-Model-Quantum-Predictor).\n","authors":["Antonio Tudisco","Deborah Volpe","Giacomo Orlandi","Giovanna Turvani"],"pdf_url":"https://arxiv.org/pdf/2507.19093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02126v1","updated":"2025-08-04T07:15:57Z","published":"2025-08-04T07:15:57Z","title":"Understanding Learning Dynamics Through Structured Representations","summary":"  While modern deep networks have demonstrated remarkable versatility, their\ntraining dynamics remain poorly understood--often driven more by empirical\ntweaks than architectural insight. This paper investigates how internal\nstructural choices shape the behavior of learning systems. Building on prior\nefforts that introduced simple architectural constraints, we explore the\nbroader implications of structure for convergence, generalization, and\nadaptation. Our approach centers on a family of enriched transformation layers\nthat incorporate constrained pathways and adaptive corrections. We analyze how\nthese structures influence gradient flow, spectral sensitivity, and fixed-point\nbehavior--uncovering mechanisms that contribute to training stability and\nrepresentational regularity. Theoretical analysis is paired with empirical\nstudies on synthetic and structured tasks, demonstrating improved robustness,\nsmoother optimization, and scalable depth behavior. Rather than prescribing\nfixed templates, we emphasize principles of tractable design that can steer\nlearning behavior in interpretable ways. Our findings support a growing view\nthat architectural design is not merely a matter of performance tuning, but a\ncritical axis for shaping learning dynamics in scalable and trustworthy neural\nsystems.\n","authors":["Saleh Nikooroo","Thomas Engel"],"pdf_url":"https://arxiv.org/pdf/2508.02126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02124v1","updated":"2025-08-04T07:05:15Z","published":"2025-08-04T07:05:15Z","title":"Trainable Dynamic Mask Sparse Attention","summary":"  In large language models, the demand for modeling long contexts is constantly\nincreasing, but the quadratic complexity of the standard self-attention\nmechanism often becomes a bottleneck. Although existing sparse attention\nmechanisms have improved efficiency, they may still encounter issues such as\nstatic patterns or information loss. We introduce a trainable dynamic mask\nsparse attention mechanism, Dynamic Mask Attention, which effectively utilizes\ncontent-aware and position-aware sparsity. DMA achieves this through two key\ninnovations: First, it dynamically generates content-aware sparse masks from\nvalue representations, enabling the model to identify and focus on critical\ninformation adaptively. Second, it implements position-aware sparse attention\ncomputation that effectively skips unnecessary calculation regions. This\ndual-sparsity design allows the model to significantly reduce the computational\ncomplexity of important information while retaining complete information,\nachieving an excellent balance between information fidelity and computational\nefficiency. We have verified the performance of DMA through comprehensive\nexperiments. Comparative studies show that DMA outperforms multi-head\nattention, sliding window attention, multi-head latent attention, and native\nsparse attention in terms of perplexity under Chinchilla Scaling Law settings.\nMoreover, in challenging multi-query associative recall tasks, DMA also\ndemonstrates superior performance and efficiency compared to these methods.\nCrucially, in the evaluation of a 1.7B parameter model, DMA significantly\noutperforms multi-head attention in both standard benchmark performance and the\nchallenging needle-in-a-haystack task. These experimental results highlight its\ncapability to balance model efficiency and long-context modeling ability\neffectively.\n","authors":["Jingze Shi","Yifan Wu","Bingheng Wu","Yiran Peng","Liangdong Wang","Guang Liu","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2508.02124v1.pdf","comment":"8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2508.02123v1","updated":"2025-08-04T07:04:58Z","published":"2025-08-04T07:04:58Z","title":"Understanding the Essence: Delving into Annotator Prototype Learning for\n  Multi-Class Annotation Aggregation","summary":"  Multi-class classification annotations have significantly advanced AI\napplications, with truth inference serving as a critical technique for\naggregating noisy and biased annotations. Existing state-of-the-art methods\ntypically model each annotator's expertise using a confusion matrix. However,\nthese methods suffer from two widely recognized issues: 1) when most annotators\nlabel only a few tasks, or when classes are imbalanced, the estimated confusion\nmatrices are unreliable, and 2) a single confusion matrix often remains\ninadequate for capturing each annotator's full expertise patterns across all\ntasks. To address these issues, we propose a novel confusion-matrix-based\nmethod, PTBCC (ProtoType learning-driven Bayesian Classifier Combination), to\nintroduce a reliable and richer annotator estimation by prototype learning.\nSpecifically, we assume that there exists a set $S$ of prototype confusion\nmatrices, which capture the inherent expertise patterns of all annotators.\nRather than a single confusion matrix, the expertise per annotator is extended\nas a Dirichlet prior distribution over these prototypes. This prototype\nlearning-driven mechanism circumvents the data sparsity and class imbalance\nissues, ensuring a richer and more flexible characterization of annotators.\nExtensive experiments on 11 real-world datasets demonstrate that PTBCC achieves\nup to a 15% accuracy improvement in the best case, and a 3% higher average\naccuracy while reducing computational cost by over 90%.\n","authors":["Ju Chen","Jun Feng","Shenyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.02123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13930v3","updated":"2025-08-04T07:02:04Z","published":"2024-06-20T01:55:08Z","title":"ME-IGM: Individual-Global-Max in Maximum Entropy Multi-Agent\n  Reinforcement Learning","summary":"  Multi-agent credit assignment is a fundamental challenge for cooperative\nmulti-agent reinforcement learning (MARL), where a team of agents learn from\nshared reward signals. The Individual-Global-Max (IGM) condition is a widely\nused principle for multi-agent credit assignment, requiring that the joint\naction determined by individual Q-functions maximizes the global Q-value.\nMeanwhile, the principle of maximum entropy has been leveraged to enhance\nexploration in MARL. However, we identify a critical limitation in existing\nmaximum entropy MARL methods: a misalignment arises between local policies and\nthe joint policy that maximizes the global Q-value, leading to violations of\nthe IGM condition. To address this misalignment, we propose an order-preserving\ntransformation. Building on it, we introduce ME-IGM, a novel maximum entropy\nMARL algorithm compatible with any credit assignment mechanism that satisfies\nthe IGM condition while enjoying the benefits of maximum entropy exploration.\nWe empirically evaluate two variants of ME-IGM: ME-QMIX and ME-QPLEX, in\nnon-monotonic matrix games, and demonstrate their state-of-the-art performance\nacross 17 scenarios in SMAC-v2 and Overcooked.\n","authors":["Wen-Tse Chen","Yuxuan Li","Shiyu Huang","Jiayu Chen","Jeff Schneider"],"pdf_url":"https://arxiv.org/pdf/2406.13930v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02109v1","updated":"2025-08-04T06:37:39Z","published":"2025-08-04T06:37:39Z","title":"Real-Time Conflict Prediction for Large Truck Merging in Mixed Traffic\n  at Work Zone Lane Closures","summary":"  Large trucks substantially contribute to work zone-related crashes, primarily\ndue to their large size and blind spots. When approaching a work zone, large\ntrucks often need to merge into an adjacent lane because of lane closures\ncaused by construction activities. This study aims to enhance the safety of\nlarge truck merging maneuvers in work zones by evaluating the risk associated\nwith merging conflicts and establishing a decision-making strategy for merging\nbased on this risk assessment. To predict the risk of large trucks merging into\na mixed traffic stream within a work zone, a Long Short-Term Memory (LSTM)\nneural network is employed. For a large truck intending to merge, it is\ncritical that the immediate downstream vehicle in the target lane maintains a\nminimum safe gap to facilitate a safe merging process. Once a conflict-free\nmerging opportunity is predicted, large trucks are instructed to merge in\nresponse to the lane closure. Our LSTM-based conflict prediction method is\ncompared against baseline approaches, which include probabilistic risk-based\nmerging, 50th percentile gap-based merging, and 85th percentile gap-based\nmerging strategies. The results demonstrate that our method yields a lower\nconflict risk, as indicated by reduced Time Exposed Time-to-Collision (TET) and\nTime Integrated Time-to-Collision (TIT) values relative to the baseline models.\nFurthermore, the findings indicate that large trucks that use our method can\nperform early merging while still in motion, as opposed to coming to a complete\nstop at the end of the current lane prior to closure, which is commonly\nobserved with the baseline approaches.\n","authors":["Abyad Enan","Abdullah Al Mamun","Gurcan Comert","Debbie Aisiana Indah","Judith Mwakalonge","Amy W. Apon","Mashrur Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2508.02109v1.pdf","comment":"This work has been submitted to the Transportation Research Record:\n  Journal of the Transportation Research Board for possible publication"},{"id":"http://arxiv.org/abs/2504.16628v2","updated":"2025-08-04T06:34:17Z","published":"2025-04-23T11:35:57Z","title":"ParetoHqD: Fast Offline Multiobjective Alignment of Large Language\n  Models using Pareto High-quality Data","summary":"  Aligning large language models with multiple human expectations and values is\ncrucial for ensuring that they adequately serve a variety of user needs. To\nthis end, offline multiobjective alignment algorithms such as the\nRewards-in-Context algorithm have shown strong performance and efficiency.\nHowever, inappropriate preference representations and training with imbalanced\nreward scores limit the performance of such algorithms. In this work, we\nintroduce ParetoHqD that addresses the above issues by representing human\npreferences as preference directions in the objective space and regarding data\nnear the Pareto front as ''high-quality'' data. For each preference, ParetoHqD\nfollows a two-stage supervised fine-tuning process, where each stage uses an\nindividual Pareto high-quality training set that best matches its preference\ndirection. The experimental results have demonstrated the superiority of\nParetoHqD over five baselines on two multiobjective alignment tasks.\n","authors":["Haoran Gu","Handing Wang","Yi Mei","Mengjie Zhang","Yaochu Jin"],"pdf_url":"https://arxiv.org/pdf/2504.16628v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02103v1","updated":"2025-08-04T06:25:45Z","published":"2025-08-04T06:25:45Z","title":"Instance-Dependent Continuous-Time Reinforcement Learning via Maximum\n  Likelihood Estimation","summary":"  Continuous-time reinforcement learning (CTRL) provides a natural framework\nfor sequential decision-making in dynamic environments where interactions\nevolve continuously over time. While CTRL has shown growing empirical success,\nits ability to adapt to varying levels of problem difficulty remains poorly\nunderstood. In this work, we investigate the instance-dependent behavior of\nCTRL and introduce a simple, model-based algorithm built on maximum likelihood\nestimation (MLE) with a general function approximator. Unlike existing\napproaches that estimate system dynamics directly, our method estimates the\nstate marginal density to guide learning. We establish instance-dependent\nperformance guarantees by deriving a regret bound that scales with the total\nreward variance and measurement resolution. Notably, the regret becomes\nindependent of the specific measurement strategy when the observation frequency\nadapts appropriately to the problem's complexity. To further improve\nperformance, our algorithm incorporates a randomized measurement schedule that\nenhances sample efficiency without increasing measurement cost. These results\nhighlight a new direction for designing CTRL algorithms that automatically\nadjust their learning behavior based on the underlying difficulty of the\nenvironment.\n","authors":["Runze Zhao","Yue Yu","Ruhan Wang","Chunfeng Huang","Dongruo Zhou"],"pdf_url":"https://arxiv.org/pdf/2508.02103v1.pdf","comment":"32 pages, 3 figures, 1 table. The first two authors contributed\n  equally"},{"id":"http://arxiv.org/abs/2409.00032v2","updated":"2025-08-04T06:14:54Z","published":"2024-08-17T14:10:41Z","title":"ADformer: A Multi-Granularity Spatial-Temporal Transformer for EEG-Based\n  Alzheimer Detection","summary":"  Electroencephalography (EEG) has emerged as a cost-effective and efficient\ntool to support neurologists in the detection of Alzheimer's Disease (AD).\nHowever, most existing approaches rely heavily on manual feature engineering or\ndata transformation. While such techniques may provide benefits when working\nwith small-scale datasets, they often lead to information loss and distortion\nwhen applied to large-scale data, ultimately limiting model performance.\nMoreover, the limited subject scale and demographic diversity of datasets used\nin prior studies hinder comprehensive evaluation of model robustness and\ngeneralizability, thus restricting their applicability in real-world clinical\nsettings. To address these challenges, we propose ADformer, a novel\nmulti-granularity spatial-temporal transformer designed to capture both\ntemporal and spatial features from raw EEG signals, enabling effective\nend-to-end representation learning. Our model introduces multi-granularity\nembedding strategies across both spatial and temporal dimensions, leveraging a\ntwo-stage intra-inter granularity self-attention mechanism to learn both local\npatterns within each granularity and global dependencies across granularities.\nWe evaluate ADformer on 4 large-scale datasets comprising a total of 1,713\nsubjects, representing one of the largest corpora for EEG-based AD detection to\ndate, under a cross-validated, subject-independent setting. Experimental\nresults demonstrate that ADformer consistently outperforms existing methods,\nachieving subject-level F1 scores of 92.82%, 89.83%, 67.99%, and 83.98% on the\n4 datasets, respectively, in distinguishing AD from healthy control (HC)\nsubjects.\n","authors":["Yihe Wang","Nadia Mammone","Darina Petrovsky","Alexandros T. Tzallas","Francesco C. Morabito","Xiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.00032v2.pdf","comment":"This work will be submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2505.16326v2","updated":"2025-08-04T06:14:13Z","published":"2025-05-22T07:32:17Z","title":"ChemMLLM: Chemical Multimodal Large Language Model","summary":"  Multimodal large language models (MLLMs) have made impressive progress in\nmany applications in recent years. However, chemical MLLMs that can handle\ncross-modal understanding and generation remain underexplored. To fill this\ngap, we propose ChemMLLM, a unified chemical multimodal large language model\nfor molecule understanding and generation. Also, we design five multimodal\ntasks across text, molecular SMILES strings, and image, and curate the\ndatasets. We benchmark ChemMLLM against a range of general leading MLLMs and\nChemical LLMs on these tasks. Experimental results show that ChemMLLM achieves\nsuperior performance across all evaluated tasks. For example, in molecule image\noptimization task, ChemMLLM outperforms the best baseline (GPT-4o) by 116.75\\%\n(4.27 vs 1.97 property improvement). The code is publicly available at\nhttps://github.com/bbsbz/ChemMLLM.git.\n","authors":["Qian Tan","Dongzhan Zhou","Peng Xia","Wanhao Liu","Wanli Ouyang","Lei Bai","Yuqiang Li","Tianfan Fu"],"pdf_url":"https://arxiv.org/pdf/2505.16326v2.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2502.14037v3","updated":"2025-08-04T06:13:07Z","published":"2025-02-19T19:00:02Z","title":"DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation","summary":"  Despite their growing capabilities, language models still frequently\nreproduce content from their training data, generate repetitive text, and favor\ncommon grammatical patterns and vocabulary. A possible cause is the decoding\nstrategy: the most common strategies either consider only the most probable\ntokens, which reduces output diversity, or increase the likelihood of unlikely\ntokens, compromising output accuracy and correctness. In this paper, we propose\nDiffSampling, a new decoding method that leverages a mathematical analysis of\nthe token probability distribution to ensure the generation of contextually\nappropriate text. In particular, the difference between consecutive, sorted\nprobabilities can be used to truncate incorrect tokens. In addition, we also\npropose two variations of the proposed method that aim to correct the subtle\ninconsistencies of common sampling strategies. Experiments involving four\ndifferent text-generation tasks demonstrate that our approach consistently\nperforms at least on par with the existing methods it builds upon in terms of\nquality, while potentially improving output diversity.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2502.14037v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02091v1","updated":"2025-08-04T05:57:46Z","published":"2025-08-04T05:57:46Z","title":"CRINN: Contrastive Reinforcement Learning for Approximate Nearest\n  Neighbor Search","summary":"  Approximate nearest-neighbor search (ANNS) algorithms have become\nincreasingly critical for recent AI applications, particularly in\nretrieval-augmented generation (RAG) and agent-based LLM applications. In this\npaper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS\noptimization as a reinforcement learning problem where execution speed serves\nas the reward signal. This approach enables the automatic generation of\nprogressively faster ANNS implementations while maintaining accuracy\nconstraints. Our experimental evaluation demonstrates CRINN's effectiveness\nacross six widely-used NNS benchmark datasets. When compared against\nstate-of-the-art open-source ANNS algorithms, CRINN achieves best performance\non three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and\nGloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean\nand GloVe-25-angular). The implications of CRINN's success reach well beyond\nANNS optimization: It validates that LLMs augmented with reinforcement learning\ncan function as an effective tool for automating sophisticated algorithmic\noptimizations that demand specialized knowledge and labor-intensive manual\nrefinement.Code can be found at https://github.com/deepreinforce-ai/CRINN\n","authors":["Xiaoya Li","Xiaofei Sun","Albert Wang","Chris Shum","Jiwei Li"],"pdf_url":"https://arxiv.org/pdf/2508.02091v1.pdf","comment":"Preprint Version"},{"id":"http://arxiv.org/abs/2504.08817v2","updated":"2025-08-04T05:55:53Z","published":"2025-04-09T11:15:21Z","title":"Exploring utilization of generative AI for research and education in\n  data-driven materials science","summary":"  Generative AI has recently had a profound impact on various fields, including\ndaily life, research, and education. To explore its efficient utilization in\ndata-driven materials science, we organized a hackathon -- AIMHack2024 -- in\nJuly 2024. In this hackathon, researchers from fields such as materials\nscience, information science, bioinformatics, and condensed matter physics\nworked together to explore how generative AI can facilitate research and\neducation. Based on the results of the hackathon, this paper presents topics\nrelated to (1) conducting AI-assisted software trials, (2) building AI tutors\nfor software, and (3) developing GUI applications for software. While\ngenerative AI continues to evolve rapidly, this paper provides an early record\nof its application in data-driven materials science and highlights strategies\nfor integrating AI into research and education.\n","authors":["Takahiro Misawa","Ai Koizumi","Ryo Tamura","Kazuyoshi Yoshimi"],"pdf_url":"https://arxiv.org/pdf/2504.08817v2.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2508.02080v1","updated":"2025-08-04T05:45:52Z","published":"2025-08-04T05:45:52Z","title":"The Geometry of Machine Learning Models","summary":"  This paper presents a mathematical framework for analyzing machine learning\n  models through the geometry of their induced partitions. By representing\n  partitions as Riemannian simplicial complexes, we capture not only adjacency\n  relationships but also geometric properties including cell volumes, volumes\nof\n  faces where cells meet, and dihedral angles between adjacent cells. For\nneural\n  networks, we introduce a differential forms approach that tracks geometric\n  structure through layers via pullback operations, making computations\n  tractable by focusing on data-containing cells. The framework enables\n  geometric regularization that directly penalizes problematic spatial\n  configurations and provides new tools for model refinement through extended\n  Laplacians and simplicial splines. We also explore how data distribution\n  induces effective geometric curvature in model partitions, developing\ndiscrete\n  curvature measures for vertices that quantify local geometric complexity and\n  statistical Ricci curvature for edges that captures pairwise relationships\n  between cells. While focused on mathematical foundations, this geometric\n  perspective offers new approaches to model interpretation, regularization,\nand\n  diagnostic tools for understanding learning dynamics.\n","authors":["Pawel Gajer","Jacques Ravel"],"pdf_url":"https://arxiv.org/pdf/2508.02080v1.pdf","comment":"61 pages, 1 figure"}],"Multimedia":[{"id":"http://arxiv.org/abs/2506.22967v2","updated":"2025-08-04T16:54:44Z","published":"2025-06-28T17:57:58Z","title":"ActAlign: Zero-Shot Fine-Grained Video Classification via\n  Language-Guided Sequence Alignment","summary":"  We address the task of zero-shot video classification for extremely\nfine-grained actions (e.g., Windmill Dunk in basketball), where no video\nexamples or temporal annotations are available for unseen classes. While\nimage-language models (e.g., CLIP, SigLIP) show strong open-set recognition,\nthey lack temporal modeling needed for video understanding. We propose\nActAlign, a truly zero-shot, training-free method that formulates video\nclassification as a sequence alignment problem, preserving the generalization\nstrength of pretrained image-language models. For each class, a large language\nmodel (LLM) generates an ordered sequence of sub-actions, which we align with\nvideo frames using Dynamic Time Warping (DTW) in a shared embedding space.\nWithout any video-text supervision or fine-tuning, ActAlign achieves 30.5%\naccuracy on ActionAtlas--the most diverse benchmark of fine-grained actions\nacross multiple sports--where human performance is only 61.6%. ActAlign\noutperforms billion-parameter video-language models while using 8x fewer\nparameters. Our approach is model-agnostic and domain-general, demonstrating\nthat structured language priors combined with classical alignment methods can\nunlock the open-set recognition potential of image-language models for\nfine-grained video understanding.\n","authors":["Amir Aghdam","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2506.22967v2.pdf","comment":"Preprint manuscript - Project page:\n  https://amir-aghdam.github.io/act-align/"},{"id":"http://arxiv.org/abs/2508.00748v2","updated":"2025-08-04T12:27:33Z","published":"2025-08-01T16:23:27Z","title":"Is It Really You? Exploring Biometric Verification Scenarios in\n  Photorealistic Talking-Head Avatar Videos","summary":"  Photorealistic talking-head avatars are becoming increasingly common in\nvirtual meetings, gaming, and social platforms. These avatars allow for more\nimmersive communication, but they also introduce serious security risks. One\nemerging threat is impersonation: an attacker can steal a user's avatar,\npreserving his appearance and voice, making it nearly impossible to detect its\nfraudulent usage by sight or sound alone. In this paper, we explore the\nchallenge of biometric verification in such avatar-mediated scenarios. Our main\nquestion is whether an individual's facial motion patterns can serve as\nreliable behavioral biometrics to verify their identity when the avatar's\nvisual appearance is a facsimile of its owner. To answer this question, we\nintroduce a new dataset of realistic avatar videos created using a\nstate-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and\nimpostor avatar videos. We also propose a lightweight, explainable\nspatio-temporal Graph Convolutional Network architecture with temporal\nattention pooling, that uses only facial landmarks to model dynamic facial\ngestures. Experimental results demonstrate that facial motion cues enable\nmeaningful identity verification with AUC values approaching 80%. The proposed\nbenchmark and biometric system are available for the research community in\norder to bring attention to the urgent need for more advanced behavioral\nbiometric defenses in avatar-based communication systems.\n","authors":["Laura Pedrouzo-Rodriguez","Pedro Delgado-DeRobles","Luis F. Gomez","Ruben Tolosana","Ruben Vera-Rodriguez","Aythami Morales","Julian Fierrez"],"pdf_url":"https://arxiv.org/pdf/2508.00748v2.pdf","comment":"Accepted at the IEEE International Joint Conference on Biometrics\n  (IJCB 2025)"},{"id":"http://arxiv.org/abs/2508.02340v1","updated":"2025-08-04T12:21:16Z","published":"2025-08-04T12:21:16Z","title":"Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search","summary":"  Ad-hoc Video Search (AVS) involves using a textual query to search for\nmultiple relevant videos in a large collection of unlabeled short videos. The\nmain challenge of AVS is the visual diversity of relevant videos. A simple\nquery such as \"Find shots of a man and a woman dancing together indoors\" can\nspan a multitude of environments, from brightly lit halls and shadowy bars to\ndance scenes in black-and-white animations. It is therefore essential to\nretrieve relevant videos as comprehensively as possible. Current solutions for\nthe AVS task primarily fuse multiple features into one or more common spaces,\nyet overlook the need for diverse spaces. To fully exploit the expressive\ncapability of individual features, we propose LPD, short for Learning Partially\nDecorrelated common spaces. LPD incorporates two key innovations:\nfeature-specific common space construction and the de-correlation loss.\nSpecifically, LPD learns a separate common space for each video and text\nfeature, and employs de-correlation loss to diversify the ordering of negative\nsamples across different spaces. To enhance the consistency of multi-space\nconvergence, we designed an entropy-based fair multi-space triplet ranking\nloss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify\nthe effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces\nhighlight its ability to enhance result diversity.\n","authors":["Fan Hu","Zijie Xin","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2508.02340v1.pdf","comment":"Accepted by ACMMM2025"},{"id":"http://arxiv.org/abs/2507.22676v2","updated":"2025-08-04T11:22:43Z","published":"2025-07-30T13:37:06Z","title":"Listening to the Unspoken: Exploring \"365\" Aspects of Multimodal\n  Interview Performance Assessment","summary":"  Interview performance assessment is essential for determining candidates'\nsuitability for professional positions. To ensure holistic and fair\nevaluations, we propose a novel and comprehensive framework that explores\n``365'' aspects of interview performance by integrating \\textit{three}\nmodalities (video, audio, and text), \\textit{six} responses per candidate, and\n\\textit{five} key evaluation dimensions. The framework employs\nmodality-specific feature extractors to encode heterogeneous data streams and\nsubsequently fused via a Shared Compression Multilayer Perceptron. This module\ncompresses multimodal embeddings into a unified latent space, facilitating\nefficient feature interaction. To enhance prediction robustness, we incorporate\na two-level ensemble learning strategy: (1) independent regression heads\npredict scores for each response, and (2) predictions are aggregated across\nresponses using a mean-pooling mechanism to produce final scores for the five\ntarget dimensions. By listening to the unspoken, our approach captures both\nexplicit and implicit cues from multimodal data, enabling comprehensive and\nunbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our\nframework secured first place in the AVI Challenge 2025, demonstrating its\neffectiveness and robustness in advancing automated and multimodal interview\nperformance assessment. The full implementation is available at\nhttps://github.com/MSA-LMC/365Aspects.\n","authors":["Jia Li","Yang Wang","Wenhao Qian","Zhenzhen Hu","Richang Hong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2507.22676v2.pdf","comment":"8 pages, 4 figures, ACM MM 2025.\n  github:https://github.com/MSA-LMC/365Aspects"},{"id":"http://arxiv.org/abs/2409.06690v3","updated":"2025-08-04T10:54:15Z","published":"2024-09-10T17:54:00Z","title":"Benchmarking Sub-Genre Classification For Mainstage Dance Music","summary":"  Music classification, a cornerstone of music information retrieval, supports\na wide array of applications. To address the lack of comprehensive datasets and\neffective methods for sub-genre classification in mainstage dance music, we\nintroduce a novel benchmark featuring a new dataset and baseline. Our dataset\nexpands the scope of sub-genres to reflect the diversity of recent mainstage\nlive sets performed by leading DJs at global music festivals, capturing the\nvibrant and rapidly evolving electronic dance music (EDM) scene that engages\nmillions of fans worldwide. We employ a continuous soft labeling approach to\naccommodate tracks blending multiple sub-genres, preserving their inherent\ncomplexity. Experiments demonstrate that even state-of-the-art multimodal large\nlanguage models (MLLMs) struggle with this task, while our specialized baseline\nmodels achieve high accuracy. This benchmark supports applications such as\nmusic recommendation, DJ set curation, and interactive multimedia systems, with\nvideo demos provided. Our code and data are all open-sourced at\nhttps://github.com/Gariscat/housex-v2.git.\n","authors":["Hongzhi Shu","Xinglin Li","Hongyu Jiang","Minghao Fu","Xinyu Li"],"pdf_url":"https://arxiv.org/pdf/2409.06690v3.pdf","comment":"WASPAA 2025"},{"id":"http://arxiv.org/abs/2412.17632v3","updated":"2025-08-04T10:47:24Z","published":"2024-12-23T15:08:08Z","title":"D-Judge: How Far Are We? Evaluating the Discrepancies Between\n  AI-synthesized Images and Natural Images through Multimodal Guidance","summary":"  In the rapidly evolving field of Artificial Intelligence Generated Content\n(AIGC), a central challenge is distinguishing AI-synthesized images from\nnatural images. Despite the impressive capabilities of advanced AI generative\nmodels in producing visually compelling content, significant discrepancies\nremain when compared to natural images. To systematically investigate and\nquantify these differences, we construct a large-scale multimodal dataset named\nDANI, comprising 5,000 natural images and over 440,000 AI-generated image\n(AIGI) samples produced by nine representative models using both unimodal and\nmultimodal prompts, including Text-to-Image (T2I), Image-to-Image (I2I), and\nText and Image-to-Image (TI2I). We then introduce D-Judge, a benchmark designed\nto answer the critical question: how far are AI-generated images from truly\nrealistic images? Our fine-grained evaluation framework assesses DANI across\nfive key dimensions: naive visual quality, semantic alignment, aesthetic\nappeal, downstream task applicability, and coordinated human validation.\nExtensive experiments reveal substantial discrepancies across these dimensions,\nhighlighting the importance of aligning quantitative metrics with human\njudgment to achieve a comprehensive understanding of AI-generated image\nquality. The code and dataset are publicly available at:\nhttps://github.com/ryliu68/DJudge and\nhttps://huggingface.co/datasets/Renyang/DANI.\n","authors":["Renyang Liu","Ziyu Lyu","Wei Zhou","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2412.17632v3.pdf","comment":"Accepted by ACM MM 2025"},{"id":"http://arxiv.org/abs/2506.10857v2","updated":"2025-08-04T09:11:48Z","published":"2025-06-12T16:17:17Z","title":"VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos","summary":"  We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 960 long videos (with an average duration of\n1.6 hours), along with 8,243 human-labeled multi-step question-answering pairs\nand 25,106 reasoning steps with timestamps. These videos are curated via a\nmulti-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 19 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.\n","authors":["Jiashuo Yu","Yue Wu","Meng Chu","Zhifei Ren","Zizheng Huang","Pei Chu","Ruijie Zhang","Yinan He","Qirui Li","Songze Li","Zhenxiang Li","Zhongying Tu","Conghui He","Yu Qiao","Yali Wang","Yi Wang","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2506.10857v2.pdf","comment":"ICCV2025"},{"id":"http://arxiv.org/abs/2508.00733v2","updated":"2025-08-04T09:11:26Z","published":"2025-08-01T16:03:57Z","title":"AudioGen-Omni: A Unified Multimodal Diffusion Transformer for\n  Video-Synchronized Audio, Speech, and Song Generation","summary":"  We present AudioGen-Omni - a unified approach based on multimodal diffusion\ntransformers (MMDit), capable of generating high-fidelity audio, speech, and\nsongs coherently synchronized with the input video. AudioGen-Omni introduces a\nnovel joint training paradigm that seamlessly integrates large-scale\nvideo-text-audio corpora, enabling a model capable of generating semantically\nrich, acoustically diverse audio conditioned on multimodal inputs and adaptable\nto a wide range of audio generation tasks. AudioGen-Omni employs a unified\nlyrics-transcription encoder that encodes graphemes and phonemes from both sung\nand spoken inputs into dense frame-level representations. Dense frame-level\nrepresentations are fused using an AdaLN-based joint attention mechanism\nenhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein\nRoPE is selectively applied to temporally structured modalities to ensure\nprecise and robust cross-modal alignment. By unfreezing all modalities and\nmasking missing inputs, AudioGen-Omni mitigates the semantic constraints of\ntext-frozen paradigms, enabling effective cross-modal conditioning. This joint\ntraining approach enhances audio quality, semantic alignment, and lip-sync\naccuracy, while also achieving state-of-the-art results on\nText-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8\nseconds of audio, it offers substantial improvements in both efficiency and\ngenerality.\n","authors":["Le Wang","Jun Wang","Feng Deng","Chen Zhang","Di Zhang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2508.00733v2.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2508.02172v1","updated":"2025-08-04T08:12:44Z","published":"2025-08-04T08:12:44Z","title":"GaussianCross: Cross-modal Self-supervised 3D Representation Learning\n  via Gaussian Splatting","summary":"  The significance of informative and robust point representations has been\nwidely acknowledged for 3D scene understanding. Despite existing\nself-supervised pre-training counterparts demonstrating promising performance,\nthe model collapse and structural information deficiency remain prevalent due\nto insufficient point discrimination difficulty, yielding unreliable\nexpressions and suboptimal performance. In this paper, we present\nGaussianCross, a novel cross-modal self-supervised 3D representation learning\narchitecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques\nto address current challenges. GaussianCross seamlessly converts\nscale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian\nrepresentation without missing details, enabling stable and generalizable\npre-training. Subsequently, a tri-attribute adaptive distillation splatting\nmodule is incorporated to construct a 3D feature field, facilitating synergetic\nfeature capturing of appearance, geometry, and semantic cues to maintain\ncross-modal consistency. To validate GaussianCross, we perform extensive\nevaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In\nparticular, GaussianCross shows a prominent parameter and data efficiency,\nachieving superior performance through linear probing (<0.1% parameters) and\nlimited data training (1% of scenes) compared to state-of-the-art methods.\nFurthermore, GaussianCross demonstrates strong generalization capabilities,\nimproving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on\nScanNet200 semantic and instance segmentation tasks, respectively, supporting\nthe effectiveness of our approach. The code, weights, and visualizations are\npublicly available at\n\\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.\n","authors":["Lei Yao","Yi Wang","Yi Zhang","Moyun Liu","Lap-Pui Chau"],"pdf_url":"https://arxiv.org/pdf/2508.02172v1.pdf","comment":"14 pages, 8 figures, accepted by MM'25"},{"id":"http://arxiv.org/abs/2508.01980v1","updated":"2025-08-04T01:39:09Z","published":"2025-08-04T01:39:09Z","title":"On-the-Fly Object-aware Representative Point Selection in Point Cloud","summary":"  Point clouds are essential for object modeling and play a critical role in\nassisting driving tasks for autonomous vehicles (AVs). However, the significant\nvolume of data generated by AVs creates challenges for storage, bandwidth, and\nprocessing cost. To tackle these challenges, we propose a representative point\nselection framework for point cloud downsampling, which preserves critical\nobject-related information while effectively filtering out irrelevant\nbackground points. Our method involves two steps: (1) Object Presence\nDetection, where we introduce an unsupervised density peak-based classifier and\na supervised Na\\\"ive Bayes classifier to handle diverse scenarios, and (2)\nSampling Budget Allocation, where we propose a strategy that selects\nobject-relevant points while maintaining a high retention rate of object\ninformation. Extensive experiments on the KITTI and nuScenes datasets\ndemonstrate that our method consistently outperforms state-of-the-art baselines\nin both efficiency and effectiveness across varying sampling rates. As a\nmodel-agnostic solution, our approach integrates seamlessly with diverse\ndownstream models, making it a valuable and scalable addition to the 3D point\ncloud downsampling toolkit for AV applications.\n","authors":["Xiaoyu Zhang","Ziwei Wang","Hai Dong","Zhifeng Bao","Jiajun Liu"],"pdf_url":"https://arxiv.org/pdf/2508.01980v1.pdf","comment":null}]},"2025-08-03T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2508.01959v1","updated":"2025-08-03T23:59:31Z","published":"2025-08-03T23:59:31Z","title":"SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic\n  Association and Long Story Comprehension","summary":"  Retrieval-augmented generation (RAG) over long documents typically involves\nsplitting the text into smaller chunks, which serve as the basic units for\nretrieval. However, due to dependencies across the original document,\ncontextual information is often essential for accurately interpreting each\nchunk. To address this, prior work has explored encoding longer context windows\nto produce embeddings for longer chunks. Despite these efforts, gains in\nretrieval and downstream tasks remain limited. This is because (1) longer\nchunks strain the capacity of embedding models due to the increased amount of\ninformation they must encode, and (2) many real-world applications still\nrequire returning localized evidence due to constraints on model or human\nbandwidth.\n  We propose an alternative approach to this challenge by representing short\nchunks in a way that is conditioned on a broader context window to enhance\nretrieval performance -- i.e., situating a chunk's meaning within its context.\nWe further show that existing embedding models are not well-equipped to encode\nsuch situated context effectively, and thus introduce a new training paradigm\nand develop the situated embedding models (SitEmb). To evaluate our method, we\ncurate a book-plot retrieval dataset specifically designed to assess situated\nretrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3\nsubstantially outperforms state-of-the-art embedding models, including several\nwith up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model\nfurther improves performance by over 10% and shows strong results across\ndifferent languages and several downstream applications.\n","authors":["Junjie Wu","Jiangnan Li","Yuqing Li","Lemao Liu","Liyan Xu","Jiwei Li","Dit-Yan Yeung","Jie Zhou","Mo Yu"],"pdf_url":"https://arxiv.org/pdf/2508.01959v1.pdf","comment":"Our trained models can be downloaded from:\n  https://huggingface.co/SituatedEmbedding"},{"id":"http://arxiv.org/abs/2405.17402v2","updated":"2025-08-03T22:56:22Z","published":"2024-05-27T17:51:24Z","title":"THREAD: Thinking Deeper with Recursive Spawning","summary":"  Large language models (LLMs) have shown impressive capabilities across\ndiverse settings, but still struggle as the length and complexity of the\ncontext increases. To address this challenge, we propose Thinking Recursively\nand Dynamically (ThReaD). THREAD frames model generation as a thread of\nexecution that, based on the context, can run to completion or dynamically\nspawn new threads. By spawning, threads can offload work (e.g., thinking,\nretrieving information) to child threads, which only return tokens needed for\nthe parent thread to do its work. In effect, this enables the model to adapt,\nas needed, the amount of intermediate work used to produce tokens. We apply\nTHREAD in the settings of LLM task solving and question answering, where the\ndynamic threading allows the model to recursively decompose the given task or\nquestion into progressively simpler sub-problems that can be solved by separate\nchild threads. We test THREAD, implemented using a few-shot learning approach,\non diverse benchmarks for agent tasks and data-grounded question answering.\nTHREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these\nbenchmarks, including ALFWorld, TextCraft, and WebShop, along with two new\nbenchmarks, DataCommons QA and MIMIC-III ICU QA. In addition, THREAD\noutperforms existing frameworks by 10% to 50% absolute points with smaller\nmodels, including Llama-3-8b and CodeLlama-7b.\n","authors":["Philip Schroeder","Nathaniel Morgan","Hongyin Luo","James Glass"],"pdf_url":"https://arxiv.org/pdf/2405.17402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01943v1","updated":"2025-08-03T22:33:43Z","published":"2025-08-03T22:33:43Z","title":"ROVER: Recursive Reasoning Over Videos with Vision-Language Models for\n  Embodied Tasks","summary":"  Vision-language models (VLMs) have exhibited impressive capabilities across\ndiverse image understanding tasks, but still struggle in settings that require\nreasoning over extended sequences of camera frames from a video. This limits\ntheir utility in embodied settings, which require reasoning over long frame\nsequences from a continuous stream of visual input at each moment of a task\nattempt. To address this limitation, we propose ROVER (Reasoning Over VidEo\nRecursively), a framework that enables the model to recursively decompose\nlong-horizon video trajectories into segments corresponding to shorter subtasks\nwithin the trajectory. In doing so, ROVER facilitates more focused and accurate\nreasoning over temporally localized frame sequences without losing global\ncontext. We evaluate ROVER, implemented using an in-context learning approach,\non diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa\nthat consists of 543 videos showing both expert and perturbed non-expert\ntrajectories across 27 robotic manipulation tasks. ROVER outperforms strong\nbaselines across three video reasoning tasks: task progress estimation,\nframe-level natural language reasoning, and video question answering. We\nobserve that, by reducing the number of frames the model reasons over at each\ntimestep, ROVER mitigates hallucinations, especially during unexpected or\nnon-optimal moments of a trajectory. In addition, by enabling the\nimplementation of a subtask-specific sliding context window, ROVER's time\ncomplexity scales linearly with video length, an asymptotic improvement over\nbaselines. Demos, code, and data available at: https://rover-vlm.github.io\n","authors":["Philip Schroeder","Ondrej Biza","Thomas Weng","Hongyin Luo","James Glass"],"pdf_url":"https://arxiv.org/pdf/2508.01943v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16306v3","updated":"2025-08-03T22:12:55Z","published":"2024-06-24T04:08:35Z","title":"Cascade Reward Sampling for Efficient Decoding-Time Alignment","summary":"  Aligning large language models (LLMs) with human preferences is essential for\ntheir applications. Recently, decoding-time alignment has emerged as an\neffective plug-and-play technique that avoids fine-tuning model parameters.\nThis approach retains the general utility of pretrained LLMs but often suffers\nfrom significant inefficiencies during decoding, primarily due to wasted token\ngeneration and excessive reward evaluations. To address these challenges, we\nintroduce Cascade Reward Sampling (CARDS) to resolve both efficiency\nbottlenecks in decoding-time alignment. Specifically, we develop a\nsegment-level rejection sampling algorithm that minimizes redundant\ncomputations of both LLMs and reward models (RMs). Central to CARDS is an\nuncertainty-based segmentation mechanism, which ensures the accuracy of RMs\nevaluations on incomplete segments. Furthermore, we provide a detailed analysis\nof reward scores on segments to elucidate the improved alignment performance.\nExperimental results demonstrate that CARDS significantly improves decoding\nefficiency, alignment quality, and general utility compared to existing\ndecoding-time alignment methods, achieving approximately a 70% reduction in\ndecoding time and over 90% win-ties in utility and safety benchmarks.\n","authors":["Bolian Li","Yifan Wang","Anamika Lochab","Ananth Grama","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.16306v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01930v1","updated":"2025-08-03T21:45:37Z","published":"2025-08-03T21:45:37Z","title":"Word Overuse and Alignment in Large Language Models: The Influence of\n  Learning from Human Feedback","summary":"  Large Language Models (LLMs) are known to overuse certain terms like \"delve\"\nand \"intricate.\" The exact reasons for these lexical choices, however, have\nbeen unclear. Using Meta's Llama model, this study investigates the\ncontribution of Learning from Human Feedback (LHF), under which we subsume\nReinforcement Learning from Human Feedback and Direct Preference Optimization.\nWe present a straightforward procedure for detecting the lexical preferences of\nLLMs that are potentially LHF-induced. Next, we more conclusively link LHF to\nlexical overuse by experimentally emulating the LHF procedure and demonstrating\nthat participants systematically prefer text variants that include certain\nwords. This lexical overuse can be seen as a sort of misalignment, though our\nstudy highlights the potential divergence between the lexical expectations of\ndifferent populations -- namely LHF workers versus LLM users. Our work\ncontributes to the growing body of research on explainable artificial\nintelligence and emphasizes the importance of both data and procedural\ntransparency in alignment research.\n","authors":["Tom S. Juzek","Zina B. Ward"],"pdf_url":"https://arxiv.org/pdf/2508.01930v1.pdf","comment":"Accepted for publication in the Proceedings of the 5th Workshop on\n  Bias and Fairness in AI (BIAS 2025) at ECML PKDD"},{"id":"http://arxiv.org/abs/2508.01918v1","updated":"2025-08-03T21:03:22Z","published":"2025-08-03T21:03:22Z","title":"Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and\n  Retrieval for the Punjabi Language","summary":"  Despite the rapid advancement of large language models (LLMs), low-resource\nlanguages remain largely excluded from the NLP landscape. We present PunGPT2,\nthe first fully open-source suite of Punjabi large language models, trained\nfrom scratch on a 35GB domain-diverse corpus encompassing literature, religious\ntexts, news, and social discourse. Unlike prior multilingual approaches,\nPunGPT2 captures rich syntactic and morphological features unique to Punjabi\nthrough a tokenizer optimised with byte pair encoding and linguistically\naligned pretraining objectives. To improve factual grounding and domain recall,\nwe introduce Pun-RAG, a retrieval-augmented generation framework combining\nPunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We\nfurther develop Pun-Instruct, a parameter-efficient, instruction-tuned variant\nusing QLoRA, enabling robust zero-shot and instruction-following performance\nwith significantly reduced compute needs.\n  As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system\nthat fuses sparse (BM25) and dense methods with quantum-inspired semantic\nmatching. By encoding queries using amplitude-based embeddings and retrieving\nvia quantum kernel similarity, Quantum-RAG achieves improved contextual\nrelevance with minimal memory overhead marking the first practical integration\nof quantum representations in low-resource language generation. Our models\nsignificantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in\nperplexity, factuality, and fluency. This work provides a scalable,\nreproducible blueprint for extending LLM capabilities to underrepresented\nlanguages and pioneers quantum-aware retrieval in low-resource NLP\n","authors":["Jaskaranjeet Singh","Rakesh Thakur"],"pdf_url":"https://arxiv.org/pdf/2508.01918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08642v3","updated":"2025-08-03T21:00:55Z","published":"2024-10-11T09:10:26Z","title":"More than Memes: A Multimodal Topic Modeling Approach to Conspiracy\n  Theories on Telegram","summary":"  To address the increasing prevalence of (audio-)visual data on social media,\nand to capture the evolving and dynamic nature of this communication,\nresearchers have begun to explore the potential of unsupervised approaches for\nanalyzing multimodal online content. However, existing research often neglects\nvisual content beyond memes, and in addition lacks methods to compare topic\nmodels across modalities. Our study addresses these gaps by applying multimodal\ntopic modeling for analyzing conspiracy theories in German-language Telegram\nchannels. We use BERTopic with CLIP for the analysis of textual and visual data\nin a corpus of ~40, 000 Telegram messages posted in October 2023 in 571\nGerman-language Telegram channels known for disseminating conspiracy theories.\nThrough this dataset, we provide insights into unimodal and multimodal topic\nmodels by analyzing symmetry and intersections of topics across modalities. We\ndemonstrate the variety of textual and visual content shared in the channels\ndiscovered through the topic modeling, and propose a conceptual framework for\nthe analysis of textual and visual discursive strategies in the communication\nof conspiracy theories. We apply the framework in a case study of the topic\ngroup Israel Gaza.\n","authors":["Elisabeth Steffen"],"pdf_url":"https://arxiv.org/pdf/2410.08642v3.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2508.01916v1","updated":"2025-08-03T20:59:29Z","published":"2025-08-03T20:59:29Z","title":"Decomposing Representation Space into Interpretable Subspaces with\n  Unsupervised Learning","summary":"  Understanding internal representations of neural models is a core interest of\nmechanistic interpretability. Due to its large dimensionality, the\nrepresentation space can encode various aspects about inputs. To what extent\nare different aspects organized and encoded in separate subspaces? Is it\npossible to find these ``natural'' subspaces in a purely unsupervised way?\nSomewhat surprisingly, we can indeed achieve this and find interpretable\nsubspaces by a seemingly unrelated training objective. Our method, neighbor\ndistance minimization (NDM), learns non-basis-aligned subspaces in an\nunsupervised manner. Qualitative analysis shows subspaces are interpretable in\nmany cases, and encoded information in obtained subspaces tends to share the\nsame abstract concept across different inputs, making such subspaces similar to\n``variables'' used by the model. We also conduct quantitative experiments using\nknown circuits in GPT-2; results show a strong connection between subspaces and\ncircuit variables. We also provide evidence showing scalability to 2B models by\nfinding separate subspaces mediating context and parametric knowledge routing.\nViewed more broadly, our findings offer a new perspective on understanding\nmodel internals and building circuits.\n","authors":["Xinting Huang","Michael Hahn"],"pdf_url":"https://arxiv.org/pdf/2508.01916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15933v2","updated":"2025-08-03T20:48:55Z","published":"2023-06-28T05:34:25Z","title":"You Can Generate It Again: Data-to-Text Generation with Verification and\n  Correction Prompting","summary":"  Small language models like T5 excel in generating high-quality text for\ndata-to-text tasks, offering adaptability and cost-efficiency compared to Large\nLanguage Models (LLMs). However, they frequently miss keywords, which is\nconsidered one of the most severe and common errors in this task. In this work,\nwe explore the potential of using feedback systems to enhance semantic fidelity\nin smaller language models for data-to-text generation tasks, through our\nVerification and Correction Prompting (VCP) approach. In the inference stage,\nour approach involves a multi-step process, including generation, verification,\nand regeneration stages. During the verification stage, we implement a simple\nrule to check for the presence of every keyword in the prediction. Recognizing\nthat this rule can be inaccurate, we have developed a carefully designed\ntraining procedure, which enabling the model to incorporate feedback from the\nerror-correcting prompt effectively, despite its potential inaccuracies. The\nVCP approach effectively reduces the Semantic Error Rate (SER) while\nmaintaining the text's quality.\n","authors":["Xuan Ren","Zeyu Zhang","Lingqiao Liu"],"pdf_url":"https://arxiv.org/pdf/2306.15933v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01913v1","updated":"2025-08-03T20:26:19Z","published":"2025-08-03T20:26:19Z","title":"A Decentralized Framework for Ethical Authorship Validation in Academic\n  Publishing: Leveraging Self-Sovereign Identity and Blockchain Technology","summary":"  Academic publishing, integral to knowledge dissemination and scientific\nadvancement, increasingly faces threats from unethical practices such as\nunconsented authorship, gift authorship, author ambiguity, and undisclosed\nconflicts of interest. While existing infrastructures like ORCID effectively\ndisambiguate researcher identities, they fall short in enforcing explicit\nauthorship consent, accurately verifying contributor roles, and robustly\ndetecting conflicts of interest during peer review. To address these\nshortcomings, this paper introduces a decentralized framework leveraging\nSelf-Sovereign Identity (SSI) and blockchain technology. The proposed model\nuses Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) to\nsecurely verify author identities and contributions, reducing ambiguity and\nensuring accurate attribution. A blockchain-based trust registry records\nauthorship consent and peer-review activity immutably. Privacy-preserving\ncryptographic techniques, especially Zero-Knowledge Proofs (ZKPs), support\nconflict-of-interest detection without revealing sensitive data. Verified\nauthorship metadata and consent records are embedded in publications,\nincreasing transparency. A stakeholder survey of researchers, editors, and\nreviewers suggests the framework improves ethical compliance and confidence in\nscholarly communication. This work represents a step toward a more transparent,\naccountable, and trustworthy academic publishing ecosystem.\n","authors":["Kamal Al-Sabahi","Yousuf Khamis Al Mabsali"],"pdf_url":"https://arxiv.org/pdf/2508.01913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01908v1","updated":"2025-08-03T20:07:15Z","published":"2025-08-03T20:07:15Z","title":"Revisiting Replay and Gradient Alignment for Continual Pre-Training of\n  Large Language Models","summary":"  Training large language models (LLMs) typically involves pre-training on\nmassive corpora, only to restart the process entirely when new data becomes\navailable. A more efficient and resource-conserving approach would be continual\npre-training, where models are updated with new data rather than retraining\nfrom scratch. However, the introduction of new data often causes distribution\nshifts, leading to performance degradation on previously learned tasks. In this\npaper, we take a deeper look at two popular proposals for addressing this\ndistribution shift within the continual learning literature: experience replay\nand gradient alignment. We consider continual pre-training of models within the\nLlama family of architectures at a large scale across languages with 100\nbillion tokens of training data in each language, finding that both replay and\ngradient alignment lead to more stable learning without forgetting. This\nconclusion holds both as we vary the model scale and as we vary the number and\ndiversity of tasks. Moreover, we are the first to demonstrate the effectiveness\nof gradient alignment techniques in the context of LLM pre-training and propose\nan efficient implementation of meta-experience replay (MER) that imbues\nexperience replay with the benefits of gradient alignment despite negligible\ncompute and memory overhead. Our scaling analysis across model sizes and replay\nrates indicates that small rates of replaying old examples are definitely a\nmore valuable use of compute than investing in model size, but that it is more\ncompute efficient to scale the size of the model than invest in high rates of\nreplaying old examples.\n","authors":["Istabrak Abbes","Gopeshh Subbaraj","Matthew Riemer","Nizar Islah","Benjamin Therien","Tsuguchika Tabaru","Hiroaki Kingetsu","Sarath Chandar","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2508.01908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.04377v2","updated":"2025-08-03T20:05:38Z","published":"2025-07-06T12:50:07Z","title":"Multi-Modal Semantic Parsing for the Interpretation of Tombstone\n  Inscriptions","summary":"  Tombstones are historically and culturally rich artifacts, encapsulating\nindividual lives, community memory, historical narratives and artistic\nexpression. Yet, many tombstones today face significant preservation\nchallenges, including physical erosion, vandalism, environmental degradation,\nand political shifts. In this paper, we introduce a novel multi-modal framework\nfor tombstones digitization, aiming to improve the interpretation, organization\nand retrieval of tombstone content. Our approach leverages vision-language\nmodels (VLMs) to translate tombstone images into structured Tombstone Meaning\nRepresentations (TMRs), capturing both image and text information. To further\nenrich semantic parsing, we incorporate retrieval-augmented generation (RAG)\nfor integrate externally dependent elements such as toponyms, occupation codes,\nand ontological concepts. Compared to traditional OCR-based pipelines, our\nmethod improves parsing accuracy from an F1 score of 36.1 to 89.5. We\nadditionally evaluate the model's robustness across diverse linguistic and\ncultural inscriptions, and simulate physical degradation through image fusion\nto assess performance under noisy or damaged conditions. Our work represents\nthe first attempt to formalize tombstone understanding using large\nvision-language models, presenting implications for heritage preservation.\n","authors":["Xiao Zhang","Johan Bos"],"pdf_url":"https://arxiv.org/pdf/2507.04377v2.pdf","comment":"ACMMM 2025"},{"id":"http://arxiv.org/abs/2508.01887v1","updated":"2025-08-03T18:43:41Z","published":"2025-08-03T18:43:41Z","title":"Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection","summary":"  AI-generated text detectors have become essential tools for maintaining\ncontent authenticity, yet their robustness against evasion attacks remains\nquestionable. We present PDFuzz, a novel attack that exploits the discrepancy\nbetween visual text layout and extraction order in PDF documents. Our method\npreserves exact textual content while manipulating character positioning to\nscramble extraction sequences. We evaluate this approach against the ArguGPT\ndetector using a dataset of human and AI-generated text. Our results\ndemonstrate complete evasion: detector performance drops from (93.6 $\\pm$ 1.4)\n% accuracy and 0.938 $\\pm$ 0.014 F1 score to random-level performance ((50.4\n$\\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity.\nOur work reveals a vulnerability in current detection systems that is inherent\nto PDF document structures and underscores the need for implementing sturdy\nsafeguards against such attacks. We make our code publicly available at\nhttps://github.com/ACMCMC/PDFuzz.\n","authors":["Aldan Creo"],"pdf_url":"https://arxiv.org/pdf/2508.01887v1.pdf","comment":"Code: https://github.com/ACMCMC/PDFuzz"},{"id":"http://arxiv.org/abs/2410.07025v3","updated":"2025-08-03T17:48:51Z","published":"2024-10-09T16:07:11Z","title":"CheXalign: Preference fine-tuning in chest X-ray interpretation models\n  without human feedback","summary":"  Radiologists play a crucial role in translating medical images into\nactionable reports. However, the field faces staffing shortages and increasing\nworkloads. While automated approaches using vision-language models (VLMs) show\npromise as assistants, they require exceptionally high accuracy. Most current\nVLMs in radiology rely solely on supervised fine-tuning. Meanwhile, additional\npreference fine-tuning in the post-training pipeline has become standard\npractice in the general domain. The challenge in radiology lies in the\nprohibitive cost of obtaining radiologist feedback at scale. To address this\nchallenge, we propose an automated pipeline for preference feedback, focusing\non chest X-ray radiology report generation (RRG). Specifically, our method\nleverages publicly available datasets containing pairs of images and\nradiologist-written reference reports with reference-based metrics, or Judges,\neliminating the need for additional radiologist feedback. We investigate reward\noveroptimization via length exploitation in this setting and introduce a\nlength-controlled version of the GREEN score. Our best-performing setup\nachieves state-of-the-art CheXbert scores on the MIMIC-CXR dataset for the RRG\ntask while on average maintaining robust performance across six additional\nimage perception and reasoning tasks.\n","authors":["Dennis Hein","Zhihong Chen","Sophie Ostmeier","Justin Xu","Maya Varma","Eduardo Pontes Reis","Arne Edward Michalson","Christian Bluethgen","Hyun Joo Shin","Curtis Langlotz","Akshay S Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2410.07025v3.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2506.21848v3","updated":"2025-08-03T17:30:11Z","published":"2025-06-27T01:45:20Z","title":"LinguaSynth: Heterogeneous Linguistic Signals for News Classification","summary":"  Deep learning has significantly advanced NLP, but its reliance on large\nblack-box models introduces critical interpretability and computational\nefficiency concerns. This paper proposes LinguaSynth, a novel text\nclassification framework that strategically integrates five complementary\nlinguistic feature types: lexical, syntactic, entity-level, word-level\nsemantics, and document-level semantics within a transparent logistic\nregression model. Unlike transformer-based architectures, LinguaSynth maintains\ninterpretability and computational efficiency, achieving an accuracy of 84.89\npercent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by\n3.32 percent. Through rigorous feature interaction analysis, we show that\nsyntactic and entity-level signals provide essential disambiguation and\neffectively complement distributional semantics. LinguaSynth sets a new\nbenchmark for interpretable, resource-efficient NLP models and challenges the\nprevailing assumption that deep neural networks are necessary for\nhigh-performing text classification.\n","authors":["Duo Zhang","Junyi Mo"],"pdf_url":"https://arxiv.org/pdf/2506.21848v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01862v1","updated":"2025-08-03T17:29:48Z","published":"2025-08-03T17:29:48Z","title":"Counterfactual Probing for Hallucination Detection and Mitigation in\n  Large Language Models","summary":"  Large Language Models have demonstrated remarkable capabilities across\ndiverse tasks, yet they frequently generate hallucinations outputs that are\nfluent but factually incorrect or unsupported. We propose Counterfactual\nProbing, a novel approach for detecting and mitigating hallucinations in LLM\noutputs. Our method dynamically generates counterfactual statements that appear\nplausible but contain subtle factual errors, then evaluates the model's\nsensitivity to these perturbations. We hypothesize that genuine knowledge\nexhibits robustness to counterfactual variations, while hallucinated content\nshows inconsistent confidence patterns when confronted with plausible\nalternatives. Our comprehensive evaluation on TruthfulQA, factual statement\ndatasets, and curated hallucination examples demonstrates that counterfactual\nprobing achieves superior detection performance compared to baseline methods,\nwhile our adaptive mitigation strategies reduce hallucination scores by an\naverage of 24.5%. The approach requires no model retraining and can be\nintegrated into existing LLM pipelines as a realtime verification mechanism.\n","authors":["Yijun Feng"],"pdf_url":"https://arxiv.org/pdf/2508.01862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01858v1","updated":"2025-08-03T17:17:52Z","published":"2025-08-03T17:17:52Z","title":"Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web\n  Agents","summary":"  Multimodal large-scale models have significantly advanced the development of\nweb agents, enabling perception and interaction with digital environments akin\nto human cognition. In this paper, we argue that web agents must first acquire\nsufficient knowledge to effectively engage in cognitive reasoning. Therefore,\nwe decompose a web agent's capabilities into two essential stages: knowledge\ncontent learning and cognitive processes. To formalize this, we propose\nWeb-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and\nProcedural. In this framework, knowledge content learning corresponds to the\nagent's processes of Memorizing and Understanding, which rely on the first two\nknowledge types, representing the \"what\" of learning. Conversely, cognitive\nprocesses correspond to Exploring, grounded in Procedural knowledge, defining\nthe \"how\" of reasoning and action. To facilitate knowledge acquisition, we\nconstruct the Web-CogDataset, a structured resource curated from 14 real-world\nwebsites, designed to systematically instill core knowledge necessary for web\nagent. This dataset serves as the agent's conceptual grounding-the \"nouns\" upon\nwhich comprehension is built-as well as the basis for learning how to reason\nand act. Building on this foundation, we operationalize these processes through\na novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing\nand training our proposed agent, the Web-CogReasoner. Extensive experimentation\nreveals its significant superiority over existing models, especially in\ngeneralizing to unseen tasks where structured knowledge is decisive. To enable\nrigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation\nsuite designed to assess and compare agent performance across the delineated\nknowledge domains and cognitive capabilities. Our code and data is open sourced\nat https://github.com/Gnonymous/Web-CogReasoner\n","authors":["Yuhan Guo","Cong Guo","Aiwen Sun","Hongliang He","Xinyu Yang","Yue Lu","Yingji Zhang","Xuntao Guo","Dong Zhang","Jianzhuang Liu","Jiang Duan","Yijia Xiao","Liangjian Wen","Hai-Ming Xu","Yong Dai"],"pdf_url":"https://arxiv.org/pdf/2508.01858v1.pdf","comment":"Our code and data is open sourced at\n  https://github.com/Gnonymous/Web-CogReasoner"},{"id":"http://arxiv.org/abs/2501.03262v8","updated":"2025-08-03T16:48:29Z","published":"2025-01-04T02:08:06Z","title":"REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt\n  and Reward Models","summary":"  Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in\naligning large language models (LLMs) with human values and preferences. While\nstate-of-the-art applications like ChatGPT or GPT-4 commonly employ Proximal\nPolicy Optimization (PPO), the inclusion of a critic network introduces\nsignificant computational overhead. REINFORCE-based methods, such as REINFORCE\nLeave One-Out (RLOO), ReMax, and Group Relative Policy Optimization (GRPO),\naddress this limitation by eliminating the critic network. However, these\napproaches face challenges in accurate advantage estimation. Specifically, they\nestimate advantages independently for responses to each prompt, which can lead\nto overfitting on simpler prompts and vulnerability to reward hacking and may\nbe biased. To address these challenges, we introduce REINFORCE++, a novel\napproach that removes the critic model while using the global advantage\nnormalization which is unbiased to improve the training stability. Our\nempirical evaluation demonstrates that REINFORCE++ exhibits robust performance\nacross various reward models without requiring prompt set truncation.\nFurthermore, it achieves superior generalization in both RLHF and long\nchain-of-thought (CoT) settings compared to existing REINFORCE-based methods.\nThe implementation is available at https://github.com/OpenRLHF/OpenRLHF.\n","authors":["Jian Hu","Jason Klein Liu","Haotian Xu","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2501.03262v8.pdf","comment":"add experiments"},{"id":"http://arxiv.org/abs/2508.01832v1","updated":"2025-08-03T16:40:53Z","published":"2025-08-03T16:40:53Z","title":"MLP Memory: Language Modeling with Retriever-pretrained External Memory","summary":"  While modern decoder-only LLMs achieve superior performance across various\ndomains, hallucinations have risen to be a common problem in their generated\ntext, hindering their application in knowledge-intensive tasks.\nRetriever-augmented generation (RAG) offers a solution, but the non-parametric\nnature of the retriever hinders its deep interaction with LLM. In this work, we\npropose to decouple memorization from the LLM decoder using a pretrained,\ndifferentiable external memory. The external memory is an MLP pretrained by\nimitating the behavior of a retriever on the entire pretraining dataset. Our\nresulting architecture, which comprises a transformer decoder and an external\nMLP memory pretrained on language modeling and retriever imitation\nrespectively, demonstrates strong perplexity and performance on downstream\ntasks. Experiments show our architecture exhibits steeper power-law scaling\nwith model size, achieving 17.5% and 24.1% improvement on WikiText-103 and Web\ndatasets compared to decoder-only models while benefiting from added training\nwithout overfitting. We demonstrate superior performance on three hallucination\nbenchmarks and nine memory-intensive tasks. Additionally, our approach delivers\n$80\\times$ speedup over $k$NN-LM (500M tokens) and $1.3\\times$ faster inference\nthan decoder-only models. Unlike $k$NN-LM, which impairs reasoning, our MLP\nmemory improves StrategyQA performance. We will open-source our code and models\nin the future.\n","authors":["Rubin Wei","Jiaqi Cao","Jiarui Wang","Jushi Kai","Qipeng Guo","Bowen Zhou","Zhouhan Lin"],"pdf_url":"https://arxiv.org/pdf/2508.01832v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.17075v2","updated":"2025-08-03T16:14:13Z","published":"2025-04-23T19:52:02Z","title":"Agree to Disagree? A Meta-Evaluation of LLM Misgendering","summary":"  Numerous methods have been proposed to measure LLM misgendering, including\nprobability-based evaluations (e.g., automatically with templatic sentences)\nand generation-based evaluations (e.g., with automatic heuristics or human\nvalidation). However, it has gone unexamined whether these evaluation methods\nhave convergent validity, that is, whether their results align. Therefore, we\nconduct a systematic meta-evaluation of these methods across three existing\ndatasets for LLM misgendering. We propose a method to transform each dataset to\nenable parallel probability- and generation-based evaluation. Then, by\nautomatically evaluating a suite of 6 models from 3 families, we find that\nthese methods can disagree with each other at the instance, dataset, and model\nlevels, conflicting on 20.2% of evaluation instances. Finally, with a human\nevaluation of 2400 LLM generations, we show that misgendering behaviour is\ncomplex and goes far beyond pronouns, which automatic evaluations are not\ncurrently designed to capture, suggesting essential disagreement with human\nevaluations. Based on our findings, we provide recommendations for future\nevaluations of LLM misgendering. Our results are also more widely relevant, as\nthey call into question broader methodological conventions in LLM evaluation,\nwhich often assume that different evaluation methods agree.\n","authors":["Arjun Subramonian","Vagrant Gautam","Preethi Seshadri","Dietrich Klakow","Kai-Wei Chang","Yizhou Sun"],"pdf_url":"https://arxiv.org/pdf/2504.17075v2.pdf","comment":"Accepted to COLM 2025"},{"id":"http://arxiv.org/abs/2508.01815v1","updated":"2025-08-03T15:58:54Z","published":"2025-08-03T15:58:54Z","title":"AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning\n  over Heterogeneous Knowledge Graphs for the Circular Economy","summary":"  Question answering over heterogeneous knowledge graphs (KGQA) involves\nreasoning across diverse schemas, incomplete alignments, and distributed data\nsources. Existing text-to-SPARQL approaches rely on large-scale domain-specific\nfine-tuning or operate within single-graph settings, limiting their\ngeneralizability in low-resource domains and their ability to handle queries\nspanning multiple graphs. These challenges are particularly relevant in domains\nsuch as the circular economy, where information about classifications,\nprocesses, and emissions is distributed across independently curated knowledge\ngraphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes\nKGQA into subtasks managed by specialized agents responsible for retrieval,\nquery generation, and verification. A scheduler assigns subgoals to different\ngraphs using weak-to-strong alignment strategies. A two-stage verifier detects\nstructurally invalid and semantically underspecified queries through symbolic\nvalidation and counterfactual consistency checks. Experiments on real-world\ncircular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy\nby 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing\nthe average prompt length by 46.4%. These results demonstrate the benefits of\nagent-based schema-aware reasoning for scalable KGQA and support\ndecision-making in sustainability domains through robust cross-graph reasoning.\n","authors":["Yang Zhao","Chengxiao Dai","Wei Zhuo","Tan Chuan Fu","Yue Xiu","Dusit Niyato","Jonathan Z. Low","Eugene Ho Hong Zhuang","Daren Zong Loong Tan"],"pdf_url":"https://arxiv.org/pdf/2508.01815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01812v1","updated":"2025-08-03T15:53:01Z","published":"2025-08-03T15:53:01Z","title":"HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark","summary":"  Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly\non morpho-syntactic tasks, neglecting the semantic dimension of language\nunderstanding. To bridge this gap, we set out to deliver a Hebrew Machine\nReading Comprehension (MRC) dataset, where MRC is to be realized as extractive\nQuestion Answering. The morphologically rich nature of Hebrew poses a challenge\nto this endeavor: the indeterminacy and non-transparency of span boundaries in\nmorphologically complex forms lead to annotation inconsistencies,\ndisagreements, and flaws in standard evaluation metrics.\n  To remedy this, we devise a novel set of guidelines, a controlled\ncrowdsourcing protocol, and revised evaluation metrics that are suitable for\nthe morphologically rich nature of the language. Our resulting benchmark, HeQ\n(Hebrew QA), features 30,147 diverse question-answer pairs derived from both\nHebrew Wikipedia articles and Israeli tech news. Our empirical investigation\nreveals that standard evaluation metrics such as F1 scores and Exact Match (EM)\nare not appropriate for Hebrew (and other MRLs), and we propose a relevant\nenhancement.\n  In addition, our experiments show low correlation between models' performance\non morpho-syntactic tasks and on MRC, which suggests that models designed for\nthe former might underperform on semantics-heavy tasks. The development and\nexploration of HeQ illustrate some of the challenges MRLs pose in natural\nlanguage understanding (NLU), fostering progression towards more and better NLU\nmodels for Hebrew and other MRLs.\n","authors":["Amir DN Cohen","Hilla Merhav","Yoav Goldberg","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2508.01812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09758v2","updated":"2025-08-03T15:52:04Z","published":"2024-10-13T07:28:26Z","title":"BiDoRA: Bi-level Optimization-Based Weight-Decomposed Low-Rank\n  Adaptation","summary":"  Parameter-efficient fine-tuning (PEFT) is a flexible and efficient method for\nadapting large language models (LLMs) to downstream tasks. Among these methods,\nweight-decomposed low-rank adaptation (DoRA) is a promising approach that\ndecomposes weight matrices into magnitude and direction components to mimic\nfull fine-tuning (FT) better. However, DoRA's simultaneous optimization of\nthese components makes it over-expressive, increases the risk of overfitting,\nand creates a coupled updating pattern that limits its learning capacity. To\naddress these issues, we propose Bi-level Optimization-Based Weight-Decomposed\nLow-Rank Adaptation (BiDoRA), a novel PEFT method based on a bi-level\noptimization framework. BiDoRA fundamentally differs from DoRA by optimizing\nthe magnitude and direction in two separate, asynchronous loops using distinct\ntraining and validation data splits. This decoupled optimization process\neffectively mitigates overfitting and allows for more flexible updates that\nalign even more closely with FT. For instance, weight decomposition analysis\nshows BiDoRA achieves a magnitude-direction update correlation of $-8.042$,\nsignificantly closer to the FT ideal compared to $-1.784$ for DoRA. Evaluation\nof BiDoRA on diverse tasks spanning natural language understanding, generation,\ntoken classification, and extremely small biomedical datasets reveals that it\nconsistently outperforms DoRA and a wide range of leading PEFT methods. This\nimprovement is statistically significant, as demonstrated on the GLUE benchmark\nwhere BiDoRA surpasses DoRA with a p-value of $2.4\\times10^{-4}$ in terms of\nthe Wilcoxon signed-rank test. The code for BiDoRA is available at\nhttps://github.com/t2ance/BiDoRA.\n","authors":["Peijia Qin","Ruiyi Zhang","Pengtao Xie"],"pdf_url":"https://arxiv.org/pdf/2410.09758v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01791v1","updated":"2025-08-03T14:58:50Z","published":"2025-08-03T14:58:50Z","title":"CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic\n  Sign Language Recognition on the Isharah Datase","summary":"  The field of Continuous Sign Language Recognition (CSLR) poses substantial\ntechnical challenges, including fluid inter-sign transitions, the absence of\ntemporal boundaries, and co-articulation effects. This paper, developed for the\nMSLR 2025 Workshop Challenge at ICCV 2025, addresses the critical challenge of\nsigner-independent recognition to advance the generalization capabilities of\nCSLR systems across diverse signers. A data-centric methodology is proposed,\ncentered on systematic feature engineering, a robust preprocessing pipeline,\nand an optimized model architecture. Key contributions include a principled\nfeature selection process guided by Exploratory Data Analysis (EDA) to isolate\ncommunicative keypoints, a rigorous preprocessing pipeline incorporating\nDBSCAN-based outlier filtering and spatial normalization, and the novel\nCSLRConformer architecture. This architecture adapts the hybrid CNN-Transformer\ndesign of the Conformer model, leveraging its capacity to model local temporal\ndependencies and global sequence context; a characteristic uniquely suited for\nthe spatio-temporal dynamics of sign language. The proposed methodology\nachieved a competitive performance, with a Word Error Rate (WER) of 5.60% on\nthe development set and 12.01% on the test set, a result that secured a 3rd\nplace ranking on the official competition platform. This research validates the\nefficacy of cross-domain architectural adaptation, demonstrating that the\nConformer model, originally conceived for speech recognition, can be\nsuccessfully repurposed to establish a new state-of-the-art performance in\nkeypoint-based CSLR.\n","authors":["Fatimah Mohamed Emad Elden"],"pdf_url":"https://arxiv.org/pdf/2508.01791v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01781v1","updated":"2025-08-03T14:37:16Z","published":"2025-08-03T14:37:16Z","title":"A comprehensive taxonomy of hallucinations in Large Language Models","summary":"  Large language models (LLMs) have revolutionized natural language processing,\nyet their propensity for hallucination, generating plausible but factually\nincorrect or fabricated content, remains a critical challenge. This report\nprovides a comprehensive taxonomy of LLM hallucinations, beginning with a\nformal definition and a theoretical framework that posits its inherent\ninevitability in computable LLMs, irrespective of architecture or training. It\nexplores core distinctions, differentiating between intrinsic (contradicting\ninput context) and extrinsic (inconsistent with training data or reality), as\nwell as factuality (absolute correctness) and faithfulness (adherence to\ninput). The report then details specific manifestations, including factual\nerrors, contextual and logical inconsistencies, temporal disorientation,\nethical violations, and task-specific hallucinations across domains like code\ngeneration and multimodal applications. It analyzes the underlying causes,\ncategorizing them into data-related issues, model-related factors, and\nprompt-related influences. Furthermore, the report examines cognitive and human\nfactors influencing hallucination perception, surveys evaluation benchmarks and\nmetrics for detection, and outlines architectural and systemic mitigation\nstrategies. Finally, it introduces web-based resources for monitoring LLM\nreleases and performance. This report underscores the complex, multifaceted\nnature of LLM hallucinations and emphasizes that, given their theoretical\ninevitability, future efforts must focus on robust detection, mitigation, and\ncontinuous human oversight for responsible and reliable deployment in critical\napplications.\n","authors":["Manuel Cossio"],"pdf_url":"https://arxiv.org/pdf/2508.01781v1.pdf","comment":"55 pages, 16 figures, 3 tables"},{"id":"http://arxiv.org/abs/2508.01780v1","updated":"2025-08-03T14:36:42Z","published":"2025-08-03T14:36:42Z","title":"LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?","summary":"  With the rapid development of Model Context Protocol (MCP), the number of MCP\nservers has surpassed 10,000. However, existing MCP benchmarks are limited to\nsingle-server settings with only a few tools, hindering effective evaluation of\nagent capabilities in large-scale, real-world scenarios. To address this\nlimitation, we present LiveMCPBench, the first comprehensive benchmark\ncomprising 95 real-world tasks grounded in the MCP ecosystem, designed to\nevaluate LLM agents at scale across diverse servers. To support a scalable and\nreproducible evaluation pipeline in large-scale MCP environments, we curate\nLiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and\n527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework\nthat enables automated and adaptive evaluation in dynamic, time-varying task\nenvironments, achieving 81% agreement with human reviewers. Finally, we propose\nthe MCP Copilot Agent, a multi-step agent that routes tools for dynamic\nplanning and executes tools for API interaction across the entire LiveMCPTool\nsuite. Our evaluation covers 10 leading models, with the best-performing model\n(Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large\nperformance variance across models, and several widely-used models perform\npoorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench\noffers the first unified framework for benchmarking LLM agents in realistic,\ntool-rich, and dynamic MCP environments, laying a solid foundation for scalable\nand reproducible research on agent capabilities. Our code and data will be\npublicly available at https://icip-cas.github.io/LiveMCPBench.\n","authors":["Guozhao Mo","Wenliang Zhong","Jiawei Chen","Xuanang Chen","Yaojie Lu","Hongyu Lin","Ben He","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2508.01780v1.pdf","comment":"Our code and data will be publicly available at\n  https://icip-cas.github.io/LiveMCPBench"},{"id":"http://arxiv.org/abs/2506.16123v2","updated":"2025-08-03T14:31:25Z","published":"2025-06-19T08:18:55Z","title":"FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning","summary":"  This paper presents FinCoT, a structured chain-of-thought (CoT) prompting\nframework that embeds domain-specific expert financial reasoning blueprints to\nguide large language models' behaviors. We identify three main prompting styles\nin financial NLP (FinNLP): (1) standard prompting (zero-shot), (2) unstructured\nCoT (free-form reasoning), and (3) structured CoT (with explicitly structured\nreasoning steps). Prior work has mainly focused on the first two, while\nstructured CoT remains underexplored and lacks domain expertise incorporation.\nTherefore, we evaluate all three prompting approaches across ten CFA-style\nfinancial domains and introduce FinCoT as the first structured finance-specific\nprompting approach incorporating blueprints from domain experts. FinCoT\nimproves the accuracy of a general-purpose model, Qwen3-8B-Base, from 63.2% to\n80.5%, and boosts Fin-R1 (7B), a finance-specific model, from 65.7% to 75.7%,\nwhile reducing output length by up to 8.9x and 1.16x compared to structured CoT\nmethods, respectively. We find that FinCoT proves most effective for models\nlacking financial post-training. Our findings show that FinCoT does not only\nimprove performance and reduce inference costs but also yields more\ninterpretable and expert-aligned reasoning traces.\n","authors":["Natapong Nitarach","Warit Sirichotedumrong","Panop Pitchayarthorn","Pittawat Taveekitworachai","Potsawee Manakul","Kunat Pipatanakul"],"pdf_url":"https://arxiv.org/pdf/2506.16123v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01773v1","updated":"2025-08-03T14:14:13Z","published":"2025-08-03T14:14:13Z","title":"Uncertainty-Based Methods for Automated Process Reward Data Construction\n  and Output Aggregation in Mathematical Reasoning","summary":"  Large language models have demonstrated remarkable capabilities in complex\nmathematical reasoning tasks, but they inevitably generate errors throughout\nmulti-step solutions. Process-level Reward Models (PRMs) have shown great\npromise by providing supervision and evaluation at each intermediate step,\nthereby effectively improving the models' reasoning abilities. However,\ntraining effective PRMs requires high-quality process reward data, yet existing\nmethods for constructing such data are often labour-intensive or inefficient.\nIn this paper, we propose an uncertainty-driven framework for automated process\nreward data construction, encompassing both data generation and annotation\nprocesses for PRMs. Additionally, we identify the limitations of both majority\nvote and PRMs, and introduce two generic uncertainty-aware output aggregation\nmethods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which\ncombine the strengths of majority vote with PRMs. Extensive experiments on\nProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the\nproposed PRM data construction framework, and demonstrate that the two output\naggregation methods further improve the mathematical reasoning abilities across\ndiverse PRMs. The code and data will be publicly available at\nhttps://github.com/Jiuzhouh/UnPRM.\n","authors":["Jiuzhou Han","Wray Buntine","Ehsan Shareghi"],"pdf_url":"https://arxiv.org/pdf/2508.01773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01754v1","updated":"2025-08-03T13:43:34Z","published":"2025-08-03T13:43:34Z","title":"AI-Generated Text is Non-Stationary: Detection via Temporal Tomography","summary":"  The field of AI-generated text detection has evolved from supervised\nclassification to zero-shot statistical analysis. However, current approaches\nshare a fundamental limitation: they aggregate token-level measurements into\nscalar scores, discarding positional information about where anomalies occur.\nOur empirical analysis reveals that AI-generated text exhibits significant\nnon-stationarity, statistical properties vary by 73.8\\% more between text\nsegments compared to human writing. This discovery explains why existing\ndetectors fail against localized adversarial perturbations that exploit this\noverlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT),\na novel detection paradigm that preserves positional information by\nreformulating detection as a signal processing task. TDT treats token-level\ndiscrepancies as a time-series signal and applies Continuous Wavelet Transform\nto generate a two-dimensional time-scale representation, capturing both the\nlocation and linguistic scale of statistical anomalies. On the RAID benchmark,\nTDT achieves 0.855 AUROC (7.1\\% improvement over the best baseline). More\nimportantly, TDT demonstrates robust performance on adversarial tasks, with\n14.1\\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its\nsophisticated analysis, TDT maintains practical efficiency with only 13\\%\ncomputational overhead. Our work establishes non-stationarity as a fundamental\ncharacteristic of AI-generated text and demonstrates that preserving temporal\ndynamics is essential for robust detection.\n","authors":["Alva West","Yixuan Weng","Minjun Zhu","Luodan Zhang","Zhen Lin","Guangsheng Bao","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.01754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14758v2","updated":"2025-08-03T12:47:07Z","published":"2025-06-17T17:54:03Z","title":"Reasoning with Exploration: An Entropy Perspective on Reinforcement\n  Learning for LLMs","summary":"  Balancing exploration and exploitation is a central goal in reinforcement\nlearning (RL). Despite recent advances in enhancing large language model (LLM)\nreasoning, most methods lean toward exploitation, and increasingly encounter\nperformance plateaus. In this work, we revisit entropy -- a signal of\nexploration in RL -- and examine its relationship to exploratory reasoning in\nLLMs. Through empirical analysis, we uncover positive correlations between\nhigh-entropy regions and three types of exploratory reasoning actions: (1)\npivotal tokens that determine or connect logical steps, (2) reflective actions\nsuch as self-verification and correction, and (3) rare behaviors under-explored\nby the base LLMs. Motivated by this, we introduce a minimal modification to\nstandard RL with only one line of code: augmenting the advantage function with\nan entropy-based term. Unlike traditional maximum-entropy methods which\nencourage exploration by promoting uncertainty, we encourage exploration by\npromoting longer and deeper reasoning chains. Notably, our method achieves\nsignificant gains on the Pass@K metric -- an upper-bound estimator of LLM\nreasoning capabilities -- even when evaluated with extremely large K values,\npushing the boundaries of LLM reasoning.\n","authors":["Daixuan Cheng","Shaohan Huang","Xuekai Zhu","Bo Dai","Wayne Xin Zhao","Zhenliang Zhang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2506.14758v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01739v1","updated":"2025-08-03T12:44:03Z","published":"2025-08-03T12:44:03Z","title":"Enhancing the Preference Extractor in Multi-turn Dialogues: From\n  Annotating Disasters to Accurate Preference Extraction","summary":"  Identifying user preferences in dialogue systems is a pivotal aspect of\nproviding satisfying services. Current research shows that using large language\nmodels (LLMs) to fine-tune a task-specific preference extractor yields\nexcellent results in terms of accuracy and generalization. However, the primary\nchallenge stems from the inherent difficulty in obtaining high-quality labeled\nmulti-turn dialogue data. Accurately tracking user preference transitions\nacross turns not only demands intensive domain expertise and contextual\nconsistency maintenance for annotators (termed \\textbf{``Annotating\nDisaster''}) but also complicates model training due to error propagation in\nsequential dependency learning. Inspired by the observation that multi-turn\npreference extraction can be decomposed into iterative executions of one-turn\nextraction processes. We propose a novel dialogue data generation framework\nnamed \\textbf{IterChat}. First, we construct a new data format that categorizes\nthe dialogue data into attributed historical preferences and one-turn\ndialogues. This reduces the probability of annotation errors and improves\nannotation efficiency. Then, to generate a high-quality and diverse dialogue\ndataset, we adopt GPT4 to pre-define the preference slots in the target\npreference extractor task and then randomly sample the subset of the slots and\ntheir corresponding schema values to create the dialogue datasets. Experimental\nresults indicate that fine-tuning or only few-shot prompting with the new\ndialogue format yields superior performance compared to the original multi-turn\ndialogues. Additionally, the new data format improves annotator efficiency with\na win rate of 28.4\\% higher than the original multi-turn dialogues.\n","authors":["Cheng Wang","ziru Liu","Pengcheng Tang","Mingyu Zhang","Quanyu Dai","Yue Zhu"],"pdf_url":"https://arxiv.org/pdf/2508.01739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01710v1","updated":"2025-08-03T10:35:05Z","published":"2025-08-03T10:35:05Z","title":"CultureGuard: Towards Culturally-Aware Dataset and Guard Model for\n  Multilingual Safety Applications","summary":"  The increasing use of Large Language Models (LLMs) in agentic applications\nhighlights the need for robust safety guard models. While content safety in\nEnglish is well-studied, non-English languages lack similar advancements due to\nthe high cost of collecting culturally aligned labeled datasets. We present\nCultureGuard, a novel solution for curating culturally aligned, high-quality\nsafety datasets across multiple languages. Our approach introduces a four-stage\nsynthetic data generation and filtering pipeline: cultural data segregation,\ncultural data adaptation, machine translation, and quality filtering. This\npipeline enables the conversion and expansion of the\nNemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct\nlanguages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.\nThe resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,\ncomprises 386,661 samples in 9 languages and facilitates the training of\nLlama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.\nThe final model achieves state-of-the-art performance on several multilingual\ncontent safety benchmarks. We also benchmark the latest open LLMs on\nmultilingual safety and observe that these LLMs are more prone to give unsafe\nresponses when prompted in non-English languages. This work represents a\nsignificant step toward closing the safety gap in multilingual LLMs by enabling\nthe development of culturally aware safety guard models.\n","authors":["Raviraj Joshi","Rakesh Paul","Kanishk Singla","Anusha Kamath","Michael Evans","Katherine Luna","Shaona Ghosh","Utkarsh Vaidya","Eileen Long","Sanjay Singh Chauhan","Niranjan Wartikar"],"pdf_url":"https://arxiv.org/pdf/2508.01710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01708v1","updated":"2025-08-03T10:29:19Z","published":"2025-08-03T10:29:19Z","title":"Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large\n  Language Models as a Symptom of Irrelevancy Disruption","summary":"  Large language models (LLMs) have advanced natural language processing (NLP)\nskills such as through next-token prediction and self-attention, but their\nability to integrate broad context also makes them prone to incorporating\nirrelevant information. Prior work has focused on semantic leakage, bias\nintroduced by semantically irrelevant context. In this paper, we introduce\nexpression leakage, a novel phenomenon where LLMs systematically generate\nsentimentally charged expressions that are semantically unrelated to the input\ncontext. To analyse the expression leakage, we collect a benchmark dataset\nalong with a scheme to automatically generate a dataset from free-form text\nfrom common-crawl. In addition, we propose an automatic evaluation pipeline\nthat correlates well with human judgment, which accelerates the benchmarking by\ndecoupling from the need of annotation for each analysed model. Our experiments\nshow that, as the model scales in the parameter space, the expression leakage\nreduces within the same LLM family. On the other hand, we demonstrate that\nexpression leakage mitigation requires specific care during the model building\nprocess, and cannot be mitigated by prompting. In addition, our experiments\nindicate that, when negative sentiment is injected in the prompt, it disrupts\nthe generation process more than the positive sentiment, causing a higher\nexpression leakage rate.\n","authors":["Berkay Köprü","Mehrzad Mashal","Yigit Gurses","Akos Kadar","Maximilian Schmitt","Ditty Mathew","Felix Burkhardt","Florian Eyben","Björn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2508.01708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16607v2","updated":"2025-08-03T10:27:19Z","published":"2025-01-28T00:52:23Z","title":"MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search","summary":"  Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy.\n","authors":["Shuozhi Yuan","Limin Chen","Miaomiao Yuan","Jin Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.16607v2.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.19147v2","updated":"2025-08-03T10:09:02Z","published":"2025-05-25T13:51:17Z","title":"Shifting AI Efficiency From Model-Centric to Data-Centric Compression","summary":"  The rapid advancement of large language models (LLMs) and multi-modal LLMs\n(MLLMs) has historically relied on model-centric scaling through increasing\nparameter counts from millions to hundreds of billions to drive performance\ngains. However, as we approach hardware limits on model size, the dominant\ncomputational bottleneck has fundamentally shifted to the quadratic cost of\nself-attention over long token sequences, now driven by ultra-long text\ncontexts, high-resolution images, and extended videos. In this position paper,\n\\textbf{we argue that the focus of research for efficient AI is shifting from\nmodel-centric compression to data-centric compression}. We position token\ncompression as the new frontier, which improves AI efficiency via reducing the\nnumber of tokens during model training or inference. Through comprehensive\nanalysis, we first examine recent developments in long-context AI across\nvarious domains and establish a unified mathematical framework for existing\nmodel efficiency strategies, demonstrating why token compression represents a\ncrucial paradigm shift in addressing long-context overhead. Subsequently, we\nsystematically review the research landscape of token compression, analyzing\nits fundamental benefits and identifying its compelling advantages across\ndiverse scenarios. Furthermore, we provide an in-depth analysis of current\nchallenges in token compression research and outline promising future\ndirections. Ultimately, our work aims to offer a fresh perspective on AI\nefficiency, synthesize existing research, and catalyze innovative developments\nto address the challenges that increasing context lengths pose to the AI\ncommunity's advancement.\n","authors":["Xuyang Liu","Zichen Wen","Shaobo Wang","Junjie Chen","Zhishan Tao","Yubo Wang","Xiangqi Jin","Chang Zou","Yiyu Wang","Chenfei Liao","Xu Zheng","Honggang Chen","Weijia Li","Xuming Hu","Conghui He","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.19147v2.pdf","comment":"Project:\n  \\url{https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression}"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2508.01867v1","updated":"2025-08-03T17:45:04Z","published":"2025-08-03T17:45:04Z","title":"Counterfactual Reciprocal Recommender Systems for User-to-User Matching","summary":"  Reciprocal recommender systems (RRS) in dating, gaming, and talent platforms\nrequire mutual acceptance for a match. Logged data, however, over-represents\npopular profiles due to past exposure policies, creating feedback loops that\nskew learning and fairness. We introduce Counterfactual Reciprocal Recommender\nSystems (CFRR), a causal framework to mitigate this bias. CFRR uses inverse\npropensity scored, self-normalized objectives. Experiments show CFRR improves\nNDCG@10 by up to 3.5% (e.g., from 0.459 to 0.475 on DBLP, from 0.299 to 0.307\non Synthetic), increases long-tail user coverage by up to 51% (from 0.504 to\n0.763 on Synthetic), and reduces Gini exposure inequality by up to 24% (from\n0.708 to 0.535 on Synthetic). CFRR offers a promising approach for more\naccurate and fair user-to-user matching.\n","authors":["Kazuki Kawamura","Takuma Udagawa","Kei Tateno"],"pdf_url":"https://arxiv.org/pdf/2508.01867v1.pdf","comment":"9 pages, 2 figures. Accepted for publication at the Workshop on\n  Two-sided Marketplace Optimization (TSMO '25), held in conjunction with the\n  31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2025),\n  Toronto, Canada"},{"id":"http://arxiv.org/abs/2503.03687v2","updated":"2025-08-03T08:19:53Z","published":"2025-03-05T17:28:16Z","title":"Fine-grained Alignment of Large Language Models for General Medication\n  Recommendation without Overprescription","summary":"  Large language models (LLMs) holds significant promise in achieving general\nmedication recommendation systems owing to their comprehensive interpretation\nof clinical notes and flexibility to medication encoding. We evaluated both\ngeneral-purpose and medical-specific LLMs for medication recommendations,\nshowing their unsatisfactory precision and severe overprescription. To address\nthis, we introduce Language-Assisted Medication Recommendation, which tailors\nLLMs for medication recommendation in a medication-aware manner, improving the\nusage of clinical notes. Fine-tuning LLMs with this framework can outperform\nexisting methods by more than 10% in internal validation and generalize across\ntemporal and external validations. Furthermore, the model maintains high\naccuracy when encountering out-of-distribution medication.\n","authors":["Zihao Zhao","Chenxiao Fan","Junlong Liu","Zheng Wang","Xiangnan He","Chongming Gao","Juan Li","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2503.03687v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01643v1","updated":"2025-08-03T08:04:44Z","published":"2025-08-03T08:04:44Z","title":"ChEmbed: Enhancing Chemical Literature Search Through Domain-Specific\n  Text Embeddings","summary":"  Retrieval-Augmented Generation (RAG) systems in chemistry heavily depend on\naccurate and relevant retrieval of chemical literature. However,\ngeneral-purpose text embedding models frequently fail to adequately represent\ncomplex chemical terminologies, resulting in suboptimal retrieval quality.\nSpecialized embedding models tailored to chemical literature retrieval have not\nyet been developed, leaving a substantial performance gap. To address this\nchallenge, we introduce ChEmbed, a domain-adapted family of text embedding\nmodels fine-tuned on a dataset comprising chemistry-specific text from the\nPubChem, Semantic Scholar, and ChemRxiv corpora. To create effective training\ndata, we employ large language models to synthetically generate queries,\nresulting in approximately 1.7 million high-quality query-passage pairs.\nAdditionally, we augment the tokenizer by adding 900 chemically specialized\ntokens to previously unused slots, which significantly reduces the\nfragmentation of chemical entities, such as IUPAC names. ChEmbed also maintains\na 8192-token context length, enabling the efficient retrieval of longer\npassages compared to many other open-source embedding models, which typically\nhave a context length of 512 or 2048 tokens. Evaluated on our newly introduced\nChemRxiv Retrieval benchmark, ChEmbed outperforms state-of-the-art general\nembedding models, raising nDCG@10 from 0.82 to 0.91 (+9 pp). ChEmbed represents\na practical, lightweight, and reproducible embedding solution that effectively\nimproves retrieval for chemical literature search.\n","authors":["Ali Shiraee Kasmaee","Mohammad Khodadad","Mehdi Astaraki","Mohammad Arshi Saloot","Nicholas Sherck","Hamidreza Mahyar","Soheila Samiee"],"pdf_url":"https://arxiv.org/pdf/2508.01643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08445v2","updated":"2025-08-03T05:07:40Z","published":"2025-07-11T09:36:45Z","title":"Clue-RAG: Towards Accurate and Cost-Efficient Graph-based RAG via\n  Multi-Partite Graph and Query-Driven Iterative Retrieval","summary":"  Despite the remarkable progress of Large Language Models (LLMs), their\nperformance in question answering (QA) remains limited by the lack of\ndomain-specific and up-to-date knowledge. Retrieval-Augmented Generation (RAG)\naddresses this limitation by incorporating external information, often from\ngraph-structured data. However, existing graph-based RAG methods suffer from\npoor graph quality due to incomplete extraction and insufficient utilization of\nquery information during retrieval. To overcome these limitations, we propose\nClue-RAG, a novel approach that introduces (1) a multi-partite graph index\nincorporates Chunk, knowledge unit, and entity to capture semantic content at\nmultiple levels of granularity, coupled with a hybrid extraction strategy that\nreduces LLM token usage while still producing accurate and disambiguated\nknowledge units, and (2) Q-Iter, a query-driven iterative retrieval strategy\nthat enhances relevance through semantic search and constrained graph\ntraversal. Experiments on three QA benchmarks show that Clue-RAG significantly\noutperforms state-of-the-art baselines, achieving up to 99.33% higher Accuracy\nand 113.51% higher F1 score while reducing indexing costs by 72.58%.\nRemarkably, Clue-RAG matches or outperforms baselines even without using an LLM\nfor indexing. These results demonstrate the effectiveness and cost-efficiency\nof Clue-RAG in advancing graph-based RAG systems.\n","authors":["Yaodong Su","Yixiang Fang","Yingli Zhou","Quanqing Xu","Chuanhui Yang"],"pdf_url":"https://arxiv.org/pdf/2507.08445v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.08642v3","updated":"2025-08-03T21:00:55Z","published":"2024-10-11T09:10:26Z","title":"More than Memes: A Multimodal Topic Modeling Approach to Conspiracy\n  Theories on Telegram","summary":"  To address the increasing prevalence of (audio-)visual data on social media,\nand to capture the evolving and dynamic nature of this communication,\nresearchers have begun to explore the potential of unsupervised approaches for\nanalyzing multimodal online content. However, existing research often neglects\nvisual content beyond memes, and in addition lacks methods to compare topic\nmodels across modalities. Our study addresses these gaps by applying multimodal\ntopic modeling for analyzing conspiracy theories in German-language Telegram\nchannels. We use BERTopic with CLIP for the analysis of textual and visual data\nin a corpus of ~40, 000 Telegram messages posted in October 2023 in 571\nGerman-language Telegram channels known for disseminating conspiracy theories.\nThrough this dataset, we provide insights into unimodal and multimodal topic\nmodels by analyzing symmetry and intersections of topics across modalities. We\ndemonstrate the variety of textual and visual content shared in the channels\ndiscovered through the topic modeling, and propose a conceptual framework for\nthe analysis of textual and visual discursive strategies in the communication\nof conspiracy theories. We apply the framework in a case study of the topic\ngroup Israel Gaza.\n","authors":["Elisabeth Steffen"],"pdf_url":"https://arxiv.org/pdf/2410.08642v3.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2507.04377v2","updated":"2025-08-03T20:05:38Z","published":"2025-07-06T12:50:07Z","title":"Multi-Modal Semantic Parsing for the Interpretation of Tombstone\n  Inscriptions","summary":"  Tombstones are historically and culturally rich artifacts, encapsulating\nindividual lives, community memory, historical narratives and artistic\nexpression. Yet, many tombstones today face significant preservation\nchallenges, including physical erosion, vandalism, environmental degradation,\nand political shifts. In this paper, we introduce a novel multi-modal framework\nfor tombstones digitization, aiming to improve the interpretation, organization\nand retrieval of tombstone content. Our approach leverages vision-language\nmodels (VLMs) to translate tombstone images into structured Tombstone Meaning\nRepresentations (TMRs), capturing both image and text information. To further\nenrich semantic parsing, we incorporate retrieval-augmented generation (RAG)\nfor integrate externally dependent elements such as toponyms, occupation codes,\nand ontological concepts. Compared to traditional OCR-based pipelines, our\nmethod improves parsing accuracy from an F1 score of 36.1 to 89.5. We\nadditionally evaluate the model's robustness across diverse linguistic and\ncultural inscriptions, and simulate physical degradation through image fusion\nto assess performance under noisy or damaged conditions. Our work represents\nthe first attempt to formalize tombstone understanding using large\nvision-language models, presenting implications for heritage preservation.\n","authors":["Xiao Zhang","Johan Bos"],"pdf_url":"https://arxiv.org/pdf/2507.04377v2.pdf","comment":"ACMMM 2025"},{"id":"http://arxiv.org/abs/2505.19233v2","updated":"2025-08-03T20:05:22Z","published":"2025-05-25T17:14:43Z","title":"RAISE: Realness Assessment for Image Synthesis and Evaluation","summary":"  The rapid advancement of generative AI has enabled the creation of highly\nphotorealistic visual content, offering practical substitutes for real images\nand videos in scenarios where acquiring real data is difficult or expensive.\nHowever, reliably substituting real visual content with AI-generated\ncounterparts requires robust assessment of the perceived realness of\nAI-generated visual content, a challenging task due to its inherent subjective\nnature. To address this, we conducted a comprehensive human study evaluating\nthe perceptual realness of both real and AI-generated images, resulting in a\nnew dataset, containing images paired with subjective realness scores,\nintroduced as RAISE in this paper. Further, we develop and train multiple\nmodels on RAISE to establish baselines for realness prediction. Our\nexperimental results demonstrate that features derived from deep foundation\nvision models can effectively capture the subjective realness. RAISE thus\nprovides a valuable resource for developing robust, objective models of\nperceptual realness assessment.\n","authors":["Aniruddha Mukherjee","Spriha Dubey","Somdyuti Paul"],"pdf_url":"https://arxiv.org/pdf/2505.19233v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01852v1","updated":"2025-08-03T17:07:49Z","published":"2025-08-03T17:07:49Z","title":"Context Guided Transformer Entropy Modeling for Video Compression","summary":"  Conditional entropy models effectively leverage spatio-temporal contexts to\nreduce video redundancy. However, incorporating temporal context often\nintroduces additional model complexity and increases computational cost. In\nparallel, many existing spatial context models lack explicit modeling the\nordering of spatial dependencies, which may limit the availability of relevant\ncontext during decoding. To address these issues, we propose the Context Guided\nTransformer (CGT) entropy model, which estimates probability mass functions of\nthe current frame conditioned on resampled temporal context and\ndependency-weighted spatial context. A temporal context resampler learns\npredefined latent queries to extract critical temporal information using\ntransformer encoders, reducing downstream computational overhead. Meanwhile, a\nteacher-student network is designed as dependency-weighted spatial context\nassigner to explicitly model the dependency of spatial context order. The\nteacher generates an attention map to represent token importance and an entropy\nmap to reflect prediction certainty from randomly masked inputs, guiding the\nstudent to select the weighted top-k tokens with the highest spatial\ndependency. During inference, only the student is used to predict undecoded\ntokens based on high-dependency context. Experimental results demonstrate that\nour CGT model reduces entropy modeling time by approximately 65% and achieves\nan 11% BD-Rate reduction compared to the previous state-of-the-art conditional\nentropy model.\n","authors":["Junlong Tong","Wei Zhang","Yaohui Jin","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2508.01852v1.pdf","comment":"ICCV 2025"},{"id":"http://arxiv.org/abs/2507.00498v3","updated":"2025-08-03T15:08:39Z","published":"2025-07-01T07:13:34Z","title":"MuteSwap: Visual-informed Silent Video Identity Conversion","summary":"  Conventional voice conversion modifies voice characteristics from a source\nspeaker to a target speaker, relying on audio input from both sides. However,\nthis process becomes infeasible when clean audio is unavailable, such as in\nsilent videos or noisy environments. In this work, we focus on the task of\nSilent Face-based Voice Conversion (SFVC), which does voice conversion entirely\nfrom visual inputs. i.e., given images of a target speaker and a silent video\nof a source speaker containing lip motion, SFVC generates speech aligning the\nidentity of the target speaker while preserving the speech content in the\nsource silent video. As this task requires generating intelligible speech and\nconverting identity using only visual cues, it is particularly challenging. To\naddress this, we introduce MuteSwap, a novel framework that employs contrastive\nlearning to align cross-modality identities and minimize mutual information to\nseparate shared visual features. Experimental results show that MuteSwap\nachieves impressive performance in both speech synthesis and identity\nconversion, especially under noisy conditions where methods dependent on audio\ninput fail to produce intelligible results, demonstrating both the\neffectiveness of our training approach and the feasibility of SFVC.\n","authors":["Yifan Liu","Yu Fang","Zhouhan Lin"],"pdf_url":"https://arxiv.org/pdf/2507.00498v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.05185v2","updated":"2025-08-03T13:19:40Z","published":"2025-03-07T07:13:59Z","title":"Towards Temporal-Aware Multi-Modal Retrieval Augmented Generation in\n  Finance","summary":"  Finance decision-making often relies on in-depth data analysis across various\ndata sources, including financial tables, news articles, stock prices, etc. In\nthis work, we introduce FinTMMBench, the first comprehensive benchmark for\nevaluating temporal-aware multi-modal Retrieval-Augmented Generation (RAG)\nsystems in finance. Built from heterologous data of NASDAQ 100 companies,\nFinTMMBench offers three significant advantages. 1) Multi-modal Corpus: It\nencompasses a hybrid of financial tables, news articles, daily stock prices,\nand visual technical charts as the corpus. 2) Temporal-aware Questions: Each\nquestion requires the retrieval and interpretation of its relevant data over a\nspecific time period, including daily, weekly, monthly, quarterly, and annual\nperiods. 3) Diverse Financial Analysis Tasks: The questions involve 10\ndifferent financial analysis tasks designed by domain experts, including\ninformation extraction, trend analysis, sentiment analysis and event detection,\netc. We further propose a novel TMMHybridRAG method, which first leverages LLMs\nto convert data from other modalities (e.g., tabular, visual and time-series\ndata) into textual format and then incorporates temporal information in each\nnode when constructing graphs and dense indexes. Its effectiveness has been\nvalidated in extensive experiments, but notable gaps remain, highlighting the\nchallenges presented by our FinTMMBench.\n","authors":["Fengbin Zhu","Junfeng Li","Liangming Pan","Wenjie Wang","Fuli Feng","Chao Wang","Huanbo Luan","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2503.05185v2.pdf","comment":"Accepted by MM 2025"},{"id":"http://arxiv.org/abs/2502.04400v2","updated":"2025-08-03T12:20:03Z","published":"2025-02-06T07:28:05Z","title":"Adaptive Prototype Knowledge Transfer for Federated Learning with Mixed\n  Modalities and Heterogeneous Tasks","summary":"  Multimodal Federated Learning (MFL) with mixed modalities enables unimodal\nand multimodal clients to collaboratively train models while ensuring clients'\nprivacy. As a representative sample of local data, prototypes offer an approach\nwith low resource consumption and no reliance on prior knowledge for MFL with\nmixed modalities. However, existing prototype-based MFL methods assume unified\nlabels across clients and identical tasks per client, which is impractical in\nMFL with mixed modalities. In this work, we propose an Adaptive prototype-based\nMultimodal Federated Learning (AproMFL) framework for mixed modalities to\naddress the aforementioned issues. Our AproMFL transfers knowledge through\nadaptively-constructed prototypes without unified labels. Clients adaptively\nselect prototype construction methods in line with labels; server converts\nclient prototypes into unified multimodal prototypes and cluster them to form\nglobal prototypes. To address model aggregation issues in task heterogeneity,\nwe develop a client relationship graph-based scheme to dynamically adjust\naggregation weights. Furthermore, we propose a global prototype knowledge\ntransfer loss and a global model knowledge transfer loss to enable the transfer\nof global knowledge to local knowledge. Experimental results show that AproMFL\noutperforms four baselines on three highly heterogeneous datasets\n($\\alpha=0.1$) and two heterogeneous tasks, with the optimal results in\naccuracy and recall being 0.42%~6.09% and 1.6%~3.89% higher than those of\nFedIoT (FedAvg-based MFL), respectively.\n","authors":["Keke Gai","Mohan Wang","Jing Yu","Dongjue Wang","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2502.04400v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01644v1","updated":"2025-08-03T08:05:57Z","published":"2025-08-03T08:05:57Z","title":"DRKF: Decoupled Representations with Knowledge Fusion for Multimodal\n  Emotion Recognition","summary":"  Multimodal emotion recognition (MER) aims to identify emotional states by\nintegrating and analyzing information from multiple modalities. However,\ninherent modality heterogeneity and inconsistencies in emotional cues remain\nkey challenges that hinder performance. To address these issues, we propose a\nDecoupled Representations with Knowledge Fusion (DRKF) method for MER. DRKF\nconsists of two main modules: an Optimized Representation Learning (ORL) Module\nand a Knowledge Fusion (KF) Module. ORL employs a contrastive mutual\ninformation estimation method with progressive modality augmentation to\ndecouple task-relevant shared representations and modality-specific features\nwhile mitigating modality heterogeneity. KF includes a lightweight\nself-attention-based Fusion Encoder (FE) that identifies the dominant modality\nand integrates emotional information from other modalities to enhance the fused\nrepresentation. To handle potential errors from incorrect dominant modality\nselection under emotionally inconsistent conditions, we introduce an Emotion\nDiscrimination Submodule (ED), which enforces the fused representation to\nretain discriminative cues of emotional inconsistency. This ensures that even\nif the FE selects an inappropriate dominant modality, the Emotion\nClassification Submodule (EC) can still make accurate predictions by leveraging\npreserved inconsistency information. Experiments show that DRKF achieves\nstate-of-the-art (SOTA) performance on IEMOCAP, MELD, and M3ED. The source code\nis publicly available at https://github.com/PANPANKK/DRKF.\n","authors":["Peiyuan Jiang","Yao Liu","Qiao Liu","Zongshun Zhang","Jiaye Yang","Lu Liu","Daibing Yao"],"pdf_url":"https://arxiv.org/pdf/2508.01644v1.pdf","comment":"Published in ACM Multimedia 2025. 10 pages, 4 figures"}]},"2025-08-02T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2508.01514v1","updated":"2025-08-02T22:46:50Z","published":"2025-08-02T22:46:50Z","title":"End-to-End Personalization: Unifying Recommender Systems with Large\n  Language Models","summary":"  Recommender systems are essential for guiding users through the vast and\ndiverse landscape of digital content by delivering personalized and relevant\nsuggestions. However, improving both personalization and interpretability\nremains a challenge, particularly in scenarios involving limited user feedback\nor heterogeneous item attributes. In this article, we propose a novel hybrid\nrecommendation framework that combines Graph Attention Networks (GATs) with\nLarge Language Models (LLMs) to address these limitations. LLMs are first used\nto enrich user and item representations by generating semantically meaningful\nprofiles based on metadata such as titles, genres, and overviews. These\nenriched embeddings serve as initial node features in a user and movie\nbipartite graph, which is processed using a GAT based collaborative filtering\nmodel. To enhance ranking accuracy, we introduce a hybrid loss function that\ncombines Bayesian Personalized Ranking (BPR), cosine similarity, and robust\nnegative sampling. Post-processing involves reranking the GAT-generated\nrecommendations using the LLM, which also generates natural-language\njustifications to improve transparency. We evaluated our model on benchmark\ndatasets, including MovieLens 100k and 1M, where it consistently outperforms\nstrong baselines. Ablation studies confirm that LLM-based embeddings and the\ncosine similarity term significantly contribute to performance gains. This work\ndemonstrates the potential of integrating LLMs to improve both the accuracy and\ninterpretability of recommender systems.\n","authors":["Danial Ebrat","Tina Aminian","Sepideh Ahmadian","Luis Rueda"],"pdf_url":"https://arxiv.org/pdf/2508.01514v1.pdf","comment":"Second Workshop on Generative AI for Recommender Systems and\n  Personalization at the ACM Conference on Knowledge Discovery and Data Mining\n  (GenAIRecP@KDD 2025)"},{"id":"http://arxiv.org/abs/2508.01502v1","updated":"2025-08-02T21:53:51Z","published":"2025-08-02T21:53:51Z","title":"Req-Rec: Enhancing Requirements Elicitation for Increasing Stakeholder's\n  Satisfaction Using a Collaborative Filtering Based Recommender System","summary":"  The success or failure of a project is highly related to recognizing the\nright stakeholders and accurately finding and discovering their requirements.\nHowever, choosing the proper elicitation technique was always a considerable\nchallenge for efficient requirement engineering. As a consequence of the swift\nimprovement of digital technologies since the past decade, recommender systems\nhave become an efficient channel for making a deeply personalized interactive\ncommunication with stakeholders. In this research, a new method, called the\nReq-Rec (Requirements Recommender), is proposed. It is a hybrid recommender\nsystem based on the collaborative filtering approach and the repertory grid\ntechnique as the core component. The primary goal of Req-Rec is to increase\nstakeholder satisfaction by assisting them in the requirement elicitation\nphase. Based on the results, the method efficiently could overcome weaknesses\nof common requirement elicitation techniques, such as time limitation,\nlocation-based restrictions, and bias in requirements' elicitation process.\nTherefore, recommending related requirements assists stakeholders in becoming\nmore aware of different aspects of the project.\n","authors":["Ali Fallahi","Amineh Amini","Azam Bastanfard","Hadi Saboohi"],"pdf_url":"https://arxiv.org/pdf/2508.01502v1.pdf","comment":"March 2023, 28 pages, 7 figures"},{"id":"http://arxiv.org/abs/2508.01375v1","updated":"2025-08-02T14:09:21Z","published":"2025-08-02T14:09:21Z","title":"SaviorRec: Semantic-Behavior Alignment for Cold-Start Recommendation","summary":"  In recommendation systems, predicting Click-Through Rate (CTR) is crucial for\naccurately matching users with items. To improve recommendation performance for\ncold-start and long-tail items, recent studies focus on leveraging item\nmultimodal features to model users' interests. However, obtaining multimodal\nrepresentations for items relies on complex pre-trained encoders, which incurs\nunacceptable computation cost to train jointly with downstream ranking models.\nTherefore, it is important to maintain alignment between semantic and behavior\nspace in a lightweight way.\n  To address these challenges, we propose a Semantic-Behavior Alignment for\nCold-start Recommendation framework, which mainly focuses on utilizing\nmultimodal representations that align with the user behavior space to predict\nCTR. First, we leverage domain-specific knowledge to train a multimodal encoder\nto generate behavior-aware semantic representations. Second, we use residual\nquantized semantic ID to dynamically bridge the gap between multimodal\nrepresentations and the ranking model, facilitating the continuous\nsemantic-behavior alignment. We conduct our offline and online experiments on\nthe Taobao, one of the world's largest e-commerce platforms, and have achieved\nan increase of 0.83% in offline AUC, 13.21% clicks increase and 13.44% orders\nincrease in the online A/B test, emphasizing the efficacy of our method.\n","authors":["Yining Yao","Ziwei Li","Shuwen Xiao","Boya Du","Jialin Zhu","Junjun Zheng","Xiangheng Kong","Yuning Jiang"],"pdf_url":"https://arxiv.org/pdf/2508.01375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01370v1","updated":"2025-08-02T13:49:15Z","published":"2025-08-02T13:49:15Z","title":"MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and\n  Analysis","summary":"  We present an autonomous framework that leverages Large Language Models\n(LLMs) to automate end-to-end business analysis and market report generation.\nAt its core, the system employs specialized agents - Researcher, Reviewer,\nWriter, and Retriever - that collaborate to analyze data and produce\ncomprehensive reports. These agents learn from real professional consultants'\npresentation materials at Amazon through in-context learning to replicate\nprofessional analytical methodologies. The framework executes a multi-step\nprocess: querying databases, analyzing data, generating insights, creating\nvisualizations, and composing market reports. We also introduce a novel\nLLM-based evaluation system for assessing report quality, which shows alignment\nwith expert human evaluations. Building on these evaluations, we implement an\niterative improvement mechanism that optimizes report quality through automated\nreview cycles. Experimental results show that report quality can be improved by\nboth automated review cycles and consultants' unstructured knowledge. In\nexperimental validation, our framework generates detailed 6-page reports in 7\nminutes at a cost of approximately \\$1. Our work could be an important step to\nautomatically create affordable market insights.\n","authors":["Roman Koshkin","Pengyu Dai","Nozomi Fujikawa","Masahito Togami","Marco Visentini-Scarzanella"],"pdf_url":"https://arxiv.org/pdf/2508.01370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01285v1","updated":"2025-08-02T09:32:52Z","published":"2025-08-02T09:32:52Z","title":"BioDisco: Multi-agent hypothesis generation with dual-mode evidence,\n  iterative feedback and temporal evaluation","summary":"  Identifying novel hypotheses is essential to scientific research, yet this\nprocess risks being overwhelmed by the sheer volume and complexity of available\ninformation. Existing automated methods often struggle to generate novel and\nevidence-grounded hypotheses, lack robust iterative refinement and rarely\nundergo rigorous temporal evaluation for future discovery potential. To address\nthis, we propose BioDisco, a multi-agent framework that draws upon language\nmodel-based reasoning and a dual-mode evidence system (biomedical knowledge\ngraphs and automated literature retrieval) for grounded novelty, integrates an\ninternal scoring and feedback loop for iterative refinement, and validates\nperformance through pioneering temporal and human evaluations and a\nBradley-Terry paired comparison model to provide statistically-grounded\nassessment. Our evaluations demonstrate superior novelty and significance over\nablated configurations representative of existing agentic architectures.\nDesigned for flexibility and modularity, BioDisco allows seamless integration\nof custom language models or knowledge graphs, and can be run with just a few\nlines of code. We anticipate researchers using this practical tool as a\ncatalyst for the discovery of new hypotheses.\n","authors":["Yujing Ke","Kevin George","Kathan Pandya","David Blumenthal","Maximilian Sprang","Gerrit Großmann","Sebastian Vollmer","David Antony Selby"],"pdf_url":"https://arxiv.org/pdf/2508.01285v1.pdf","comment":"7 pages main content + 11 pages appendices"},{"id":"http://arxiv.org/abs/2508.01265v1","updated":"2025-08-02T08:49:45Z","published":"2025-08-02T08:49:45Z","title":"A Study on Enhancing User Engagement by Employing Gamified Recommender\n  Systems","summary":"  Providing customized products and services in the modern business world is\none of the most efficient solutions to improve users' experience and their\nengagements with the industries. To aim, recommender systems, by producing\npersonalized recommendations, have a crucial role in the digital age. As a\nconsequence of modern improvements in the internet and online-based\ntechnologies, using gamification rules also increased in various fields. Recent\nstudies showed that considering gamification concepts in implementing\nrecommendation systems not only can become helpful to overcome the cold start\nand lack of sufficient data, moreover, can effectively improve user engagement.\nGamification can motivate individuals to have more activities on the system;\nthese interactions are valuable resources of data for recommender engines.\nUnlike the past related works about using gamified recommendation systems in\ndifferent environments or studies that particularly surveyed gamification\nstrategies or recommenders separately, this work provides a comprehensive\nreview of how gamified recommender systems can enhance user engagement in\nvarious domain applications. Furthermore, comparing different approaches for\nbuilding recommender systems is followed by in-depth surveying about\ninvestigating the gamified recommender systems, including their approaches,\nlimitations, evaluation metrics, proposed achievements, datasets, domain areas,\nand their recommendation techniques. This exhaustive analysis provides a\ndetailed picture of the topic's popularity, gaps, and unexplored regions. It is\nenvisaged that the proposed research and introduced possible future directions\nwould serve as a stepping stone for researchers interested in using gamified\nrecommender systems for user satisfaction and engagement.\n","authors":["Ali Fallahi","Azam Bastanfard","Amineh Amini","Hadi Saboohi"],"pdf_url":"https://arxiv.org/pdf/2508.01265v1.pdf","comment":"June 2023, 21 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.20227v3","updated":"2025-08-02T07:32:08Z","published":"2025-07-27T11:13:03Z","title":"CTR-Driven Ad Text Generation via Online Feedback Preference\n  Optimization","summary":"  Advertising text plays a critical role in determining click-through rates\n(CTR) in online advertising. Large Language Models (LLMs) offer significant\nefficiency advantages over manual ad text creation. However, LLM-generated ad\ntexts do not guarantee higher CTR performance compared to human-crafted texts,\nrevealing a gap between generation quality and online performance of ad texts.\nIn this work, we propose a novel ad text generation method which optimizes for\nCTR through preference optimization from online feedback. Our approach adopts\nan innovative two-stage framework: (1) diverse ad text sampling via one-shot\nin-context learning, using retrieval-augmented generation (RAG) to provide\nexemplars with chain-of-thought (CoT) reasoning; (2) CTR-driven preference\noptimization from online feedback, which weighs preference pairs according to\ntheir CTR gains and confidence levels. Through our method, the resulting model\nenables end-to-end generation of high-CTR ad texts. Extensive experiments have\ndemonstrated the effectiveness of our method in both offline and online\nmetrics. Notably, we have applied our method on a large-scale online shopping\nplatform and achieved significant CTR improvements, showcasing its strong\napplicability and effectiveness in advertising systems.\n","authors":["Yanda Chen","Zihui Ren","Qixiang Gao","Jiale Chen","Si Chen","Xubin Li","Tiezheng Ge","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2507.20227v3.pdf","comment":"13 pages, 7 figures, 8 tables"},{"id":"http://arxiv.org/abs/2508.01226v1","updated":"2025-08-02T06:44:59Z","published":"2025-08-02T06:44:59Z","title":"CM$^3$: Calibrating Multimodal Recommendation","summary":"  Alignment and uniformity are fundamental principles within the domain of\ncontrastive learning. In recommender systems, prior work has established that\noptimizing the Bayesian Personalized Ranking (BPR) loss contributes to the\nobjectives of alignment and uniformity. Specifically, alignment aims to draw\ntogether the representations of interacting users and items, while uniformity\nmandates a uniform distribution of user and item embeddings across a unit\nhypersphere. This study revisits the alignment and uniformity properties within\nthe context of multimodal recommender systems, revealing a proclivity among\nextant models to prioritize uniformity to the detriment of alignment. Our\nhypothesis challenges the conventional assumption of equitable item treatment\nthrough a uniformity loss, proposing a more nuanced approach wherein items with\nsimilar multimodal attributes converge toward proximal representations within\nthe hyperspheric manifold. Specifically, we leverage the inherent similarity\nbetween items' multimodal data to calibrate their uniformity distribution,\nthereby inducing a more pronounced repulsive force between dissimilar entities\nwithin the embedding space. A theoretical analysis elucidates the relationship\nbetween this calibrated uniformity loss and the conventional uniformity\nfunction. Moreover, to enhance the fusion of multimodal features, we introduce\na Spherical B\\'ezier method designed to integrate an arbitrary number of\nmodalities while ensuring that the resulting fused features are constrained to\nthe same hyperspherical manifold. Empirical evaluations conducted on five\nreal-world datasets substantiate the superiority of our approach over competing\nbaselines. We also shown that the proposed methods can achieve up to a 5.4%\nincrease in NDCG@20 performance via the integration of MLLM-extracted features.\nSource code is available at: https://github.com/enoche/CM3.\n","authors":["Xin Zhou","Yongjie Wang","Zhiqi Shen"],"pdf_url":"https://arxiv.org/pdf/2508.01226v1.pdf","comment":"Working Paper: https://github.com/enoche/CM3"},{"id":"http://arxiv.org/abs/2508.01136v1","updated":"2025-08-02T01:36:57Z","published":"2025-08-02T01:36:57Z","title":"DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance\n  System using Knowledge Graphs","summary":"  The operation and maintenance (O&M) of database systems is critical to\nensuring system availability and performance, typically requiring expert\nexperience (e.g., identifying metric-to-anomaly relations) for effective\ndiagnosis and recovery. However, existing automatic database O&M methods,\nincluding commercial products, cannot effectively utilize expert experience. On\nthe one hand, rule-based methods only support basic O&M tasks (e.g.,\nmetric-based anomaly detection), which are mostly numerical equations and\ncannot effectively incorporate literal O&M experience (e.g., troubleshooting\nguidance in manuals). On the other hand, LLM-based methods, which retrieve\nfragmented information (e.g., standard documents + RAG), often generate\ninaccurate or generic results. To address these limitations, we present\nDBAIOps, a novel hybrid database O&M system that combines reasoning LLMs with\nknowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a\nheterogeneous graph model for representing the diagnosis experience, and\nproposes a semi-automatic graph construction algorithm to build that graph from\nthousands of documents. Second, DBAIOps develops a collection of (800+)\nreusable anomaly models that identify both directly alerted metrics and\nimplicitly correlated experience and metrics. Third, for each anomaly, DBAIOps\nproposes a two-stage graph evolution mechanism to explore relevant diagnosis\npaths and identify missing relations automatically. It then leverages a\nreasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear\ndiagnosis reports for both DBAs and common users. Our evaluation over four\nmainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates\nthat DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher\nin root cause and human evaluation accuracy, respectively.\n","authors":["Wei Zhou","Peng Sun","Xuanhe Zhou","Qianglei Zang","Ji Xu","Tieying Zhang","Guoliang Li","Fan Wu"],"pdf_url":"https://arxiv.org/pdf/2508.01136v1.pdf","comment":"DBAIOps supports 25 database systems and has been deployed in 20\n  real-world scenarios, covering domains like finance, energy, and healthcare.\n  See website at: https://www.dbaiops.com; See code at:\n  https://github.com/weAIDB/DBAIOps/"},{"id":"http://arxiv.org/abs/2508.01128v1","updated":"2025-08-02T00:53:40Z","published":"2025-08-02T00:53:40Z","title":"Towards Bridging Review Sparsity in Recommendation with Textual Edge\n  Graph Representation","summary":"  Textual reviews enrich recommender systems with fine-grained preference\nsignals and enhanced explainability. However, in real-world scenarios, users\nrarely leave reviews, resulting in severe sparsity that undermines the\neffectiveness of existing models. A natural solution is to impute or generate\nmissing reviews to enrich the data. However, conventional imputation techniques\n-- such as matrix completion and LLM-based augmentation -- either lose\ncontextualized semantics by embedding texts into vectors, or overlook\nstructural dependencies among user-item interactions. To address these\nshortcomings, we propose TWISTER (ToWards Imputation on Sparsity with Textual\nEdge Graph Representation), a unified framework that imputes missing reviews by\njointly modeling semantic and structural signals. Specifically, we represent\nuser-item interactions as a Textual-Edge Graph (TEG), treating reviews as edge\nattributes. To capture relational context, we construct line-graph views and\nemploy a large language model as a graph-aware aggregator. For each interaction\nlacking a textual review, our model aggregates the neighborhood's\nnatural-language representations to generate a coherent and personalized\nreview. Experiments on the Amazon and Goodreads datasets show that TWISTER\nconsistently outperforms traditional numeric, graph-based, and LLM baselines,\ndelivering higher-quality imputed reviews and, more importantly, enhanced\nrecommendation performance. In summary, TWISTER generates reviews that are more\nhelpful, authentic, and specific, while smoothing structural signals for\nimproved recommendations.\n","authors":["Leyao Wang","Xutao Mao","Xuhui Zhan","Yuying Zhao","Bo Ni","Ryan A. Rossi","Nesreen K. Ahmed","Tyler Derr"],"pdf_url":"https://arxiv.org/pdf/2508.01128v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2505.19849v2","updated":"2025-08-02T00:31:18Z","published":"2025-05-26T11:35:04Z","title":"HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems","summary":"  Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model.\n","authors":["Haoqiang Yang","Congde Yuan","Kun Bai","Mengzhuo Guo","Wei Yang","Chao Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.19849v2.pdf","comment":"7 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2505.12332v3","updated":"2025-08-02T15:09:52Z","published":"2025-05-18T09:58:48Z","title":"VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized\n  Diffusion-based Voice Cloning","summary":"  Diffusion Models (DMs) have achieved remarkable success in realistic voice\ncloning (VC), while they also increase the risk of malicious misuse. Existing\nproactive defenses designed for traditional VC models aim to disrupt the\nforgery process, but they have been proven incompatible with DMs due to the\nintricate generative mechanisms of diffusion. To bridge this gap, we introduce\nVoiceCloak, a multi-dimensional proactive defense framework with the goal of\nobfuscating speaker identity and degrading perceptual quality in potential\nunauthorized VC. To achieve these goals, we conduct a focused analysis to\nidentify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt\nthe cloning process by introducing adversarial perturbations into the reference\naudio. Specifically, to obfuscate speaker identity, VoiceCloak first targets\nspeaker identity by distorting representation learning embeddings to maximize\nidentity variation, which is guided by auditory perception principles.\nAdditionally, VoiceCloak disrupts crucial conditional guidance processes,\nparticularly attention context, thereby preventing the alignment of vocal\ncharacteristics that are essential for achieving convincing cloning. Then, to\naddress the second objective, VoiceCloak introduces score magnitude\namplification to actively steer the reverse trajectory away from the generation\nof high-quality speech. Noise-guided semantic corruption is further employed to\ndisrupt structural speech semantics captured by DMs, degrading output quality.\nExtensive experiments highlight VoiceCloak's outstanding defense success rate\nagainst unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak\nare available at https://voice-cloak.github.io/VoiceCloak/.\n","authors":["Qianyue Hu","Junyan Wu","Wei Lu","Xiangyang Luo"],"pdf_url":"https://arxiv.org/pdf/2505.12332v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15640v2","updated":"2025-08-02T13:50:28Z","published":"2023-07-28T16:00:21Z","title":"CLIP Brings Better Features to Visual Aesthetics Learners","summary":"  Image Aesthetics Assessment (IAA) is a challenging task due to its subjective\nnature and expensive manual annotations. Recent large-scale vision-language\nmodels, such as Contrastive Language-Image Pre-training (CLIP), have shown\ntheir promising representation capability for various downstream tasks.\nHowever, the application of CLIP to resource-constrained and low-data IAA tasks\nremains limited. While few attempts to leverage CLIP in IAA have mainly focused\non carefully designed prompts, we extend beyond this by allowing models from\ndifferent domains and with different model sizes to acquire knowledge from\nCLIP. To achieve this, we propose a unified and flexible two-phase CLIP-based\nSemi-supervised Knowledge Distillation (CSKD) paradigm, aiming to learn a\nlightweight IAA model while leveraging CLIP's strong generalization capability.\nSpecifically, CSKD employs a feature alignment strategy to facilitate the\ndistillation of heterogeneous CLIP teacher and IAA student models, effectively\ntransferring valuable features from pre-trained visual representations to two\nlightweight IAA models, respectively. To efficiently adapt to downstream IAA\ntasks in a low-data regime, the two strong visual aesthetics learners then\nconduct distillation with unlabeled examples for refining and transferring the\ntask-specific knowledge collaboratively. Extensive experiments demonstrate that\nthe proposed CSKD achieves state-of-the-art performance on multiple widely used\nIAA benchmarks. Furthermore, analysis of attention distance and entropy before\nand after feature alignment shows the effective transfer of CLIP's feature\nrepresentation to IAA models, which not only provides valuable guidance for the\nmodel initialization of IAA but also enhances the aesthetic feature\nrepresentation of IAA models. Code will be made publicly available.\n","authors":["Liwu Xu","Jinjin Xu","Yuzhe Yang","Xilu Wang","Yijie Huang","Yaqian Li"],"pdf_url":"https://arxiv.org/pdf/2307.15640v2.pdf","comment":"ICME 2025 Oral"},{"id":"http://arxiv.org/abs/2507.19821v2","updated":"2025-08-02T13:22:34Z","published":"2025-07-26T06:38:07Z","title":"LAVA: Language Driven Scalable and Versatile Traffic Video Analytics","summary":"  In modern urban environments, camera networks generate massive amounts of\noperational footage -- reaching petabytes each day -- making scalable video\nanalytics essential for efficient processing. Many existing approaches adopt an\nSQL-based paradigm for querying such large-scale video databases; however, this\nconstrains queries to rigid patterns with predefined semantic categories,\nsignificantly limiting analytical flexibility. In this work, we explore a\nlanguage-driven video analytics paradigm aimed at enabling flexible and\nefficient querying of high-volume video data driven by natural language.\nParticularly, we build \\textsc{Lava}, a system that accepts natural language\nqueries and retrieves traffic targets across multiple levels of granularity and\narbitrary categories. \\textsc{Lava} comprises three main components: 1) a\nmulti-armed bandit-based efficient sampling method for video segment-level\nlocalization;\n  2) a video-specific open-world detection module for object-level retrieval;\nand 3) a long-term object trajectory extraction scheme for temporal object\nassociation, yielding complete trajectories for object-of-interests. To support\ncomprehensive evaluation, we further develop a novel benchmark by providing\ndiverse, semantically rich natural language predicates and fine-grained\nannotations for multiple videos. Experiments on this benchmark demonstrate that\n\\textsc{Lava} improves $F_1$-scores for selection queries by $\\mathbf{14\\%}$,\nreduces MPAE for aggregation queries by $\\mathbf{0.39}$, and achieves top-$k$\nprecision of $\\mathbf{86\\%}$, while processing videos $ \\mathbf{9.6\\times} $\nfaster than the most accurate baseline. Our code and dataset are available at\nhttps://github.com/yuyanrui/LAVA.\n","authors":["Yanrui Yu","Tianfei Zhou","Jiaxin Sun","Lianpeng Qiao","Lizhong Ding","Ye Yuan","Guoren Wang"],"pdf_url":"https://arxiv.org/pdf/2507.19821v2.pdf","comment":"Accepted by ACM MM 2025, code: https://github.com/yuyanrui/LAVA"},{"id":"http://arxiv.org/abs/2505.13023v3","updated":"2025-08-02T11:16:27Z","published":"2025-05-19T12:07:29Z","title":"Anti-Inpainting: A Proactive Defense Approach against Malicious\n  Diffusion-based Inpainters under Unknown Conditions","summary":"  With the increasing prevalence of diffusion-based malicious image\nmanipulation, existing proactive defense methods struggle to safeguard images\nagainst tampering under unknown conditions. To address this, we propose\nAnti-Inpainting, a proactive defense approach that achieves protection\ncomprising three novel modules. First, we introduce a multi-level deep feature\nextractor to obtain intricate features from the diffusion denoising process,\nenhancing protective effectiveness. Second, we design a multi-scale,\nsemantic-preserving data augmentation technique to enhance the transferability\nof adversarial perturbations across unknown conditions. Finally, we propose a\nselection-based distribution deviation optimization strategy to bolster\nprotection against manipulations guided by diverse random seeds. Extensive\nexperiments on InpaintGuardBench and CelebA-HQ demonstrate that Anti-Inpainting\neffectively defends against diffusion-based inpainters under unknown\nconditions. Additionally, our approach demonstrates robustness against various\nimage purification methods and transferability across different diffusion model\nversions.\n","authors":["Yimao Guo","Zuomin Qu","Wei Lu","Xiangyang Luo"],"pdf_url":"https://arxiv.org/pdf/2505.13023v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01226v1","updated":"2025-08-02T06:44:59Z","published":"2025-08-02T06:44:59Z","title":"CM$^3$: Calibrating Multimodal Recommendation","summary":"  Alignment and uniformity are fundamental principles within the domain of\ncontrastive learning. In recommender systems, prior work has established that\noptimizing the Bayesian Personalized Ranking (BPR) loss contributes to the\nobjectives of alignment and uniformity. Specifically, alignment aims to draw\ntogether the representations of interacting users and items, while uniformity\nmandates a uniform distribution of user and item embeddings across a unit\nhypersphere. This study revisits the alignment and uniformity properties within\nthe context of multimodal recommender systems, revealing a proclivity among\nextant models to prioritize uniformity to the detriment of alignment. Our\nhypothesis challenges the conventional assumption of equitable item treatment\nthrough a uniformity loss, proposing a more nuanced approach wherein items with\nsimilar multimodal attributes converge toward proximal representations within\nthe hyperspheric manifold. Specifically, we leverage the inherent similarity\nbetween items' multimodal data to calibrate their uniformity distribution,\nthereby inducing a more pronounced repulsive force between dissimilar entities\nwithin the embedding space. A theoretical analysis elucidates the relationship\nbetween this calibrated uniformity loss and the conventional uniformity\nfunction. Moreover, to enhance the fusion of multimodal features, we introduce\na Spherical B\\'ezier method designed to integrate an arbitrary number of\nmodalities while ensuring that the resulting fused features are constrained to\nthe same hyperspherical manifold. Empirical evaluations conducted on five\nreal-world datasets substantiate the superiority of our approach over competing\nbaselines. We also shown that the proposed methods can achieve up to a 5.4%\nincrease in NDCG@20 performance via the integration of MLLM-extracted features.\nSource code is available at: https://github.com/enoche/CM3.\n","authors":["Xin Zhou","Yongjie Wang","Zhiqi Shen"],"pdf_url":"https://arxiv.org/pdf/2508.01226v1.pdf","comment":"Working Paper: https://github.com/enoche/CM3"},{"id":"http://arxiv.org/abs/2508.01205v1","updated":"2025-08-02T05:41:52Z","published":"2025-08-02T05:41:52Z","title":"Conquering High Packet-Loss Erasure: MoE Swin Transformer-Based Video\n  Semantic Communication","summary":"  Semantic communication with joint semantic-channel coding robustly transmits\ndiverse data modalities but faces challenges in mitigating semantic information\nloss due to packet drops in packet-based systems. Under current protocols,\npackets with errors are discarded, preventing the receiver from utilizing\nerroneous semantic data for robust decoding. To address this issue, a\npacket-loss-resistant MoE Swin Transformer-based Video Semantic Communication\n(MSTVSC) system is proposed in this paper. Semantic vectors are encoded by\nMSTVSC and transmitted through upper-layer protocol packetization. To\ninvestigate the impact of the packetization, a theoretical analysis of the\npacketization strategy is provided. To mitigate the semantic loss caused by\npacket loss, a 3D CNN at the receiver recovers missing information using\nun-lost semantic data and an packet-loss mask matrix. Semantic-level\ninterleaving is employed to reduce concentrated semantic loss from packet\ndrops. To improve compression, a common-individual decomposition approach is\nadopted, with downsampling applied to individual information to minimize\nredundancy. The model is lightweighted for practical deployment. Extensive\nsimulations and comparisons demonstrate strong performance, achieving an\nMS-SSIM greater than 0.6 and a PSNR exceeding 20 dB at a 90% packet loss rate.\n","authors":["Lei Teng","Senran Fan","Chen Dong","Haotai Liang","Zhicheng Bao","Xiaodong Xu","Rui Meng","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.01205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01181v1","updated":"2025-08-02T04:03:44Z","published":"2025-08-02T04:03:44Z","title":"Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion\n  Reasoning","summary":"  Despite their strong performance in multimodal emotion reasoning, existing\nMultimodal Large Language Models (MLLMs) often overlook the scenarios involving\nemotion conflicts, where emotional cues from different modalities are\ninconsistent. To fill this gap, we first introduce CA-MER, a new benchmark\ndesigned to examine MLLMs under realistic emotion conflicts. It consists of\nthree subsets: video-aligned, audio-aligned, and consistent, where only one or\nall modalities reflect the true emotion. However, evaluations on our CA-MER\nreveal that current state-of-the-art emotion MLLMs systematically over-rely on\naudio signal during emotion conflicts, neglecting critical cues from visual\nmodality. To mitigate this bias, we propose MoSEAR, a parameter-efficient\nframework that promotes balanced modality integration. MoSEAR consists of two\nmodules: (1)MoSE, modality-specific experts with a regularized gating mechanism\nthat reduces modality bias in the fine-tuning heads; and (2)AR, an attention\nreallocation mechanism that rebalances modality contributions in frozen\nbackbones during inference. Our framework offers two key advantages: it\nmitigates emotion conflicts and improves performance on consistent\nsamples-without incurring a trade-off between audio and visual modalities.\nExperiments on multiple benchmarks-including MER2023, EMER, DFEW, and our\nCA-MER-demonstrate that MoSEAR achieves state-of-the-art performance,\nparticularly under modality conflict conditions.\n","authors":["Zhiyuan Han","Beier Zhu","Yanlong Xu","Peipei Song","Xun Yang"],"pdf_url":"https://arxiv.org/pdf/2508.01181v1.pdf","comment":"ACM Multimedia 2025"},{"id":"http://arxiv.org/abs/2508.01168v1","updated":"2025-08-02T03:11:03Z","published":"2025-08-02T03:11:03Z","title":"Graph-based Interaction Augmentation Network for Robust Multimodal\n  Sentiment Analysis","summary":"  The inevitable modality imperfection in real-world scenarios poses\nsignificant challenges for Multimodal Sentiment Analysis (MSA). While existing\nmethods tailor reconstruction or joint representation learning strategies to\nrestore missing semantics, they often overlook complex dependencies within and\nacross modalities. Consequently, they fail to fully leverage available\nmodalities to capture complementary semantics. To this end, this paper proposes\na novel graph-based framework to exploit both intra- and inter-modality\ninteractions, enabling imperfect samples to derive missing semantics from\ncomplementary parts for robust MSA. Specifically, we first devise a learnable\nhypergraph to model intra-modality temporal dependencies to exploit contextual\ninformation within each modality. Then, a directed graph is employed to explore\ninter-modality correlations based on attention mechanism, capturing\ncomplementary information across different modalities. Finally, the knowledge\nfrom perfect samples is integrated to supervise our interaction processes,\nguiding the model toward learning reliable and robust joint representations.\nExtensive experiments on MOSI and MOSEI datasets demonstrate the effectiveness\nof our method.\n","authors":["Hu Zhangfeng","Shi mengxin"],"pdf_url":"https://arxiv.org/pdf/2508.01168v1.pdf","comment":null}]},"2025-08-01T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2507.08336v2","updated":"2025-08-01T22:33:29Z","published":"2025-07-11T06:28:35Z","title":"Distillation versus Contrastive Learning: How to Train Your Rerankers","summary":"  Training effective text rerankers is crucial for information retrieval. Two\nstrategies are widely used: contrastive learning (optimizing directly on\nground-truth labels) and knowledge distillation (transferring knowledge from a\nlarger reranker). While both have been studied extensively, a clear comparison\nof their effectiveness for training cross-encoder rerankers under practical\nconditions is needed.\n  This paper empirically compares these strategies by training rerankers of\ndifferent sizes and architectures using both methods on the same data, with a\nstrong contrastive learning model acting as the distillation teacher. Our\nresults show that knowledge distillation generally yields better in-domain and\nout-of-domain ranking performance than contrastive learning when distilling\nfrom a larger teacher model. This finding is consistent across student model\nsizes and architectures. However, distilling from a teacher of the same\ncapacity does not provide the same advantage, particularly for out-of-domain\ntasks. These findings offer practical guidance for choosing a training strategy\nbased on available teacher models. We recommend using knowledge distillation to\ntrain smaller rerankers if a larger, more powerful teacher is accessible; in\nits absence, contrastive learning remains a robust baseline.\n","authors":["Zhichao Xu","Zhiqi Huang","Shengyao Zhuang","Vivek Srikumar"],"pdf_url":"https://arxiv.org/pdf/2507.08336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01096v1","updated":"2025-08-01T22:22:35Z","published":"2025-08-01T22:22:35Z","title":"Cross-Domain Web Information Extraction at Pinterest","summary":"  The internet offers a massive repository of unstructured information, but\nit's a significant challenge to convert this into a structured format. At\nPinterest, the ability to accurately extract structured product data from\ne-commerce websites is essential to enhance user experiences and improve\ncontent distribution. In this paper, we present Pinterest's system for\nattribute extraction, which achieves remarkable accuracy and scalability at a\nmanageable cost. Our approach leverages a novel webpage representation that\ncombines structural, visual, and text modalities into a compact form,\noptimizing it for small model learning. This representation captures each\nvisible HTML node with its text, style and layout information. We show how this\nallows simple models such as eXtreme Gradient Boosting (XGBoost) to extract\nattributes more accurately than much more complex Large Language Models (LLMs)\nsuch as Generative Pre-trained Transformer (GPT). Our results demonstrate a\nsystem that is highly scalable, processing over 1,000 URLs per second, while\nbeing 1000 times more cost-effective than the cheapest GPT alternatives.\n","authors":["Michael Farag","Patrick Halina","Andrey Zaytsev","Alekhya Munagala","Imtihan Ahmed","Junhao Wang"],"pdf_url":"https://arxiv.org/pdf/2508.01096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01036v1","updated":"2025-08-01T19:28:57Z","published":"2025-08-01T19:28:57Z","title":"Addressing Cold Start For next-article Recommendation","summary":"  This replication study modifies ALMM, the Adaptive Linear Mapping Model\nconstructed for the next song recommendation, to the news recommendation\nproblem on the MIND dataset. The original version of ALMM computes latent\nrepresentations for users, last-time items, and current items in a tensor\nfactorization structure and learns a linear mapping from content features to\nlatent item vectors. Our replication aims to improve recommendation performance\nin cold-start scenarios by restructuring this model to sequential news click\nbehavior, viewing consecutively read articles as (last news, next news) tuples.\nInstead of the original audio features, we apply BERT and a TF-IDF (Term\nFrequency-Inverse Document Frequency) to news titles and abstracts to extract\ntoken contextualized representations and align them with triplet-based user\nreading patterns. We also propose a reproducibly thorough pre-processing\npipeline combining news filtering and feature integrity validation. Our\nimplementation of ALMM with TF-IDF shows relatively improved recommendation\naccuracy and robustness over Forbes and Oord baseline models in the cold-start\nscenario. We demonstrate that ALMM in a minimally modified state is not\nsuitable for next news recommendation.\n","authors":["Omar Elgohary","Nathan Jorgenson","Trenton Marple"],"pdf_url":"https://arxiv.org/pdf/2508.01036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08379v2","updated":"2025-08-01T19:19:55Z","published":"2025-03-11T12:39:04Z","title":"JurisTCU: A Brazilian Portuguese Information Retrieval Dataset with\n  Query Relevance Judgments","summary":"  This paper introduces JurisTCU, a Brazilian Portuguese dataset for legal\ninformation retrieval (LIR). The dataset is freely available and consists of\n16,045 jurisprudential documents from the Brazilian Federal Court of Accounts,\nalong with 150 queries annotated with relevance judgments. It addresses the\nscarcity of Portuguese-language LIR datasets with query relevance annotations.\nThe queries are organized into three groups: real user keyword-based queries,\nsynthetic keyword-based queries, and synthetic question-based queries.\nRelevance judgments were produced through a hybrid approach combining LLM-based\nscoring with expert domain validation. We used JurisTCU in 14 experiments using\nlexical search (document expansion methods) and semantic search (BERT-based and\nOpenAI embeddings). We show that the document expansion methods significantly\nimprove the performance of standard BM25 search on this dataset, with\nimprovements exceeding 45% in P@10, R@10, and nDCG@10 metrics when evaluating\nshort keyword-based queries. Among the embedding models, the OpenAI models\nproduced the best results, with improvements of approximately 70% in P@10,\nR@10, and nDCG@10 metrics for short keyword-based queries, suggesting that\nthese dense embeddings capture semantic relationships in this domain,\nsurpassing the reliance on lexical terms. Besides offering a dataset for the\nPortuguese-language IR research community, suitable for evaluating search\nsystems, the results also contribute to enhancing a search system highly\nrelevant to Brazilian citizens.\n","authors":["Leandro Carísio Fernandes","Leandro dos Santos Ribeiro","Marcos Vinícius Borela de Castro","Leonardo Augusto da Silva Pacheco","Edans Flávius de Oliveira Sandes"],"pdf_url":"https://arxiv.org/pdf/2503.08379v2.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2508.01005v1","updated":"2025-08-01T18:15:22Z","published":"2025-08-01T18:15:22Z","title":"MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented\n  Generation","summary":"  In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has\nbecome pivotal in enhancing response accuracy and reducing hallucination\nissues. The architecture of RAG systems varies significantly, encompassing\nsingle-round RAG, iterative RAG, and reasoning RAG, each tailored to address\ndifferent types of queries. Due to the varying complexity of real-world\nqueries, a fixed RAG pipeline often struggles to balance performance and cost\nefficiency across different queries. To address this challenge, we propose an\nadaptive RAG framework called MAO-ARAG, which leverages multi-agent\norchestration. Our adaptive RAG is conceived as a multi-turn framework.\nSpecifically, we define multiple executor agents, representing typical RAG\nmodules such as query reformulation agents, document selection agent, and\ngeneration agents. A planner agent intelligently selects and integrates the\nappropriate agents from these executors into a suitable workflow tailored for\neach query, striving for high-quality answers while maintaining reasonable\ncosts. During each turn, the planner agent is trained using reinforcement\nlearning, guided by an outcome-based reward (F1 score) and a cost-based\npenalty, continuously improving answer quality while keeping costs within a\nreasonable range. Experiments conducted on multiple QA datasets demonstrate\nthat our approach, which dynamically plans workflows for each query, not only\nachieves high answer quality but also maintains both cost and latency within\nacceptable limits.The code of MAO-ARAG is on\nhttps://github.com/chenyiqun/Agentic-RAG.\n","authors":["Yiqun Chen","Erhan Zhang","Lingyong Yan","Shuaiqiang Wang","Jizhou Huang","Dawei Yin","Jiaxin Mao"],"pdf_url":"https://arxiv.org/pdf/2508.01005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18365v3","updated":"2025-08-01T17:19:56Z","published":"2025-07-24T12:46:30Z","title":"RecPS: Privacy Risk Scoring for Recommender Systems","summary":"  Recommender systems (RecSys) have become an essential component of many web\napplications. The core of the system is a recommendation model trained on\nhighly sensitive user-item interaction data. While privacy-enhancing techniques\nare actively studied in the research community, the real-world model\ndevelopment still depends on minimal privacy protection, e.g., via controlled\naccess. Users of such systems should have the right to choose \\emph{not} to\nshare highly sensitive interactions. However, there is no method allowing the\nuser to know which interactions are more sensitive than others. Thus,\nquantifying the privacy risk of RecSys training data is a critical step to\nenabling privacy-aware RecSys model development and deployment. We propose a\nmembership-inference attack (MIA)- based privacy scoring method, RecPS, to\nmeasure privacy risks at both the interaction and user levels. The RecPS\ninteraction-level score definition is motivated and derived from differential\nprivacy, which is then extended to the user-level scoring method. A critical\ncomponent is the interaction-level MIA method RecLiRA, which gives high-quality\nmembership estimation. We have conducted extensive experiments on well-known\nbenchmark datasets and RecSys models to show the unique features and benefits\nof RecPS scoring in risk assessment and RecSys model unlearning.\n","authors":["Jiajie He","Yuechun Gu","Keke Chen"],"pdf_url":"https://arxiv.org/pdf/2507.18365v3.pdf","comment":"Accepted by ACM RecSys 2025; to appear"},{"id":"http://arxiv.org/abs/2508.00751v1","updated":"2025-08-01T16:28:18Z","published":"2025-08-01T16:28:18Z","title":"Harnessing the Power of Interleaving and Counterfactual Evaluation for\n  Airbnb Search Ranking","summary":"  Evaluation plays a crucial role in the development of ranking algorithms on\nsearch and recommender systems. It enables online platforms to create\nuser-friendly features that drive commercial success in a steady and effective\nmanner. The online environment is particularly conducive to applying causal\ninference techniques, such as randomized controlled experiments (known as A/B\ntest), which are often more challenging to implement in fields like medicine\nand public policy. However, businesses face unique challenges when it comes to\neffective A/B test. Specifically, achieving sufficient statistical power for\nconversion-based metrics can be time-consuming, especially for significant\npurchases like booking accommodations. While offline evaluations are quicker\nand more cost-effective, they often lack accuracy and are inadequate for\nselecting candidates for A/B test. To address these challenges, we developed\ninterleaving and counterfactual evaluation methods to facilitate rapid online\nassessments for identifying the most promising candidates for A/B tests. Our\napproach not only increased the sensitivity of experiments by a factor of up to\n100 (depending on the approach and metrics) compared to traditional A/B testing\nbut also streamlined the experimental process. The practical insights gained\nfrom usage in production can also benefit organizations with similar interests.\n","authors":["Qing Zhang","Alex Deng","Michelle Du","Huiji Gao","Liwei He","Sanjeev Katariya"],"pdf_url":"https://arxiv.org/pdf/2508.00751v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2503.22675v3","updated":"2025-08-01T16:17:57Z","published":"2025-03-28T17:59:03Z","title":"Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation","summary":"  Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose \\textbf{ReaRec}, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.\n","authors":["Jiakai Tang","Sunhao Dai","Teng Shi","Jun Xu","Xu Chen","Wen Chen","Jian Wu","Yuning Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.22675v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00710v1","updated":"2025-08-01T15:26:27Z","published":"2025-08-01T15:26:27Z","title":"Experimental Evaluation of Dynamic Topic Modeling Algorithms","summary":"  The amount of text generated daily on social media is gigantic and analyzing\nthis text is useful for many purposes. To understand what lies beneath a huge\namount of text, we need dependable and effective computing techniques from\nself-powered topic models. Nevertheless, there are currently relatively few\nthorough quantitative comparisons between these models. In this study, we\ncompare these models and propose an assessment metric that documents how the\ntopics change in time.\n","authors":["Ngozichukwuka Onah","Nadine Steinmetz","Hani Al-Sayeh","Kai-Uwe Sattler"],"pdf_url":"https://arxiv.org/pdf/2508.00710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00709v1","updated":"2025-08-01T15:23:20Z","published":"2025-08-01T15:23:20Z","title":"NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian\n  Common Law System","summary":"  Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality.\n","authors":["Shubham Kumar Nigam","Balaramamahanthi Deepak Patnaik","Shivam Mishra","Ajay Varghese Thomas","Noel Shallum","Kripabandhu Ghosh","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2508.00709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00679v1","updated":"2025-08-01T14:49:33Z","published":"2025-08-01T14:49:33Z","title":"Segment First, Retrieve Better: Realistic Legal Search via Rhetorical\n  Role-Based Queries","summary":"  Legal precedent retrieval is a cornerstone of the common law system, governed\nby the principle of stare decisis, which demands consistency in judicial\ndecisions. However, the growing complexity and volume of legal documents\nchallenge traditional retrieval methods. TraceRetriever mirrors real-world\nlegal search by operating with limited case information, extracting only\nrhetorically significant segments instead of requiring complete documents. Our\npipeline integrates BM25, Vector Database, and Cross-Encoder models, combining\ninitial results through Reciprocal Rank Fusion before final re-ranking.\nRhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier\ntrained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,\nTraceRetriever addresses growing document volume challenges while aligning with\npractical search constraints, reliable and scalable foundation for precedent\nretrieval enhancing legal research when only partial case knowledge is\navailable.\n","authors":["Shubham Kumar Nigam","Tanmay Dubey","Noel Shallum","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2508.00679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00589v1","updated":"2025-08-01T12:41:52Z","published":"2025-08-01T12:41:52Z","title":"Context-based Motion Retrieval using Open Vocabulary Methods for\n  Autonomous Driving","summary":"  Autonomous driving systems must operate reliably in safety-critical\nscenarios, particularly those involving unusual or complex behavior by\nVulnerable Road Users (VRUs). Identifying these edge cases in driving datasets\nis essential for robust evaluation and generalization, but retrieving such rare\nhuman behavior scenarios within the long tail of large-scale datasets is\nchallenging. To support targeted evaluation of autonomous driving systems in\ndiverse, human-centered scenarios, we propose a novel context-aware motion\nretrieval framework. Our method combines Skinned Multi-Person Linear\n(SMPL)-based motion sequences and corresponding video frames before encoding\nthem into a shared multimodal embedding space aligned with natural language.\nOur approach enables the scalable retrieval of human behavior and their context\nthrough text queries. This work also introduces our dataset WayMoCo, an\nextension of the Waymo Open Dataset. It contains automatically labeled motion\nand scene context descriptions derived from generated pseudo-ground-truth SMPL\nsequences and corresponding image data. Our approach outperforms\nstate-of-the-art models by up to 27.5% accuracy in motion-context retrieval,\nwhen evaluated on the WayMoCo dataset.\n","authors":["Stefan Englmeier","Max A. Büttner","Katharina Winter","Fabian B. Flohr"],"pdf_url":"https://arxiv.org/pdf/2508.00589v1.pdf","comment":"9 pages, 10 figure, project page\n  https://iv.ee.hm.edu/contextmotionclip/, submitted to IEEE Transactions on\n  Intelligent Vehicles (T-IV), This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2508.00579v1","updated":"2025-08-01T12:22:53Z","published":"2025-08-01T12:22:53Z","title":"MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for\n  Document Question-Answering with Hierarchical Index and Multi-Granularity\n  Retrieval","summary":"  The multi-modal long-context document question-answering task aims to locate\nand integrate multi-modal evidences (such as texts, tables, charts, images, and\nlayouts) distributed across multiple pages, for question understanding and\nanswer generation. The existing methods can be categorized into Large\nVision-Language Model (LVLM)-based and Retrieval-Augmented Generation\n(RAG)-based methods. However, the former were susceptible to hallucinations,\nwhile the latter struggled for inter-modal disconnection and cross-page\nfragmentation. To address these challenges, a novel multi-modal RAG model,\nnamed MMRAG-DocQA, was proposed, leveraging both textual and visual information\nacross long-range pages to facilitate accurate question answering. A\nhierarchical indexing method with the integration of flattened in-page chunks\nand topological cross-page chunks was designed to jointly establish in-page\nmulti-modal associations and long-distance cross-page dependencies. By means of\njoint similarity evaluation and large language model (LLM)-based re-ranking, a\nmulti-granularity semantic retrieval method, including the page-level parent\npage retrieval and document-level summary retrieval, was proposed to foster\nmulti-modal evidence connection and long-distance evidence integration and\nreasoning. Experimental results performed on public datasets, MMLongBench-Doc\nand LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in\nunderstanding and answering modality-rich and multi-page documents.\n","authors":["Ziyu Gong","Yihua Huang","Chengcheng Mai"],"pdf_url":"https://arxiv.org/pdf/2508.00579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00570v1","updated":"2025-08-01T12:11:10Z","published":"2025-08-01T12:11:10Z","title":"Session-Based Recommendation with Validated and Enriched LLM Intents","summary":"  Session-based recommendation (SBR) aims to predict the next item for an\nanonymous user in a timely manner. However, SBR suffers from data sparsity due\nto the short and anonymous nature of sessions. Recently, an emerging line of\nwork has explored inferring the underlying user intents of a session using\nlarge language models (LLMs), with the generated intents serving as auxiliary\ntraining signals to enhance SBR models. Despite its promise, this approach\nfaces three key challenges: validating intent quality, incorporating\nsession-level multi-intents, and complementing inevitable LLM failure cases. In\nthis paper, we propose VELI4SBR, a two-stage framework that leverages Validated\nand Enriched LLM-generated Intents for SBR. In the first stage, we generate\nhigh-quality intents using a predict-and-correct loop that validates the\ninformativeness of LLM-generated intents with a global intent pool to constrain\nthe LLM's output space and reduce hallucination. In the second stage, we\nenhance the SBR model using the generated intents through a lightweight\nmulti-intent prediction and fusion mechanism. Furthermore, we introduce a\ntraining strategy that compensates for LLM failures by inferring intents from\ninter-session behavioral similarities. Extensive experiments show that VELI4SBR\noutperforms state-of-the-art baselines while improving explainability.\n","authors":["Gyuseok Lee","Yaokun Liu","Yifan Liu","Susik Yoon","Dong Wang","SeongKu Kang"],"pdf_url":"https://arxiv.org/pdf/2508.00570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.01233v4","updated":"2025-08-01T09:49:52Z","published":"2022-12-02T15:23:15Z","title":"Safe machine learning model release from Trusted Research Environments:\n  The SACRO-ML package","summary":"  We present SACRO-ML, an integrated suite of open source Python tools to\nfacilitate the statistical disclosure control (SDC) of machine learning (ML)\nmodels trained on confidential data prior to public release. SACRO-ML combines\n(i) a SafeModel package that extends commonly used ML models to provide\nante-hoc SDC by assessing the vulnerability of disclosure posed by the training\nregime; and (ii) an Attacks package that provides post-hoc SDC by rigorously\nassessing the empirical disclosure risk of a model through a variety of\nsimulated attacks after training. The SACRO-ML code and documentation are\navailable under an MIT license at https://github.com/AI-SDC/SACRO-ML\n","authors":["Jim Smith","Richard J. Preen","Andrew McCarthy","Maha Albashir","Alba Crespi-Boixader","Shahzad Mumtaz","Christian Cole","James Liley","Jost Migenda","Simon Rogers","Yola Jones"],"pdf_url":"https://arxiv.org/pdf/2212.01233v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02962v4","updated":"2025-08-01T09:41:05Z","published":"2025-06-30T09:02:45Z","title":"RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs\n  through Multi-query Parallelism","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, while LLMs remain prone to generating hallucinated or outdated\nresponses due to their static internal knowledge. Recent advancements in\nRetrieval-Augmented Generation (RAG) methods have aimed to enhance models'\nsearch and reasoning capabilities through reinforcement learning (RL). Although\nthese methods demonstrate promising results, they face challenges in training\nstability and encounter issues such as substantial inference time and\nrestricted capabilities due to reliance on single-query mode. In this paper, we\npropose RAG-R1, a novel training framework designed to enable LLMs to\nadaptively leverage internal and external knowledge during the reasoning\nprocess. We further expand the generation and retrieval processes within the\nframework from single-query mode to multi-query parallelism, with the aim of\nreducing inference time and enhancing the model's capabilities. Extensive\nexperiments on seven question-answering benchmarks demonstrate that our method\noutperforms the strongest baseline by up to 13.2% and decreases inference time\nby 11.1%.\n","authors":["Zhiwen Tan","Jiaming Huang","Qintong Wu","Hongxuan Zhang","Chenyi Zhuang","Jinjie Gu"],"pdf_url":"https://arxiv.org/pdf/2507.02962v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00452v1","updated":"2025-08-01T09:16:26Z","published":"2025-08-01T09:16:26Z","title":"M^2VAE: Multi-Modal Multi-View Variational Autoencoder for Cold-start\n  Item Recommendation","summary":"  Cold-start item recommendation is a significant challenge in recommendation\nsystems, particularly when new items are introduced without any historical\ninteraction data. While existing methods leverage multi-modal content to\nalleviate the cold-start issue, they often neglect the inherent multi-view\nstructure of modalities, the distinction between shared and modality-specific\nfeatures. In this paper, we propose Multi-Modal Multi-View Variational\nAutoEncoder (M^2VAE), a generative model that addresses the challenges of\nmodeling common and unique views in attribute and multi-modal features, as well\nas user preferences over single-typed item features. Specifically, we generate\ntype-specific latent variables for item IDs, categorical attributes, and image\nfeatures, and use Product-of-Experts (PoE) to derive a common representation. A\ndisentangled contrastive loss decouples the common view from unique views while\npreserving feature informativeness. To model user inclinations, we employ a\npreference-guided Mixture-of-Experts (MoE) to adaptively fuse representations.\nWe further incorporate co-occurrence signals via contrastive learning,\neliminating the need for pretraining. Extensive experiments on real-world\ndatasets validate the effectiveness of our approach.\n","authors":["Chuan He","Yongchao Liu","Qiang Li","Wenliang Zhong","Chuntao Hong","Xinwei Yao"],"pdf_url":"https://arxiv.org/pdf/2508.00452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00450v1","updated":"2025-08-01T09:10:56Z","published":"2025-08-01T09:10:56Z","title":"When Relevance Meets Novelty: Dual-Stable Periodic Optimization for\n  Exploratory Recommendation","summary":"  Traditional recommendation systems tend to trap users in strong feedback\nloops by excessively pushing content aligned with their historical preferences,\nthereby limiting exploration opportunities and causing content fatigue.\nAlthough large language models (LLMs) demonstrate potential with their diverse\ncontent generation capabilities, existing LLM-enhanced dual-model frameworks\nface two major limitations: first, they overlook long-term preferences driven\nby group identity, leading to biased interest modeling; second, they suffer\nfrom static optimization flaws, as a one-time alignment process fails to\nleverage incremental user data for closed-loop optimization. To address these\nchallenges, we propose the Co-Evolutionary Alignment (CoEA) method. For\ninterest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE)\nmodule, jointly modeling long-term group identity and short-term individual\ninterests through parallel processing of behavioral sequences. For static\noptimization limitations, we design a Periodic Collaborative Optimization (PCO)\nmechanism. This mechanism regularly conducts preference verification on\nincremental data using the Relevance LLM, then guides the Novelty LLM to\nperform fine-tuning based on the verification results, and subsequently feeds\nback the output of the incrementally fine-tuned Novelty LLM to the Relevance\nLLM for re-evaluation, thereby achieving a dynamic closed-loop optimization.\nExtensive online and offline experiments verify the effectiveness of the CoEA\nmodel in exploratory recommendation.\n","authors":["Hongxiang Lin","Hao Guo","Zeshun Li","Erpeng Xue","Yongqian He","Xiangyu Hou","Zhaoyu Hu","Lei Wang","Sheng Chen"],"pdf_url":"https://arxiv.org/pdf/2508.00450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00956v1","updated":"2025-08-01T08:35:32Z","published":"2025-08-01T08:35:32Z","title":"Learning Unified User Quantized Tokenizers for User Representation","summary":"  Multi-source user representation learning plays a critical role in enabling\npersonalized services on web platforms (e.g., Alipay). While prior works have\nadopted late-fusion strategies to combine heterogeneous data sources, they\nsuffer from three key limitations: lack of unified representation frameworks,\nscalability and storage issues in data compression, and inflexible cross-task\ngeneralization. To address these challenges, we propose U^2QT (Unified User\nQuantized Tokenizers), a novel framework that integrates cross-domain knowledge\ntransfer with early fusion of heterogeneous domains. Our framework employs a\ntwo-stage architecture: first, a causal Q-Former projects domain-specific\nfeatures into a shared causal representation space to preserve inter-modality\ndependencies; second, a multi-view RQ-VAE discretizes causal embeddings into\ncompact tokens through shared and source-specific codebooks, enabling efficient\nstorage while maintaining semantic coherence. Experimental results showcase\nU^2QT's advantages across diverse downstream tasks, outperforming task-specific\nbaselines in future behavior prediction and recommendation tasks while\nachieving efficiency gains in storage and computation. The unified tokenization\nframework enables seamless integration with language models and supports\nindustrial-scale applications.\n","authors":["Chuan He","Yang Chen","Wuliang Huang","Tianyi Zheng","Jianhu Chen","Bin Dou","Yice Luo","Yun Zhu","Baokun Wang","Yongchao Liu","Xing Fu","Yu Cheng","Chuntao Hong","Weiqiang Wang","Xin-Wei Yao"],"pdf_url":"https://arxiv.org/pdf/2508.00956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00955v1","updated":"2025-08-01T07:31:24Z","published":"2025-08-01T07:31:24Z","title":"From Generator to Embedder: Harnessing Innate Abilities of Multimodal\n  LLMs via Building Zero-Shot Discriminative Embedding Model","summary":"  Multimodal Large Language Models (MLLMs) have emerged as a promising solution\nfor universal embedding tasks, yet adapting their generative nature for\ndiscriminative representation learning remains a significant challenge. The\ndominant paradigm of large-scale contrastive pre-training suffers from critical\ninefficiencies, including prohibitive computational costs and a failure to\nleverage the intrinsic, instruction-following capabilities of MLLMs. To\novercome these limitations, we propose an efficient framework for universal\nmultimodal embeddings, which bridges this gap by centering on two synergistic\ncomponents. First, our hierarchical embedding prompt template employs a\ntwo-level instruction architecture that forces the model to produce\ndiscriminative representations. Building on this strong foundation, our second\ncomponent, self-aware hard negative sampling, redefines the fine-tuning process\nby leveraging the model's own understanding to efficiently mine challenging\nnegatives while actively filtering out potential false negatives. Our\ncomprehensive experiments show that our hierarchical prompt achieves zero-shot\nperformance competitive with contrastively trained baselines and enhances the\nfine-tuning process by lifting a simple in-batch negative baseline by 4.8\npoints on the MMEB benchmark. We further boost the performance via our\nself-aware hard negative sampling, achieving the state-of-the-art performance\nwithout the contrative pre-training. Our work presents an effective and\nefficient pathway to adapt MLLMs for universal embedding tasks, significantly\nreducing training time.\n","authors":["Yeong-Joon Ju","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2508.00955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00271v1","updated":"2025-08-01T02:30:32Z","published":"2025-08-01T02:30:32Z","title":"MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning","summary":"  In this work, we propose MetaAgent, an agentic paradigm inspired by the\nprinciple of learning-by-doing, where expertise is developed through hands-on\npractice and continual self-improvement. MetaAgent starts with a minimal\nworkflow, equipped only with basic reasoning and adaptive help-seeking\nabilities. When a knowledge gap is encountered, MetaAgent generates natural\nlanguage help requests, which are routed to the most suitable external tool by\na dedicated tool router. As MetaAgent solves tasks, it continually conducts\nself-reflection and answer verification, distilling actionable experience into\nconcise texts that are dynamically incorporated into future task contexts.\nBesides, MetaAgent autonomously builds in-house tools and a persistent\nknowledge base by organizing its tool-use history, further enhancing its\nability to retrieve and integrate relevant information We term this continual,\ndata-driven process as \\textit{meta tool learning}, through which MetaAgent\nincrementally refines its reasoning and tool-use strategies, without changing\nmodel parameters or requiring further post-training. Evaluated on challenging\nknowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,\nMetaAgent consistently outperforms workflow-based baselines and matches or\nexceeds end-to-end trained agents, demonstrating the promise of self-evolving\nagentic systems for robust, general-purpose knowledge discovery. We provide our\nsource codes in https://github.com/qhjqhj00/MetaAgent.\n","authors":["Hongjin Qian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2508.00271v1.pdf","comment":"Technical Report, 14 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2506.01109v2","updated":"2025-08-01T23:31:48Z","published":"2025-06-01T18:19:47Z","title":"CountingFruit: Language-Guided 3D Fruit Counting with Semantic Gaussian\n  Splatting","summary":"  Accurate 3D fruit counting in orchards is challenging due to heavy occlusion,\nsemantic ambiguity between fruits and surrounding structures, and the high\ncomputational cost of volumetric reconstruction. Existing pipelines often rely\non multi-view 2D segmentation and dense volumetric sampling, which lead to\naccumulated fusion errors and slow inference. We introduce FruitLangGS, a\nlanguage-guided 3D fruit counting framework that reconstructs orchard-scale\nscenes using an adaptive-density Gaussian Splatting pipeline with radius-aware\npruning and tile-based rasterization, enabling scalable 3D representation.\nDuring inference, compressed CLIP-aligned semantic vectors embedded in each\nGaussian are filtered via a dual-threshold cosine similarity mechanism,\nretrieving Gaussians relevant to target prompts while suppressing common\ndistractors (e.g., foliage), without requiring retraining or image-space masks.\nThe selected Gaussians are then sampled into dense point clouds and clustered\ngeometrically to estimate fruit instances, remaining robust under severe\nocclusion and viewpoint variation. Experiments on nine different orchard-scale\ndatasets demonstrate that FruitLangGS consistently outperforms existing\npipelines in instance counting recall, avoiding multi-view segmentation fusion\nerrors and achieving up to 99.2\\% recall on Fuji-SfM orchard dataset. Ablation\nstudies further confirm that language-conditioned semantic embedding and\ndual-threshold prompt filtering are essential for suppressing distractors and\nimproving counting accuracy under heavy occlusion. Beyond fruit counting, the\nsame framework enables prompt-driven 3D semantic retrieval without retraining,\nhighlighting the potential of language-guided 3D perception for scalable\nagricultural scene understanding.\n","authors":["Fengze Li","Yangle Liu","Jieming Ma","Hai-Ning Liang","Yaochun Shen","Huangxiang Li","Zhijing Wu"],"pdf_url":"https://arxiv.org/pdf/2506.01109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12571v2","updated":"2025-08-01T21:40:57Z","published":"2025-07-16T18:41:42Z","title":"Catching Dark Signals in Algorithms: Unveiling Audiovisual and Thematic\n  Markers of Unsafe Content Recommended for Children and Teenagers","summary":"  The prevalence of short form video platforms, combined with the\nineffectiveness of age verification mechanisms, raises concerns about the\npotential harms facing children and teenagers in an algorithm-moderated online\nenvironment. We conducted multimodal feature analysis and thematic topic\nmodeling of 4,492 short videos recommended to children and teenagers on\nInstagram Reels, TikTok, and YouTube Shorts, collected as a part of an\nalgorithm auditing experiment. This feature-level and content-level analysis\nrevealed that unsafe (i.e., problematic, mentally distressing) short videos (a)\npossess darker visual features and (b) contain explicitly harmful content and\nimplicit harm from anxiety-inducing ordinary content. We introduce a useful\nframework of online harm (i.e., explicit, implicit, unintended), providing a\nunique lens for understanding the dynamic, multifaceted online risks facing\nchildren and teenagers. The findings highlight the importance of protecting\nyounger audiences in critical developmental stages from both explicit and\nimplicit risks on social media, calling for nuanced content moderation, age\nverification, and platform regulation.\n","authors":["Haoning Xue","Brian Nishimine","Martin Hilbert","Drew Cingel","Samantha Vigil","Jane Shawcroft","Arti Thakur","Zubair Shafiq","Jingwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.12571v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09766v2","updated":"2025-08-01T19:32:09Z","published":"2024-07-13T04:15:02Z","title":"User Digital Twin-Driven Video Streaming for Customized Preferences and\n  Adaptive Transcoding","summary":"  In the rapidly evolving field of multimedia services, video streaming has\nbecome increasingly prevalent, demanding innovative solutions to enhance user\nexperience and system efficiency. This paper introduces a novel approach that\nintegrates user digital twins-a dynamic digital representation of a user's\npreferences and behaviors-with traditional video streaming systems. We explore\nthe potential of this integration to dynamically adjust video preferences and\noptimize transcoding processes according to real-time data. The methodology\nleverages advanced machine learning algorithms to continuously update the\nuser's digital twin, which in turn informs the transcoding service to adapt\nvideo parameters for optimal quality and minimal buffering. Experimental\nresults show that our approach not only improves the personalization of content\ndelivery but also significantly enhances the overall efficiency of video\nstreaming services by reducing bandwidth usage and improving video playback\nquality. The implications of such advancements suggest a shift towards more\nadaptive, user-centric multimedia services, potentially transforming how video\ncontent is consumed and delivered.\n","authors":["Stephen Jimmy","Kalkidan Berhane","Kevin Muhammad"],"pdf_url":"https://arxiv.org/pdf/2407.09766v2.pdf","comment":"arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship and affiliation"},{"id":"http://arxiv.org/abs/2508.00782v1","updated":"2025-08-01T17:05:04Z","published":"2025-08-01T17:05:04Z","title":"SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware\n  Video Generation","summary":"  Audio-driven video generation aims to synthesize realistic videos that align\nwith input audio recordings, akin to the human ability to visualize scenes from\nauditory input. However, existing approaches predominantly focus on exploring\nsemantic information, such as the classes of sounding sources present in the\naudio, limiting their ability to generate videos with accurate content and\nspatial composition. In contrast, we humans can not only naturally identify the\nsemantic categories of sounding sources but also determine their deeply encoded\nspatial attributes, including locations and movement directions. This useful\ninformation can be elucidated by considering specific spatial indicators\nderived from the inherent physical properties of sound, such as loudness or\nfrequency. As prior methods largely ignore this factor, we present SpA2V, the\nfirst framework explicitly exploits these spatial auditory cues from audios to\ngenerate videos with high semantic and spatial correspondence. SpA2V decomposes\nthe generation process into two stages: 1) Audio-guided Video Planning: We\nmeticulously adapt a state-of-the-art MLLM for a novel task of harnessing\nspatial and semantic cues from input audio to construct Video Scene Layouts\n(VSLs). This serves as an intermediate representation to bridge the gap between\nthe audio and video modalities. 2) Layout-grounded Video Generation: We develop\nan efficient and effective approach to seamlessly integrate VSLs as conditional\nguidance into pre-trained diffusion models, enabling VSL-grounded video\ngeneration in a training-free manner. Extensive experiments demonstrate that\nSpA2V excels in generating realistic videos with semantic and spatial alignment\nto the input audios.\n","authors":["Kien T. Pham","Yingqing He","Yazhou Xing","Qifeng Chen","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2508.00782v1.pdf","comment":"The 33rd ACM Multimedia Conference (MM '25)"},{"id":"http://arxiv.org/abs/2502.05695v3","updated":"2025-08-01T15:02:11Z","published":"2025-02-08T21:14:28Z","title":"Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models\n  for Wireless Networks","summary":"  This paper proposes a novel Semantic Communication (SemCom) framework for\nreal-time adaptive-bitrate video streaming by integrating Latent Diffusion\nModels (LDMs) within the FFmpeg techniques. This solution addresses the\nchallenges of high bandwidth usage, storage inefficiencies, and quality of\nexperience (QoE) degradation associated with traditional Constant Bitrate\nStreaming (CBS) and Adaptive Bitrate Streaming (ABS). The proposed approach\nleverages LDMs to compress I-frames into a latent space, offering significant\nstorage and semantic transmission savings without sacrificing high visual\nquality. While retaining B-frames and P-frames as adjustment metadata to\nsupport efficient refinement of video reconstruction at the user side, the\nproposed framework further incorporates state-of-the-art denoising and Video\nFrame Interpolation (VFI) techniques. These techniques mitigate semantic\nambiguity and restore temporal coherence between frames, even in noisy wireless\ncommunication environments. Experimental results demonstrate the proposed\nmethod achieves high-quality video streaming with optimized bandwidth usage,\noutperforming state-of-the-art solutions in terms of QoE and resource\nefficiency. This work opens new possibilities for scalable real-time video\nstreaming in 5G and future post-5G networks.\n","authors":["Zijiang Yan","Jianhua Pei","Hongda Wu","Hina Tabassum","Ping Wang"],"pdf_url":"https://arxiv.org/pdf/2502.05695v3.pdf","comment":"Accepted in IEEE Wireless Communications"},{"id":"http://arxiv.org/abs/2508.00632v1","updated":"2025-08-01T13:45:13Z","published":"2025-08-01T13:45:13Z","title":"Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings","summary":"  While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches.\n","authors":["Alexia Jolicoeur-Martineau"],"pdf_url":"https://arxiv.org/pdf/2508.00632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15066v2","updated":"2025-08-01T13:03:05Z","published":"2025-07-20T18:02:50Z","title":"Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback","summary":"  Time series anomaly detection is critical across various domains, yet current\napproaches often limit analysis to mere binary anomaly classification without\ndetailed categorization or further explanatory reasoning. To address these\nlimitations, we propose a novel task, Time-series Reasoning for Anomaly\n(Time-RA) that transforms classical time series anomaly detection from a\ndiscriminative into a generative, reasoning-intensive task leveraging Large\nLanguage Models (LLMs). Also, we introduce the first real-world multimodal\nbenchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,\ncomprising approximately 40,000 samples across 10 real-world domains. Each\nsample includes numeric time series data, contextual text information, and\nvisual representations, each annotated with fine-grained categories (14 types\nfor univariate anomalies and 6 for multivariate anomalies) and structured\nexplanatory reasoning. We develop a sophisticated annotation framework\nutilizing ensemble-generated labels refined through GPT-4-driven feedback,\nensuring accuracy and interpretability. Extensive benchmarking of LLMs and\nmultimodal LLMs demonstrates the capabilities and limitations of current\nmodels, highlighting the critical role of supervised fine-tuning. Our dataset\nand task pave the way for significant advancements in interpretable time series\nanomaly detection and reasoning. The code\n(https://github.com/yyysjz1997/Time-RA) and dataset\n(https://huggingface.co/datasets/Time-RA/RATs40K) have been fully open-sourced\nto support and accelerate future research in this area.\n","authors":["Yiyuan Yang","Zichuan Liu","Lei Song","Kai Ying","Zhiguang Wang","Tom Bamford","Svitlana Vyetrenko","Jiang Bian","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2507.15066v2.pdf","comment":"Under review. 19 pages, 8 figures, 12 tables. Code and dataset are\n  publicly available"},{"id":"http://arxiv.org/abs/2507.19209v2","updated":"2025-08-01T12:27:28Z","published":"2025-07-25T12:29:21Z","title":"Querying Autonomous Vehicle Point Clouds: Enhanced by 3D Object Counting\n  with CounterNet","summary":"  Autonomous vehicles generate massive volumes of point cloud data, yet only a\nsubset is relevant for specific tasks such as collision detection, traffic\nanalysis, or congestion monitoring. Effectively querying this data is essential\nto enable targeted analytics. In this work, we formalize point cloud querying\nby defining three core query types: RETRIEVAL, COUNT, and AGGREGATION, each\naligned with distinct analytical scenarios. All these queries rely heavily on\naccurate object counts to produce meaningful results, making precise object\ncounting a critical component of query execution. Prior work has focused on\nindexing techniques for 2D video data, assuming detection models provide\naccurate counting information. However, when applied to 3D point cloud data,\nstate-of-the-art detection models often fail to generate reliable object\ncounts, leading to substantial errors in query results. To address this\nlimitation, we propose CounterNet, a heatmap-based network designed for\naccurate object counting in large-scale point cloud data. Rather than focusing\non accurate object localization, CounterNet detects object presence by finding\nobject centers to improve counting accuracy. We further enhance its performance\nwith a feature map partitioning strategy using overlapping regions, enabling\nbetter handling of both small and large objects in complex traffic scenes. To\nadapt to varying frame characteristics, we introduce a per-frame dynamic model\nselection strategy that selects the most effective configuration for each\ninput. Evaluations on three real-world autonomous vehicle datasets show that\nCounterNet improves counting accuracy by 5% to 20% across object categories,\nresulting in more reliable query outcomes across all supported query types.\n","authors":["Xiaoyu Zhang","Zhifeng Bao","Hai Dong","Ziwei Wang","Jiajun Liu"],"pdf_url":"https://arxiv.org/pdf/2507.19209v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00579v1","updated":"2025-08-01T12:22:53Z","published":"2025-08-01T12:22:53Z","title":"MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for\n  Document Question-Answering with Hierarchical Index and Multi-Granularity\n  Retrieval","summary":"  The multi-modal long-context document question-answering task aims to locate\nand integrate multi-modal evidences (such as texts, tables, charts, images, and\nlayouts) distributed across multiple pages, for question understanding and\nanswer generation. The existing methods can be categorized into Large\nVision-Language Model (LVLM)-based and Retrieval-Augmented Generation\n(RAG)-based methods. However, the former were susceptible to hallucinations,\nwhile the latter struggled for inter-modal disconnection and cross-page\nfragmentation. To address these challenges, a novel multi-modal RAG model,\nnamed MMRAG-DocQA, was proposed, leveraging both textual and visual information\nacross long-range pages to facilitate accurate question answering. A\nhierarchical indexing method with the integration of flattened in-page chunks\nand topological cross-page chunks was designed to jointly establish in-page\nmulti-modal associations and long-distance cross-page dependencies. By means of\njoint similarity evaluation and large language model (LLM)-based re-ranking, a\nmulti-granularity semantic retrieval method, including the page-level parent\npage retrieval and document-level summary retrieval, was proposed to foster\nmulti-modal evidence connection and long-distance evidence integration and\nreasoning. Experimental results performed on public datasets, MMLongBench-Doc\nand LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in\nunderstanding and answering modality-rich and multi-page documents.\n","authors":["Ziyu Gong","Yihua Huang","Chengcheng Mai"],"pdf_url":"https://arxiv.org/pdf/2508.00579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01064v4","updated":"2025-08-01T09:23:18Z","published":"2024-12-02T02:50:07Z","title":"FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking\n  Portrait","summary":"  With the rapid advancement of diffusion-based generative models, portrait\nimage animation has achieved remarkable results. However, it still faces\nchallenges in temporally consistent video generation and fast sampling due to\nits iterative sampling nature. This paper presents FLOAT, an audio-driven\ntalking portrait video generation method based on flow matching generative\nmodel. Instead of a pixel-based latent space, we take advantage of a learned\northogonal motion latent space, enabling efficient generation and editing of\ntemporally consistent motion. To achieve this, we introduce a transformer-based\nvector field predictor with an effective frame-wise conditioning mechanism.\nAdditionally, our method supports speech-driven emotion enhancement, enabling a\nnatural incorporation of expressive motions. Extensive experiments demonstrate\nthat our method outperforms state-of-the-art audio-driven talking portrait\nmethods in terms of visual quality, motion fidelity, and efficiency.\n","authors":["Taekyung Ki","Dongchan Min","Gyeongsu Chae"],"pdf_url":"https://arxiv.org/pdf/2412.01064v4.pdf","comment":"ICCV 2025. Project page:\n  https://deepbrainai-research.github.io/float/"},{"id":"http://arxiv.org/abs/2508.00260v1","updated":"2025-08-01T02:08:09Z","published":"2025-08-01T02:08:09Z","title":"Instruction-Grounded Visual Projectors for Continual Learning of\n  Generative Vision-Language Models","summary":"  Continual learning enables pre-trained generative vision-language models\n(VLMs) to incorporate knowledge from new tasks without retraining data from\nprevious ones. Recent methods update a visual projector to translate visual\ninformation for new tasks, connecting pre-trained vision encoders with large\nlanguage models. However, such adjustments may cause the models to prioritize\nvisual inputs over language instructions, particularly learning tasks with\nrepetitive types of textual instructions. To address the neglect of language\ninstructions, we propose a novel framework that grounds the translation of\nvisual information on instructions for language models. We introduce a mixture\nof visual projectors, each serving as a specialized visual-to-language\ntranslation expert based on the given instruction context to adapt to new\ntasks. To avoid using experts for irrelevant instruction contexts, we propose\nan expert recommendation strategy that reuses experts for tasks similar to\nthose previously learned. Additionally, we introduce expert pruning to\nalleviate interference from the use of experts that cumulatively activated in\nprevious tasks. Extensive experiments on diverse vision-language tasks\ndemonstrate that our method outperforms existing continual learning approaches\nby generating instruction-following responses.\n","authors":["Hyundong Jin","Hyung Jin Chang","Eunwoo Kim"],"pdf_url":"https://arxiv.org/pdf/2508.00260v1.pdf","comment":"Accepted to ICCV 2025"}]},"2025-07-31T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2508.00194v1","updated":"2025-07-31T22:27:22Z","published":"2025-07-31T22:27:22Z","title":"Audio Prototypical Network For Controllable Music Recommendation","summary":"  Traditional recommendation systems represent user preferences in dense\nrepresentations obtained through black-box encoder models. While these models\noften provide strong recommendation performance, they lack interpretability for\nusers, leaving users unable to understand or control the system's modeling of\ntheir preferences. This limitation is especially challenging in music\nrecommendation, where user preferences are highly personal and often evolve\nbased on nuanced qualities like mood, genre, tempo, or instrumentation. In this\npaper, we propose an audio prototypical network for controllable music\nrecommendation. This network expresses user preferences in terms of prototypes\nrepresentative of semantically meaningful features pertaining to musical\nqualities. We show that the model obtains competitive recommendation\nperformance compared to popular baseline models while also providing\ninterpretable and controllable user profiles.\n","authors":["Fırat Öncel","Emiliano Penaloza","Haolun Wu","Shubham Gupta","Mirco Ravanelli","Laurent Charlin","Cem Subakan"],"pdf_url":"https://arxiv.org/pdf/2508.00194v1.pdf","comment":"Accepted to MLSP2025"},{"id":"http://arxiv.org/abs/2507.22268v2","updated":"2025-07-31T20:53:24Z","published":"2025-07-29T22:38:39Z","title":"Multi-modal Relational Item Representation Learning for Inferring\n  Substitutable and Complementary Items","summary":"  We introduce a novel self-supervised multi-modal relational item\nrepresentation learning framework designed to infer substitutable and\ncomplementary items. Existing approaches primarily focus on modeling item-item\nassociations deduced from user behaviors using graph neural networks (GNNs) or\nleveraging item content information. However, these methods often overlook\ncritical challenges, such as noisy user behavior data and data sparsity due to\nthe long-tailed distribution of these behaviors. In this paper, we propose\nMMSC, a self-supervised multi-modal relational item representation learning\nframework to address these challenges. Specifically, MMSC consists of three\nmain components: (1) a multi-modal item representation learning module that\nleverages a multi-modal foundational model and learns from item metadata, (2) a\nself-supervised behavior-based representation learning module that denoises and\nlearns from user behavior data, and (3) a hierarchical representation\naggregation mechanism that integrates item representations at both the semantic\nand task levels. Additionally, we leverage LLMs to generate augmented training\ndata, further enhancing the denoising process during training. We conduct\nextensive experiments on five real-world datasets, showing that MMSC\noutperforms existing baselines by 26.1% for substitutable recommendation and\n39.2% for complementary recommendation. In addition, we empirically show that\nMMSC is effective in modeling cold-start items.\n","authors":["Junting Wang","Chenghuan Guo","Jiao Yang","Yanhui Guo","Yan Gao","Hari Sundaram"],"pdf_url":"https://arxiv.org/pdf/2507.22268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05885v2","updated":"2025-07-31T20:41:49Z","published":"2025-05-09T08:53:59Z","title":"Cost-Effective, Low Latency Vector Search with Azure Cosmos DB","summary":"  Vector indexing enables semantic search over diverse corpora and has become\nan important interface to databases for both users and AI agents. Efficient\nvector search requires deep optimizations in database systems. This has\nmotivated a new class of specialized vector databases that optimize for vector\nsearch quality and cost. Instead, we argue that a scalable, high-performance,\nand cost-efficient vector search system can be built inside a cloud-native\noperational database like Azure Cosmos DB while leveraging the benefits of a\ndistributed database such as high availability, durability, and scale. We do\nthis by deeply integrating DiskANN, a state-of-the-art vector indexing library,\ninside Azure Cosmos DB NoSQL. This system uses a single vector index per\npartition stored in existing index trees, and kept in sync with underlying\ndata. It supports < 20ms query latency over an index spanning 10 million\nvectors, has stable recall over updates, and offers approximately 43x and 12x\nlower query cost compared to Pinecone and Zilliz serverless enterprise\nproducts. It also scales out to billions of vectors via automatic partitioning.\nThis convergent design presents a point in favor of integrating vector indices\ninto operational databases in the context of recent debates on specialized\nvector databases, and offers a template for vector indexing in other databases.\n","authors":["Nitish Upreti","Harsha Vardhan Simhadri","Hari Sudan Sundar","Krishnan Sundaram","Samer Boshra","Balachandar Perumalswamy","Shivam Atri","Martin Chisholm","Revti Raman Singh","Greg Yang","Tamara Hass","Nitesh Dudhey","Subramanyam Pattipaka","Mark Hildebrand","Magdalen Manohar","Jack Moffitt","Haiyang Xu","Naren Datha","Suryansh Gupta","Ravishankar Krishnaswamy","Prashant Gupta","Abhishek Sahu","Hemeswari Varada","Sudhanshu Barthwal","Ritika Mor","James Codella","Shaun Cooper","Kevin Pilch","Simon Moreno","Aayush Kataria","Santosh Kulkarni","Neil Deshpande","Amar Sagare","Dinesh Billa","Zishan Fu","Vipul Vishal"],"pdf_url":"https://arxiv.org/pdf/2505.05885v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.00123v1","updated":"2025-07-31T19:23:57Z","published":"2025-07-31T19:23:57Z","title":"Melody-Lyrics Matching with Contrastive Alignment Loss","summary":"  The connection between music and lyrics is far beyond semantic bonds.\nConceptual pairs in the two modalities such as rhythm and rhyme, note duration\nand syllabic stress, and structure correspondence, raise a compelling yet\nseldom-explored direction in the field of music information retrieval. In this\npaper, we present melody-lyrics matching (MLM), a new task which retrieves\npotential lyrics for a given symbolic melody from text sources. Rather than\ngenerating lyrics from scratch, MLM essentially exploits the relationships\nbetween melody and lyrics. We propose a self-supervised representation learning\nframework with contrastive alignment loss for melody and lyrics. This has the\npotential to leverage the abundance of existing songs with paired melody and\nlyrics. No alignment annotations are required. Additionally, we introduce\nsylphone, a novel representation for lyrics at syllable-level activated by\nphoneme identity and vowel stress. We demonstrate that our method can match\nmelody with coherent and singable lyrics with empirical results and intuitive\nexamples. We open source code and provide matching examples on the companion\nwebpage: https://github.com/changhongw/mlm.\n","authors":["Changhong Wang","Michel Olvera","Gaël Richard"],"pdf_url":"https://arxiv.org/pdf/2508.00123v1.pdf","comment":"10 pages, 7 figures, 3 tables. This work has been submitted to the\n  IEEE for possible publication"},{"id":"http://arxiv.org/abs/2507.22879v2","updated":"2025-07-31T16:54:43Z","published":"2025-07-30T17:55:06Z","title":"RecGPT Technical Report","summary":"  Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem.\n","authors":["Chao Yi","Dian Chen","Gaoyang Guo","Jiakai Tang","Jian Wu","Jing Yu","Mao Zhang","Sunhao Dai","Wen Chen","Wenjun Yang","Yuning Jiang","Zhujin Gao","Bo Zheng","Chi Li","Dimin Wang","Dixuan Wang","Fan Li","Fan Zhang","Haibin Chen","Haozhuang Liu","Jialin Zhu","Jiamang Wang","Jiawei Wu","Jin Cui","Ju Huang","Kai Zhang","Kan Liu","Lang Tian","Liang Rao","Longbin Li","Lulu Zhao","Na He","Peiyang Wang","Qiqi Huang","Tao Luo","Wenbo Su","Xiaoxiao He","Xin Tong","Xu Chen","Xunke Xi","Yang Li","Yaxuan Wu","Yeqiu Yang","Yi Hu","Yinnan Song","Yuchen Li","Yujie Luo","Yujin Yuan","Yuliang Yan","Zhengyang Wang","Zhibo Xiao","Zhixin Ma","Zile Zhou","Ziqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.22879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12311v3","updated":"2025-07-31T16:17:52Z","published":"2025-07-16T15:06:29Z","title":"An Ecosystem for Ontology Interoperability","summary":"  Ontology interoperability is one of the complicated issues that restricts the\nuse of ontologies in knowledge graphs (KGs). Different ontologies with\nconflicting and overlapping concepts make it difficult to design, develop, and\ndeploy an interoperable ontology for downstream tasks. We propose an ecosystem\nfor ontology interoperability. The ecosystem employs three state-of-the-art\nsemantic techniques in different phases of the ontology engineering life cycle:\nontology design patterns (ODPs) in the design phase, ontology matching and\nversioning (OM\\&OV) in the develop phase, and ontology-compliant knowledge\ngraphs (OCKGs) in the deploy phase, to achieve better ontology interoperability\nand data integration in real-world applications. A case study of sensor\nobservation in the building domain validates the usefulness of the proposed\necosystem.\n","authors":["Zhangcheng Qiang"],"pdf_url":"https://arxiv.org/pdf/2507.12311v3.pdf","comment":"5 pages, 8 figures"},{"id":"http://arxiv.org/abs/2507.23669v1","updated":"2025-07-31T15:48:12Z","published":"2025-07-31T15:48:12Z","title":"Automating AI Failure Tracking: Semantic Association of Reports in AI\n  Incident Database","summary":"  Artificial Intelligence (AI) systems are transforming critical sectors such\nas healthcare, finance, and transportation, enhancing operational efficiency\nand decision-making processes. However, their deployment in high-stakes domains\nhas exposed vulnerabilities that can result in significant societal harm. To\nsystematically study and mitigate these risk, initiatives like the AI Incident\nDatabase (AIID) have emerged, cataloging over 3,000 real-world AI failure\nreports. Currently, associating a new report with the appropriate AI Incident\nrelies on manual expert intervention, limiting scalability and delaying the\nidentification of emerging failure patterns.\n  To address this limitation, we propose a retrieval-based framework that\nautomates the association of new reports with existing AI Incidents through\nsemantic similarity modeling. We formalize the task as a ranking problem, where\neach report-comprising a title and a full textual description-is compared to\npreviously documented AI Incidents based on embedding cosine similarity.\nBenchmarking traditional lexical methods, cross-encoder architectures, and\ntransformer-based sentence embedding models, we find that the latter\nconsistently achieve superior performance. Our analysis further shows that\ncombining titles and descriptions yields substantial improvements in ranking\naccuracy compared to using titles alone. Moreover, retrieval performance\nremains stable across variations in description length, highlighting the\nrobustness of the framework. Finally, we find that retrieval performance\nconsistently improves as the training set expands. Our approach provides a\nscalable and efficient solution for supporting the maintenance of the AIID.\n","authors":["Diego Russo","Gian Marco Orlando","Valerio La Gatta","Vincenzo Moscato"],"pdf_url":"https://arxiv.org/pdf/2507.23669v1.pdf","comment":"Accepted at the 28th European Conference on Artificial Intelligence\n  (ECAI 2025)"},{"id":"http://arxiv.org/abs/2507.23664v1","updated":"2025-07-31T15:43:51Z","published":"2025-07-31T15:43:51Z","title":"Personalized Education with Ranking Alignment Recommendation","summary":"  Personalized question recommendation aims to guide individual students\nthrough questions to enhance their mastery of learning targets. Most previous\nmethods model this task as a Markov Decision Process and use reinforcement\nlearning to solve, but they struggle with efficient exploration, failing to\nidentify the best questions for each student during training. To address this,\nwe propose Ranking Alignment Recommendation (RAR), which incorporates\ncollaborative ideas into the exploration mechanism, enabling more efficient\nexploration within limited training episodes. Experiments show that RAR\neffectively improves recommendation performance, and our framework can be\napplied to any RL-based question recommender. Our code is available in\nhttps://github.com/wuming29/RAR.git.\n","authors":["Haipeng Liu","Yuxuan Liu","Ting Long"],"pdf_url":"https://arxiv.org/pdf/2507.23664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.21903v2","updated":"2025-07-31T15:33:32Z","published":"2025-07-29T15:14:39Z","title":"Who's important? -- SUnSET: Synergistic Understanding of Stakeholder,\n  Events and Time for Timeline Generation","summary":"  As news reporting becomes increasingly global and decentralized online,\ntracking related events across multiple sources presents significant\nchallenges. Existing news summarization methods typically utilizes Large\nLanguage Models and Graphical methods on article-based summaries. However, this\nis not effective since it only considers the textual content of similarly dated\narticles to understand the gist of the event. To counteract the lack of\nanalysis on the parties involved, it is essential to come up with a novel\nframework to gauge the importance of stakeholders and the connection of related\nevents through the relevant entities involved. Therefore, we present SUnSET:\nSynergistic Understanding of Stakeholder, Events and Time for the task of\nTimeline Summarization (TLS). We leverage powerful Large Language Models (LLMs)\nto build SET triplets and introduced the use of stakeholder-based ranking to\nconstruct a $Relevancy$ metric, which can be extended into general situations.\nOur experimental results outperform all prior baselines and emerged as the new\nState-of-the-Art, highlighting the impact of stakeholder information within\nnews article.\n","authors":["Tiviatis Sim","Kaiwen Yang","Shen Xin","Kenji Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2507.21903v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06254v2","updated":"2025-07-31T11:48:39Z","published":"2024-11-09T19:03:56Z","title":"KeyB2: Selecting Key Blocks is Also Important for Long Document Ranking\n  with Large Language Models","summary":"  The emergence of large language models (LLMs) such as Llama has significantly\nadvanced neural information retrieval (IR). However, applying LLMs to long\ndocument reranking remains computationally expensive and may be ineffective.\nMoreover, the internal behavior of LLMs during document relevance judgment is\nstill underexplored. In this paper, we begin with an in-depth analysis of\ndecoder-only LLM attention patterns and find that several attention heads\nconsistently align with relevance signals, yet this alignment deteriorates as\nirrelevant content increases. Motivated by this observation, we revisit and\nextend the block selection paradigm, introducing KeyB2, a scalable reranking\nframework that combines block pre-selection with powerful decoder-only LLMs.\nKeyB2 generalizes the selection stage to support BM25, cross-encoder, and\nbi-encoder, and adapts LLM to compute fine-grained relevance scores. We further\nintroduce a new bi-encoder strategy that performs strongly and efficiently.\nExtensive experiments on TREC DL 2019/2023 document task, Robust04, and MLDR-zh\ndemonstrate that KeyB2 outperforms baselines including RankLLaMA,\nRankLLaMA-MaxP/AvgP, and KeyB, achieving new state-of-the-art (SOTA) results on\nTREC DL 2019 document reranking task. In addition, KeyB2 reduces reranking\nlatency compared with RankLLaMA by over 83% and memory usage by over 74%,\npositioning it as a practical and effective solution for long document ranking\nwith LLMs.\n","authors":["Minghan Li","Eric Gaussier","Juntao Li","Guodong Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.06254v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.23459v1","updated":"2025-07-31T11:37:11Z","published":"2025-07-31T11:37:11Z","title":"KLAN: Kuaishou Landing-page Adaptive Navigator","summary":"  Modern online platforms configure multiple pages to accommodate diverse user\nneeds. This multi-page architecture inherently establishes a two-stage\ninteraction paradigm between the user and the platform: (1) Stage I: page\nnavigation, navigating users to a specific page and (2) Stage II: in-page\ninteraction, where users engage with customized content within the specific\npage. While the majority of research has been focusing on the sequential\nrecommendation task that improves users' feedback in Stage II, there has been\nlittle investigation on how to achieve better page navigation in Stage I. To\nfill this gap, we formally define the task of Personalized Landing Page\nModeling (PLPM) into the field of recommender systems: Given a user upon app\nentry, the goal of PLPM is to proactively select the most suitable landing page\nfrom a set of candidates (e.g., functional tabs, content channels, or\naggregation pages) to optimize the short-term PDR metric and the long-term user\nengagement and satisfaction metrics, while adhering to industrial constraints.\nAdditionally, we propose KLAN (Kuaishou Landing-page Adaptive Navigator), a\nhierarchical solution framework designed to provide personalized landing pages\nunder the formulation of PLPM. KLAN comprises three key components: (1)\nKLAN-ISP captures inter-day static page preference; (2) KLAN-IIT captures\nintra-day dynamic interest transitions and (3) KLAN-AM adaptively integrates\nboth components for optimal navigation decisions. Extensive online experiments\nconducted on the Kuaishou platform demonstrate the effectiveness of KLAN,\nobtaining +0.205% and +0.192% improvements on in Daily Active Users (DAU) and\nuser Lifetime (LT). Our KLAN is ultimately deployed on the online platform at\nfull traffic, serving hundreds of millions of users. To promote further\nresearch in this important area, we will release our dataset and code upon\npaper acceptance.\n","authors":["Fan Li","Chang Meng","Jiaqi Fu","Shuchang Liu","Jiashuo Zhang","Tianke Zhang","Xueliang Wang","Xiaoqiang Feng"],"pdf_url":"https://arxiv.org/pdf/2507.23459v1.pdf","comment":"We propose PLPM, a new task for selecting optimal landing pages upon\n  user entry. Our solution, KLAN, models static and dynamic user interests and\n  is successfully deployed on Kuaishou, improving DAU and user lifetime"},{"id":"http://arxiv.org/abs/2507.23410v1","updated":"2025-07-31T10:33:47Z","published":"2025-07-31T10:33:47Z","title":"Towards LLM-Enhanced Product Line Scoping","summary":"  The idea of product line scoping is to identify the set of features and\nconfigurations that a product line should include, i.e., offer for\nconfiguration purposes. In this context, a major scoping task is to find a\nbalance between commercial relevance and technical feasibility. Traditional\nproduct line scoping approaches rely on formal feature models and require a\nmanual analysis which can be quite time-consuming. In this paper, we sketch how\nLarge Language Models (LLMs) can be applied to support product line scoping\ntasks with a natural language interaction based scoping process. Using a\nworking example from the smarthome domain, we sketch how LLMs can be applied to\nevaluate different feature model alternatives. We discuss open research\nchallenges regarding the integration of LLMs with product line scoping.\n","authors":["Alexander Felfernig","Damian Garber","Viet-Man Le","Sebastian Lubos","Thi Ngoc Trang Tran"],"pdf_url":"https://arxiv.org/pdf/2507.23410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16725v2","updated":"2025-07-31T10:20:56Z","published":"2025-07-22T16:08:12Z","title":"RAVine: Reality-Aligned Evaluation for Agentic Search","summary":"  Agentic search, as a more autonomous and adaptive paradigm of retrieval\naugmentation, is driving the evolution of intelligent search systems. However,\nexisting evaluation frameworks fail to align well with the goals of agentic\nsearch. First, the complex queries commonly used in current benchmarks often\ndeviate from realistic user search scenarios. Second, prior approaches tend to\nintroduce noise when extracting ground truth for end-to-end evaluations,\nleading to distorted assessments at a fine-grained level. Third, most current\nframeworks focus solely on the quality of final answers, neglecting the\nevaluation of the iterative process inherent to agentic search. To address\nthese limitations, we propose RAVine -- a Reality-Aligned eValuation framework\nfor agentic LLMs with search. RAVine targets multi-point queries and long-form\nanswers that better reflect user intents, and introduces an attributable ground\ntruth construction strategy to enhance the accuracy of fine-grained evaluation.\nMoreover, RAVine examines model's interaction with search tools throughout the\niterative process, and accounts for factors of efficiency. We benchmark a\nseries of models using RAVine and derive several insights, which we hope will\ncontribute to advancing the development of agentic search systems. The code and\ndatasets are available at https://github.com/SwordFaith/RAVine.\n","authors":["Yilong Xu","Xiang Long","Zhi Zheng","Jinhua Gao"],"pdf_url":"https://arxiv.org/pdf/2507.16725v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.23400v1","updated":"2025-07-31T10:14:03Z","published":"2025-07-31T10:14:03Z","title":"MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based\n  on Multi-Relational Graphs and Structural Entropy Minimization","summary":"  The core challenge faced by multi-document summarization is the complexity of\nrelationships among documents and the presence of information redundancy. Graph\nclustering is an effective paradigm for addressing this issue, as it models the\ncomplex relationships among documents using graph structures and reduces\ninformation redundancy through clustering, achieving significant research\nprogress. However, existing methods often only consider single-relational\ngraphs and require a predefined number of clusters, which hinders their ability\nto fully represent rich relational information and adaptively partition\nsentence groups to reduce redundancy. To overcome these limitations, we propose\nMRGSEM-Sum, an unsupervised multi-document summarization framework based on\nmulti-relational graphs and structural entropy minimization. Specifically, we\nconstruct a multi-relational graph that integrates semantic and discourse\nrelations between sentences, comprehensively modeling the intricate and dynamic\nconnections among sentences across documents. We then apply a two-dimensional\nstructural entropy minimization algorithm for clustering, automatically\ndetermining the optimal number of clusters and effectively organizing sentences\ninto coherent groups. Finally, we introduce a position-aware compression\nmechanism to distill each cluster, generating concise and informative\nsummaries. Extensive experiments on four benchmark datasets (Multi-News,\nDUC-2004, PubMed, and WikiSum) demonstrate that our approach consistently\noutperforms previous unsupervised methods and, in several cases, achieves\nperformance comparable to supervised models and large language models. Human\nevaluation demonstrates that the summaries generated by MRGSEM-Sum exhibit high\nconsistency and coverage, approaching human-level quality.\n","authors":["Yongbing Zhang","Fang Nan","Shengxiang Gao","Yuxin Huang","Kaiwen Tan","Zhengtao Yu"],"pdf_url":"https://arxiv.org/pdf/2507.23400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.23364v1","updated":"2025-07-31T09:20:04Z","published":"2025-07-31T09:20:04Z","title":"Holistic Evaluations of Topic Models","summary":"  Topic models are gaining increasing commercial and academic interest for\ntheir ability to summarize large volumes of unstructured text. As unsupervised\nmachine learning methods, they enable researchers to explore data and help\ngeneral users understand key themes in large text collections. However, they\nrisk becoming a 'black box', where users input data and accept the output as an\naccurate summary without scrutiny. This article evaluates topic models from a\ndatabase perspective, drawing insights from 1140 BERTopic model runs. The goal\nis to identify trade-offs in optimizing model parameters and to reflect on what\nthese findings mean for the interpretation and responsible use of topic models\n","authors":["Thomas Compton"],"pdf_url":"https://arxiv.org/pdf/2507.23364v1.pdf","comment":"10 pages, 6 tables"},{"id":"http://arxiv.org/abs/2507.23358v1","updated":"2025-07-31T09:08:59Z","published":"2025-07-31T09:08:59Z","title":"Text-to-SQL Task-oriented Dialogue Ontology Construction","summary":"  Large language models (LLMs) are widely used as general-purpose knowledge\nsources, but they rely on parametric knowledge, limiting explainability and\ntrustworthiness. In task-oriented dialogue (TOD) systems, this separation is\nexplicit, using an external database structured by an explicit ontology to\nensure explainability and controllability. However, building such ontologies\nrequires manual labels or supervised training. We introduce TeQoDO: a\nText-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM\nautonomously builds a TOD ontology from scratch without supervision using its\ninherent SQL programming capabilities combined with dialogue theory provided in\nthe prompt. We show that TeQoDO outperforms transfer learning approaches, and\nits constructed ontology is competitive on a downstream dialogue state tracking\ntask. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also\nscales to allow construction of much larger ontologies, which we investigate on\na Wikipedia and ArXiv dataset. We view this as a step towards broader\napplication of ontologies to increase LLM explainability.\n","authors":["Renato Vukovic","Carel van Niekerk","Michael Heck","Benjamin Ruppik","Hsien-Chin Lin","Shutong Feng","Nurul Lubis","Milica Gasic"],"pdf_url":"https://arxiv.org/pdf/2507.23358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.23334v1","updated":"2025-07-31T08:31:05Z","published":"2025-07-31T08:31:05Z","title":"MUST-RAG: MUSical Text Question Answering with Retrieval Augmented\n  Generation","summary":"  Recent advancements in Large language models (LLMs) have demonstrated\nremarkable capabilities across diverse domains. While they exhibit strong\nzero-shot performance on various tasks, LLMs' effectiveness in music-related\napplications remains limited due to the relatively small proportion of\nmusic-specific knowledge in their training data. To address this limitation, we\npropose MusT-RAG, a comprehensive framework based on Retrieval Augmented\nGeneration (RAG) to adapt general-purpose LLMs for text-only music question\nanswering (MQA) tasks. RAG is a technique that provides external knowledge to\nLLMs by retrieving relevant context information when generating answers to\nquestions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a\nmusic-specialized vector database for the retrieval stage, and (2) utilizes\ncontext information during both inference and fine-tuning processes to\neffectively transform general-purpose LLMs into music-specific models. Our\nexperiment demonstrates that MusT-RAG significantly outperforms traditional\nfine-tuning approaches in enhancing LLMs' music domain adaptation capabilities,\nshowing consistent improvements across both in-domain and out-of-domain MQA\nbenchmarks. Additionally, our MusWikiDB proves substantially more effective\nthan general Wikipedia corpora, delivering superior performance and\ncomputational efficiency.\n","authors":["Daeyong Kwon","SeungHeon Doh","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2507.23334v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.18518v2","updated":"2025-07-31T06:47:49Z","published":"2025-07-24T15:41:34Z","title":"Transform Before You Query: A Privacy-Preserving Approach for Vector\n  Retrieval with Embedding Space Alignment","summary":"  Vector Database (VDB) can efficiently index and search high-dimensional\nvector embeddings from unstructured data, crucially enabling fast semantic\nsimilarity search essential for modern AI applications like generative AI and\nrecommendation systems. Since current VDB service providers predominantly use\nproprietary black-box models, users are forced to expose raw query text to them\nvia API in exchange for the vector retrieval services. Consequently, if query\ntext involves confidential records from finance or healthcare domains, this\nmechanism inevitably leads to critical leakage of user's sensitive information.\nTo address this issue, we introduce STEER (\\textbf{S}ecure \\textbf{T}ransformed\n\\textbf{E}mbedding v\\textbf{E}ctor\\textbf{ R}etrieval), a private vector\nretrieval framework that leverages the alignment relationship between the\nsemantic spaces of different embedding models to derive approximate embeddings\nfor the query text. STEER performs the retrieval using the approximate\nembeddings within the original VDB and requires no modifications to the server\nside. Our theoretical and experimental analyses demonstrate that STEER\neffectively safeguards query text privacy while maintaining the retrieval\naccuracy. Even though approximate embeddings are approximations of the\nembeddings from proprietary models, they still prevent the providers from\nrecovering the query text through Embedding Inversion Attacks (EIAs). Extensive\nexperimental results show that Recall@100 of STEER can basically achieve a\ndecrease of less than 5\\%. Furthermore, even when searching within a text\ncorpus of millions of entries, STEER achieves a Recall@20 accuracy 20\\% higher\nthan current baselines.\n","authors":["Ruiqi He","Zekun Fei","Jiaqi Li","Xinyuan Zhu","Biao Yi","Siyi Lv","Weijie Liu","Zheli Liu"],"pdf_url":"https://arxiv.org/pdf/2507.18518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.23267v1","updated":"2025-07-31T05:56:21Z","published":"2025-07-31T05:56:21Z","title":"Your Spending Needs Attention: Modeling Financial Habits with\n  Transformers","summary":"  Predictive models play a crucial role in the financial industry, enabling\nrisk prediction, fraud detection, and personalized recommendations, where\nslight changes in core model performance can result in billions of dollars in\nrevenue or losses. While financial institutions have access to enormous amounts\nof user data (e.g., bank transactions, in-app events, and customer support\nlogs), leveraging this data effectively remains challenging due to its\ncomplexity and scale. Thus, in many financial institutions, most production\nmodels follow traditional machine learning (ML) approaches by converting\nunstructured data into manually engineered tabular features. Conversely, other\ndomains (e.g., natural language processing) have effectively utilized\nself-supervised learning (SSL) to learn rich representations from raw data,\nremoving the need for manual feature extraction. In this paper, we investigate\nusing transformer-based representation learning models for transaction data,\nhypothesizing that these models, trained on massive data, can provide a novel\nand powerful approach to understanding customer behavior. We propose a new\nmethod enabling the use of SSL with transaction data by adapting\ntransformer-based models to handle both textual and structured attributes. Our\napproach, denoted nuFormer, includes an end-to-end fine-tuning method that\nintegrates user embeddings with existing tabular features. Our experiments\ndemonstrate improvements for large-scale recommendation problems at Nubank.\nNotably, these gains are achieved solely through enhanced representation\nlearning rather than incorporating new data sources.\n","authors":["D. T. Braithwaite","Misael Cavalcanti","R. Austin McEver","Hiroto Udagawa","Daniel Silva","Rohan Ramanath","Felipe Meneses","Arissa Yoshida","Evan Wingert","Matheus Ramos","Brian Zanfelice","Aman Gupta"],"pdf_url":"https://arxiv.org/pdf/2507.23267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01124v2","updated":"2025-07-31T05:19:47Z","published":"2024-02-02T03:52:21Z","title":"TransFR: Transferable Federated Recommendation with Adapter Tuning on\n  Pre-trained Language Models","summary":"  Federated recommendations (FRs), facilitating multiple local clients to\ncollectively learn a global model without disclosing user private data, have\nemerged as a prevalent on-device service. In conventional FRs, a dominant\nparadigm is to utilize discrete identities to represent clients and items,\nwhich are then mapped to domain-specific embeddings to participate in model\ntraining. Despite considerable performance, we reveal three inherent\nlimitations that can not be ignored in federated settings, i.e.,\nnon-transferability across domains, ineffectiveness in cold-start settings, and\npotential privacy violations during federated training. To this end, we propose\na transferable federated recommendation model, TransFR, which delicately\nincorporates the general capabilities empowered by pre-trained models and the\npersonalized abilities by fine-tuning local private data. Specifically, it\nfirst learns domain-agnostic representations of items by exploiting pre-trained\nmodels with public textual corpora. To tailor for FR tasks, we further\nintroduce efficient federated adapter-tuning and test-time adaptation\nmechanisms, which facilitate personalized local adapters for each client by\nfitting their private data distributions. We theoretically prove the advantages\nof incorporating adapter tuning in FRs regarding both effectiveness and\nprivacy. Through extensive experiments, we show that our TransFR model\nsurpasses several state-of-the-art FRs on transferability.\n","authors":["Honglei Zhang","Zhiwei Li","Haoxuan Li","Xin Zhou","Jie Zhang","Yidong Li"],"pdf_url":"https://arxiv.org/pdf/2402.01124v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.23209v1","updated":"2025-07-31T03:05:05Z","published":"2025-07-31T03:05:05Z","title":"Not Just What, But When: Integrating Irregular Intervals to LLM for\n  Sequential Recommendation","summary":"  Time intervals between purchasing items are a crucial factor in sequential\nrecommendation tasks, whereas existing approaches focus on item sequences and\noften overlook by assuming the intervals between items are static. However,\ndynamic intervals serve as a dimension that describes user profiling on not\nonly the history within a user but also different users with the same item\nhistory. In this work, we propose IntervalLLM, a novel framework that\nintegrates interval information into LLM and incorporates the novel\ninterval-infused attention to jointly consider information of items and\nintervals. Furthermore, unlike prior studies that address the cold-start\nscenario only from the perspectives of users and items, we introduce a new\nviewpoint: the interval perspective to serve as an additional metric for\nevaluating recommendation methods on the warm and cold scenarios. Extensive\nexperiments on 3 benchmarks with both traditional- and LLM-based baselines\ndemonstrate that our IntervalLLM achieves not only 4.4% improvements in average\nbut also the best-performing warm and cold scenarios across all users, items,\nand the proposed interval perspectives. In addition, we observe that the cold\nscenario from the interval perspective experiences the most significant\nperformance drop among all recommendation methods. This finding underscores the\nnecessity of further research on interval-based cold challenges and our\nintegration of interval information in the realm of sequential recommendation\ntasks. Our code is available here:\nhttps://github.com/sony/ds-research-code/tree/master/recsys25-IntervalLLM.\n","authors":["Wei-Wei Du","Takuma Udagawa","Kei Tateno"],"pdf_url":"https://arxiv.org/pdf/2507.23209v1.pdf","comment":"Accepted by RecSys 2025 short paper track"},{"id":"http://arxiv.org/abs/2507.23208v1","updated":"2025-07-31T03:04:34Z","published":"2025-07-31T03:04:34Z","title":"Are Recommenders Self-Aware? Label-Free Recommendation Performance\n  Estimation via Model Uncertainty","summary":"  Can a recommendation model be self-aware? This paper investigates the\nrecommender's self-awareness by quantifying its uncertainty, which provides a\nlabel-free estimation of its performance. Such self-assessment can enable more\ninformed understanding and decision-making before the recommender engages with\nany users. To this end, we propose an intuitive and effective method,\nprobability-based List Distribution uncertainty (LiDu). LiDu measures\nuncertainty by determining the probability that a recommender will generate a\ncertain ranking list based on the prediction distributions of individual items.\nWe validate LiDu's ability to represent model self-awareness in two settings:\n(1) with a matrix factorization model on a synthetic dataset, and (2) with\npopular recommendation algorithms on real-world datasets. Experimental results\nshow that LiDu is more correlated with recommendation performance than a series\nof label-free performance estimators. Additionally, LiDu provides valuable\ninsights into the dynamic inner states of models throughout training and\ninference. This work establishes an empirical connection between recommendation\nuncertainty and performance, framing it as a step towards more transparent and\nself-evaluating recommender systems.\n","authors":["Jiayu Li","Ziyi Ye","Guohao Jian","Zhiqiang Guo","Weizhi Ma","Qingyao Ai","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.23208v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2507.23779v1","updated":"2025-07-31T17:59:09Z","published":"2025-07-31T17:59:09Z","title":"Phi-Ground Tech Report: Advancing Perception in GUI Grounding","summary":"  With the development of multimodal reasoning models, Computer Use Agents\n(CUAs), akin to Jarvis from \\textit{\"Iron Man\"}, are becoming a reality. GUI\ngrounding is a core component for CUAs to execute actual actions, similar to\nmechanical control in robotics, and it directly leads to the success or failure\nof the system. It determines actions such as clicking and typing, as well as\nrelated parameters like the coordinates for clicks. Current end-to-end\ngrounding models still achieve less than 65\\% accuracy on challenging\nbenchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from\nbeing ready for deployment. % , as a single misclick can result in unacceptable\nconsequences. In this work, we conduct an empirical study on the training of\ngrounding models, examining details from data collection to model training.\nUltimately, we developed the \\textbf{Phi-Ground} model family, which achieves\nstate-of-the-art performance across all five grounding benchmarks for models\nunder $10B$ parameters in agent settings. In the end-to-end model setting, our\nmodel still achieves SOTA results with scores of \\textit{\\textbf{43.2}} on\nScreenSpot-pro and \\textit{\\textbf{27.2}} on UI-Vision. We believe that the\nvarious details discussed in this paper, along with our successes and failures,\nnot only clarify the construction of grounding models but also benefit other\nperception tasks. Project homepage:\n\\href{https://zhangmiaosen2000.github.io/Phi-Ground/}{https://zhangmiaosen2000.github.io/Phi-Ground/}\n","authors":["Miaosen Zhang","Ziqiang Xu","Jialiang Zhu","Qi Dai","Kai Qiu","Yifan Yang","Chong Luo","Tianyi Chen","Justin Wagle","Tim Franklin","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2507.23779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.23585v1","updated":"2025-07-31T14:18:28Z","published":"2025-07-31T14:18:28Z","title":"Agency Among Agents: Designing with Hypertextual Friction in the\n  Algorithmic Web","summary":"  Today's algorithm-driven interfaces, from recommendation feeds to GenAI\ntools, often prioritize engagement and efficiency at the expense of user\nagency. As systems take on more decision-making, users have less control over\nwhat they see and how meaning or relationships between content are constructed.\nThis paper introduces \"Hypertextual Friction,\" a conceptual design stance that\nrepositions classical hypertext principles--friction, traceability, and\nstructure--as actionable values for reclaiming agency in algorithmically\nmediated environments. Through a comparative analysis of real-world\ninterfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image\ntools--we examine how different systems structure user experience, navigation,\nand authorship. We show that hypertext systems emphasize provenance,\nassociative thinking, and user-driven meaning-making, while algorithmic systems\ntend to obscure process and flatten participation. We contribute: (1) a\ncomparative analysis of how interface structures shape agency in user-driven\nversus agent-driven systems, and (2) a conceptual stance that offers\nhypertextual values as design commitments for reclaiming agency in an\nincreasingly algorithmic web.\n","authors":["Sophia Liu","Shm Garanganao Almeda"],"pdf_url":"https://arxiv.org/pdf/2507.23585v1.pdf","comment":"To appear in: Adjunct Proceedings of the 36th ACM Conference on\n  Hypertext and Social Media, Chicago, IL, USA, September 15-18, 2025"},{"id":"http://arxiv.org/abs/2503.15621v2","updated":"2025-07-31T12:41:25Z","published":"2025-03-19T18:10:12Z","title":"LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for\n  Enhanced Visual Instruction Tuning","summary":"  Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE.\n","authors":["Federico Cocchi","Nicholas Moratelli","Davide Caffagni","Sara Sarto","Lorenzo Baraldi","Marcella Cornia","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2503.15621v2.pdf","comment":"ICCV 2025 Workshop on What is Next in Multimodal Foundation Models"},{"id":"http://arxiv.org/abs/2507.23444v1","updated":"2025-07-31T11:23:12Z","published":"2025-07-31T11:23:12Z","title":"Hybrid CNN-Mamba Enhancement Network for Robust Multimodal Sentiment\n  Analysis","summary":"  Multimodal Sentiment Analysis (MSA) with missing modalities has recently\nattracted increasing attention. Although existing research mainly focuses on\ndesigning complex model architectures to handle incomplete data, it still faces\nsignificant challenges in effectively aligning and fusing multimodal\ninformation. In this paper, we propose a novel framework called the Hybrid\nCNN-Mamba Enhancement Network (HCMEN) for robust multimodal sentiment analysis\nunder missing modality conditions. HCMEN is designed around three key\ncomponents: (1) hierarchical unimodal modeling, (2) cross-modal enhancement and\nalignment, and (3) multimodal mix-up fusion. First, HCMEN integrates the\nstrengths of Convolutional Neural Network (CNN) for capturing local details and\nthe Mamba architecture for modeling global contextual dependencies across\ndifferent modalities. Furthermore, grounded in the principle of Mutual\nInformation Maximization, we introduce a cross-modal enhancement mechanism that\ngenerates proxy modalities from mixed token-level representations and learns\nfine-grained token-level correspondences between modalities. The enhanced\nunimodal features are then fused and passed through the CNN-Mamba backbone,\nenabling local-to-global cross-modal interaction and comprehensive multimodal\nintegration. Extensive experiments on two benchmark MSA datasets demonstrate\nthat HCMEN consistently outperforms existing state-of-the-art methods,\nachieving superior performance across various missing modality scenarios. The\ncode will be released publicly in the near future.\n","authors":["Xiang Li","Xianfu Cheng","Xiaoming Zhang","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2507.23444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10006v2","updated":"2025-07-31T07:57:18Z","published":"2025-04-12T11:24:06Z","title":"HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic\n  Bidirectional Reconstruction","summary":"  In breast cancer HER2 assessment, clinical evaluation relies on combined H&E\nand IHC images, yet acquiring both modalities is often hindered by clinical\nconstraints and cost. We propose an adaptive bimodal prediction framework that\nflexibly supports single- or dual-modality inputs through two core innovations:\na dynamic branch selector activating modality completion or joint inference\nbased on input availability, and a cross-modal GAN (CM-GAN) enabling\nfeature-space reconstruction of missing modalities. This design dramatically\nimproves H&E-only accuracy from 71.44% to 94.25%, achieves 95.09% with full\ndual-modality inputs, and maintains 90.28% reliability under single-modality\nconditions. The \"dual-modality preferred, single-modality compatible\"\narchitecture delivers near-dual-modality accuracy without mandatory\nsynchronized acquisition, offering a cost-effective solution for\nresource-limited regions and significantly improving HER2 assessment\naccessibility.\n","authors":["Jie Qin","Wei Yang","Yan Su","Yiran Zhu","Weizhen Li","Yunyue Pan","Chengchang Pan","Honggang Qi"],"pdf_url":"https://arxiv.org/pdf/2506.10006v2.pdf","comment":"8 pages,6 figures,3 tables,accepted by the 33rd ACM International\n  Conference on Multimedia(ACM MM 2025)"}]},"2025-07-30T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2507.22880v1","updated":"2025-07-30T17:55:09Z","published":"2025-07-30T17:55:09Z","title":"AUV-Fusion: Cross-Modal Adversarial Fusion of User Interactions and\n  Visual Perturbations Against VARS","summary":"  Modern Visual-Aware Recommender Systems (VARS) exploit the integration of\nuser interaction data and visual features to deliver personalized\nrecommendations with high precision. However, their robustness against\nadversarial attacks remains largely underexplored, posing significant risks to\nsystem reliability and security. Existing attack strategies suffer from notable\nlimitations: shilling attacks are costly and detectable, and visual-only\nperturbations often fail to align with user preferences. To address these\nchallenges, we propose AUV-Fusion, a cross-modal adversarial attack framework\nthat adopts high-order user preference modeling and cross-modal adversary\ngeneration. Specifically, we obtain robust user embeddings through multi-hop\nuser-item interactions and transform them via an MLP into semantically aligned\nperturbations. These perturbations are injected onto the latent space of a\npre-trained VAE within the diffusion model. By synergistically integrating\ngenuine user interaction data with visually plausible perturbations, AUV-Fusion\neliminates the need for injecting fake user profiles and effectively mitigates\nthe challenge of insufficient user preference extraction inherent in\ntraditional visual-only attacks. Comprehensive evaluations on diverse VARS\narchitectures and real-world datasets demonstrate that AUV-Fusion significantly\nenhances the exposure of target (cold-start) items compared to conventional\nbaseline methods. Moreover, AUV-Fusion maintains exceptional stealth under\nrigorous scrutiny.\n","authors":["Hai Ling","Tianchi Wang","Xiaohao Liu","Zhulin Tao","Lifang Yang","Xianglin Huang"],"pdf_url":"https://arxiv.org/pdf/2507.22880v1.pdf","comment":"14 pages,6 figures"},{"id":"http://arxiv.org/abs/2507.22878v1","updated":"2025-07-30T17:54:38Z","published":"2025-07-30T17:54:38Z","title":"GeoOutageKG: A Multimodal Geospatiotemporal Knowledge Graph for\n  Multiresolution Power Outage Analysis","summary":"  Detecting, analyzing, and predicting power outages is crucial for grid risk\nassessment and disaster mitigation. Numerous outages occur each year,\nexacerbated by extreme weather events such as hurricanes. Existing outage data\nare typically reported at the county level, limiting their spatial resolution\nand making it difficult to capture localized patterns. However, it offers\nexcellent temporal granularity. In contrast, nighttime light satellite image\ndata provides significantly higher spatial resolution and enables a more\ncomprehensive spatial depiction of outages, enhancing the accuracy of assessing\nthe geographic extent and severity of power loss after disaster events.\nHowever, these satellite data are only available on a daily basis. Integrating\nspatiotemporal visual and time-series data sources into a unified knowledge\nrepresentation can substantially improve power outage detection, analysis, and\npredictive reasoning. In this paper, we propose GeoOutageKG, a multimodal\nknowledge graph that integrates diverse data sources, including nighttime light\nsatellite image data, high-resolution spatiotemporal power outage maps, and\ncounty-level timeseries outage reports in the U.S. We describe our method for\nconstructing GeoOutageKG by aligning source data with a developed ontology,\nGeoOutageOnto. Currently, GeoOutageKG includes over 10.6 million individual\noutage records spanning from 2014 to 2024, 300,000 NTL images spanning from\n2012 to 2024, and 15,000 outage maps. GeoOutageKG is a novel, modular and\nreusable semantic resource that enables robust multimodal data integration. We\ndemonstrate its use through multiresolution analysis of geospatiotemporal power\noutages.\n","authors":["Ethan Frakes","Yinghui Wu","Roger H. French","Mengjie Li"],"pdf_url":"https://arxiv.org/pdf/2507.22878v1.pdf","comment":"Accepted to the 24th International Semantic Web Conference Resource\n  Track (ISWC 2025)"},{"id":"http://arxiv.org/abs/2507.01053v2","updated":"2025-07-30T13:27:00Z","published":"2025-06-27T16:24:17Z","title":"Conversational LLMs Simplify Secure Clinical Data Access, Understanding,\n  and Analysis","summary":"  As ever-larger clinical datasets become available, they have the potential to\nunlock unprecedented opportunities for medical research. Foremost among them is\nMedical Information Mart for Intensive Care (MIMIC-IV), the world's largest\nopen-source EHR database. However, the inherent complexity of these datasets,\nparticularly the need for sophisticated querying skills and the need to\nunderstand the underlying clinical settings, often presents a significant\nbarrier to their effective use. M3 lowers the technical barrier to\nunderstanding and querying MIMIC-IV data. With a single command it retrieves\nMIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the\nhosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers\nconverse with the database in plain English. Ask a clinical question in natural\nlanguage; M3 uses a language model to translate it into SQL, executes the query\nagainst the MIMIC-IV dataset, and returns structured results alongside the\nunderlying query for verifiability and reproducibility. Demonstrations show\nthat minutes of dialogue with M3 yield the kind of nuanced cohort analyses that\nonce demanded hours of handcrafted SQL and relied on understanding the\ncomplexities of clinical workflows. By simplifying access, M3 invites the\nbroader research community to mine clinical critical-care data and accelerates\nthe translation of raw records into actionable insight.\n","authors":["Rafi Al Attrach","Pedro Moreira","Rajna Fani","Renato Umeton","Leo Anthony Celi"],"pdf_url":"https://arxiv.org/pdf/2507.01053v2.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2507.22520v1","updated":"2025-07-30T09:46:56Z","published":"2025-07-30T09:46:56Z","title":"Sustainability Evaluation Metrics for Recommender Systems","summary":"  Sustainability-oriented evaluation metrics can help to assess the quality of\nrecommender systems beyond wide-spread metrics such as accuracy, precision,\nrecall, and satisfaction. Following the United Nations`s sustainable\ndevelopment goals (SDGs), such metrics can help to analyse the impact of\nrecommender systems on environmental, social, and economic aspects. We discuss\ndifferent basic sustainability evaluation metrics for recommender systems and\nanalyze their applications.\n","authors":["Alexander Felfernig","Damian Garber","Viet-Man Le","Sebastian Lubos","Thi Ngoc Trang Tran"],"pdf_url":"https://arxiv.org/pdf/2507.22520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16695v6","updated":"2025-07-30T05:26:45Z","published":"2023-05-26T07:31:30Z","title":"The Search for Stability: Learning Dynamics of Strategic Publishers with\n  Initial Documents","summary":"  We study a game-theoretic information retrieval model in which strategic\npublishers aim to maximize their chances of being ranked first by the search\nengine while maintaining the integrity of their original documents. We show\nthat the commonly used Probability Ranking Principle (PRP) ranking scheme\nresults in an unstable environment where games often fail to reach pure Nash\nequilibrium. We propose two families of ranking functions that do not adhere to\nthe PRP principle. We provide both theoretical and empirical evidence that\nthese methods lead to a stable search ecosystem, by providing positive results\non the learning dynamics convergence. We also define the publishers' and users'\nwelfare, demonstrate a possible publisher-user trade-off, and provide means for\na search system designer to control it. Finally, we show how instability harms\nlong-term users' welfare.\n","authors":["Omer Madmon","Idan Pipano","Itamar Reinman","Moshe Tennenholtz"],"pdf_url":"https://arxiv.org/pdf/2305.16695v6.pdf","comment":"Published in the Journal of Artificial Intelligence Research 83\n  (2025)"},{"id":"http://arxiv.org/abs/2507.22337v1","updated":"2025-07-30T02:44:20Z","published":"2025-07-30T02:44:20Z","title":"A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers","summary":"  Understanding and solving complex reasoning tasks is vital for addressing the\ninformation needs of a user. Although dense neural models learn contextualised\nembeddings, they still underperform on queries containing negation. To\nunderstand this phenomenon, we study negation in both traditional neural\ninformation retrieval and LLM-based models. We (1) introduce a taxonomy of\nnegation that derives from philosophical, linguistic, and logical definitions;\n(2) generate two benchmark datasets that can be used to evaluate the\nperformance of neural information retrieval models and to fine-tune models for\na more robust performance on negation; and (3) propose a logic-based\nclassification mechanism that can be used to analyze the performance of\nretrieval models on existing datasets. Our taxonomy produces a balanced data\ndistribution over negation types, providing a better training setup that leads\nto faster convergence on the NevIR dataset. Moreover, we propose a\nclassification schema that reveals the coverage of negation types in existing\ndatasets, offering insights into the factors that might affect the\ngeneralization of fine-tuned models on negation.\n","authors":["Roxana Petcu","Samarth Bhargav","Maarten de Rijke","Evangelos Kanoulas"],"pdf_url":"https://arxiv.org/pdf/2507.22337v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2507.23042v1","updated":"2025-07-30T19:12:42Z","published":"2025-07-30T19:12:42Z","title":"Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language\n  Driving","summary":"  Autonomous vehicles must react in milliseconds while reasoning about road\ngeometry and traffic intent to navigate complex situations. We introduce\nNovaDrive, a single-branch vision-language architecture that processes\nfront-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a\nsingle branch. A lightweight, two-stage cross-attention block first aligns\nwaypoint tokens with the HD map, then refines attention over fine-grained image\nand depth patches. Coupled with a novel smoothness loss that discourages abrupt\nsteering and speed changes, this design eliminates the need for recurrent\nmemory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language\nbackbone, enabling real-time inference. On the nuScenes / Waymo subset of the\nMD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts\npath-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from\n2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations\nconfirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention\nfusion each contribute the most to these gains. Beyond safety, NovaDrive's\nshorter routes (resulting from the novel smoothness loss) translate to lower\nfuel or battery usage, pointing toward leaner, more easily updated driving\nstacks. NovaDrive can be extended to other embodied-AI domains as well.\n","authors":["Santosh Patapati","Trisanth Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2507.23042v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2507.22731v1","updated":"2025-07-30T14:50:38Z","published":"2025-07-30T14:50:38Z","title":"GestureHYDRA: Semantic Co-speech Gesture Synthesis via Hybrid Modality\n  Diffusion Transformer and Cascaded-Synchronized Retrieval-Augmented\n  Generation","summary":"  While increasing attention has been paid to co-speech gesture synthesis, most\nprevious works neglect to investigate hand gestures with explicit and essential\nsemantics. In this paper, we study co-speech gesture generation with an\nemphasis on specific hand gesture activation, which can deliver more\ninstructional information than common body movements. To achieve this, we first\nbuild a high-quality dataset of 3D human body movements including a set of\nsemantically explicit hand gestures that are commonly used by live streamers.\nThen we present a hybrid-modality gesture generation system GestureHYDRA built\nupon a hybrid-modality diffusion transformer architecture with novelly designed\nmotion-style injective transformer layers, which enables advanced gesture\nmodeling ability and versatile gesture operations. To guarantee these specific\nhand gestures can be activated, we introduce a cascaded retrieval-augmented\ngeneration strategy built upon a semantic gesture repository annotated for each\nsubject and an adaptive audio-gesture synchronization mechanism, which\nsubstantially improves semantic gesture activation and production efficiency.\nQuantitative and qualitative experiments demonstrate that our proposed approach\nachieves superior performance over all the counterparts. The project page can\nbe found at https://mumuwei.github.io/GestureHYDRA/.\n","authors":["Quanwei Yang","Luying Huang","Kaisiyuan Wang","Jiazhi Guan","Shengyi He","Fengguo Li","Hang Zhou","Lingyun Yu","Yingying Li","Haocheng Feng","Hongtao Xie"],"pdf_url":"https://arxiv.org/pdf/2507.22731v1.pdf","comment":"10 pages, 5 figures, Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2507.22481v1","updated":"2025-07-30T08:31:54Z","published":"2025-07-30T08:31:54Z","title":"Towards Blind Bitstream-corrupted Video Recovery via a Visual Foundation\n  Model-driven Framework","summary":"  Video signals are vulnerable in multimedia communication and storage systems,\nas even slight bitstream-domain corruption can lead to significant pixel-domain\ndegradation. To recover faithful spatio-temporal content from corrupted inputs,\nbitstream-corrupted video recovery has recently emerged as a challenging and\nunderstudied task. However, existing methods require time-consuming and\nlabor-intensive annotation of corrupted regions for each corrupted video frame,\nresulting in a large workload in practice. In addition, high-quality recovery\nremains difficult as part of the local residual information in corrupted frames\nmay mislead feature completion and successive content recovery. In this paper,\nwe propose the first blind bitstream-corrupted video recovery framework that\nintegrates visual foundation models with a recovery model, which is adapted to\ndifferent types of corruption and bitstream-level prompts. Within the\nframework, the proposed Detect Any Corruption (DAC) model leverages the rich\npriors of the visual foundation model while incorporating bitstream and\ncorruption knowledge to enhance corruption localization and blind recovery.\nAdditionally, we introduce a novel Corruption-aware Feature Completion (CFC)\nmodule, which adaptively processes residual contributions based on high-level\ncorruption understanding. With VFM-guided hierarchical feature augmentation and\nhigh-level coordination in a mixture-of-residual-experts (MoRE) structure, our\nmethod suppresses artifacts and enhances informative residuals. Comprehensive\nevaluations show that the proposed method achieves outstanding performance in\nbitstream-corrupted video recovery without requiring a manually labeled mask\nsequence. The demonstrated effectiveness will help to realize improved user\nexperience, wider application scenarios, and more reliable multimedia\ncommunication and storage systems.\n","authors":["Tianyi Liu","Kejun Wu","Chen Cai","Yi Wang","Kim-Hui Yap","Lap-Pui Chau"],"pdf_url":"https://arxiv.org/pdf/2507.22481v1.pdf","comment":"10 pages, 5 figures, accepted by ACMMM 2025"},{"id":"http://arxiv.org/abs/2503.11026v2","updated":"2025-07-30T05:08:14Z","published":"2025-03-14T02:48:43Z","title":"MAVFlow: Preserving Paralinguistic Elements with Conditional Flow\n  Matching for Zero-Shot AV2AV Multilingual Translation","summary":"  Despite recent advances in text-to-speech (TTS) models,\naudio-visual-to-audio-visual (AV2AV) translation still faces a critical\nchallenge: maintaining speaker consistency between the original and translated\nvocal and facial features. To address this issue, we propose a conditional flow\nmatching (CFM) zero-shot audio-visual renderer that utilizes strong dual\nguidance from both audio and visual modalities. By leveraging multimodal\nguidance with CFM, our model robustly preserves speaker-specific\ncharacteristics and enhances zero-shot AV2AV translation abilities. For the\naudio modality, we enhance the CFM process by integrating robust speaker\nembeddings with x-vectors, which serve to bolster speaker consistency.\nAdditionally, we convey emotional nuances to the face rendering module. The\nguidance provided by both audio and visual cues remains independent of semantic\nor linguistic content, allowing our renderer to effectively handle zero-shot\ntranslation tasks for monolingual speakers in different languages. We\nempirically demonstrate that the inclusion of high-quality mel-spectrograms\nconditioned on facial information not only enhances the quality of the\nsynthesized speech but also positively influences facial generation, leading to\noverall performance improvements in LSE and FID score. Our code is available at\nhttps://github.com/Peter-SungwooCho/MAVFlow.\n","authors":["Sungwoo Cho","Jeongsoo Choi","Sungnyun Kim","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2503.11026v2.pdf","comment":"Accepted to ICCV 2025"},{"id":"http://arxiv.org/abs/2507.22367v1","updated":"2025-07-30T04:12:14Z","published":"2025-07-30T04:12:14Z","title":"Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided\n  LLM Representations and Multimodal Apparent Behaviors","summary":"  Accurate and reliable personality assessment plays a vital role in many\nfields, such as emotional intelligence, mental health diagnostics, and\npersonalized education. Unlike fleeting emotions, personality traits are\nstable, often subconsciously leaked through language, facial expressions, and\nbody behaviors, with asynchronous patterns across modalities. It was hard to\nmodel personality semantics with traditional superficial features and seemed\nimpossible to achieve effective cross-modal understanding. To address these\nchallenges, we propose a novel personality assessment framework called\n\\textit{\\textbf{Traits Run Deep}}. It employs\n\\textit{\\textbf{psychology-informed prompts}} to elicit high-level\npersonality-relevant semantic representations. Besides, it devises a\n\\textit{\\textbf{Text-Centric Trait Fusion Network}} that anchors rich text\nsemantics to align and integrate asynchronous signals from other modalities. To\nbe specific, such fusion module includes a Chunk-Wise Projector to decrease\ndimensionality, a Cross-Modal Connector and a Text Feature Enhancer for\neffective modality fusion and an ensemble regression head to improve\ngeneralization in data-scarce situations. To our knowledge, we are the first to\napply personality-specific prompts to guide large language models (LLMs) in\nextracting personality-aware semantics for improved representation quality.\nFurthermore, extracting and fusing audio-visual apparent behavior features\nfurther improves the accuracy. Experimental results on the AVI validation set\nhave demonstrated the effectiveness of the proposed components, i.e.,\napproximately a 45\\% reduction in mean squared error (MSE). Final evaluations\non the test set of the AVI Challenge 2025 confirm our method's superiority,\nranking first in the Personality Assessment track. The source code will be made\navailable at https://github.com/MSA-LMC/TraitsRunDeep.\n","authors":["Jia Li","Yichao He","Jiacheng Xu","Tianhao Luo","Zhenzhen Hu","Richang Hong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2507.22367v1.pdf","comment":"8 pages, 3 figures, ACM MM 2025"},{"id":"http://arxiv.org/abs/2507.20300v2","updated":"2025-07-30T04:02:44Z","published":"2025-07-27T14:34:29Z","title":"Talking-to-Build: How LLM-Assisted Interface Shapes Player Performance\n  and Experience in Minecraft","summary":"  With large language models (LLMs) on the rise, in-game interactions are\nshifting from rigid commands to natural conversations. However, the impacts of\nLLMs on player performance and game experience remain underexplored. This work\nexplores LLM's role as a co-builder during gameplay, examining its impact on\ntask performance, usability, and player experience. Using Minecraft as a\nsandbox, we present an LLM-assisted interface that engages players through\nnatural language, aiming to facilitate creativity and simplify complex gaming\ncommands. We conducted a mixed-methods study with 30 participants, comparing\nLLM-assisted and command-based interfaces across simple and complex game tasks.\nQuantitative and qualitative analyses reveal that the LLM-assisted interface\nsignificantly improves player performance, engagement, and overall game\nexperience. Additionally, task complexity has a notable effect on player\nperformance and experience across both interfaces. Our findings highlight the\npotential of LLM-assisted interfaces to revolutionize virtual experiences,\nemphasizing the importance of balancing intuitiveness with predictability,\ntransparency, and user agency in AI-driven, multimodal gaming environments.\n","authors":["Xin Sun","Lei Wang","Yue Li","Jie Li","Massimo Poesio","Julian Frommel","Koen Hinriks","Jiahuan Pei"],"pdf_url":"https://arxiv.org/pdf/2507.20300v2.pdf","comment":null}]},"2025-07-29T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2507.22224v1","updated":"2025-07-29T20:41:51Z","published":"2025-07-29T20:41:51Z","title":"Generative Recommendation with Semantic IDs: A Practitioner's Handbook","summary":"  Generative recommendation (GR) has gained increasing attention for its\npromising performance compared to traditional models. A key factor contributing\nto the success of GR is the semantic ID (SID), which converts continuous\nsemantic representations (e.g., from large language models) into discrete ID\nsequences. This enables GR models with SIDs to both incorporate semantic\ninformation and learn collaborative filtering signals, while retaining the\nbenefits of discrete decoding. However, varied modeling techniques,\nhyper-parameters, and experimental setups in existing literature make direct\ncomparisons between GR proposals challenging. Furthermore, the absence of an\nopen-source, unified framework hinders systematic benchmarking and extension,\nslowing model iteration. To address this challenge, our work introduces and\nopen-sources a framework for Generative Recommendation with semantic ID, namely\nGRID, specifically designed for modularity to facilitate easy component\nswapping and accelerate idea iteration. Using GRID, we systematically\nexperiment with and ablate different components of GR models with SIDs on\npublic benchmarks. Our comprehensive experiments with GRID reveal that many\noverlooked architectural components in GR models with SIDs substantially impact\nperformance. This offers both novel insights and validates the utility of an\nopen-source platform for robust benchmarking and GR research advancement. GRID\nis open-sourced at https://github.com/snap-research/GRID.\n","authors":["Clark Mingxuan Ju","Liam Collins","Leonardo Neves","Bhuvesh Kumar","Louis Yufeng Wang","Tong Zhao","Neil Shah"],"pdf_url":"https://arxiv.org/pdf/2507.22224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.22213v1","updated":"2025-07-29T20:20:07Z","published":"2025-07-29T20:20:07Z","title":"Intent-Aware Neural Query Reformulation for Behavior-Aligned Product\n  Search","summary":"  Understanding and modeling buyer intent is a foundational challenge in\noptimizing search query reformulation within the dynamic landscape of\ne-commerce search systems. This work introduces a robust data pipeline designed\nto mine and analyze large-scale buyer query logs, with a focus on extracting\nfine-grained intent signals from both explicit interactions and implicit\nbehavioral cues. Leveraging advanced sequence mining techniques and supervised\nlearning models, the pipeline systematically captures patterns indicative of\nlatent purchase intent, enabling the construction of a high-fidelity,\nintent-rich dataset. The proposed framework facilitates the development of\nadaptive query rewrite strategies by grounding reformulations in inferred user\nintent rather than surface-level lexical signals. This alignment between query\nrewriting and underlying user objectives enhances both retrieval relevance and\ndownstream engagement metrics. Empirical evaluations across multiple product\nverticals demonstrate measurable gains in precision-oriented relevance metrics,\nunderscoring the efficacy of intent-aware reformulation. Our findings highlight\nthe value of intent-centric modeling in bridging the gap between sparse user\ninputs and complex product discovery goals, and establish a scalable foundation\nfor future research in user-aligned neural retrieval and ranking systems.\n","authors":["Jayanth Yetukuri","Ishita Khan"],"pdf_url":"https://arxiv.org/pdf/2507.22213v1.pdf","comment":"Accepted at SIGIR eCom'25.\n  https://sigir-ecom.github.io/eCom25Papers/paper_23.pdf"},{"id":"http://arxiv.org/abs/2507.22019v1","updated":"2025-07-29T17:12:16Z","published":"2025-07-29T17:12:16Z","title":"Not Here, Go There: Analyzing Redirection Patterns on the Web","summary":"  URI redirections are integral to web management, supporting structural\nchanges, SEO optimization, and security. However, their complexities affect\nusability, SEO performance, and digital preservation. This study analyzed 11\nmillion unique redirecting URIs, following redirections up to 10 hops per URI,\nto uncover patterns and implications of redirection practices. Our findings\nrevealed that 50% of the URIs terminated successfully, while 50% resulted in\nerrors, including 0.06% exceeding 10 hops. Canonical redirects, such as HTTP to\nHTTPS transitions, were prevalent, reflecting adherence to SEO best practices.\nNon-canonical redirects, often involving domain or path changes, highlighted\nsignificant web migrations, rebranding, and security risks. Notable patterns\nincluded \"sink\" URIs, where multiple redirects converged, ranging from traffic\nconsolidation by global websites to deliberate \"Rickrolling.\" The study also\nidentified 62,000 custom 404 URIs, almost half being soft 404s, which could\ncompromise SEO and user experience. These findings underscore the critical role\nof URI redirects in shaping the web while exposing challenges such as outdated\nURIs, server instability, and improper error handling. This research offers a\ndetailed analysis of URI redirection practices, providing insights into their\nprevalence, types, and outcomes. By examining a large dataset, we highlight\ninefficiencies in redirection chains and examine patterns such as the use of\n\"sink\" URIs and custom error pages. This information can help webmasters,\nresearchers, and digital archivists improve web usability, optimize resource\nallocation, and safeguard valuable online content.\n","authors":["Kritika Garg","Sawood Alam","Dietrich Ayala","Michele C. Weigle","Michael L. Nelson"],"pdf_url":"https://arxiv.org/pdf/2507.22019v1.pdf","comment":"Extended version of the paper accepted at the 2025 ACM Web Science\n  Conference (WebSci 2025)"},{"id":"http://arxiv.org/abs/2507.21989v1","updated":"2025-07-29T16:39:54Z","published":"2025-07-29T16:39:54Z","title":"Benchmarking Filtered Approximate Nearest Neighbor Search Algorithms on\n  Transformer-based Embedding Vectors","summary":"  Advances in embedding models for text, image, audio, and video drive progress\nacross multiple domains, including retrieval-augmented generation,\nrecommendation systems, vehicle/person reidentification, and face recognition.\nMany applications in these domains require an efficient method to retrieve\nitems that are close to a given query in the embedding space while satisfying a\nfilter condition based on the item's attributes, a problem known as Filtered\nApproximate Nearest Neighbor Search (FANNS). In this work, we present a\ncomprehensive survey and taxonomy of FANNS methods and analyze how they are\nbenchmarked in the literature. By doing so, we identify a key challenge in the\ncurrent FANNS landscape: the lack of diverse and realistic datasets,\nparticularly ones derived from the latest transformer-based text embedding\nmodels. To address this, we introduce a novel dataset consisting of embedding\nvectors for the abstracts of over 2.7 million research articles from the arXiv\nrepository, accompanied by 11 real-world attributes such as authors and\ncategories. We benchmark a wide range of FANNS methods on our novel dataset and\nfind that each method has distinct strengths and limitations; no single\napproach performs best across all scenarios. ACORN, for example, supports\nvarious filter types and performs reliably across dataset scales but is often\noutperformed by more specialized methods. SeRF shows excellent performance for\nrange filtering on ordered attributes but cannot handle categorical attributes.\nFiltered-DiskANN and UNG excel on the medium-scale dataset but fail on the\nlarge-scale dataset, highlighting the challenge posed by transformer-based\nembeddings, which are often more than an order of magnitude larger than earlier\nembeddings. We conclude that no universally best method exists.\n","authors":["Patrick Iff","Paul Bruegger","Marcin Chrapek","Maciej Besta","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2507.21989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.21939v1","updated":"2025-07-29T15:51:44Z","published":"2025-07-29T15:51:44Z","title":"The Curious Case of High-Dimensional Indexing as a File Structure: A\n  Case Study of eCP-FS","summary":"  Modern analytical pipelines routinely deploy multiple deep learning and\nretrieval models that rely on approximate nearest-neighbor (ANN) indexes to\nsupport efficient similarity-based search. While many state-of-the-art\nANN-indexes are memory-based (e.g., HNSW and IVF), using multiple ANN indexes\ncreates a competition for limited GPU/CPU memory resources, which in turn\nnecessitates disk-based index structures (e.g., DiskANN or eCP). In typical\nindex implementations, the main component is a complex data structure that is\nserialized to disk and is read either fully at startup time, for memory-based\nindexes, or incrementally at query time, for disk-based indexes. To visualize\nthe index structure, or analyze its quality, complex coding is needed that is\neither embedded in the index implementation or replicates the code that reads\nthe data structure. In this paper, we consider an alternative approach that\nmaps the data structure to a file structure, using a file library, making the\nindex easily readable for any programming language and even human-readable. The\ndisadvantage is that the serialized index is verbose, leading to overhead of\nsearching through the index. The question addressed in this paper is how severe\nthis performance penalty is. To that end, this paper presents eCP-FS, a\nfile-based implementation of eCP, a well-known disk-based ANN index. A\ncomparison with state-of-the-art indexes shows that while eCP-FS is slower, the\nimplementation is nevertheless somewhat competitive even when memory is not\nconstrained. In a memory-constrained scenario, eCP-FS offers a minimal memory\nfootprint, making it ideal for resource-constrained or multi-index\nenvironments.\n","authors":["Omar Shahbaz Khan","Gylfi Þór Guðmundsson","Björn Þór Jónsson"],"pdf_url":"https://arxiv.org/pdf/2507.21939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.21884v1","updated":"2025-07-29T14:57:26Z","published":"2025-07-29T14:57:26Z","title":"Exploration on Demand: From Algorithmic Control to User Empowerment","summary":"  Recommender systems often struggle with over-specialization, which severely\nlimits users' exposure to diverse content and creates filter bubbles that\nreduce serendipitous discovery. To address this fundamental limitation, this\npaper introduces an adaptive clustering framework with user-controlled\nexploration that effectively balances personalization and diversity in movie\nrecommendations. Our approach leverages sentence-transformer embeddings to\ngroup items into semantically coherent clusters through an online algorithm\nwith dynamic thresholding, thereby creating a structured representation of the\ncontent space. Building upon this clustering foundation, we propose a novel\nexploration mechanism that empowers users to control recommendation diversity\nby strategically sampling from less-engaged clusters, thus expanding their\ncontent horizons while preserving relevance. Experiments on the MovieLens\ndataset demonstrate the system's effectiveness, showing that exploration\nsignificantly reduces intra-list similarity from 0.34 to 0.26 while\nsimultaneously increasing unexpectedness to 0.73. Furthermore, our Large\nLanguage Model-based A/B testing methodology, conducted with 300 simulated\nusers, reveals that 72.7% of long-term users prefer exploratory recommendations\nover purely exploitative ones, providing strong evidence for the system's\nability to promote meaningful content discovery without sacrificing user\nsatisfaction.\n","authors":["Edoardo Bianchi"],"pdf_url":"https://arxiv.org/pdf/2507.21884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.21770v1","updated":"2025-07-29T12:55:45Z","published":"2025-07-29T12:55:45Z","title":"Proposing a Semantic Movie Recommendation System Enhanced by ChatGPT's\n  NLP Results","summary":"  The importance of recommender systems on the web has grown, especially in the\nmovie industry, with a vast selection of options to watch. To assist users in\ntraversing available items and finding relevant results, recommender systems\nanalyze operational data and investigate users' tastes and habits. Providing\nhighly individualized suggestions can boost user engagement and satisfaction,\nwhich is one of the fundamental goals of the movie industry, significantly in\nonline platforms. According to recent studies and research, using\nknowledge-based techniques and considering the semantic ideas of the textual\ndata is a suitable way to get more appropriate results. This study provides a\nnew method for building a knowledge graph based on semantic information. It\nuses the ChatGPT, as a large language model, to assess the brief descriptions\nof movies and extract their tone of voice. Results indicated that using the\nproposed method may significantly enhance accuracy rather than employing the\nexplicit genres supplied by the publishers.\n","authors":["Ali Fallahi","Azam Bastanfard","Amineh Amini","Hadi Saboohi"],"pdf_url":"https://arxiv.org/pdf/2507.21770v1.pdf","comment":"May 2023, 6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.10541v2","updated":"2025-07-29T12:44:24Z","published":"2025-04-13T09:12:35Z","title":"Multi-Modal Hypergraph Enhanced LLM Learning for Recommendation","summary":"  The burgeoning presence of Large Language Models (LLM) is propelling the\ndevelopment of personalized recommender systems. Most existing LLM-based\nmethods fail to sufficiently explore the multi-view graph structure\ncorrelations inherent in recommendation scenarios. To this end, we propose a\nnovel framework, Hypergraph Enhanced LLM Learning for multimodal Recommendation\n(HeLLM), designed to equip LLMs with the capability to capture intricate\nhigher-order semantic correlations by fusing graph-level contextual signals\nwith sequence-level behavioral patterns. In the recommender pre-training phase,\nwe design a user hypergraph to uncover shared interest preferences among users\nand an item hypergraph to capture correlations within multimodal similarities\namong items. The hypergraph convolution and synergistic contrastive learning\nmechanism are introduced to enhance the distinguishability of learned\nrepresentations. In the LLM fine-tuning phase, we inject the learned\ngraph-structured embeddings directly into the LLM's architecture and integrate\nsequential features capturing each user's chronological behavior. This process\nenables hypergraphs to leverage graph-structured information as global context,\nenhancing the LLM's ability to perceive complex relational patterns and\nintegrate multimodal information, while also modeling local temporal dynamics.\nExtensive experiments demonstrate the superiority of our proposed method over\nstate-of-the-art baselines, confirming the advantages of fusing\nhypergraph-based context with sequential user behavior in LLMs for\nrecommendation.\n","authors":["Xu Guo","Tong Zhang","Yuanzhi Wang","Chenxu Wang","Fuyun Wang","Xudong Wang","Xiaoya Zhang","Xin Liu","Zhen Cui"],"pdf_url":"https://arxiv.org/pdf/2504.10541v2.pdf","comment":"12 pages, 4 figures, submitted to IEEE Transactions on Knowledge and\n  Data Engineering"},{"id":"http://arxiv.org/abs/2410.10381v4","updated":"2025-07-29T09:19:23Z","published":"2024-10-14T11:10:15Z","title":"Collaborative filtering based on nonnegative/binary matrix factorization","summary":"  Collaborative filtering generates recommendations by exploiting user-item\nsimilarities based on rating data, which often contains numerous unrated items.\nTo predict scores for unrated items, matrix factorization techniques such as\nnonnegative matrix factorization (NMF) are often employed. Nonnegative/binary\nmatrix factorization (NBMF), which is an extension of NMF, approximates a\nnonnegative matrix as the product of nonnegative and binary matrices. While\nprevious studies have applied NBMF primarily to dense data such as images, this\npaper proposes a modified NBMF algorithm tailored for collaborative filtering\nwith sparse data. In the modified method, unrated entries in the rating matrix\nare masked, enhancing prediction accuracy. Furthermore, utilizing a low-latency\nIsing machine in NBMF is advantageous in terms of the computation time, making\nthe proposed method beneficial.\n","authors":["Yukino Terui","Yuka Inoue","Yohei Hamakawa","Kosuke Tatsumura","Kazue Kudo"],"pdf_url":"https://arxiv.org/pdf/2410.10381v4.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2504.03524v2","updated":"2025-07-29T09:14:58Z","published":"2025-04-04T15:22:02Z","title":"RANa: Retrieval-Augmented Navigation","summary":"  Methods for navigation based on large-scale learning typically treat each\nepisode as a new problem, where the agent is spawned with a clean memory in an\nunknown environment. While these generalization capabilities to an unknown\nenvironment are extremely important, we claim that, in a realistic setting, an\nagent should have the capacity of exploiting information collected during\nearlier robot operations. We address this by introducing a new\nretrieval-augmented agent, trained with RL, capable of querying a database\ncollected from previous episodes in the same environment and learning how to\nintegrate this additional context information. We introduce a unique agent\narchitecture for the general navigation task, evaluated on ImageNav,\nInstance-ImageNav and ObjectNav. Our retrieval and context encoding methods are\ndata-driven and employ vision foundation models (FM) for both semantic and\ngeometric understanding. We propose new benchmarks for these settings and we\nshow that retrieval allows zero-shot transfer across tasks and environments\nwhile significantly improving performance.\n","authors":["Gianluca Monaci","Rafael S. Rezende","Romain Deffayet","Gabriela Csurka","Guillaume Bono","Hervé Déjean","Stéphane Clinchant","Christian Wolf"],"pdf_url":"https://arxiv.org/pdf/2504.03524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.21563v1","updated":"2025-07-29T07:51:56Z","published":"2025-07-29T07:51:56Z","title":"Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank\n  Augmentation","summary":"  Recommendation systems often suffer from data sparsity caused by limited\nuser-item interactions, which degrade their performance and amplify popularity\nbias in real-world scenarios. This paper proposes a novel data augmentation\nframework that leverages Large Language Models (LLMs) and item textual\ndescriptions to enrich interaction data. By few-shot prompting LLMs multiple\ntimes to rerank items and aggregating the results via majority voting, we\ngenerate high-confidence synthetic user-item interactions, supported by\ntheoretical guarantees based on the concentration of measure. To effectively\nleverage the augmented data in the context of a graph recommendation system, we\nintegrate it into a graph contrastive learning framework to mitigate\ndistributional shift and alleviate popularity bias. Extensive experiments show\nthat our method improves accuracy and reduces popularity bias, outperforming\nstrong baselines.\n","authors":["Minh-Anh Nguyen","Bao Nguyen","Ha Lan N. T.","Tuan Anh Hoang","Duc-Trong Le","Dung D. Le"],"pdf_url":"https://arxiv.org/pdf/2507.21563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.19846v2","updated":"2025-07-29T06:26:46Z","published":"2025-07-26T07:42:12Z","title":"A Scalable and High Availability Solution for Recommending Resolutions\n  to Problem Tickets","summary":"  Resolution of incidents or problem tickets is a common theme in service\nindustries in any sector, including billing and charging systems in telecom\ndomain. Machine learning can help to identify patterns and suggest resolutions\nfor the problem tickets, based on patterns in the historical data of the\ntickets. However, this process may be complicated due to a variety of phenomena\nsuch as data drift and issues such as missing data, lack of data pertaining to\nresolutions of past incidents, too many similar sounding resolutions due to\nfree text and similar sounding text. This paper proposes a robust ML-driven\nsolution employing clustering, supervised learning, and advanced NLP models to\ntackle these challenges effectively. Building on previous work, we demonstrate\nclustering-based resolution identification, supervised classification with LDA,\nSiamese networks, and One-shot learning, Index embedding. Additionally, we\npresent a real-time dashboard and a highly available Kubernetes-based\nproduction deployment. Our experiments with both the open-source Bitext\ncustomer-support dataset and proprietary telecom datasets demonstrate high\nprediction accuracy.\n","authors":["Harish Saragadam","Chetana K Nayak","Joy Bose"],"pdf_url":"https://arxiv.org/pdf/2507.19846v2.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2507.21520v1","updated":"2025-07-29T06:07:59Z","published":"2025-07-29T06:07:59Z","title":"Solution for Meta KDD Cup'25: A Comprehensive Three-Step Framework for\n  Vision Question Answering","summary":"  Vision Large Language Models (VLLMs) have improved multi-modal understanding\nand visual question answering (VQA), but still suffer from hallucinated\nanswers. Multi-modal Retrieval-Augmented Generation (RAG) helps address these\nissues by incorporating external information, yet challenges remain in visual\ncontext comprehension, multi-source retrieval, and multi-turn interactions. To\naddress these challenges, Meta constructed the CRAG-MM benchmark and launched\nthe CRAG-MM Challenge at KDD Cup 2025, which consists of three tasks. This\npaper describes the solutions of all tasks in Meta KDD Cup'25 from BlackPearl\nteam. We use a single model for each task, with key methods including data\naugmentation, RAG, reranking, and multi-task fine-tuning. Our solution achieve\nautomatic evaluation rankings of 3rd, 3rd, and 1st on the three tasks, and win\nsecond place in Task3 after human evaluation.\n","authors":["Zijian Zhang","Xiaocheng Zhang","Yang Zhou","Zhimin Lin","Peng Yan"],"pdf_url":"https://arxiv.org/pdf/2507.21520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.21490v1","updated":"2025-07-29T04:16:21Z","published":"2025-07-29T04:16:21Z","title":"Conversations over Clicks: Impact of Chatbots on Information Search in\n  Interdisciplinary Learning","summary":"  This full research paper investigates the impact of generative AI (GenAI) on\nthe learner experience, with a focus on how learners engage with and utilize\nthe information it provides. In e-learning environments, learners often need to\nnavigate a complex information space on their own. This challenge is further\ncompounded in interdisciplinary fields like bioinformatics, due to the varied\nprior knowledge and backgrounds. In this paper, we studied how GenAI influences\ninformation search in bioinformatics research: (1) How do interactions with a\nGenAI chatbot influence learner orienteering behaviors?; and (2) How do\nlearners identify information scent in GenAI chatbot responses? We adopted an\nautoethnographic approach to investigate these questions. GenAI was found to\nsupport orienteering once a learning plan was established, but it was\ncounterproductive prior to that. Moreover, traditionally value-rich information\nsources such as bullet points and related terms proved less effective when\napplied to GenAI responses. Information scents were primarily recognized\nthrough the presence or absence of prior knowledge of the domain. These\nfindings suggest that GenAI should be adopted into e-learning environments with\ncaution, particularly in interdisciplinary learning contexts.\n","authors":["Hannah Kim","Sergei L. Kosakovsky Pond","Stephen MacNeil"],"pdf_url":"https://arxiv.org/pdf/2507.21490v1.pdf","comment":"9 pages, 2 tables, 3 figures, 2025 ASEE/IEEE Frontiers in Education\n  (FIE) Conference preprint"},{"id":"http://arxiv.org/abs/2502.07327v2","updated":"2025-07-29T03:43:44Z","published":"2025-02-11T07:43:47Z","title":"Generative Ghost: Investigating Ranking Bias Hidden in AI-Generated\n  Videos","summary":"  With the rapid development of AI-generated content (AIGC), the creation of\nhigh-quality AI-generated videos has become faster and easier, resulting in the\nInternet being flooded with all kinds of video content. However, the impact of\nthese videos on the content ecosystem remains largely unexplored. Video\ninformation retrieval remains a fundamental approach for accessing video\ncontent. Building on the observation that retrieval models often favor\nAI-generated content in ad-hoc and image retrieval tasks, we investigate\nwhether similar biases emerge in the context of challenging video retrieval,\nwhere temporal and visual factors may further influence model behavior. To\nexplore this, we first construct a comprehensive benchmark dataset containing\nboth real and AI-generated videos, along with a set of fair and rigorous\nmetrics to assess bias. This benchmark consists of 13,000 videos generated by\ntwo state-of-the-art open-source video generation models. We meticulously\ndesign a suite of rigorous metrics to accurately measure this preference,\naccounting for potential biases arising from the limited frame rate and\nsuboptimal quality of AIGC videos. We then applied three off-the-shelf video\nretrieval models to perform retrieval tasks on this hybrid dataset. Our\nfindings reveal a clear preference for AI-generated videos in retrieval.\nFurther investigation shows that incorporating AI-generated videos into the\ntraining set of retrieval models exacerbates this bias. Unlike the preference\nobserved in image modalities, we find that video retrieval bias arises from\nboth unseen visual and temporal information, making the root causes of video\nbias a complex interplay of these two factors. To mitigate this bias, we\nfine-tune the retrieval models using a contrastive learning approach. The\nresults of this study highlight the potential implications of AI-generated\nvideos on retrieval systems.\n","authors":["Haowen Gao","Liang Pang","Shicheng Xu","Leigang Qu","Tat-Seng Chua","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.07327v2.pdf","comment":"13 pages, Accepted at ACMMM2025"},{"id":"http://arxiv.org/abs/2408.11557v5","updated":"2025-07-29T03:37:44Z","published":"2024-08-21T12:09:37Z","title":"Enhancing Spectral Knowledge Interrogation: A Reliable\n  Retrieval-Augmented Generative Framework on Large Language Models","summary":"  Large Language Model (LLM) has demonstrated significant success in a range of\nnatural language processing (NLP) tasks within general domain. The emergence of\nLLM has introduced innovative methodologies across diverse fields, including\nthe natural sciences. Researchers aim to implement automated, concurrent\nprocess driven by LLM to supplant conventional manual, repetitive and\nlabor-intensive work. In the domain of spectral analysis and detection, it is\nimperative for researchers to autonomously acquire pertinent knowledge across\nvarious research objects, which encompasses the spectroscopic techniques and\nthe chemometric methods that are employed in experiments and analysis.\nParadoxically, despite the recognition of spectroscopic detection as an\neffective analytical method, the fundamental process of knowledge retrieval\nremains both time-intensive and repetitive. In response to this challenge, we\nfirst introduced the Spectral Detection and Analysis Based Paper(SDAAP)\ndataset, which is the first open-source textual knowledge dataset for spectral\nanalysis and detection and contains annotated literature data as well as\ncorresponding knowledge instruction data. Subsequently, we also designed an\nautomated Q\\&A framework based on the SDAAP dataset, which can retrieve\nrelevant knowledge and generate high-quality responses by extracting entities\nin the input as retrieval parameters. It is worth noting that: within this\nframework, LLM is only used as a tool to provide generalizability, while RAG\ntechnique is used to accurately capture the source of the knowledge.This\napproach not only improves the quality of the generated responses, but also\nensures the traceability of the knowledge. Experimental results show that our\nframework generates responses with more reliable expertise compared to the\nbaseline.\n","authors":["Jiheng Liang","Zujie Xie","Ziru Yu","Xiangyang Yu"],"pdf_url":"https://arxiv.org/pdf/2408.11557v5.pdf","comment":"16 pages,10 figures,3 tables"},{"id":"http://arxiv.org/abs/2507.21474v1","updated":"2025-07-29T03:34:32Z","published":"2025-07-29T03:34:32Z","title":"Hebbian Memory-Augmented Recurrent Networks: Engram Neurons in Deep\n  Learning","summary":"  Despite success across diverse tasks, current artificial recurrent network\narchitectures rely primarily on implicit hidden-state memories, limiting their\ninterpretability and ability to model long-range dependencies. In contrast,\nbiological neural systems employ explicit, associative memory traces (i.e.,\nengrams) strengthened through Hebbian synaptic plasticity and activated\nsparsely during recall. Motivated by these neurobiological insights, we\nintroduce the Engram Neural Network (ENN), a novel recurrent architecture\nincorporating an explicit, differentiable memory matrix with Hebbian plasticity\nand sparse, attention-driven retrieval mechanisms. The ENN explicitly models\nmemory formation and recall through dynamic Hebbian traces, improving\ntransparency and interpretability compared to conventional RNN variants. We\nevaluate the ENN architecture on three canonical benchmarks: MNIST digit\nclassification, CIFAR-10 image sequence modeling, and WikiText-103 language\nmodeling. Our empirical results demonstrate that the ENN achieves accuracy and\ngeneralization performance broadly comparable to classical RNN, GRU, and LSTM\narchitectures, with all models converging to similar accuracy and perplexity on\nthe large-scale WikiText-103 task. At the same time, the ENN offers significant\nenhancements in interpretability through observable memory dynamics. Hebbian\ntrace visualizations further reveal biologically plausible, structured memory\nformation processes, validating the potential of neuroscience-inspired\nmechanisms to inform the development of more interpretable and robust deep\nlearning models.\n","authors":["Daniel Szelogowski"],"pdf_url":"https://arxiv.org/pdf/2507.21474v1.pdf","comment":"20 pages, 11 figures, 4 tables"},{"id":"http://arxiv.org/abs/2507.21467v1","updated":"2025-07-29T03:13:41Z","published":"2025-07-29T03:13:41Z","title":"Efficient Data Retrieval and Comparative Bias Analysis of Recommendation\n  Algorithms for YouTube Shorts and Long-Form Videos","summary":"  The growing popularity of short-form video content, such as YouTube Shorts,\nhas transformed user engagement on digital platforms, raising critical\nquestions about the role of recommendation algorithms in shaping user\nexperiences. These algorithms significantly influence content consumption, yet\nconcerns about biases, echo chambers, and content diversity persist. This study\ndevelops an efficient data collection framework to analyze YouTube's\nrecommendation algorithms for both short-form and long-form videos, employing\nparallel computing and advanced scraping techniques to overcome limitations of\nYouTube's API. The analysis uncovers distinct behavioral patterns in\nrecommendation algorithms across the two formats, with short-form videos\nshowing a more immediate shift toward engaging yet less diverse content\ncompared to long-form videos. Furthermore, a novel investigation into biases in\npolitically sensitive topics, such as the South China Sea dispute, highlights\nthe role of these algorithms in shaping narratives and amplifying specific\nviewpoints. By providing actionable insights for designing equitable and\ntransparent recommendation systems, this research underscores the importance of\nresponsible AI practices in the evolving digital media landscape.\n","authors":["Selimhan Dagtas","Mert Can Cakmak","Nitin Agarwal"],"pdf_url":"https://arxiv.org/pdf/2507.21467v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2506.15677v3","updated":"2025-07-29T22:40:49Z","published":"2025-06-18T17:58:17Z","title":"Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence","summary":"  AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.\n","authors":["Yining Hong","Rui Sun","Bingxuan Li","Xingcheng Yao","Maxine Wu","Alexander Chien","Da Yin","Ying Nian Wu","Zhecan James Wang","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2506.15677v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07336v2","updated":"2025-07-29T19:14:56Z","published":"2024-10-09T18:00:09Z","title":"Positive-Augmented Contrastive Learning for Vision-and-Language\n  Evaluation and Training","summary":"  Despite significant advancements in caption generation, existing evaluation\nmetrics often fail to capture the full quality or fine-grained details of\ncaptions. This is mainly due to their reliance on non-specific human-written\nreferences or noisy pre-training data. Still, finding an effective metric is\ncrucial not only for captions evaluation but also for the generation phase.\nMetrics can indeed play a key role in the fine-tuning stage of captioning\nmodels, ultimately enhancing the quality of the generated captions. In this\npaper, we propose PAC-S++, a learnable metric that leverages the CLIP model,\npre-trained on both web-collected and cleaned data and regularized through\nadditional pairs of generated visual and textual positive samples. Exploiting\nthis stronger and curated pre-training, we also apply PAC-S++ as a reward in\nthe Self-Critical Sequence Training (SCST) stage typically employed to\nfine-tune captioning models. Extensive experiments on different image and video\ndatasets highlight the effectiveness of PAC-S++ compared to popular metrics for\nthe task, including its sensitivity to object hallucinations. Furthermore, we\nshow that integrating PAC-S++ into the fine-tuning stage of a captioning model\nresults in semantically richer captions with fewer repetitions and grammatical\nerrors. Evaluations on out-of-domain benchmarks further demonstrate the\nefficacy of our fine-tuning approach in enhancing model capabilities. Source\ncode and trained models are publicly available at:\nhttps://github.com/aimagelab/pacscore.\n","authors":["Sara Sarto","Nicholas Moratelli","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2410.07336v2.pdf","comment":"International Journal of Computer Vision (2025)"},{"id":"http://arxiv.org/abs/2507.22099v1","updated":"2025-07-29T17:58:41Z","published":"2025-07-29T17:58:41Z","title":"Runtime Failure Hunting for Physics Engine Based Software Systems: How\n  Far Can We Go?","summary":"  Physics Engines (PEs) are fundamental software frameworks that simulate\nphysical interactions in applications ranging from entertainment to\nsafety-critical systems. Despite their importance, PEs suffer from physics\nfailures, deviations from expected physical behaviors that can compromise\nsoftware reliability, degrade user experience, and potentially cause critical\nfailures in autonomous vehicles or medical robotics. Current testing approaches\nfor PE-based software are inadequate, typically requiring white-box access and\nfocusing on crash detection rather than semantically complex physics failures.\nThis paper presents the first large-scale empirical study characterizing\nphysics failures in PE-based software. We investigate three research questions\naddressing the manifestations of physics failures, the effectiveness of\ndetection techniques, and developer perceptions of current detection practices.\nOur contributions include: (1) a taxonomy of physics failure manifestations;\n(2) a comprehensive evaluation of detection methods including deep learning,\nprompt-based techniques, and large multimodal models; and (3) actionable\ninsights from developer experiences for improving detection approaches. To\nsupport future research, we release PhysiXFails, code, and other materials at\nhttps://sites.google.com/view/physics-failure-detection.\n","authors":["Shuqing Li","Qiang Chen","Xiaoxue Ren","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2507.22099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.20630v6","updated":"2025-07-29T16:39:59Z","published":"2025-04-29T10:56:44Z","title":"ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting","summary":"  Multimodal immersive spatial drama generation focuses on creating continuous\nmulti-speaker binaural speech with dramatic prosody based on multimodal\nprompts, with potential applications in AR, VR, and others. This task requires\nsimultaneous modeling of spatial information and dramatic prosody based on\nmultimodal inputs, with high data collection costs. To the best of our\nknowledge, our work is the first attempt to address these challenges. We\nconstruct MRSDrama, the first multimodal recorded spatial drama dataset,\ncontaining binaural drama audios, scripts, videos, geometric poses, and textual\nprompts. Then, we propose ISDrama, the first immersive spatial drama generation\nmodel through multimodal prompting. ISDrama comprises these primary components:\n1) Multimodal Pose Encoder, based on contrastive learning, considering the\nDoppler effect caused by moving speakers to extract unified pose information\nfrom multimodal prompts. 2) Immersive Drama Transformer, a flow-based\nmamba-transformer model that generates high-quality drama, incorporating\nDrama-MOE to select proper experts for enhanced prosody and pose control. We\nalso design a context-consistent classifier-free guidance strategy to\ncoherently generate complete drama. Experimental results show that ISDrama\noutperforms baseline models on objective and subjective metrics. The demos are\navailable at https://aaronz345.github.io/ISDramaDemo. We provide the dataset\nand the evaluation code at https://huggingface.co/datasets/AaronZ345/MRSDrama\nand https://github.com/AaronZ345/ISDrama.\n","authors":["Yu Zhang","Wenxiang Guo","Changhao Pan","Zhiyuan Zhu","Tao Jin","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.20630v6.pdf","comment":"Accepted by ACM Multimedia 2025"},{"id":"http://arxiv.org/abs/2507.14915v3","updated":"2025-07-29T16:07:01Z","published":"2025-07-20T11:06:47Z","title":"Music-Aligned Holistic 3D Dance Generation via Hierarchical Motion\n  Modeling","summary":"  Well-coordinated, music-aligned holistic dance enhances emotional\nexpressiveness and audience engagement. However, generating such dances remains\nchallenging due to the scarcity of holistic 3D dance datasets, the difficulty\nof achieving cross-modal alignment between music and dance, and the complexity\nof modeling interdependent motion across the body, hands, and face. To address\nthese challenges, we introduce SoulDance, a high-precision music-dance paired\ndataset captured via professional motion capture systems, featuring\nmeticulously annotated holistic dance movements. Building on this dataset, we\npropose SoulNet, a framework designed to generate music-aligned, kinematically\ncoordinated holistic dance sequences. SoulNet consists of three principal\ncomponents: (1) Hierarchical Residual Vector Quantization, which models\ncomplex, fine-grained motion dependencies across the body, hands, and face; (2)\nMusic-Aligned Generative Model, which composes these hierarchical motion units\ninto expressive and coordinated holistic dance; (3) Music-Motion Retrieval\nModule, a pre-trained cross-modal model that functions as a music-dance\nalignment prior, ensuring temporal synchronization and semantic coherence\nbetween generated dance and input music throughout the generation process.\nExtensive experiments demonstrate that SoulNet significantly surpasses existing\napproaches in generating high-quality, music-coordinated, and well-aligned\nholistic 3D dance sequences.\n","authors":["Xiaojie Li","Ronghui Li","Shukai Fang","Shuzhao Xie","Xiaoyang Guo","Jiaqing Zhou","Junkun Peng","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2507.14915v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.21926v1","updated":"2025-07-29T15:41:17Z","published":"2025-07-29T15:41:17Z","title":"Efficient Sub-pixel Motion Compensation in Learned Video Codecs","summary":"  Motion compensation is a key component of video codecs. Conventional codecs\n(HEVC and VVC) have carefully refined this coding step, with an important focus\non sub-pixel motion compensation. On the other hand, learned codecs achieve\nsub-pixel motion compensation through simple bilinear filtering. This paper\noffers to improve learned codec motion compensation by drawing inspiration from\nconventional codecs. It is shown that the usage of more advanced interpolation\nfilters, block-based motion information and finite motion accuracy lead to\nbetter compression performance and lower decoding complexity. Experimental\nresults are provided on the Cool-chic video codec, where we demonstrate a rate\ndecrease of more than 10% and a lowering of motion-related decoding complexity\nfrom 391 MAC per pixel to 214 MAC per pixel. All contributions are made\nopen-source at https://github.com/Orange-OpenSource/Cool-Chic\n","authors":["Théo Ladune","Thomas Leguay","Pierrick Philippe","Gordon Clare","Félix Henry"],"pdf_url":"https://arxiv.org/pdf/2507.21926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.21741v1","updated":"2025-07-29T12:17:46Z","published":"2025-07-29T12:17:46Z","title":"MAGE: Multimodal Alignment and Generation Enhancement via Bridging\n  Visual and Semantic Spaces","summary":"  In the latest advancements in multimodal learning, effectively addressing the\nspatial and semantic losses of visual data after encoding remains a critical\nchallenge. This is because the performance of large multimodal models is\npositively correlated with the coupling between visual encoders and large\nlanguage models. Existing approaches often face issues such as vector gaps or\nsemantic disparities, resulting in information loss during the propagation\nprocess. To address these issues, we propose MAGE (Multimodal Alignment and\nGeneration Enhancement), a novel framework that bridges the semantic spaces of\nvision and text through an innovative alignment mechanism. By introducing the\nIntelligent Alignment Network (IAN), MAGE achieves dimensional and semantic\nalignment. To reduce the gap between synonymous heterogeneous data, we employ a\ntraining strategy that combines cross-entropy and mean squared error,\nsignificantly enhancing the alignment effect. Moreover, to enhance MAGE's\n\"Any-to-Any\" capability, we developed a fine-tuning dataset for multimodal\ntool-calling instructions to expand the model's output capability boundaries.\nFinally, our proposed multimodal large model architecture, MAGE, achieved\nsignificantly better performance compared to similar works across various\nevaluation benchmarks, including MME, MMBench, and SEED. Complete code and\nappendix are available at: https://github.com/GTCOM-NLP/MAGE.\n","authors":["Shaojun E","Yuchen Yang","Jiaheng Wu","Yan Zhang","Tiejun Zhao","Ziyan Chen"],"pdf_url":"https://arxiv.org/pdf/2507.21741v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2507.21557v1","updated":"2025-07-29T07:42:14Z","published":"2025-07-29T07:42:14Z","title":"PC-JND: Subjective Study and Dataset on Just Noticeable Difference for\n  Point Clouds in 6DoF Virtual Reality","summary":"  The Just Noticeable Difference (JND) accounts for the minimum distortion at\nwhich humans can perceive a difference between a pristine stimulus and its\ndistorted version. The JND concept has been widely applied in visual signal\nprocessing tasks, including coding, transmission, rendering, and quality\nassessment, to optimize human-centric media experiences. A point cloud is a\nmainstream volumetric data representation consisting of both geometry\ninformation and attributes (e.g. color). Point clouds are used for advanced\nimmersive 3D media such as Virtual Reality (VR). However, the JND\ncharacteristics of viewing point clouds in VR have not been explored before. In\nthis paper, we study the point cloud-wise JND (PCJND) characteristics in a Six\nDegrees of Freedom (6DoF) VR environment using a head-mounted display. Our\nfindings reveal that the texture PCJND of human eyes is smaller than the\ngeometry PCJND for most point clouds. Furthermore, we identify a correlation\nbetween colorfulness and texture PCJND. However, there is no significant\ncorrelation between colorfulness and the geometry PCJND, nor between the number\nof points and neither the texture or geometry PCJND. To support future research\nin JND prediction and perception-driven signal processing, we introduce PC-JND,\na novel point cloud-based JND dataset. This dataset will be made publicly\navailable to facilitate advancements in perceptual optimization for immersive\nmedia.\n","authors":["Chunling Fan","Yun Zhang","Dietmar Saupe","Raouf Hamzaoui","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2507.21557v1.pdf","comment":"13 pages, 10 figures, Journal"},{"id":"http://arxiv.org/abs/2502.05130v3","updated":"2025-07-29T07:31:59Z","published":"2025-02-07T18:02:47Z","title":"Latent Swap Joint Diffusion for 2D Long-Form Latent Generation","summary":"  This paper introduces Swap Forward (SaFa), a modality-agnostic and efficient\nmethod to generate seamless and coherence long spectrum and panorama through\nlatent swap joint diffusion across multi-views. We first investigate the\nspectrum aliasing problem in spectrum-based audio generation caused by existing\njoint diffusion methods. Through a comparative analysis of the VAE latent\nrepresentation of Mel-spectra and RGB images, we identify that the failure\narises from excessive suppression of high-frequency components during the\nspectrum denoising process due to the averaging operator. To address this\nissue, we propose Self-Loop Latent Swap, a frame-level bidirectional swap\napplied to the overlapping region of adjacent views. Leveraging stepwise\ndifferentiated trajectories of adjacent subviews, this swap operator adaptively\nenhances high-frequency components and avoid spectrum distortion. Furthermore,\nto improve global cross-view consistency in non-overlapping regions, we\nintroduce Reference-Guided Latent Swap, a unidirectional latent swap operator\nthat provides a centralized reference trajectory to synchronize subview\ndiffusions. By refining swap timing and intervals, we can achieve a cross-view\nsimilarity-diversity balance in a forward-only manner. Quantitative and\nqualitative experiments demonstrate that SaFa significantly outperforms\nexisting joint diffusion methods and even training-based methods in audio\ngeneration using both U-Net and DiT models, along with effective longer length\nadaptation. It also adapts well to panorama generation, achieving comparable\nperformance with 2 $\\sim$ 20 $\\times$ faster speed and greater model\ngeneralizability. More generation demos are available at\nhttps://swapforward.github.io/\n","authors":["Yusheng Dai","Chenxi Wang","Chang Li","Chen Wang","Jun Du","Kewei Li","Ruoyu Wang","Jiefeng Ma","Lei Sun","Jianqing Gao"],"pdf_url":"https://arxiv.org/pdf/2502.05130v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.16405v4","updated":"2025-07-29T07:22:26Z","published":"2025-04-23T04:18:17Z","title":"EEmo-Bench: A Benchmark for Multi-modal Large Language Models on Image\n  Evoked Emotion Assessment","summary":"  The furnishing of multi-modal large language models (MLLMs) has led to the\nemergence of numerous benchmark studies, particularly those evaluating their\nperception and understanding capabilities. Among these, understanding\nimage-evoked emotions aims to enhance MLLMs' empathy, with significant\napplications such as human-machine interaction and advertising recommendations.\nHowever, current evaluations of this MLLM capability remain coarse-grained, and\na systematic and comprehensive assessment is still lacking. To this end, we\nintroduce EEmo-Bench, a novel benchmark dedicated to the analysis of the evoked\nemotions in images across diverse content categories. Our core contributions\ninclude: 1) Regarding the diversity of the evoked emotions, we adopt an emotion\nranking strategy and employ the Valence-Arousal-Dominance (VAD) as emotional\nattributes for emotional assessment. In line with this methodology, 1,960\nimages are collected and manually annotated. 2) We design four tasks to\nevaluate MLLMs' ability to capture the evoked emotions by single images and\ntheir associated attributes: Perception, Ranking, Description, and Assessment.\nAdditionally, image-pairwise analysis is introduced to investigate the model's\nproficiency in performing joint and comparative analysis. In total, we collect\n6,773 question-answer pairs and perform a thorough assessment on 19\ncommonly-used MLLMs. The results indicate that while some proprietary and\nlarge-scale open-source MLLMs achieve promising overall performance, the\nanalytical capabilities in certain evaluation dimensions remain suboptimal. Our\nEEmo-Bench paves the path for further research aimed at enhancing the\ncomprehensive perceiving and understanding capabilities of MLLMs concerning\nimage-evoked emotions, which is crucial for machine-centric emotion perception\nand understanding.\n","authors":["Lancheng Gao","Ziheng Jia","Yunhao Zeng","Wei Sun","Yiming Zhang","Wei Zhou","Guangtao Zhai","Xiongkuo Min"],"pdf_url":"https://arxiv.org/pdf/2504.16405v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.21507v1","updated":"2025-07-29T05:17:48Z","published":"2025-07-29T05:17:48Z","title":"VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly\n  Grounding and Understanding","summary":"  Video Anomaly Detection (VAD) aims to identify anomalous events in videos and\naccurately determine their time intervals. Current VAD methods mainly fall into\ntwo categories: traditional DNN-based approaches that focus on temporal\nlocalization, and LLM-based approaches that emphasize semantic understanding.\nBoth anomaly understanding and grounding are essential for comprehensive video\nanomaly detection and can complement each other. However, no existing model or\ndataset supports both tasks simultaneously. To address this, we introduce VAGU\n(Video Anomaly Grounding and Understanding), the first benchmark to integrate\nboth tasks. Each VAGU instance includes annotations for anomaly category,\nsemantic explanation, precise temporal grounding and Video QA. We also provide\nmultiple-choice Video QA for objective evaluation. Based on this dataset, we\npropose Glance then Scrutinize (GtS), a training-free framework guided by\ntextual prompts. The framework first enables coarse localization of\nhigh-probability anomalous regions, followed by detailed anomaly interpretation\nand temporal boundary refinement. Additionally, we propose the JeAUG metric,\nwhich jointly evaluates semantic interpretability and temporal precision,\novercoming the limitations of traditional metrics. Extensive experiments verify\nthe effectiveness of our benchmark, framework, and evaluation metric.\n","authors":["Shibo Gao","Peipei Yang","Yangyang Liu","Yi Chen","Han Zhu","Xuyao Zhang","Linlin Huang"],"pdf_url":"https://arxiv.org/pdf/2507.21507v1.pdf","comment":"21 pages, 19 figures, 8 tables"},{"id":"http://arxiv.org/abs/2507.21395v1","updated":"2025-07-29T00:03:28Z","published":"2025-07-29T00:03:28Z","title":"Sync-TVA: A Graph-Attention Framework for Multimodal Emotion Recognition\n  with Cross-Modal Fusion","summary":"  Multimodal emotion recognition (MER) is crucial for enabling emotionally\nintelligent systems that perceive and respond to human emotions. However,\nexisting methods suffer from limited cross-modal interaction and imbalanced\ncontributions across modalities. To address these issues, we propose Sync-TVA,\nan end-to-end graph-attention framework featuring modality-specific dynamic\nenhancement and structured cross-modal fusion. Our design incorporates a\ndynamic enhancement module for each modality and constructs heterogeneous\ncross-modal graphs to model semantic relations across text, audio, and visual\nfeatures. A cross-attention fusion mechanism further aligns multimodal cues for\nrobust emotion inference. Experiments on MELD and IEMOCAP demonstrate\nconsistent improvements over state-of-the-art models in both accuracy and\nweighted F1 score, especially under class-imbalanced conditions.\n","authors":["Zeyu Deng","Yanhui Lu","Jiashu Liao","Shuang Wu","Chongfeng Wei"],"pdf_url":"https://arxiv.org/pdf/2507.21395v1.pdf","comment":null}]},"2025-07-28T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2507.21340v1","updated":"2025-07-28T21:20:44Z","published":"2025-07-28T21:20:44Z","title":"StructText: A Synthetic Table-to-Text Approach for Benchmark Generation\n  with Multi-Dimensional Evaluation","summary":"  Extracting structured information from text, such as key-value pairs that\ncould augment tabular data, is quite useful in many enterprise use cases.\nAlthough large language models (LLMs) have enabled numerous automated pipelines\nfor converting natural language into structured formats, there is still a lack\nof benchmarks for evaluating their extraction quality, especially in specific\ndomains or focused documents specific to a given organization. Building such\nbenchmarks by manual annotations is labour-intensive and limits the size and\nscalability of the benchmarks. In this work, we present StructText, an\nend-to-end framework for automatically generating high-fidelity benchmarks for\nkey-value extraction from text using existing tabular data. It uses available\ntabular data as structured ground truth, and follows a two-stage\n``plan-then-execute'' pipeline to synthetically generate corresponding\nnatural-language text. To ensure alignment between text and structured source,\nwe introduce a multi-dimensional evaluation strategy that combines (a)\nLLM-based judgments on factuality, hallucination, and coherence and (b)\nobjective extraction metrics measuring numeric and temporal accuracy. We\nevaluated the proposed method on 71,539 examples across 49 datasets. Results\nreveal that while LLMs achieve strong factual accuracy and avoid hallucination,\nthey struggle with narrative coherence in producing extractable text. Notably,\nmodels presume numerical and temporal information with high fidelity yet this\ninformation becomes embedded in narratives that resist automated extraction. We\nrelease a framework, including datasets, evaluation tools, and baseline\nextraction systems, to support continued research.\n","authors":["Satyananda Kashyap","Sola Shirai","Nandana Mihindukulasooriya","Horst Samulowitz"],"pdf_url":"https://arxiv.org/pdf/2507.21340v1.pdf","comment":"Data available:\n  https://huggingface.co/datasets/ibm-research/struct-text and code available\n  at: https://github.com/ibm/struct-text"},{"id":"http://arxiv.org/abs/2507.20919v1","updated":"2025-07-28T15:19:54Z","published":"2025-07-28T15:19:54Z","title":"Modeling User Behavior from Adaptive Surveys with Supplemental Context","summary":"  Modeling user behavior is critical across many industries where understanding\npreferences, intent, or decisions informs personalization, targeting, and\nstrategic outcomes. Surveys have long served as a classical mechanism for\ncollecting such behavioral data due to their interpretability, structure, and\nease of deployment. However, surveys alone are inherently limited by user\nfatigue, incomplete responses, and practical constraints on their length making\nthem insufficient for capturing user behavior. In this work, we present LANTERN\n(Late-Attentive Network for Enriched Response Modeling), a modular architecture\nfor modeling user behavior by fusing adaptive survey responses with\nsupplemental contextual signals. We demonstrate the architectural value of\nmaintaining survey primacy through selective gating, residual connections and\nlate fusion via cross-attention, treating survey data as the primary signal\nwhile incorporating external modalities only when relevant. LANTERN outperforms\nstrong survey-only baselines in multi-label prediction of survey responses. We\nfurther investigate threshold sensitivity and the benefits of selective\nmodality reliance through ablation and rare/frequent attribute analysis.\nLANTERN's modularity supports scalable integration of new encoders and evolving\ndatasets. This work provides a practical and extensible blueprint for behavior\nmodeling in survey-centric applications.\n","authors":["Aman Shukla","Daniel Patrick Scantlebury","Rishabh Kumar"],"pdf_url":"https://arxiv.org/pdf/2507.20919v1.pdf","comment":"Best Paper, NewInML @ ICML 2025"},{"id":"http://arxiv.org/abs/2507.20762v1","updated":"2025-07-28T12:16:52Z","published":"2025-07-28T12:16:52Z","title":"Watermarking Large Language Model-based Time Series Forecasting","summary":"  Large Language Model-based Time Series Forecasting (LLMTS) has shown\nremarkable promise in handling complex and diverse temporal data, representing\na significant step toward foundation models for time series analysis. However,\nthis emerging paradigm introduces two critical challenges. First, the\nsubstantial commercial potential and resource-intensive development raise\nurgent concerns about intellectual property (IP) protection. Second, their\npowerful time series forecasting capabilities may be misused to produce\nmisleading or fabricated deepfake time series data. To address these concerns,\nwe explore watermarking the outputs of LLMTS models, that is, embedding\nimperceptible signals into the generated time series data that remain\ndetectable by specialized algorithms. We propose a novel post-hoc watermarking\nframework, Waltz, which is broadly compatible with existing LLMTS models. Waltz\nis inspired by the empirical observation that time series patch embeddings are\nrarely aligned with a specific set of LLM tokens, which we term ``cold\ntokens''. Leveraging this insight, Waltz embeds watermarks by rewiring the\nsimilarity statistics between patch embeddings and cold token embeddings, and\ndetects watermarks using similarity z-scores. To minimize potential side\neffects, we introduce a similarity-based embedding position identification\nstrategy and employ projected gradient descent to constrain the watermark noise\nwithin a defined boundary. Extensive experiments using two popular LLMTS models\nacross seven benchmark datasets demonstrate that Waltz achieves high watermark\ndetection accuracy with minimal impact on the quality of the generated time\nseries.\n","authors":["Wei Yuan","Chaoqun Yang","Yu Xing","Tong Chen","Nguyen Quoc Viet Hung","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2507.20762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.20753v1","updated":"2025-07-28T12:02:02Z","published":"2025-07-28T12:02:02Z","title":"Industry Insights from Comparing Deep Learning and GBDT Models for\n  E-Commerce Learning-to-Rank","summary":"  In e-commerce recommender and search systems, tree-based models, such as\nLambdaMART, have set a strong baseline for Learning-to-Rank (LTR) tasks.\nDespite their effectiveness and widespread adoption in industry, the debate\ncontinues whether deep neural networks (DNNs) can outperform traditional\ntree-based models in this domain. To contribute to this discussion, we\nsystematically benchmark DNNs against our production-grade LambdaMART model. We\nevaluate multiple DNN architectures and loss functions on a proprietary dataset\nfrom OTTO and validate our findings through an 8-week online A/B test. The\nresults show that a simple DNN architecture outperforms a strong tree-based\nbaseline in terms of total clicks and revenue, while achieving parity in total\nunits sold.\n","authors":["Yunus Lutz","Timo Wilm","Philipp Duwe"],"pdf_url":"https://arxiv.org/pdf/2507.20753v1.pdf","comment":"This work was accepted for publication in the 19th ACM Conference on\n  Recommender Systems (RecSys 2025). The final published version will be\n  available at the ACM Digital Library"},{"id":"http://arxiv.org/abs/2507.20578v1","updated":"2025-07-28T07:22:06Z","published":"2025-07-28T07:22:06Z","title":"Beyond Interactions: Node-Level Graph Generation for Knowledge-Free\n  Augmentation in Recommender Systems","summary":"  Recent advances in recommender systems rely on external resources such as\nknowledge graphs or large language models to enhance recommendations, which\nlimit applicability in real-world settings due to data dependency and\ncomputational overhead. Although knowledge-free models are able to bolster\nrecommendations by direct edge operations as well, the absence of augmentation\nprimitives drives them to fall short in bridging semantic and structural gaps\nas high-quality paradigm substitutes. Unlike existing diffusion-based works\nthat remodel user-item interactions, this work proposes NodeDiffRec, a\npioneering knowledge-free augmentation framework that enables fine-grained\nnode-level graph generation for recommendations and expands the scope of\nrestricted augmentation primitives via diffusion. By synthesizing pseudo-items\nand corresponding interactions that align with the underlying distribution for\ninjection, and further refining user preferences through a denoising preference\nmodeling process, NodeDiffRec dramatically enhances both semantic diversity and\nstructural connectivity without external knowledge. Extensive experiments\nacross diverse datasets and recommendation algorithms demonstrate the\nsuperiority of NodeDiffRec, achieving State-of-the-Art (SOTA) performance, with\nmaximum average performance improvement 98.6% in Recall@5 and 84.0% in NDCG@5\nover selected baselines.\n","authors":["Zhaoyan Wang","Hyunjun Ahn","In-Young Ko"],"pdf_url":"https://arxiv.org/pdf/2507.20578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.20564v1","updated":"2025-07-28T06:58:35Z","published":"2025-07-28T06:58:35Z","title":"ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided\n  Captioning","summary":"  We present ZSE-Cap (Zero-Shot Ensemble for Captioning), our 4th place system\nin Event-Enriched Image Analysis (EVENTA) shared task on article-grounded image\nretrieval and captioning. Our zero-shot approach requires no finetuning on the\ncompetition's data. For retrieval, we ensemble similarity scores from CLIP,\nSigLIP, and DINOv2. For captioning, we leverage a carefully engineered prompt\nto guide the Gemma 3 model, enabling it to link high-level events from the\narticle to the visual content in the image. Our system achieved a final score\nof 0.42002, securing a top-4 position on the private test set, demonstrating\nthe effectiveness of combining foundation models through ensembling and\nprompting. Our code is available at https://github.com/ductai05/ZSE-Cap.\n","authors":["Duc-Tai Dinh","Duc Anh Khoa Dinh"],"pdf_url":"https://arxiv.org/pdf/2507.20564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20308v2","updated":"2025-07-28T05:26:47Z","published":"2025-05-20T18:27:22Z","title":"Large Language Model Powered Decision Support for a Metal Additive\n  Manufacturing Knowledge Graph","summary":"  Metal additive manufacturing (AM) involves complex interdependencies among\nprocesses, materials, feedstock, and post-processing steps. However, the\nunderlying relationships and domain knowledge remain fragmented across\nliterature and static databases that often require expert-level queries,\nlimiting their applicability in design and planning. To address these\nlimitations, we develop a novel and structured knowledge graph (KG),\nrepresenting 53 distinct metals and alloys across seven material categories,\nnine AM processes, four feedstock types, and corresponding post-processing\nrequirements. A large language model (LLM) interface, guided by a few-shot\nprompting strategy, enables natural language querying without the need for\nformal query syntax. The system supports a range of tasks, including\ncompatibility evaluation, constraint-based filtering, and design for AM (DfAM)\nguidance. User queries in natural language are normalized, translated into\nCypher, and executed on the KG, with results returned in a structured format.\nThis work introduces the first interactive system that connects a\ndomain-specific metal AM KG with an LLM interface, delivering accessible and\nexplainable decision support for engineers and promoting human-centered tools\nin manufacturing knowledge systems.\n","authors":["Muhammad Tayyab Khan","Lequn Chen","Wenhe Feng","Seung Ki Moon"],"pdf_url":"https://arxiv.org/pdf/2505.20308v2.pdf","comment":"The paper has been accepted at 11th International Conference of Asian\n  Society for Precision Engineering and Nanotechnology"},{"id":"http://arxiv.org/abs/2312.02031v2","updated":"2025-07-28T02:39:19Z","published":"2023-12-04T16:51:28Z","title":"Virtual Quantum Markov Chains","summary":"  Quantum Markov chains generalize classical Markov chains for random variables\nto the quantum realm and exhibit unique inherent properties, making them an\nimportant feature in quantum information theory. In this work, we propose the\nconcept of virtual quantum Markov chains (VQMCs), focusing on scenarios where\nsubsystems retain classical information about global systems from measurement\nstatistics. As a generalization of quantum Markov chains, VQMCs characterize\nstates where arbitrary global shadow information can be recovered from\nsubsystems through local quantum operations and measurements. We present an\nalgebraic characterization for virtual quantum Markov chains and show that the\nvirtual quantum recovery is fully determined by the block matrices of a quantum\nstate on its subsystems. Notably, we find a distinction between two classes of\ntripartite entanglement by showing that the W state is a VQMC while the GHZ\nstate is not. Furthermore, we introduce the virtual non-Markovianity to\nquantify the non-Markovianity of a given quantum state, which also assesses the\noptimal sampling overhead for virtually recovering this state. Our findings\nelucidate distinctions between quantum Markov chains and virtual quantum Markov\nchains, extending our understanding of quantum recovery to scenarios\nprioritizing classical information from measurement statistics.\n","authors":["Yu-Ao Chen","Chengkai Zhu","Keming He","Mingrui Jing","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2312.02031v2.pdf","comment":"19 pages including appendix, 6 figures, v2: results and references\n  updated"},{"id":"http://arxiv.org/abs/2507.20449v1","updated":"2025-07-28T00:48:33Z","published":"2025-07-28T00:48:33Z","title":"Improving Community Detection in Academic Networks by Handling\n  Publication Bias","summary":"  Finding potential research collaborators is a challenging task, especially in\ntoday's fast-growing and interdisciplinary research landscape. While\ntraditional methods often rely on observable relationships such as\nco-authorships and citations to construct the research network, in this work,\nwe focus solely on publication content to build a topic-based research network\nusing BERTopic with a fine-tuned SciBERT model that connects and recommends\nresearchers across disciplines based on shared topical interests. A major\nchallenge we address is publication imbalance, where some researchers publish\nmuch more than others, often across several topics. Without careful handling,\ntheir less frequent interests are hidden under dominant topics, limiting the\nnetwork's ability to detect their full research scope. To tackle this, we\nintroduce a cloning strategy that clusters a researcher's publications and\ntreats each cluster as a separate node. This allows researchers to be part of\nmultiple communities, improving the detection of interdisciplinary links.\nEvaluation on the proposed method shows that the cloned network structure leads\nto more meaningful communities and uncovers a broader set of collaboration\nopportunities.\n","authors":["Md Asaduzzaman Noor","John Sheppard","Jason Clark"],"pdf_url":"https://arxiv.org/pdf/2507.20449v1.pdf","comment":"This paper is an extended version of a work accepted at ASONAM 2025"}],"Multimedia":[{"id":"http://arxiv.org/abs/2507.20900v1","updated":"2025-07-28T14:52:57Z","published":"2025-07-28T14:52:57Z","title":"Music Arena: Live Evaluation for Text-to-Music","summary":"  We present Music Arena, an open platform for scalable human preference\nevaluation of text-to-music (TTM) models. Soliciting human preferences via\nlistening studies is the gold standard for evaluation in TTM, but these studies\nare expensive to conduct and difficult to compare, as study protocols may\ndiffer across systems. Moreover, human preferences might help researchers align\ntheir TTM systems or improve automatic evaluation metrics, but an open and\nrenewable source of preferences does not currently exist. We aim to fill these\ngaps by offering *live* evaluation for TTM. In Music Arena, real-world users\ninput text prompts of their choosing and compare outputs from two TTM systems,\nand their preferences are used to compile a leaderboard. While Music Arena\nfollows recent evaluation trends in other AI domains, we also design it with\nkey features tailored to music: an LLM-based routing system to navigate the\nheterogeneous type signatures of TTM systems, and the collection of *detailed*\npreferences including listening data and natural language feedback. We also\npropose a rolling data release policy with user privacy guarantees, providing a\nrenewable source of preference data and increasing platform transparency.\nThrough its standardized evaluation protocol, transparent data access policies,\nand music-specific features, Music Arena not only addresses key challenges in\nthe TTM ecosystem but also demonstrates how live evaluation can be thoughtfully\nadapted to unique characteristics of specific AI domains.\n  Music Arena is available at: https://music-arena.org\n","authors":["Yonghyun Kim","Wayne Chi","Anastasios N. Angelopoulos","Wei-Lin Chiang","Koichi Saito","Shinji Watanabe","Yuki Mitsufuji","Chris Donahue"],"pdf_url":"https://arxiv.org/pdf/2507.20900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08064v2","updated":"2025-07-28T13:36:38Z","published":"2025-07-10T16:47:25Z","title":"PUMA: Layer-Pruned Language Model for Efficient Unified Multimodal\n  Retrieval with Modality-Adaptive Learning","summary":"  As multimedia content expands, the demand for unified multimodal retrieval\n(UMR) in real-world applications increases. Recent work leverages multimodal\nlarge language models (MLLMs) to tackle this task. However, their large\nparameter size results in high training costs and low inference efficiency. To\naddress this, we propose PUMA: a Layer-Pruned Language Model for Efficient\nUnified Multimodal Retrieval with Modality-Adaptive Learning. Our approach\nimproves UMR from both structural and learning perspectives. (1) Structurally,\nwe propose Layer-Pruned Self-Distillation, which prunes MLLMs by keeping only\nshallow layers while distilling features from dropped deep layers as teacher\nsignals. This reduces parameters and preserves representation capability. (2)\nOn the learning side, we introduce Modality-Adaptive Contrastive Learning Loss\n(MAC-Loss), which separates in-batch negatives into harder intra-modality and\neasier inter-modality groups based on the target modality, assigning different\ntemperature strategies to enhance learning efficiency. Experiments show our\nmethod significantly reduces resource usage while maintaining strong\nperformance.\n","authors":["Yibo Lyu","Rui Shao","Gongwei Chen","Yijie Zhu","Weili Guan","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2507.08064v2.pdf","comment":"Accepted to ACM MM 2025"},{"id":"http://arxiv.org/abs/2507.20745v1","updated":"2025-07-28T11:52:56Z","published":"2025-07-28T11:52:56Z","title":"Regularizing Subspace Redundancy of Low-Rank Adaptation","summary":"  Low-Rank Adaptation (LoRA) and its variants have delivered strong capability\nin Parameter-Efficient Transfer Learning (PETL) by minimizing trainable\nparameters and benefiting from reparameterization. However, their projection\nmatrices remain unrestricted during training, causing high representation\nredundancy and diminishing the effectiveness of feature adaptation in the\nresulting subspaces. While existing methods mitigate this by manually adjusting\nthe rank or implicitly applying channel-wise masks, they lack flexibility and\ngeneralize poorly across various datasets and architectures. Hence, we propose\nReSoRA, a method that explicitly models redundancy between mapping subspaces\nand adaptively Regularizes Subspace redundancy of Low-Rank Adaptation.\nSpecifically, it theoretically decomposes the low-rank submatrices into\nmultiple equivalent subspaces and systematically applies de-redundancy\nconstraints to the feature distributions across different projections.\nExtensive experiments validate that our proposed method consistently\nfacilitates existing state-of-the-art PETL methods across various backbones and\ndatasets in vision-language retrieval and standard visual classification\nbenchmarks. Besides, as a training supervision, ReSoRA can be seamlessly\nintegrated into existing approaches in a plug-and-play manner, with no\nadditional inference costs. Code is publicly available at:\nhttps://github.com/Lucenova/ReSoRA.\n","authors":["Yue Zhu","Haiwen Diao","Shang Gao","Jiazuo Yu","Jiawen Zhu","Yunzhi Zhuge","Shuai Hao","Xu Jia","Lu Zhang","Ying Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2507.20745v1.pdf","comment":"10 pages, 4 figures, Accepted by ACMMM2025"},{"id":"http://arxiv.org/abs/2507.20738v1","updated":"2025-07-28T11:42:17Z","published":"2025-07-28T11:42:17Z","title":"Dark Side of Modalities: Reinforced Multimodal Distillation for\n  Multimodal Knowledge Graph Reasoning","summary":"  The multimodal knowledge graph reasoning (MKGR) task aims to predict the\nmissing facts in the incomplete MKGs by leveraging auxiliary images and\ndescriptions of entities. Existing approaches are trained with single-target\nobjectives, which neglect the probabilistic correlations of entity labels,\nespecially in non-target entities. Moreover, previous studies incorporate all\nmodalities statically or adaptively, overlooking the negative impacts of\nirrelevant or misleading information in the incompetent modalities. To address\nthese issues, we introduce a novel Reinforced Multimodal Distillation\nframework, exploiting the Dark Side of Modalities (DSoM) from two perspectives:\n(1) Dark knowledge from non-target entities: We propose to train a unimodal KGR\nmodel through logit distillation to mimic the multimodal soft labels provided\nby pre-trained multimodal teacher models. The multimodal soft labels could\nprovide rich supervision signals with subtle correlations among both target and\nnon-target entities from multiple perspectives. We further decouple logits into\nneighbor entities and non-neighbor entities to divide into two types of\ncorrelations. (2) Dark side in unhelpful modalities: To exclude the adverse\neffects of unhelpful modalities, we introduce a reinforced teacher combination\nmechanism that dynamically selects the optimal set of multimodal teachers for\neach triple. The agent is trained to maximize the rewards, which are only\nassigned to the beneficial multimodal combination strategies for the student\nmodel. Comprehensive experiments demonstrate the effectiveness of DSoM\nframework on 5 MKGR datasets. Codes are available at github.com/OreOZhao/DSoM.\n","authors":["Yu Zhao","Ying Zhang","Xuhui Sui","Baohang Zhou","Haoze Zhu","Jeff Z. Pan","Xiaojie Yuan"],"pdf_url":"https://arxiv.org/pdf/2507.20738v1.pdf","comment":"Accepted by ACM MM 2025"},{"id":"http://arxiv.org/abs/2507.20730v1","updated":"2025-07-28T11:26:39Z","published":"2025-07-28T11:26:39Z","title":"Vocalize: Lead Acquisition and User Engagement through Gamified Voice\n  Competitions","summary":"  This paper explores the prospect of creating engaging user experiences and\ncollecting leads through an interactive and gamified platform. We introduce\nVocalize, an end-to-end system for increasing user engagement and lead\nacquisition through gamified voice competitions. Using audio processing\ntechniques and LLMs, we create engaging and interactive experiences that have\nthe potential to reach a wide audience, foster brand recognition, and increase\ncustomer loyalty. We describe the system from a technical standpoint and report\nresults from launching Vocalize at 4 different live events. Our user study\nshows that Vocalize is capable of generating significant user engagement, which\nshows potential for gamified audio campaigns in marketing and similar\nverticals.\n","authors":["Edvin Teskeredzic","Muamer Paric","Adna Sestic","Petra Fribert","Anamarija Lukac","Hadzem Hadzic","Kemal Altwlkany","Emanuel Lacic"],"pdf_url":"https://arxiv.org/pdf/2507.20730v1.pdf","comment":"Accepted to ACM Hypertext 2025"},{"id":"http://arxiv.org/abs/2507.12060v2","updated":"2025-07-28T08:51:08Z","published":"2025-07-16T09:16:51Z","title":"InstructFLIP: Exploring Unified Vision-Language Model for Face\n  Anti-spoofing","summary":"  Face anti-spoofing (FAS) aims to construct a robust system that can withstand\ndiverse attacks. While recent efforts have concentrated mainly on cross-domain\ngeneralization, two significant challenges persist: limited semantic\nunderstanding of attack types and training redundancy across domains. We\naddress the first by integrating vision-language models (VLMs) to enhance the\nperception of visual input. For the second challenge, we employ a meta-domain\nstrategy to learn a unified model that generalizes well across multiple\ndomains. Our proposed InstructFLIP is a novel instruction-tuned framework that\nleverages VLMs to enhance generalization via textual guidance trained solely on\na single domain. At its core, InstructFLIP explicitly decouples instructions\ninto content and style components, where content-based instructions focus on\nthe essential semantics of spoofing, and style-based instructions consider\nvariations related to the environment and camera characteristics. Extensive\nexperiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA\nmodels in accuracy and substantially reducing training redundancy across\ndiverse domains in FAS. Project website is available at\nhttps://kunkunlin1221.github.io/InstructFLIP.\n","authors":["Kun-Hsiang Lin","Yu-Wen Tseng","Kang-Yang Huang","Jhih-Ciang Wu","Wen-Huang Cheng"],"pdf_url":"https://arxiv.org/pdf/2507.12060v2.pdf","comment":"Accepted by MM'25"},{"id":"http://arxiv.org/abs/2507.20627v1","updated":"2025-07-28T08:41:20Z","published":"2025-07-28T08:41:20Z","title":"Controllable Video-to-Music Generation with Multiple Time-Varying\n  Conditions","summary":"  Music enhances video narratives and emotions, driving demand for automatic\nvideo-to-music (V2M) generation. However, existing V2M methods relying solely\non visual features or supplementary textual inputs generate music in a\nblack-box manner, often failing to meet user expectations. To address this\nchallenge, we propose a novel multi-condition guided V2M generation framework\nthat incorporates multiple time-varying conditions for enhanced control over\nmusic generation. Our method uses a two-stage training strategy that enables\nlearning of V2M fundamentals and audiovisual temporal synchronization while\nmeeting users' needs for multi-condition control. In the first stage, we\nintroduce a fine-grained feature selection module and a progressive temporal\nalignment attention mechanism to ensure flexible feature alignment. For the\nsecond stage, we develop a dynamic conditional fusion module and a\ncontrol-guided decoder module to integrate multiple conditions and accurately\nguide the music composition process. Extensive experiments demonstrate that our\nmethod outperforms existing V2M pipelines in both subjective and objective\nevaluations, significantly enhancing control and alignment with user\nexpectations.\n","authors":["Junxian Wu","Weitao You","Heda Zuo","Dengming Zhang","Pei Chen","Lingyun Sun"],"pdf_url":"https://arxiv.org/pdf/2507.20627v1.pdf","comment":"Accepted by the 33rd ACM International Conference on Multimedia\n  (ACMMM 2025). The project page is available at\n  https://kita-wjx.github.io/MCV2M/"},{"id":"http://arxiv.org/abs/2504.06010v2","updated":"2025-07-28T08:37:35Z","published":"2025-04-08T13:16:48Z","title":"Latent Multimodal Reconstruction for Misinformation Detection","summary":"  Multimodal misinformation, such as miscaptioned images, where captions\nmisrepresent an image's origin, context, or meaning, poses a growing challenge\nin the digital age. To support fact-checkers, researchers have focused on\ndeveloping datasets and methods for multimodal misinformation detection (MMD).\nDue to the scarcity of large-scale annotated MMD datasets, recent approaches\nrely on synthetic training data created via out-of-context pairings or named\nentity manipulations (e.g., altering names, dates, or locations). However,\nthese often yield simplistic examples that lack real-world complexity, limiting\nmodel robustness. Meanwhile, Large Vision-Language Models (LVLMs) remain\nunderexplored for generating diverse and realistic synthetic data for MMD. To\naddress, we introduce \"Miscaption This!\", a collection of LVLM-generated\nmiscaptioned image datasets. Additionally, we introduce \"Latent Multimodal\nReconstruction\" (LAMAR), a network trained to reconstruct the embeddings of\ntruthful captions, providing a strong auxiliary signal to guide detection. We\nexplore various training strategies (end-to-end vs. large-scale pre-training)\nand integration mechanisms (direct, mask, gate, and attention). Extensive\nexperiments show that models trained on \"MisCaption This!\" generalize better to\nreal-world misinformation while LAMAR achieves new state-of-the-art on both\nNewsCLIPpings and VERITE benchmarks; highlighting the value of LVLM-generated\ndata and reconstruction-based networks for advancing MMD. Our code is available\nat https://github.com/stevejpapad/miscaptioned-image-reconstruction\n","authors":["Stefanos-Iordanis Papadopoulos","Christos Koutlis","Symeon Papadopoulos","Panagiotis C. Petrantonakis"],"pdf_url":"https://arxiv.org/pdf/2504.06010v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16421v2","updated":"2025-07-28T05:39:21Z","published":"2025-03-20T17:59:42Z","title":"MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance","summary":"  Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps://quanhaol.github.io/magicmotion-site.\n","authors":["Quanhao Li","Zhen Xing","Rui Wang","Hui Zhang","Qi Dai","Zuxuan Wu"],"pdf_url":"https://arxiv.org/pdf/2503.16421v2.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2507.20518v1","updated":"2025-07-28T04:55:27Z","published":"2025-07-28T04:55:27Z","title":"T2VParser: Adaptive Decomposition Tokens for Partial Alignment in Text\n  to Video Retrieval","summary":"  Text-to-video retrieval essentially aims to train models to align visual\ncontent with textual descriptions accurately. Due to the impressive general\nmultimodal knowledge demonstrated by image-text pretrained models such as CLIP,\nexisting work has primarily focused on extending CLIP knowledge for video-text\ntasks. However, videos typically contain richer information than images. In\ncurrent video-text datasets, textual descriptions can only reflect a portion of\nthe video content, leading to partial misalignment in video-text matching.\nTherefore, directly aligning text representations with video representations\ncan result in incorrect supervision, ignoring the inequivalence of information.\nIn this work, we propose T2VParser to extract multiview semantic\nrepresentations from text and video, achieving adaptive semantic alignment\nrather than aligning the entire representation. To extract corresponding\nrepresentations from different modalities, we introduce Adaptive Decomposition\nTokens, which consist of a set of learnable tokens shared across modalities.\nThe goal of T2VParser is to emphasize precise alignment between text and video\nwhile retaining the knowledge of pretrained models. Experimental results\ndemonstrate that T2VParser achieves accurate partial alignment through\neffective cross-modal content decomposition. The code is available at\nhttps://github.com/Lilidamowang/T2VParser.\n","authors":["Yili Li","Gang Xiong","Gaopeng Gou","Xiangyan Qu","Jiamin Zhuang","Zhen Li","Junzheng Shi"],"pdf_url":"https://arxiv.org/pdf/2507.20518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.21195v1","updated":"2025-07-28T00:51:47Z","published":"2025-07-28T00:51:47Z","title":"MaXsive: High-Capacity and Robust Training-Free Generative Image\n  Watermarking in Diffusion Models","summary":"  The great success of the diffusion model in image synthesis led to the\nrelease of gigantic commercial models, raising the issue of copyright\nprotection and inappropriate content generation. Training-free diffusion\nwatermarking provides a low-cost solution for these issues. However, the prior\nworks remain vulnerable to rotation, scaling, and translation (RST) attacks.\nAlthough some methods employ meticulously designed patterns to mitigate this\nissue, they often reduce watermark capacity, which can result in identity (ID)\ncollusion. To address these problems, we propose MaXsive, a training-free\ndiffusion model generative watermarking technique that has high capacity and\nrobustness. MaXsive best utilizes the initial noise to watermark the diffusion\nmodel. Moreover, instead of using a meticulously repetitive ring pattern, we\npropose injecting the X-shape template to recover the RST distortions. This\ndesign significantly increases robustness without losing any capacity, making\nID collusion less likely to happen. The effectiveness of MaXsive has been\nverified on two well-known watermarking benchmarks under the scenarios of\nverification and identification.\n","authors":["Po-Yuan Mao","Cheng-Chang Tsai","Chun-Shien Lu"],"pdf_url":"https://arxiv.org/pdf/2507.21195v1.pdf","comment":null}]}}